# layer_dropout


Applies Dropout to the input.




## Description

Dropout consists in randomly setting a fraction ``rate`` of input units to 0 at
each update during training time, which helps prevent overfitting.





## Usage
```r
layer_dropout(
  object,
  rate,
  noise_shape = NULL,
  seed = NULL,
  input_shape = NULL,
  batch_input_shape = NULL,
  batch_size = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL
)
```




## Arguments


Argument      |Description
------------- |----------------
object | What to compose the new ``Layer`` instance with. Typically a Sequential model or a Tensor (e.g., as returned by ``layer_input()``). The return value depends on ``object``. If ``object`` is:   *  missing or `NULL`, the `Layer` instance is returned.  *  a `Sequential` model, the model with an additional layer is returned.  *  a Tensor, the output tensor from `layer_instance(object)` is returned.
rate | float between 0 and 1. Fraction of the input units to drop.
noise_shape | 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use ``noise_shape=c(batch_size, 1, features)``.
seed | integer to use as random seed.
input_shape | Dimensionality of the input (integer) not including the samples axis. This argument is required when using this layer as the first layer in a model.
batch_input_shape | Shapes, including the batch size. For instance, ``batch_input_shape=c(10, 32)`` indicates that the expected input will be batches of 10 32-dimensional vectors. ``batch_input_shape=list(NULL, 32)`` indicates batches of an arbitrary number of 32-dimensional vectors.
batch_size | Fixed batch size for layer
name | An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.
trainable | Whether the layer weights will be updated during training.
weights | Initial weights for layer.







## See Also

Other core layers: 
`layer_activation()`,
`layer_activity_regularization()`,
`layer_attention()`,
`layer_dense_features()`,
`layer_dense()`,
`layer_flatten()`,
`layer_input()`,
`layer_lambda()`,
`layer_masking()`,
`layer_permute()`,
`layer_repeat_vector()`,
`layer_reshape()`

Other dropout layers: 
`layer_spatial_dropout_1d()`,
`layer_spatial_dropout_2d()`,
`layer_spatial_dropout_3d()`



