# layer_activation_selu


Scaled Exponential Linear Unit.




## Description

SELU is equal to: ``scale * elu(x, alpha)``, where alpha and scale
are pre-defined constants.





## Usage
```r
layer_activation_selu(
  object,
  input_shape = NULL,
  batch_input_shape = NULL,
  batch_size = NULL,
  dtype = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL
)
```




## Arguments


Argument      |Description
------------- |----------------
object | What to compose the new ``Layer`` instance with. Typically a Sequential model or a Tensor (e.g., as returned by ``layer_input()``). The return value depends on ``object``. If ``object`` is:   *  missing or `NULL`, the `Layer` instance is returned.  *  a `Sequential` model, the model with an additional layer is returned.  *  a Tensor, the output tensor from `layer_instance(object)` is returned.
input_shape | Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.
batch_input_shape | Shapes, including the batch size. For instance, ``batch_input_shape=c(10, 32)`` indicates that the expected input will be batches of 10 32-dimensional vectors. ``batch_input_shape=list(NULL, 32)`` indicates batches of an arbitrary number of 32-dimensional vectors.
batch_size | Fixed batch size for layer
dtype | The data type expected by the input, as a string (``float32``, ``float64``, ``int32``...)
name | An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.
trainable | Whether the layer weights will be updated during training.
weights | Initial weights for layer.




## Details

The values of ``alpha`` and ``scale`` are
chosen so that the mean and variance of the inputs are preserved
between two consecutive layers as long as the weights are initialized
correctly (see initializer_lecun_normal) and the number of inputs
is "large enough" (see article for more information).

Note:


*  To be used together with the initialization "lecun_normal".

*  To be used together with the dropout variant "AlphaDropout".








## See Also

https://arxiv.org/abs/1706.02515Self-Normalizing Neural Networks, `initializer_lecun_normal`, `layer_alpha_dropout`

Other activation layers: 
`layer_activation_elu()`,
`layer_activation_leaky_relu()`,
`layer_activation_parametric_relu()`,
`layer_activation_relu()`,
`layer_activation_softmax()`,
`layer_activation_thresholded_relu()`,
`layer_activation()`



