---
format:
  html:
    css: /reference/assets/reference.css
---

| <button class="button"> ![](/reference/assets/GitHub-Mark-32px.png){width="20"} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class="button"> ![](/reference/assets/GitHub-Mark-32px.png){width="20"} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
|:------------------------------:|:--------------------------------------:|

*R/initializers.R*

# initializer_lecun_normal

## LeCun normal initializer.

## Description
It draws samples from a truncated normal distribution centered on 0 with `stddev <- sqrt(1 / fan_in)` where `fan_in` is the number of input units in the weight tensor.. 


## Usage
```r
initializer_lecun_normal(seed = NULL) 
```

## Arguments
|Arguments|Description|
|---|---|
| seed | A Python integer. Used to seed the random generator. |


## Section

## References

- [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

- Efficient Backprop, *LeCun, Yann et al. 1998*




## See Also
Other initializers:  `initializer_constant()`, `initializer_glorot_normal()`, `initializer_glorot_uniform()`, `initializer_he_normal()`, `initializer_he_uniform()`, `initializer_identity()`, `initializer_lecun_uniform()`, `initializer_ones()`, `initializer_orthogonal()`, `initializer_random_normal()`, `initializer_random_uniform()`, `initializer_truncated_normal()`, `initializer_variance_scaling()`, `initializer_zeros()`

