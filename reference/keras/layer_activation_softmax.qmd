---
format:
  html:
    css: /reference/assets/reference.css
---

| <button class="button"> ![](/reference/assets/GitHub-Mark-32px.png){width="20"} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layers-activations.R#L222) </button> |
|:------------------------------:|:--------------------------------------:|

# layer_activation_softmax

## Softmax activation function.

## Description
It follows: `f(x) =  alpha * (exp(x) - 1.0)` for `x < 0`, `f(x) = x` for `x >= 0`. 


## Usage
```r
layer_activation_softmax( 
  object, 
  axis = -1, 
  input_shape = NULL, 
  batch_input_shape = NULL, 
  batch_size = NULL, 
  dtype = NULL, 
  name = NULL, 
  trainable = NULL, 
  weights = NULL 
) 
```

## Arguments
|Arguments|Description|
|---|---|
| object | What to compose the new `Layer` instance with. Typically a Sequential model or a Tensor (e.g., as returned by `layer_input()`). The return value depends on `object`. If `object` is: <br>- missing or `NULL`, the `Layer` instance is returned. <br>- a `Sequential` model, the model with an additional layer is returned. <br>- a Tensor, the output tensor from `layer_instance(object)` is returned.  |
| axis | Integer, axis along which the softmax normalization is applied. |
| input_shape | Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model. |
| batch_input_shape | Shapes, including the batch size. For instance, `batch_input_shape=c(10, 32)` indicates that the expected input will be batches of 10 32-dimensional vectors. `batch_input_shape=list(NULL, 32)`<br>indicates batches of an arbitrary number of 32-dimensional vectors. |
| batch_size | Fixed batch size for layer |
| dtype | The data type expected by the input, as a string (`float32`, `float64`, `int32`...) |
| name | An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided. |
| trainable | Whether the layer weights will be updated during training. |
| weights | Initial weights for layer. |






## See Also
Other activation layers:  `layer_activation_elu()`, `layer_activation_leaky_relu()`, `layer_activation_parametric_relu()`, `layer_activation_relu()`, `layer_activation_selu()`, `layer_activation_thresholded_relu()`, `layer_activation()`

