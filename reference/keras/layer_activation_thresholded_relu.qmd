---
execute:
  freeze: true
---

[R/layers-activations.R](https://github.com/rstudio/keras//blob/main/R/layers-activations.R#L124) 

# layer_activation_thresholded_relu

## Thresholded Rectified Linear Unit.

## Description
 It follows: `f(x) = x` for `x > theta`, `f(x) = 0` otherwise. 


## Usage
```r
 
layer_activation_thresholded_relu( 
  object, 
  theta = 1, 
  input_shape = NULL, 
  batch_input_shape = NULL, 
  batch_size = NULL, 
  dtype = NULL, 
  name = NULL, 
  trainable = NULL, 
  weights = NULL 
) 
```

## Arguments
|Arguments|Description|
|---|---|
| object | What to compose the new `Layer` instance with. Typically a Sequential model or a Tensor (e.g., as returned by `layer_input()`). The return value depends on `object`. If `object` is:  <br>- missing or `NULL`, the `Layer` instance is returned. <br>- a `Sequential` model, the model with an additional layer is returned. <br>- a Tensor, the output tensor from `layer_instance(object)` is returned.  |
| theta | float >= 0. Threshold location of activation. |
| input_shape | Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model. |
| batch_input_shape | Shapes, including the batch size. For instance, `batch_input_shape=c(10, 32)` indicates that the expected input will be batches of 10 32-dimensional vectors. `batch_input_shape=list(NULL, 32)` indicates batches of an arbitrary number of 32-dimensional vectors. |
| batch_size | Fixed batch size for layer |
| dtype | The data type expected by the input, as a string (`float32`, `float64`, `int32`...) |
| name | An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided. |
| trainable | Whether the layer weights will be updated during training. |
| weights | Initial weights for layer. |






## See Also
 [Zero-bias autoencoders and the benefits of co-adapting features](https://arxiv.org/abs/1402.3337).  Other activation layers:  `layer_activation_elu()`, `layer_activation_leaky_relu()`, `layer_activation_parametric_relu()`, `layer_activation_relu()`, `layer_activation_selu()`, `layer_activation_softmax()`, `layer_activation()` 

