---
title: Text Classification with TF Hub
description: "Use a pretrained model from TF Hub to classify text"
aliases:
  - ../beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/index.html
---

This tutorial classifies movie reviews as *positive* or *negative* using the text of the review.
This is an example of *binary*---or two-class---classification, an important and widely applicable kind of machine learning problem.

The tutorial demonstrates the basic application of transfer learning with [TensorFlow Hub](https://tfhub.dev) and Keras.

It uses the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/).
These are split into 25,000 reviews for training and 25,000 reviews for testing.
The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews.

This notebook uses [`keras`](/guides/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow hub](https://www.tensorflow.org/hub), a library for loading trained models from [TFHub](https://tfhub.dev) in a single line of code.
For a more advanced text classification tutorial using Keras, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/).

```{r setup}
library(tensorflow)
library(tfhub)
library(keras)
```

## Download the IMDB dataset

The IMDB dataset is available on [imdb reviews](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) or on [TensorFlow datasets](https://www.tensorflow.org/datasets).
The following code downloads the IMDB dataset to your machine:

```{r}
if (dir.exists("aclImdb/"))
  unlink("aclImdb/", recursive = TRUE)
url <- "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
dataset <- get_file(
  "aclImdb_v1",
  url,
  untar = TRUE,
  cache_dir = '.',
  cache_subdir = ''
)
unlink("aclImdb/train/unsup/", recursive = TRUE)
```

We can then create a TensorFlow dataset from the directory structure using the `text_dataset_from_directory()` function:

```{r}
batch_size <- 512
seed <- 42

train_data <- text_dataset_from_directory(
  'aclImdb/train',
  batch_size = batch_size,
  validation_split = 0.2,
  subset = 'training',
  seed = seed
)
validation_data <- text_dataset_from_directory(
  'aclImdb/train',
  batch_size = batch_size,
  validation_split = 0.2,
  subset = 'validation',
  seed = seed
)
test_data <- text_dataset_from_directory(
  'aclImdb/test',
  batch_size = batch_size
)
```

## Explore the data

Let's take a moment to understand the format of the data.
Each example is a sentence representing the movie review and a corresponding label.
The sentence is not preprocessed in any way.
The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.

Let's print first 10 examples.

```{r}
batch <- train_data %>%
  reticulate::as_iterator() %>%
  reticulate::iter_next()

batch[[1]][1]
```

Let's also print the first 10 labels.

```{r}
batch[[2]][1:10]
```

## Build the model

The neural network is created by stacking layers---this requires three main architectural decisions:

-   How to represent the text?
-   How many layers to use in the model?
-   How many *hidden units* to use for each layer?

In this example, the input data consists of sentences.
The labels to predict are either 0 or 1.

One way to represent the text is to convert sentences into embeddings vectors.
Use a pre-trained text embedding as the first layer, which will have three advantages:

-   You don't have to worry about text preprocessing,
-   Benefit from transfer learning,
-   the embedding has a fixed size, so it's simpler to process.

For this example you use a **pre-trained text embedding model** from [TensorFlow Hub](https://tfhub.dev) called [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).

There are many other pre-trained text embeddings from TFHub that can be used in this tutorial:

-   [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2) - trained with the same NNLM architecture on the same data as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with a larger embedding dimension. Larger dimensional embeddings can improve on your task but it may take longer to train your model.
-   [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - the same as [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2), but with additional text normalization such as removing punctuation. This can help if the text in your task contains additional characters or punctuation.
-   [google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4) - a much larger model yielding 512 dimensional embeddings trained with a deep averaging network (DAN) encoder.

And many more!
Find more [text embedding models](https://tfhub.dev/s?module-type%20=%20text-embedding) on TFHub.

Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples.
Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`.

```{r}
embedding <- "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer <- tfhub::layer_hub(handle = embedding, trainable = TRUE)
hub_layer(batch[[1]][1:2])
```

Let's now build the full model:

```{r}
model <- keras_model_sequential() %>%
  hub_layer() %>%
  layer_dense(16, activation = 'relu') %>%
  layer_dense(1)

summary(model)
```

The layers are stacked sequentially to build the classifier:

1.  The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that you are using ([google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`. For this NNLM model, the `embedding_dimension` is 50.
2.  This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.
3.  The last layer is densely connected with a single output node.

Let's compile the model.

### Loss function and optimizer

A model needs a loss function and an optimizer for training.
Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), you'll use the `binary_crossentropy` loss function.

This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`.
But, generally, `binary_crossentropy` is better for dealing with probabilities---it measures the "distance" between probability distributions, or in our case, between the ground-truth distribution and the predictions.

Later, when you are exploring regression problems (say, to predict the price of a house), you'll see how to use another loss function called mean squared error.

Now, configure the model to use an optimizer and a loss function:

```{r}
model %>% compile(
  optimizer = 'adam',
  loss = loss_binary_crossentropy(from_logits = TRUE),
  metrics = 'accuracy'
)
```

## Train the model

Train the model for 10 epochs in mini-batches of 512 samples.
This is 10 iterations over all samples in the `x_train` and `y_train` tensors.
While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:

```{r}
history <- model %>% fit(
  train_data,
  epochs = 10,
  validation_data = validation_data,
  verbose <- 1
)
```

## Evaluate the model

And let's see how the model performs.
Two values will be returned.
Loss (a number which represents our error, lower values are better), and accuracy.

```{r}
results <- model %>% evaluate(test_data, verbose = 2)
results
```

This fairly naive approach achieves an accuracy of about 87%.
With more advanced approaches, the model should get closer to 95%.

## Further reading

-   For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, see the [Text classification with preprocessed text](./text_classification.qmd) tutorial.
-   Try out more [text-related tutorials](https://www.tensorflow.org/hub/tutorials#text-related-tutorials) using trained models from TFHub.
