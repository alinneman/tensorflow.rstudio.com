{
  "hash": "3444ed8c86b67b0b48f08951cec717e9",
  "result": {
    "markdown": "---\ntitle: cifar10_cnn\ndescription: Trains a simple deep CNN on the CIFAR10 small images dataset.\n---\n\nTrain a simple deep CNN on the CIFAR10 small images dataset.\n \nIt gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs,\nthough it is still underfitting at that point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'keras' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 32\nepochs <- 200\ndata_augmentation <- TRUE\n\n\n# Data Preparation --------------------------------------------------------\n\n# See ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Feature scale RGB values in test and train inputs  \nx_train <- cifar10$train$x/255\nx_test <- cifar10$test$x/255\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n\n# Defining Model ----------------------------------------------------------\n\n# Initialize sequential model\nmodel <- keras_model_sequential()\n\nmodel %>%\n \n  # Start with hidden 2D convolutional layer being fed 32x32 pixel images\n  layer_conv_2d(\n    filter = 32, kernel_size = c(3,3), padding = \"same\", \n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation(\"relu\") %>%\n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling once more\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # Flatten max filtered output into feature vector \n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(512) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10 unit output layer\n  layer_dense(10) %>%\n  layer_activation(\"softmax\")\n\nopt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in backcompat_fix_rename_lr_to_learning_rate(...): the `lr` argument has\nbeen renamed to `learning_rate`.\n```\n:::\n\n```{.r .cell-code}\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = opt,\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nif(!data_augmentation){\n  \n  model %>% fit(\n    x_train, y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, y_test),\n    shuffle = TRUE\n  )\n  \n} else {\n  \n  datagen <- image_data_generator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    horizontal_flip = TRUE\n  )\n  \n  datagen %>% fit_image_data_generator(x_train)\n  \n  model %>% fit_generator(\n    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),\n    steps_per_epoch = as.integer(50000/batch_size), \n    epochs = epochs, \n    validation_data = list(x_test, y_test)\n  )\n  \n}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in fit_generator(., flow_images_from_data(x_train, y_train, datagen, :\n`fit_generator` is deprecated. Use `fit` instead, it now accept generators.\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}