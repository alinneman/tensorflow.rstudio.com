{
  "hash": "1339a9cda802511b9260e63d4d746f00",
  "result": {
    "markdown": "---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\"button\"> ![](/reference/assets/GitHub-Mark-32px.png){width=\"20\"} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L370) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# save_text_tokenizer\n\n## Save a text tokenizer to an external file\n\n## Description\nEnables persistence of text tokenizers alongside saved models. \n\n\n## Usage\n```r\nsave_text_tokenizer(object, filename) \n\nload_text_tokenizer(filename) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Text tokenizer fit with `fit_text_tokenizer()` |\n| filename | File to save/load |\n\n## Details\n\nYou should always use the same text tokenizer for training and prediction. In many cases however prediction will occur in another session with a version of the model loaded via `load_model_hdf5()`. \n\nIn this case you need to save the text tokenizer object after training and then reload it prior to prediction. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# vectorize texts then save for use in prediction \ntokenizer <- text_tokenizer(num_words = 10000) %>% \nfit_text_tokenizer(tokenizer, texts) \nsave_text_tokenizer(tokenizer, \"tokenizer\") \n# (train model, etc.) \n# ...later in another session \ntokenizer <- load_text_tokenizer(\"tokenizer\") \n# (use tokenizer to preprocess data for prediction) \n```\n:::\n\n\n## See Also\nOther text tokenization:  `fit_text_tokenizer()`, `sequences_to_matrix()`, `text_tokenizer()`, `texts_to_matrix()`, `texts_to_sequences_generator()`, `texts_to_sequences()`\n\n",
    "supporting": [
      "save_text_tokenizer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}