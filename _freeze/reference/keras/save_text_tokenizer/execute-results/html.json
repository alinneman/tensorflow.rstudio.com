{
  "hash": "8d49c184cd1ea505bb04707a1d6a64e0",
  "result": {
    "markdown": "---\nexecute:\n  freeze: true\n---\n\n\n[R/preprocessing.R](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L370) \n\n# save_text_tokenizer\n\n## Save a text tokenizer to an external file\n\n## Description\n Enables persistence of text tokenizers alongside saved models. \n\n\n## Usage\n```r\n \nsave_text_tokenizer(object, filename) \n \nload_text_tokenizer(filename) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Text tokenizer fit with `fit_text_tokenizer()` |\n| filename | File to save/load |\n\n## Details\n You should always use the same text tokenizer for training and prediction. In many cases however prediction will occur in another session with a version of the model loaded via `load_model_hdf5()`.  In this case you need to save the text tokenizer object after training and then reload it prior to prediction. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n \n \n# vectorize texts then save for use in prediction \ntokenizer <- text_tokenizer(num_words = 10000) %>% \nfit_text_tokenizer(tokenizer, texts) \nsave_text_tokenizer(tokenizer, \"tokenizer\") \n \n# (train model, etc.) \n \n# ...later in another session \ntokenizer <- load_text_tokenizer(\"tokenizer\") \n \n# (use tokenizer to preprocess data for prediction) \n```\n:::\n\n\n## See Also\n Other text tokenization:  `fit_text_tokenizer()`, `sequences_to_matrix()`, `text_tokenizer()`, `texts_to_matrix()`, `texts_to_sequences_generator()`, `texts_to_sequences()` \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}