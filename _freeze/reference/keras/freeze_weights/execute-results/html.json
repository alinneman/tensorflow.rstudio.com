{
  "hash": "07884df630ea4bffd113909f63d4a99b",
  "result": {
    "markdown": "---\nexecute:\n  freeze: true\n---\n\n\n[R/freeze.R](https://github.com/rstudio/keras//blob/main/R/freeze.R#L89) \n\n# freeze_weights\n\n## Freeze and unfreeze weights\n\n## Description\n Freeze weights in a model or layer so that they are no longer trainable. \n\n\n## Usage\n```r\n \nfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n \nunfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Keras model or layer object |\n| from | Layer instance, layer name, or layer index within model |\n| to | Layer instance, layer name, or layer index within model |\n| which | layer names, integer positions, layers, logical vector (of `length(object$layers)`), or a function returning a logical vector. |\n\n\n\n\n## Note\n The `from` and `to` layer arguments are both inclusive.  When applied to a model, the freeze or unfreeze is a global operation over all layers in the model (i.e. layers not within the specified range will be set to the opposite value, e.g. unfrozen for a call to freeze).  Models must be compiled again after weights are frozen or unfrozen. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n \nconv_base <- application_vgg16( \n  weights = \"imagenet\", \n  include_top = FALSE, \n  input_shape = c(150, 150, 3) \n) \n \n# freeze it's weights \nfreeze_weights(conv_base) \n \nconv_base \n \n# create a composite model that includes the base + more layers \nmodel <- keras_model_sequential() %>% \n  conv_base() %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\") \n \n# compile \nmodel %>% compile( \n  loss = \"binary_crossentropy\", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\"accuracy\") \n) \n \nmodel \nprint(model, expand_nested = TRUE) \n \n \n \n# unfreeze weights from \"block5_conv1\" on \nunfreeze_weights(conv_base, from = \"block5_conv1\") \n \n# compile again since we froze or unfroze weights \nmodel %>% compile( \n  loss = \"binary_crossentropy\", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\"accuracy\") \n) \n \nconv_base \nprint(model, expand_nested = TRUE) \n \n# freeze only the last 5 layers \nfreeze_weights(conv_base, from = -5) \nconv_base \n# equivalently, also freeze only the last 5 layers \nunfreeze_weights(conv_base, to = -6) \nconv_base \n \n# Freeze only layers of a certain type, e.g, BatchNorm layers \nbatch_norm_layer_class_name <- class(layer_batch_normalization())[1] \nis_batch_norm_layer <- function(x) inherits(x, batch_norm_layer_class_name) \n \nmodel <- application_efficientnet_b0() \nfreeze_weights(model, which = is_batch_norm_layer) \nmodel \n# equivalent to: \nfor(layer in model$layers) { \n  if(is_batch_norm_layer(layer)) \n    layer$trainable <- FALSE \n  else \n    layer$trainable <- TRUE \n} \n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}