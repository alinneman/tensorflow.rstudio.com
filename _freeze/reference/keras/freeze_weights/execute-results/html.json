{
  "hash": "6f149be162c80e7f5bd2a3b21bc3844d",
  "result": {
    "markdown": "---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\"button\"> ![](/reference/assets/GitHub-Mark-32px.png){width=\"20\"} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/freeze.R) </button> | <button class=\"button\"> ![](/reference/assets/GitHub-Mark-32px.png){width=\"20\"} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/freeze.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/freeze.R*\n\n# freeze_weights\n\n## Freeze and unfreeze weights\n\n## Description\nFreeze weights in a model or layer so that they are no longer trainable. \n\n\n## Usage\n```r\nfreeze_weights(object, from = NULL, to = NULL, which = NULL) \nunfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Keras model or layer object |\n| from | Layer instance, layer name, or layer index within model |\n| to | Layer instance, layer name, or layer index within model |\n| which | layer names, integer positions, layers, logical vector (of `length(object$layers)`), or a function returning a logical vector. |\n\n\n\n\n## Note\n\nThe `from` and `to` layer arguments are both inclusive. \n\nWhen applied to a model, the freeze or unfreeze is a global operation over all layers in the model (i.e. layers not within the specified range will be set to the opposite value, e.g. unfrozen for a call to freeze). \n\nModels must be compiled again after weights are frozen or unfrozen. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nconv_base <- application_vgg16( \n  weights = \"imagenet\", \n  include_top = FALSE, \n  input_shape = c(150, 150, 3) \n) \n# freeze it's weights \nfreeze_weights(conv_base) \nconv_base \n# create a composite model that includes the base + more layers \nmodel <- keras_model_sequential() %>% \n  conv_base() %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\") \n# compile \nmodel %>% compile( \n  loss = \"binary_crossentropy\", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\"accuracy\") \n) \nmodel \nprint(model, expand_nested = TRUE) \n# unfreeze weights from \"block5_conv1\" on \nunfreeze_weights(conv_base, from = \"block5_conv1\") \n# compile again since we froze or unfroze weights \nmodel %>% compile( \n  loss = \"binary_crossentropy\", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\"accuracy\") \n) \nconv_base \nprint(model, expand_nested = TRUE) \n# freeze only the last 5 layers \nfreeze_weights(conv_base, from = -5) \nconv_base \n# equivalently, also freeze only the last 5 layers \nunfreeze_weights(conv_base, to = -6) \nconv_base \n# Freeze only layers of a certain type, e.g, BatchNorm layers \nbatch_norm_layer_class_name <- class(layer_batch_normalization())[1] \nis_batch_norm_layer <- function(x) inherits(x, batch_norm_layer_class_name) \nmodel <- application_efficientnet_b0() \nfreeze_weights(model, which = is_batch_norm_layer) \nmodel \n# equivalent to: \nfor(layer in model$layers) { \n  if(is_batch_norm_layer(layer)) \n    layer$trainable <- FALSE \n  else \n    layer$trainable <- TRUE \n} \n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}