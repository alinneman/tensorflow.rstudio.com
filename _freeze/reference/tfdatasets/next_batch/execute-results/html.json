{
  "hash": "db2a6bcdab6830acae945bb6e9eae590",
  "result": {
    "markdown": "---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\"button\"> ![](/reference/assets/GitHub-Mark-32px.png){width=\"20\"} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R#L67) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# next_batch\n\n## Tensor(s) for retrieving the next batch from a dataset\n\n## Description\nTensor(s) for retrieving the next batch from a dataset \n\n\n## Usage\n```r\nnext_batch(dataset) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n\n## Details\n\nTo access the underlying data within the dataset you iteratively evaluate the tensor(s) to read batches of data. \n\nNote that in many cases you won't need to explicitly evaluate the tensors. Rather, you will pass the tensors to another function that will perform the evaluation (e.g. the Keras layer_input() and compile() functions). \n\nIf you do need to perform iteration manually by evaluating the tensors, there are a couple of possible approaches to controlling/detecting when iteration should end. \n\nOne approach is to create a dataset that yields batches infinitely (traversing the dataset multiple times with different batches randomly drawn). In this case you'd use another mechanism like a global step counter or detecting a learning plateau. \n\nAnother approach is to detect when all batches have been yielded from the dataset. When the tensor reaches the end of iteration a runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in the `with_dataset()` function. \n\nSee the examples below for a demonstration of each of these methods of iteration. \n\n\n## Value\nTensor(s) that can be evaluated to yield the next batch of training data. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# iteration with 'infinite' dataset and explicit step counter \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_shuffle(5000) %>% \n  dataset_batch(128) %>% \n  dataset_repeat() # repeat infinitely \nbatch <- next_batch(dataset) \nsteps <- 200 \nfor (i in 1:steps) { \n  # use batch$x and batch$y tensors \n} \n# iteration that detects and ignores end of iteration error \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) \nbatch <- next_batch(dataset) \nwith_dataset({ \n  while(TRUE) { \n    # use batch$x and batch$y tensors \n  } \n}) \n```\n:::\n",
    "supporting": [
      "next_batch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}