{
  "hash": "f84b3a2b7beba54cafb3d1aad3c58c6c",
  "result": {
    "markdown": "---\ntitle: addition_rnn\ndescription: Implementation of sequence to sequence learning for performing addition of two numbers (as strings).\n---\n\nAn implementation of sequence to sequence learning for performing addition\n\nInput: \"535+61\"  \nOutput: \"596\"  \n\nPadding is handled by using a repeated sentinel character (space)\n\nInput may optionally be reversed, shown to increase performance in many tasks in:\n\"Learning to Execute\"\nhttp://arxiv.org/abs/1410.4615\nand\n\"Sequence to Sequence Learning with Neural Networks\"\nhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nTheoretically it introduces shorter term dependencies between source and target.\n\nTwo digits reversed:\n One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n\nThree digits reversed:\n One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n\nFour digits reversed:\n One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n\nFive digits reversed:\n One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in reticulate::use_virtualenv(\"r-tensorflow-site\", required = TRUE) : \n  Directory ~/.virtualenvs/r-tensorflow-site is not a Python virtualenv\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringi)\n\n# Function Definitions ----------------------------------------------------\n\n# Creates the char table and sorts them.\nlearn_encoding <- function(chars){\n  sort(chars)\n}\n\n# Encode from a character sequence to a one hot integer representation.\n# > encode(\"22+22\", char_table)\n# [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# +    0    1    0    0    0    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\nencode <- function(char, char_table){\n  strsplit(char, \"\") %>%\n    unlist() %>%\n    sapply(function(x){\n      as.numeric(x == char_table)\n    }) %>% \n    t()\n}\n\n# Decode the one hot representation/probabilities representation\n# to their character output.\ndecode <- function(x, char_table){\n  apply(x,1, function(y){\n    char_table[which.max(y)]\n  }) %>% paste0(collapse = \"\")\n}\n\n# Returns a list of questions and expected answers.\ngenerate_data <- function(size, digits, invert = TRUE){\n  \n  max_num <- as.integer(paste0(rep(9, digits), collapse = \"\"))\n  \n  # generate integers for both sides of question\n  x <- sample(1:max_num, size = size, replace = TRUE)\n  y <- sample(1:max_num, size = size, replace = TRUE)\n  \n  # make left side always smaller than right side\n  left_side <- ifelse(x <= y, x, y)\n  right_side <- ifelse(x >= y, x, y)\n  \n  results <- left_side + right_side\n  \n  # pad with spaces on the right\n  questions <- paste0(left_side, \"+\", right_side)\n  questions <- stri_pad(questions, width = 2*digits+1, \n                        side = \"right\", pad = \" \")\n  if(invert){\n    questions <- stri_reverse(questions)\n  }\n  # pad with spaces on the left\n  results <- stri_pad(results, width = digits + 1, \n                      side = \"left\", pad = \" \")\n  \n  list(\n    questions = questions,\n    results = results\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\n# Parameters for the model and dataset\nTRAINING_SIZE <- 50000\nDIGITS <- 2\n\n# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n# int is DIGITS\nMAXLEN <- DIGITS + 1 + DIGITS\n\n# All the numbers, plus sign and space for padding\ncharset <- c(0:9, \"+\", \" \")\nchar_table <- learn_encoding(charset)\n\n\n# Data Preparation --------------------------------------------------------\n\n# Generate Data\nexamples <- generate_data(size = TRAINING_SIZE, digits = DIGITS)\n\n# Vectorization\nx <- array(0, dim = c(length(examples$questions), MAXLEN, length(char_table)))\ny <- array(0, dim = c(length(examples$questions), DIGITS + 1, length(char_table)))\n\nfor(i in 1:TRAINING_SIZE){\n  x[i,,] <- encode(examples$questions[i], char_table)\n  y[i,,] <- encode(examples$results[i], char_table)\n}\n\n# Shuffle\nindices <- sample(1:TRAINING_SIZE, size = TRAINING_SIZE)\nx <- x[indices,,]\ny <- y[indices,,]\n\n\n# Explicitly set apart 10% for validation data that we never train over\nsplit_at <- trunc(TRAINING_SIZE/10)\nx_val <- x[1:split_at,,]\ny_val <- y[1:split_at,,]\nx_train <- x[(split_at + 1):TRAINING_SIZE,,]\ny_train <- y[(split_at + 1):TRAINING_SIZE,,]\n\nprint('Training Data:')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Training Data:\"\n```\n:::\n\n```{.r .cell-code}\nprint(dim(x_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 45000     5    12\n```\n:::\n\n```{.r .cell-code}\nprint(dim(y_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 45000     3    12\n```\n:::\n\n```{.r .cell-code}\nprint('Validation Data:')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Validation Data:\"\n```\n:::\n\n```{.r .cell-code}\nprint(dim(x_val))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5000    5   12\n```\n:::\n\n```{.r .cell-code}\nprint(dim(y_val))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5000    3   12\n```\n:::\n\n```{.r .cell-code}\n# Training ----------------------------------------------------------------\n\nHIDDEN_SIZE <- 128\nBATCH_SIZE <- 128\nLAYERS <- 1\n\n# Initialize sequential model\nmodel <- keras_model_sequential() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n  # Note: In a situation where your input sequences have a variable length,\n  # use input_shape=(None, num_feature).\n  layer_lstm(HIDDEN_SIZE, input_shape=c(MAXLEN, length(char_table))) %>%\n  # As the decoder RNN's input, repeatedly provide with the last hidden state of\n  # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n  # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n  layer_repeat_vector(DIGITS + 1)\n\n# The decoder RNN could be multiple layers stacked or a single layer.\n# By setting return_sequences to True, return not only the last output but\n# all the outputs so far in the form of (num_samples, timesteps,\n# output_dim). This is necessary as TimeDistributed in the below expects\n# the first dimension to be the timesteps.\nfor(i in 1:LAYERS)\n  model %>% layer_lstm(HIDDEN_SIZE, return_sequences = TRUE)\n\nmodel %>% \n  # Apply a dense layer to the every temporal slice of an input. For each of step\n  # of the output sequence, decide which character should be chosen.\n  time_distributed(layer_dense(units = length(char_table))) %>%\n  layer_activation(\"softmax\")\n\n# Compiling the model\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = \"adam\", \n  metrics = \"accuracy\"\n)\n\n# Get the model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n lstm (LSTM)                      (None, 128)                   72192       \n repeat_vector (RepeatVector)     (None, 3, 128)                0           \n lstm_1 (LSTM)                    (None, 3, 128)                131584      \n time_distributed (TimeDistribute  (None, 3, 12)                1548        \n d)                                                                         \n activation (Activation)          (None, 3, 12)                 0           \n============================================================================\nTotal params: 205,324\nTrainable params: 205,324\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Fitting loop\nmodel %>% fit( \n  x = x_train, \n  y = y_train, \n  batch_size = BATCH_SIZE, \n  epochs = 70,\n  validation_data = list(x_val, y_val)\n)\n\n# Predict for a new observation\nnew_obs <- encode(\"55+22\", char_table) %>%\n  array(dim = c(1,5,12))\nresult <- predict(model, new_obs)\nresult <- result[1,,]\ndecode(result, char_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \" 77\"\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}