{
  "hash": "41b055340d1db6927a51ee42af152654",
  "result": {
    "markdown": "---\ntitle: Writing your own callbacks\nauthor: Rick Chao, Francois Chollet, Tomasz Kalinowski\n---\n\n\n## Introduction\n\nA callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include `callback_tensorboard()` to visualize training progress and results with TensorBoard, or `callback_model_checkpoint()` to periodically save your model during training.\n\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started.\n\n\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()\n```\n:::\n\n\n## Keras callbacks overview\n\nAll callbacks subclass the `keras$callbacks$Callback` class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\n\nYou can pass a list of callbacks (as a named argument `callbacks`) to the following keras model methods:\n\n-   `fit()`\n-   `evaluate()`\n-   `predict()`\n\n<!-- -->\n\n## An overview of callback methods\n\n### Global methods\n\n#### `on_(train|test|predict)_begin(self, logs=None)`\n\nCalled at the beginning of `fit`/`evaluate`/`predict`.\n\n#### `on_(train|test|predict)_end(self, logs=None)`\n\nCalled at the end of `fit`/`evaluate`/`predict`.\n\n### Batch-level methods for training/testing/predicting\n\n#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`\n\nCalled right before processing a batch during training/testing/predicting.\n\n#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`\n\nCalled at the end of training/testing/predicting a batch. Within this method, `logs` is a dict containing the metrics results.\n\n### Epoch-level methods (training only)\n\n#### `on_epoch_begin(self, epoch, logs=None)`\n\nCalled at the beginning of an epoch during training.\n\n#### `on_epoch_end(self, epoch, logs=None)`\n\nCalled at the end of an epoch during training.\n\n## A basic example\n\nLet's take a look at a concrete example. To get started, let's import tensorflow and define a simple Sequential Keras model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n```\n:::\n\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n```\n:::\n\n\nNow, define a simple custom callback that logs:\n\n-   When `fit`/`evaluate`/`predict` starts & ends\n-   When each epoch starts & ends\n-   When each training batch starts & ends\n-   When each evaluation (test) batch starts & ends\n-   When each inference (prediction) batch starts & ends\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  str(logs); cat(\"\\n\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n```\n:::\n\n\nLet's try it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n```\n:::\n\n\n### Usage of `logs` dict\n\nThe `logs` dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n```\n:::\n\n\n## Usage of `self$model` attribute\n\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: `self$model`.\n\nHere are of few of the things you can do with `self$model` in a callback:\n\n-   Set `self$model$stop_training <- TRUE` to immediately interrupt training.\n-   Mutate hyperparameters of the optimizer (available as `self$model$optimizer`), such as `self$model$optimizer$learning_rate`.\n-   Save the model at period intervals.\n-   Record the output of `predict(model)` on a few test samples at the end of each epoch, to use as a sanity check during training.\n-   Extract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\n-   etc.\n\nLet's see this in action in a couple of examples.\n\n## Examples of Keras callback applications\n\n### Early stopping at minimum loss\n\nThis first example shows the creation of a `Callback` that stops training when the minimum of loss has been reached, by setting the attribute `self$model$stop_training` (boolean). Optionally, you can provide an argument `patience` to specify how many epochs we should wait before stopping after having reached a local minimum.\n\n`keras$callbacks$EarlyStopping` provides a more complete and general implementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n```\n:::\n\n\n### Learning rate scheduling\n\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\n\nSee `keras$callbacks$LearningRateScheduler` for a more general implementations (in RStudio, press F1 while the cursor is over `LearningRateScheduler` and a browser will open to [this page](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n```\n:::\n\n\n### Built-in Keras callbacks\n\nBe sure to check out the existing Keras callbacks by reading the [API docs](https://keras.io/api/callbacks/). Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!\n\n---\nformat: html\n---\n\n## Environment Details \n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_version()\n```\n:::\n\n\n:::\n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.info()\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "writing_your_own_callbacks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}