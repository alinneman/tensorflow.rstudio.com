{
  "hash": "4dd285d7ba12186fdecbc69028929802",
  "result": {
    "markdown": "---\ntitle: Writing your own callbacks\n# author: Rick Chao, Francois Chollet, Tomasz Kalinowski\ndescription: > \n  Guide to writing Keras callbacks for customizing the behavior during model\n  training, evaluation, or inference.\naliases:\n  - ../../articles/new-guides/writing_your_own_callbacks.html\n  - ../../articles/training_callbacks.html\n  - ../../guide/keras/training_callbacks/index.html\n---\n\n\n## Introduction\n\nA callback is a powerful tool to customize the behavior of a Keras model\nduring training, evaluation, or inference. Examples include\n`callback_tensorboard()` to visualize training progress and results with\nTensorBoard, or `callback_model_checkpoint()` to periodically save your\nmodel during training.\n\nIn this guide, you will learn what a Keras callback is, what it can do,\nand how you can build your own. We provide a few demos of simple\ncallback applications to get you started.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(dplyr, last)\n\ntf_version()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n## Keras callbacks overview\n\nAll callbacks subclass the `keras$callbacks$Callback` class, and\noverride a set of methods called at various stages of training, testing,\nand predicting. Callbacks are useful to get a view on internal states\nand statistics of the model during training.\n\nYou can pass a list of callbacks (as a named argument `callbacks`) to\nthe following keras model methods:\n\n-   `fit()`\n-   `evaluate()`\n-   `predict()`\n\n<!-- -->\n\n## An overview of callback methods\n\n### Global methods\n\n#### `on_(train|test|predict)_begin(logs = NULL)`\n\nCalled at the beginning of `fit`/`evaluate`/`predict`.\n\n#### `on_(train|test|predict)_end(logs = NULL)`\n\nCalled at the end of `fit`/`evaluate`/`predict`.\n\n### Batch-level methods for training/testing/predicting\n\n#### `on_(train|test|predict)_batch_begin(logs = NULL)`\n\nCalled right before processing a batch during\ntraining/testing/predicting.\n\n#### `on_(train|test|predict)_batch_end(batch, logs = NULL)`\n\nCalled at the end of training/testing/predicting a batch. Within this\nmethod, `logs` is a dict containing the metrics results.\n\n### Epoch-level methods (training only)\n\n#### `on_epoch_begin(epoch, logs = NULL)`\n\nCalled at the beginning of an epoch during training.\n\n#### `on_epoch_end(epoch, logs = NULL)`\n\nCalled at the end of an epoch during training.\n\n## A basic example\n\nLet's take a look at a concrete example. To get started, let's import\ntensorflow and define a simple Sequential Keras model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n```\n:::\n\n\nThen, load the MNIST data for training and testing from Keras datasets\nAPI:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500, ]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500, ]\nmnist$test$y  <- mnist$test$y[1:500]\n```\n:::\n\n\nNow, define a simple custom callback that logs:\n\n-   When `fit`/`evaluate`/`predict` starts & ends\n-   When each epoch starts & ends\n-   When each training batch starts & ends\n-   When each evaluation (test) batch starts & ends\n-   When each inference (prediction) batch starts & ends\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  logs %>% \n    lapply(signif, digits = 3) %>% \n    dput(control = \"niceNames\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n```\n:::\n\n\nLet's try it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting training; got logs: list()\nStart epoch 0 of training; got logs: list()\n...Training: start of batch 0; got logs: list()\n...Training: end of batch 0; got logs: list(loss = 29.3, mean_absolute_error = 4.58)\n...Training: start of batch 1; got logs: list()\n...Training: end of batch 1; got logs: list(loss = 462, mean_absolute_error = 16.1)\nStart testing; got logs: list()\n...Evaluating: start of batch 0; got logs: list()\n...Evaluating: end of batch 0; got logs: list(loss = 28.3, mean_absolute_error = 4.53)\n...Evaluating: start of batch 1; got logs: list()\n...Evaluating: end of batch 1; got logs: list(loss = 27.2, mean_absolute_error = 4.36)\nStop testing; got logs: list(loss = 27.2, mean_absolute_error = 4.36)\nEnd epoch 0 of training; got logs: list(loss = 462, mean_absolute_error = 16.1, val_loss = 27.2, \n    val_mean_absolute_error = 4.36)\nStart epoch 1 of training; got logs: list()\n...Training: start of batch 0; got logs: list()\n...Training: end of batch 0; got logs: list(loss = 24.9, mean_absolute_error = 4.17)\n...Training: start of batch 1; got logs: list()\n...Training: end of batch 1; got logs: list(loss = 17.4, mean_absolute_error = 3.43)\nStart testing; got logs: list()\n...Evaluating: start of batch 0; got logs: list()\n...Evaluating: end of batch 0; got logs: list(loss = 7.04, mean_absolute_error = 2.18)\n...Evaluating: start of batch 1; got logs: list()\n...Evaluating: end of batch 1; got logs: list(loss = 7.23, mean_absolute_error = 2.23)\nStop testing; got logs: list(loss = 7.23, mean_absolute_error = 2.23)\nEnd epoch 1 of training; got logs: list(loss = 17.4, mean_absolute_error = 3.43, val_loss = 7.23, \n    val_mean_absolute_error = 2.23)\nStop training; got logs: list(loss = 17.4, mean_absolute_error = 3.43, val_loss = 7.23, \n    val_mean_absolute_error = 2.23)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart testing; got logs: list()\n...Evaluating: start of batch 0; got logs: list()\n...Evaluating: end of batch 0; got logs: list(loss = 7.08, mean_absolute_error = 2.19)\n...Evaluating: start of batch 1; got logs: list()\n...Evaluating: end of batch 1; got logs: list(loss = 6.53, mean_absolute_error = 2.1)\n...Evaluating: start of batch 2; got logs: list()\n...Evaluating: end of batch 2; got logs: list(loss = 6.48, mean_absolute_error = 2.1)\n...Evaluating: start of batch 3; got logs: list()\n...Evaluating: end of batch 3; got logs: list(loss = 6.74, mean_absolute_error = 2.14)\nStop testing; got logs: list(loss = 6.74, mean_absolute_error = 2.14)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart predicting; got logs: list()\n...Predicting: start of batch 0; got logs: list()\n...Predicting: end of batch 0; got logs: list(outputs = c(3.73, 2.91, 1.57, 3.98, 2.77, 2.2, 3.85, 3.83, \n3.76, 6.35, 1.77, 2.12, 5.25, 2.15, 3.28, 2.09, 3.89, 3.57, 3.51, \n3.11, 5.52, 3.23, 2.82, 3.28, 2.34, 4.9, 3.95, 3.55, 3.44, 2.36, \n3.96, 2.23, 3.69, 2.18, 4.83, 3.4, 4.18, 2.8, 1.79, 3.35, 1.44, \n3.12, 4.49, 1.41, 3.52, 3.06, 3.6, 3.03, 6.5, 3.17, 3.39, 4.19, \n3.69, 1.63, 2.74, 3.25, 3.29, 1.63, 5.23, 2, 5.38, 4.39, 2.71, \n3.48, 5.54, 3.21, 2.5, 3.17, 5.52, 1.4, 3.59, 2.49, 2.8, 5.57, \n3, 4.96, 2.65, 2.41, 4.04, 7.09, 5.13, 4.08, 3.72, 4.51, 3.37, \n5.2, 5.02, 3.77, 3.88, 2.96, 2.97, 4.07, 3.22, 6.16, 4.7, 6.25, \n2.36, 5.17, 3.09, 4.85, 2.12, 1.82, 4.42, 3.06, 2.82, 4.23, 2.07, \n1.8, 4.46, 3.69, 4.29, 3.09, 3.51, 5.25, 3.39, 3.7, 3.46, 3.51, \n3.51, 4.25, 4.68, 3.41, 3.3, 4.27, 5.21, 3.73, 1.91, 4.52))\n...Predicting: start of batch 1; got logs: list()\n...Predicting: end of batch 1; got logs: list(outputs = c(6.3, 3.93, 5.46, 5.58, 4.1, 4.88, 4.95, 3.42, \n2.63, 3.22, 2.91, 4.2, 2.62, 5.14, 2.46, 1.99, 4.55, 2.01, 4.91, \n4.76, 2.63, 1.41, 3.81, 3.57, 3.52, 2.31, 2.29, 5.03, 2.46, 2.46, \n2, 3.24, 4.28, 2.84, 4.51, 5.17, 2.89, 3.11, 3.71, 2.9, 2.87, \n3.72, 3.29, 5.59, 2.89, 2.22, 3.63, 1.26, 1.58, 3.97, 2.7, 4.23, \n2.48, 4.28, 3.4, 1.98, 4.14, 4.09, 4.41, 3.32, 3.17, 1.69, 3.78, \n2.64, 3.16, 3.69, 2.95, 2.41, 3.61, 5.06, 4.14, 5.59, 6.71, 2.81, \n1.74, 1.93, 1.64, 2.52, 6.5, 2.4, 2.71, 6.07, 3.25, 3.41, 6, \n2.82, 6.09, 4.12, 3.75, 3.77, 3.47, 2.94, 4.78, 3.35, 5.59, 4.97, \n1.03, 3.19, 5.46, 2.55, 2.1, 2.51, 3.67, 2.39, 4.19, 5.91, 2.84, \n3.72, 3.88, 3.27, 4.1, 2.55, 3.4, 2.97, 5.79, 4.28, 2.77, 2.56, \n1.31, 4.13, 4.52, 2.59, 6.05, 1.87, 7.07, 2.97, 3.55, 3.35))\n...Predicting: start of batch 2; got logs: list()\n...Predicting: end of batch 2; got logs: list(outputs = c(3.32, 4.43, 4.23, 2.23, 3.96, 3.17, 6.49, 4.19, \n4.02, 3.5, 4.57, 3.91, 3.1, 1.33, 3.32, 2.86, 2.45, 4.88, 4.56, \n4.63, 2.74, 5.64, 3.96, 2.14, 3.82, 5.21, 3.49, 3.05, 3.19, 2.26, \n4.19, 3.75, 2.84, 2.29, 4.25, 2.79, 6.24, 4.85, 2.57, 4.04, 3.23, \n1.4, 3.09, 4.28, 2.84, 5.26, 2.62, 1.88, 4.48, 1.69, 2.63, 3.37, \n4.86, 2.3, 2.38, 4.68, 3.68, 3.16, 3.25, 4.62, 3.62, 3.92, 3.06, \n2.76, 3.28, 2.9, 6.26, 3.08, 1.79, 3.57, 2.96, 3.56, 4.28, 3.41, \n2.85, 2.35, 2.78, 3.58, 2.47, 2.45, 4.53, 3.81, 3.35, 2.87, 4.06, \n1.74, 2.78, 2.96, 3.06, 3.9, 4.25, 5.86, 2.56, 3.59, 3.81, 3.84, \n2.16, 5.83, 1.67, 4.34, 4.41, 1.57, 4.11, 4.92, 3.9, 3.77, 3.56, \n2.11, 3.02, 3.08, 3.66, 2.65, 3.32, 2.69, 5.2, 2.48, 1.02, 4.39, \n3.8, 3.97, 4.54, 2.01, 2.4, 3.49, 0.672, 2.63, 4.1, 2.74))\n...Predicting: start of batch 3; got logs: list()\n...Predicting: end of batch 3; got logs: list(outputs = c(3.98, 1.8, 3.16, 2.09, 1.68, 5.87, 3.35, 4.76, \n4.12, 2.59, 3.35, 3.28, 3.7, 2.26, 2.86, 3.58, 3.97, 3.06, 2.84, \n3.92, 2.83, 4.27, 4.36, 2.73, 2.96, 3.29, 6.36, 2.56, 6.46, 5.96, \n3.87, 4.73, 2.9, 4.97, 3.12, 2.08, 2.39, 2.92, 4.73, 3.37, 1.61, \n4.6, 3.39, 2.29, 3.82, 3.82, 2.12, 3.38, 3.31, 3.72, 3.8, 5.14, \n4.24, 6.69, 3.59, 3.56, 4.39, 2.86, 4.56, 2.15, 3, 1.73, 4.48, \n5.53, 5.82, 4.98, 6.2, 5.78, 2.63, 1.99, 4.37, 2.94, 2.06, 3.41, \n4.61, 1.8, 4.61, 4.69, 5, 3.79, 4.89, 5.85, 4.24, 3.96, 5.09, \n3.62, 3.51, 3.61, 3.04, 1.54, 5.57, 3.78, 2.81, 2.8, 2.63, 2.34, \n1.32, 4.24, 3.71, 1.58, 3.5, 2.3, 4.65, 4.94, 3.75, 1.57, 1.92, \n3.89, 3.01, 4.08, 4.72, 6.54, 3.71, 3.6, 4.11, 2.43))\nStop predicting; got logs: list()\n```\n:::\n:::\n\n\n### Usage of `logs`\n\nThe `logs` named list contains the loss value, and all the metrics at\nthe end of a batch or epoch. Example includes the loss and mean absolute\nerror.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUp to batch 0, the average loss is   23.28.\nUp to batch 1, the average loss is  440.17.\nUp to batch 2, the average loss is  299.87.\nUp to batch 3, the average loss is  232.65.\nThe average loss for epoch  0 is    232.65 and mean absolute error is    9.52.\nUp to batch 0, the average loss is    7.37.\nUp to batch 1, the average loss is    6.63.\nUp to batch 2, the average loss is    6.33.\nUp to batch 3, the average loss is    6.07.\nThe average loss for epoch  1 is      6.07 and mean absolute error is    2.03.\n```\n:::\n\n```{.r .cell-code}\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUp to batch 0, the average loss is    5.40.\nUp to batch 1, the average loss is    4.96.\nUp to batch 2, the average loss is    4.97.\nUp to batch 3, the average loss is    4.99.\n```\n:::\n:::\n\n\n## Usage of `self$model` attribute\n\nIn addition to receiving log information when one of their methods is\ncalled, callbacks have access to the model associated with the current\nround of training/evaluation/inference: `self$model`.\n\nHere are of few of the things you can do with `self$model` in a\ncallback:\n\n-   Set `self$model$stop_training <- TRUE` to immediately interrupt\n    training.\n-   Mutate hyperparameters of the optimizer (available as\n    `self$model$optimizer`), such as\n    `self$model$optimizer$learning_rate`.\n-   Save the model at period intervals.\n-   Record the output of `predict(model)` on a few test samples at the\n    end of each epoch, to use as a sanity check during training.\n-   Extract visualizations of intermediate features at the end of each\n    epoch, to monitor what the model is learning over time.\n-   etc.\n\nLet's see this in action in a couple of examples.\n\n## Examples of Keras callback applications\n\n### Early stopping at minimum loss\n\nThis first example shows the creation of a `Callback` that stops\ntraining when the minimum of loss has been reached, by setting the\nattribute `self$model$stop_training` (boolean). Optionally, you can\nprovide an argument `patience` to specify how many epochs we should wait\nbefore stopping after having reached a local minimum.\n\n`keras$callbacks$EarlyStopping` provides a more complete and general\nimplementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUp to batch 0, the average loss is   20.51.\nUp to batch 1, the average loss is  457.14.\nUp to batch 2, the average loss is  313.42.\nUp to batch 3, the average loss is  237.60.\nUp to batch 4, the average loss is  191.72.\nThe average loss for epoch  0 is    191.72 and mean absolute error is    8.08.\nUp to batch 0, the average loss is    6.77.\nUp to batch 1, the average loss is    5.98.\nUp to batch 2, the average loss is    5.80.\nUp to batch 3, the average loss is    5.73.\nUp to batch 4, the average loss is    5.50.\nThe average loss for epoch  1 is      5.50 and mean absolute error is    1.98.\nUp to batch 0, the average loss is    4.17.\nUp to batch 1, the average loss is    4.51.\nUp to batch 2, the average loss is    4.30.\nUp to batch 3, the average loss is    4.32.\nUp to batch 4, the average loss is    4.21.\nThe average loss for epoch  2 is      4.21 and mean absolute error is    1.67.\nUp to batch 0, the average loss is    4.06.\nUp to batch 1, the average loss is    3.78.\nUp to batch 2, the average loss is    4.21.\nUp to batch 3, the average loss is    4.23.\nUp to batch 4, the average loss is    4.55.\nThe average loss for epoch  3 is      4.55 and mean absolute error is    1.74.\nRestoring model weights from the end of the best epoch.\nEpoch 00004: early stopping\n```\n:::\n:::\n\n\n### Learning rate scheduling\n\nIn this example, we show how a custom Callback can be used to\ndynamically change the learning rate of the optimizer during the course\nof training.\n\nSee `keras$callbacks$LearningRateScheduler` for a more general\nimplementations (in RStudio, press F1 while the cursor is over\n`LearningRateScheduler` and a browser will open to [this\npage](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                                           0,           0.1  ,\n                                           3,           0.05 ,\n                                           6,           0.01 ,\n                                           9,           0.005,\n                                          12,           0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEpoch 00000: Learning rate is 0.1000.\nUp to batch 0, the average loss is   26.16.\nUp to batch 1, the average loss is  517.86.\nUp to batch 2, the average loss is  355.71.\nUp to batch 3, the average loss is  268.95.\nUp to batch 4, the average loss is  216.62.\nThe average loss for epoch  0 is    216.62 and mean absolute error is    8.60.\n\nEpoch 00001: Learning rate is 0.0500.\nUp to batch 0, the average loss is    6.34.\nUp to batch 1, the average loss is    5.76.\nUp to batch 2, the average loss is    6.21.\nUp to batch 3, the average loss is    6.12.\nUp to batch 4, the average loss is    5.98.\nThe average loss for epoch  1 is      5.98 and mean absolute error is    2.03.\n\nEpoch 00002: Learning rate is 0.0500.\nUp to batch 0, the average loss is    5.61.\nUp to batch 1, the average loss is    5.14.\nUp to batch 2, the average loss is    5.06.\nUp to batch 3, the average loss is    4.66.\nUp to batch 4, the average loss is    4.47.\nThe average loss for epoch  2 is      4.47 and mean absolute error is    1.71.\n\nEpoch 00003: Learning rate is 0.0500.\nUp to batch 0, the average loss is    5.25.\nUp to batch 1, the average loss is    4.60.\nUp to batch 2, the average loss is    4.37.\nUp to batch 3, the average loss is    4.35.\nUp to batch 4, the average loss is    4.25.\nThe average loss for epoch  3 is      4.25 and mean absolute error is    1.70.\n\nEpoch 00004: Learning rate is 0.0100.\nUp to batch 0, the average loss is    4.58.\nUp to batch 1, the average loss is    3.97.\nUp to batch 2, the average loss is    4.12.\nUp to batch 3, the average loss is    3.85.\nUp to batch 4, the average loss is    3.58.\nThe average loss for epoch  4 is      3.58 and mean absolute error is    1.54.\n\nEpoch 00005: Learning rate is 0.0100.\nUp to batch 0, the average loss is    3.71.\nUp to batch 1, the average loss is    3.69.\nUp to batch 2, the average loss is    3.72.\nUp to batch 3, the average loss is    3.84.\nUp to batch 4, the average loss is    3.79.\nThe average loss for epoch  5 is      3.79 and mean absolute error is    1.58.\n\nEpoch 00006: Learning rate is 0.0100.\nUp to batch 0, the average loss is    4.96.\nUp to batch 1, the average loss is    3.81.\nUp to batch 2, the average loss is    3.82.\nUp to batch 3, the average loss is    3.72.\nUp to batch 4, the average loss is    3.74.\nThe average loss for epoch  6 is      3.74 and mean absolute error is    1.53.\n\nEpoch 00007: Learning rate is 0.0050.\nUp to batch 0, the average loss is    3.24.\nUp to batch 1, the average loss is    3.59.\nUp to batch 2, the average loss is    3.25.\nUp to batch 3, the average loss is    3.20.\nUp to batch 4, the average loss is    3.34.\nThe average loss for epoch  7 is      3.34 and mean absolute error is    1.51.\n\nEpoch 00008: Learning rate is 0.0050.\nUp to batch 0, the average loss is    4.55.\nUp to batch 1, the average loss is    3.63.\nUp to batch 2, the average loss is    3.25.\nUp to batch 3, the average loss is    3.15.\nUp to batch 4, the average loss is    3.24.\nThe average loss for epoch  8 is      3.24 and mean absolute error is    1.42.\n\nEpoch 00009: Learning rate is 0.0050.\nUp to batch 0, the average loss is    3.04.\nUp to batch 1, the average loss is    3.14.\nUp to batch 2, the average loss is    3.58.\nUp to batch 3, the average loss is    3.50.\nUp to batch 4, the average loss is    3.64.\nThe average loss for epoch  9 is      3.64 and mean absolute error is    1.57.\n\nEpoch 00010: Learning rate is 0.0010.\nUp to batch 0, the average loss is    3.22.\nUp to batch 1, the average loss is    3.00.\nUp to batch 2, the average loss is    2.91.\nUp to batch 3, the average loss is    3.10.\nUp to batch 4, the average loss is    3.11.\nThe average loss for epoch 10 is      3.11 and mean absolute error is    1.41.\n\nEpoch 00011: Learning rate is 0.0010.\nUp to batch 0, the average loss is    3.17.\nUp to batch 1, the average loss is    3.73.\nUp to batch 2, the average loss is    3.25.\nUp to batch 3, the average loss is    3.37.\nUp to batch 4, the average loss is    3.54.\nThe average loss for epoch 11 is      3.54 and mean absolute error is    1.51.\n\nEpoch 00012: Learning rate is 0.0010.\nUp to batch 0, the average loss is    1.88.\nUp to batch 1, the average loss is    2.45.\nUp to batch 2, the average loss is    2.88.\nUp to batch 3, the average loss is    2.80.\nUp to batch 4, the average loss is    2.82.\nThe average loss for epoch 12 is      2.82 and mean absolute error is    1.35.\n\nEpoch 00013: Learning rate is 0.0010.\nUp to batch 0, the average loss is    3.00.\nUp to batch 1, the average loss is    2.92.\nUp to batch 2, the average loss is    2.96.\nUp to batch 3, the average loss is    3.11.\nUp to batch 4, the average loss is    3.06.\nThe average loss for epoch 13 is      3.06 and mean absolute error is    1.40.\n\nEpoch 00014: Learning rate is 0.0010.\nUp to batch 0, the average loss is    3.70.\nUp to batch 1, the average loss is    3.69.\nUp to batch 2, the average loss is    3.58.\nUp to batch 3, the average loss is    3.52.\nUp to batch 4, the average loss is    3.29.\nThe average loss for epoch 14 is      3.29 and mean absolute error is    1.45.\n```\n:::\n:::\n\n\n### Built-in Keras callbacks\n\nBe sure to check out the existing Keras callbacks by reading the [API\ndocs](https://keras.io/api/callbacks/). Applications include logging to\nCSV, saving the model, visualizing metrics in TensorBoard, and a lot\nmore!\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           compiler_4.2.1       pillar_1.7.0        \n [4] envir_0.2.1          base64enc_0.1-3      tools_4.2.1         \n [7] zeallot_0.1.0        digest_0.6.29        tibble_3.1.7        \n[10] jsonlite_1.8.0       evaluate_0.15        lifecycle_1.0.1     \n[13] lattice_0.20-45      pkgconfig_2.0.3      png_0.1-7           \n[16] rlang_1.0.4          Matrix_1.4-1         DBI_1.1.3           \n[19] cli_3.3.0            yaml_2.3.5           xfun_0.31           \n[22] fastmap_1.1.0        stringr_1.4.0        dplyr_1.0.9         \n[25] knitr_1.39           generics_0.1.3       htmlwidgets_1.5.4   \n[28] vctrs_0.4.1          rprojroot_2.0.3      tidyselect_1.1.2    \n[31] grid_4.2.1           here_1.0.1           reticulate_1.25-9000\n[34] glue_1.6.2           R6_2.5.1             fansi_1.0.3         \n[37] rmarkdown_2.14       purrr_0.3.4          magrittr_2.0.3      \n[40] whisker_0.4          tfruns_1.5.0         htmltools_0.5.2     \n[43] ellipsis_0.3.2       assertthat_0.2.1     utf8_1.2.2          \n[46] stringi_1.7.8        crayon_1.5.1        \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\nbeautifulsoup4==4.11.1\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nfilelock==3.7.1\nflatbuffers==1.12\ngast==0.4.0\ngdown==4.5.1\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\nPySocks==1.7.1\npython-dateutil==2.8.2\npytz==2022.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nsoupsieve==2.3.2.post1\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-14 \nPage render time: 8 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}