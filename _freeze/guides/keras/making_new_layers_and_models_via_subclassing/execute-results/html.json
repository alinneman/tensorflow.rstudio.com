{
  "hash": "0b85a7033b801d2c5bdb04d5975c58de",
  "result": {
    "markdown": "---\ntitle: \"Writing `Layer` and `Model` objects from scratch.\"\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'magrittr' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n## The `Layer` class: a combination of state (weights) and some computation\n\nOne of the central abstractions in Keras is the `Layer` class. A layer\nencapsulates both a state (the layer's \"weights\") and a transformation\nfrom inputs to outputs (a \"call\", the layer's forward pass).\n\nHere's a densely-connected layer. It has a state: the variables `w` and\n`b`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \"float32\"\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \"float32\"),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nYou would use a layer by calling it on some tensor input(s), much like a\nregular function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.00647149 -0.05578769  0.03210767  0.07069797]\n [-0.00647149 -0.05578769  0.03210767  0.07069797]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n\n`Linear` behaves similarly to a layer present in the Python interface to\nkeras (e.g., `keras$layers$Dense`).\n\nHowever, one additional step is needed to make it behave like the\nbuiltin layers present in the keras R package (e.g., `layer_dense()`).\n\nKeras layers in R are designed to compose nicely with the pipe operator\n(`%>%`), so that the layer instance is conveniently created on demand\nwhen an existing model or tensor is piped in. In order to make a custom\nlayer similarly compose nicely with the pipe, you can call\n`create_layer_wrapper()` on the layer class constructor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- create_layer_wrapper(Linear)\n```\n:::\n\n\nNow `layer_linear` is a layer constructor that composes nicely with\n`%>%`, just like the built-in layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.03112034  0.06114008  0.09245743  0.0617287 ]\n [-0.03112034  0.06114008  0.09245743  0.0617287 ]], shape=(2, 4), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n linear_1 (Linear)                (2, 4)                        12          \n============================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nBecause the pattern above is so common, there is a convenience function\nthat combines the steps of subclassing `keras$layers$Layer` and calling\n`create_layer_wrapper` on the output: the `Layer` function. The\n`layer_linear` defined below is identical to the `layer_linear` defined\nabove.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- Layer(\n  \"Linear\",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n```\n:::\n\n\nFor the remainder of this vignette we'll be using the `%py_class%`\nconstructor. However, in your own code feel free to use\n`create_layer_wrapper` and/or `Layer` if you prefer.\n\nNote that the weights `w` and `b` are automatically tracked by the layer\nupon being set as layer attributes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n```\n:::\n\n\nYou also have access to a quicker shortcut for adding a weight to a\nlayer: the `add_weight()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.04482888 -0.07073172  0.01847215  0.08916104]\n [-0.04482888 -0.07073172  0.01847215  0.08916104]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n\n## Layers can have non-trainable weights\n\nBesides trainable weights, you can add non-trainable weights to a layer\nas well. Such weights are meant not to be taken into account during\nbackpropagation, when you are training the layer.\n\nHere's how to add and use a non-trainable weight:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 2\n```\n:::\n\n```{.r .cell-code}\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 4\n```\n:::\n:::\n\n\nIt's part of `layer$weights`, but it gets categorized as a non-trainable\nweight:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"weights:\", length(my_sum$weights), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 1 \n```\n:::\n\n```{.r .cell-code}\ncat(\"non-trainable weights:\", length(my_sum$non_trainable_weights), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon-trainable weights: 1 \n```\n:::\n\n```{.r .cell-code}\n# It's not included in the trainable weights:\ncat(\"trainable_weights:\", my_sum$trainable_weights, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights:  \n```\n:::\n:::\n\n\n## Best practice: deferring weight creation until the shape of the inputs is known\n\nOur `Linear` layer above took an `input_dim`argument that was used to\ncompute the shape of the weights `w` and `b` in `initialize()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nIn many cases, you may not know in advance the size of your inputs, and\nyou would like to lazily create weights when that value becomes known,\nsome time after instantiating the layer.\n\nIn the Keras API, we recommend creating layer weights in the\n`build(self, inputs_shape)` method of your layer. Like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nThe `build()` method of your layer will automatically run the first time\nyour layer instance is called. You now have a layer that can handle an\narbitrary number of input features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n```\n:::\n\n\nImplementing `build()` separately as shown above nicely separates\ncreating weights only once from using weights in every call. However,\nfor some advanced custom layers, it can become impractical to separate\nthe state creation and computation. Layer implementers are allowed to\ndefer weight creation to the first `call()`, but need to take care that\nlater calls use the same weights. In addition, since `call()` is likely\nto be executed for the first time inside a `tf_function()`, any variable\ncreation that takes place in `call()` should be wrapped in a\n`tf$init_scope()`.\n\n## Layers are recursively composable\n\nIf you assign a Layer instance as an attribute of another Layer, the\nouter layer will start tracking the weights created by the inner layer.\n\nWe recommend creating such sublayers in the `initialize()` method and\nleave it to the first `call()` to trigger building their weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\ncat(\"weights:\", length(mlp$weights), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 6 \n```\n:::\n\n```{.r .cell-code}\ncat(\"trainable weights:\", length(mlp$trainable_weights), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable weights: 6 \n```\n:::\n:::\n\n\n## The `add_loss()` method\n\nWhen writing the `call()` method of a layer, you can create loss tensors\nthat you will want to use later, when writing your training loop. This\nis doable by calling `self$add_loss(value)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n```\n:::\n\n\nThese losses (including those created by any inner layer) can be\nretrieved via `layer$losses`. This property is reset at the start of\nevery `call()` to the top-level layer, so that `layer$losses` always\ncontains the loss values created during the last forward pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n```\n:::\n\n\nIn addition, the `loss` property also contains regularization losses\ncreated for the weights of any inner layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor(0.0019065848, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThese losses are meant to be taken into account when writing training\nloops, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n```\n:::\n\n\nFor a detailed guide about writing training loops, see the [guide to\nwriting a training loop from\nscratch](/guides/writing_a_training_loop_from_scratch/).\n\nThese losses also work seamlessly with `fit()` (they get automatically\nsummed and added to the main loss, if any):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \"adam\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n```\n:::\n\n\n## The `add_metric()` method\n\nSimilarly to `add_loss()`, layers also have an `add_metric()` method for\ntracking the moving average of a quantity during training.\n\nConsider the following layer: a \"logistic endpoint\" layer. It takes as\ninputs predictions and targets, it computes a loss which it tracks via\n`add_loss()`, and it computes an accuracy scalar, which it tracks via\n`add_metric()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n```\n:::\n\n\nMetrics tracked in this way are accessible via `layer$metrics`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\"layer$metrics: \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlayer$metrics: \n```\n:::\n\n```{.r .cell-code}\nstr(layer$metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n```\n:::\n\n```{.r .cell-code}\ncat(\"current accuracy value:\", as.numeric(layer$metrics[[1]]$result()), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncurrent accuracy value: 1 \n```\n:::\n:::\n\n\nJust like for `add_loss()`, these metrics are tracked by `fit()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape(3), name = \"inputs\")\ntargets <- layer_input(shape(10), name = \"targets\")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)\n```\n:::\n\n\n## You can optionally enable serialization on your layers\n\nIf you need your custom layers to be serializable as part of a\n[Functional model](/guides/functional_api/), you can optionally\nimplement a `get_config()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$units\n[1] 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\n\nNote that the `initialize()` method of the base `Layer` class takes some\nadditional named arguments, in particular a `name` and a `dtype`. It's\ngood practice to pass these arguments to the parent class in\n`initialize()` and to include them in the layer config:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ name     : chr \"linear_9\"\n $ trainable: logi TRUE\n $ dtype    : chr \"float32\"\n $ units    : num 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\n\nIf you need more flexibility when deserializing the layer from its\nconfig, you can also override the `from_config()` class method. This is\nthe base implementation of `from_config()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrom_config <- function(cls, config) do.call(cls, config)\n```\n:::\n\n\nTo learn more about serialization and saving, see the complete [guide to\nsaving and serializing models](/guides/serialization_and_saving/).\n\n## Privileged `training` argument in the `call()` method\n\nSome layers, in particular the `BatchNormalization` layer and the\n`Dropout` layer, have different behaviors during training and inference.\nFor such layers, it is standard practice to expose a `training`\n(boolean) argument in the `call()` method.\n\nBy exposing this argument in `call()`, you enable the built-in training\nand evaluation loops (e.g. `fit()`) to correctly use the layer in\ntraining and inference. Note, the default of `NULL` means that the\ntraining parameter will be inferred by keras from the training context\n(e.g., it will be `TRUE` if called from `fit()`, `FALSE` if called from\n`predict()`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}\n```\n:::\n\n\n## Privileged `mask` argument in the `call()` method\n\nThe other privileged argument supported by `call()` is the `mask`\nargument.\n\nYou will find it in all Keras RNN layers. A mask is a boolean tensor\n(one boolean value per timestep in the input) used to skip certain input\ntimesteps when processing timeseries data.\n\nKeras will automatically pass the correct `mask` argument to `call()`\nfor layers that support it, when a mask is generated by a prior layer.\nMask-generating layers are the `Embedding` layer configured with\n`mask_zero=True`, and the `Masking` layer.\n\nTo learn more about masking and how to write masking-enabled layers,\nplease check out the guide [\"understanding padding and\nmasking\"](/guides/understanding_masking_and_padding/).\n\n## The `Model` class\n\nIn general, you will use the `Layer` class to define inner computation\nblocks, and will use the `Model` class to define the outer model -- the\nobject you will train.\n\nFor instance, in a ResNet50 model, you would have several ResNet blocks\nsubclassing `Layer`, and a single `Model` encompassing the entire\nResNet50 network.\n\nThe `Model` class has the same API as `Layer`, with the following\ndifferences:\n\n-   It has support for built-in training, evaluation, and prediction\n    methods (`fit()`, `evaluate()`, `predict()`).\n-   It exposes the list of its inner layers, via the `model$layers`\n    property.\n-   It exposes saving and serialization APIs (`save_model_tf()`,\n    `save_model_weights_tf()`, ...)\n\nEffectively, the `Layer` class corresponds to what we refer to in the\nliterature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\")\nor as a \"block\" (as in \"ResNet block\" or \"Inception block\").\n\nMeanwhile, the `Model` class corresponds to what is referred to in the\nliterature as a \"model\" (as in \"deep learning model\") or as a \"network\"\n(as in \"deep neural network\").\n\nSo if you're wondering, \"should I use the `Layer` class or the `Model`\nclass?\", ask yourself: will I need to call `fit()` on it? Will I need to\ncall `save()` on it? If so, go with `Model`. If not (either because your\nclass is just a block in a bigger system, or because you are writing\ntraining & saving code yourself), use `Layer`.\n\nFor instance, we could take our mini-resnet example above, and use it to\nbuild a `Model` that we could train with `fit()`, and that we could save\nwith `save_model_weights_tf()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)\n```\n:::\n\n\n## Putting it all together: an end-to-end example\n\nHere's what you've learned so far:\n\n-   A `Layer` encapsulates a state (created in `initialize()` or\n    `build()`), and some computation (defined in `call()`).\n-   Layers can be recursively nested to create new, bigger computation\n    blocks.\n-   Layers can create and track losses (typically regularization losses)\n    as well as metrics, via `add_loss()` and `add_metric()`\n-   The outer container, the thing you want to train, is a `Model`. A\n    `Model` is just like a `Layer`, but with added training and\n    serialization utilities.\n\nLet's put all of these things together into an end-to-end example: we're\ngoing to implement a Variational AutoEncoder (VAE). We'll train it on\nMNIST digits.\n\nOur VAE will be a subclass of `Model`, built as a nested composition of\nlayers that subclass `Layer`. It will feature a regularization loss (KL\ndivergence).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \"encoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \"Converts z, the encoded digit vector, back into a readable digit.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \"decoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \"Combines the encoder and decoder into an end-to-end model for training.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \"autoencoder\", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n```\n:::\n\n\nLet's write a simple training loop on MNIST:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\"Start of epoch %d\\n\", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\"step %d: mean loss = %.4f\\n\", step, loss_metric$result()))\n      }\n    }\n  })\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart of epoch 1\nstep 100: mean loss = 0.1279\nstep 200: mean loss = 0.1003\nstep 300: mean loss = 0.0899\nstep 400: mean loss = 0.0848\nstep 500: mean loss = 0.0813\nstep 600: mean loss = 0.0791\nstep 700: mean loss = 0.0774\nstep 800: mean loss = 0.0762\nstep 900: mean loss = 0.0752\nStart of epoch 2\nstep 100: mean loss = 0.0742\nstep 200: mean loss = 0.0737\nstep 300: mean loss = 0.0732\nstep 400: mean loss = 0.0728\nstep 500: mean loss = 0.0724\nstep 600: mean loss = 0.0721\nstep 700: mean loss = 0.0718\nstep 800: mean loss = 0.0716\nstep 900: mean loss = 0.0713\n```\n:::\n:::\n\n\nNote that since the VAE is subclassing `Model`, it features built-in\ntraining loops. So you could also have trained it like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)\n```\n:::\n\n\n## Beyond object-oriented development: the Functional API\n\nIf you prefer a less object-oriented way of programming, you can also\nbuild models using the [Functional API](/guides/functional_api/).\nImportantly, choosing one style or another does not prevent you from\nleveraging components written in the other style: you can always\nmix-and-match.\n\nFor instance, the Functional API example below reuses the same\n`Sampling` layer we defined in the example above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \"encoder_input\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \"z_mean\")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \"z_log_var\")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \"encoder\")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \"z_sampling\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \"sigmoid\")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \"decoder\")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \"vae\")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n```\n:::\n\n\nFor more information, make sure to read the [Functional API\nguide](/guides/functional_api/).\n\n## Defining custom layers and models in an R package\n\nUnfortunately you can't use anything that creates references to Python\nobjects, at the top-level of an R package.\n\nHere is why: when you build an R package, all the R files in the `R/`\ndirectory get sourced in an R environment (the package namespace), and\nthen that environment is saved as part of the package bundle. Loading\nthe package means restoring the saved R environment. This means that the\nR code only gets sourced once, at build time. If you create references\nto external objects (e.g., Python objects) at package build time, they\nwill be NULL pointers when the package is loaded, because the external\nobjects they pointed to at build time no longer exist at load time.\n\nThe solution is to delay creating references to Python objects until run\ntime. Fortunately, `%py_class%`, `Layer()`, and\n`create_layer_wrapper(R6Class(...))` are all lazy about initializing the\nPython reference, so they are safe to define and export in an R package.\n\nIf you're writing an R package that uses keras and reticulate, [this\narticle](https://rstudio.github.io/reticulate/articles/package.html)\nmight be helpful to read over.\n\n## Summary\n\nIn this guide you learned about creating custom layers and models in\nkeras.\n\n-   The constructors available: `new_layer_class()`, `%py_class%`,\n    `create_layer_wrapper()`, `R6Class()`, `Layer()`.\n-   What methods to you might want to define to your model:\n    `initialize()`, `build()`, `call()`, and `get_config()`.\n-   What convenience methods are available when you subclass\n    `keras$layers$Layer`: `add_weight()`, `add_loss()`, and\n    `add_metric()`\n\n\n---\nformat: html\n---\n\n## Environment Details \n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_version()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\" \n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "making_new_layers_and_models_via_subclassing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}