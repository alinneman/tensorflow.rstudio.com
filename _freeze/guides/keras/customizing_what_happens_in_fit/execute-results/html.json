{
  "hash": "33d3e6bd3c97680d3eed35a5313c6cbd",
  "result": {
    "markdown": "---\ntitle: Customizing what happens in `fit()`\nauthor: \n  - name: Francois Chollet\n    url: https://twitter.com/fchollet\n  - name: Tomasz Kalinowski\n    url: https://github.com/t-kalinowski\nexecute:\n  eval: true\naliases:\n  - ../../articles/new-guides/customizing_what_happens_in_fit.html\n  - ../../guide/keras/custom_models/index.html\n---\n\n\n## Introduction\n\nWhen you're doing supervised learning, you can use `fit()` and\neverything works smoothly.\n\nWhen you need to write your own training loop from scratch, you can use\nthe `GradientTape` and take control of every little detail.\n\nBut what if you need a custom training algorithm, but you still want to\nbenefit from the convenient features of `fit()`, such as callbacks,\nbuilt-in distribution support, or step fusing?\n\nA core principle of Keras is **progressive disclosure of complexity**.\nYou should always be able to get into lower-level workflows in a gradual\nway. You shouldn't fall off a cliff if the high-level functionality\ndoesn't exactly match your use case. You should be able to gain more\ncontrol over the small details while retaining a commensurate amount of\nhigh-level convenience.\n\nWhen you need to customize what `fit()` does, you should **override the\ntraining step function of the `Model` class**. This is the function that\nis called by `fit()` for every batch of data. You will then be able to\ncall `fit()` as usual -- and it will be running your own learning\nalgorithm.\n\nNote that this pattern does not prevent you from building models with\nthe Functional API. You can do this whether you're building `Sequential`\nmodels, Functional API models, or subclassed models.\n\nLet's see how that works.\n\n## Setup\n\nRequires TensorFlow 2.2 or later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## A first simple example\n\nLet's start from a simple example:\n\n-   We create a new model class by calling `new_model_class()`.\n-   We just override the method `train_step(data)`.\n-   We return a dictionary mapping metric names (including the loss) to\n    their current value.\n\nThe input argument `data` is what gets passed to fit as training data:\n\n-   If you pass arrays, by calling `fit(x, y, ...)`, then `data` will be\n    the tuple `(x, y)`\n-   If you pass a `tf$data$Dataset`, by calling `fit(dataset, ...)`,\n    then `data` will be what gets yielded by `dataset` at each batch.\n\nIn the body of the `train_step` method, we implement a regular training\nupdate, similar to what you are already familiar with. Importantly, **we\ncompute the loss via `self$compiled_loss`**, which wraps the loss(es)\nfunction(s) that were passed to `compile()`.\n\nSimilarly, we call `self$compiled_metrics$update_state(y, y_pred)` to\nupdate the state of the metrics that were passed in `compile()`, and we\nquery results from `self$metrics` at the end to retrieve their current\nvalue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nLet's try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)\n```\n:::\n\n\n## Going lower-level\n\nNaturally, you could just skip passing a loss function in `compile()`,\nand instead do everything *manually* in `train_step`. Likewise for\nmetrics.\n\nHere's a lower-level example, that only uses `compile()` to configure\nthe optimizer:\n\n-   We start by creating `Metric` instances to track our loss and a MAE\n    score.\n-   We implement a custom `train_step()` that updates the state of these\n    metrics (by calling `update_state()` on them), then query them (via\n    `result()`) to return their current average value, to be displayed\n    by the progress bar and to be pass to any callback.\n-   Note that we would need to call `reset_states()` on our metrics\n    between each epoch! Otherwise calling `result()` would return an\n    average since the start of training, whereas we usually work with\n    per-epoch averages. Thankfully, the framework can do that for us:\n    just list any metric you want to reset in the `metrics` property of\n    the model. The model will call `reset_states()` on any object listed\n    here at the beginning of each `fit()` epoch or at the beginning of a\n    call to `evaluate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)\n```\n:::\n\n\n## Supporting `sample_weight` & `class_weight`\n\nYou may have noticed that our first basic example didn't make any\nmention of sample weighting. If you want to support the `fit()`\narguments `sample_weight` and `class_weight`, you'd simply do the\nfollowing:\n\n-   Unpack `sample_weight` from the `data` argument\n-   Pass it to `compiled_loss` & `compiled_metrics` (of course, you\n    could also just apply it manually if you don't rely on `compile()`\n    for losses & metrics)\n-   That's it. That's the list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)\n```\n:::\n\n\n## Providing your own evaluation step\n\nWhat if you want to do the same for calls to `model$evaluate()`? Then\nyou would override `test_step` in exactly the same way. Here's what it\nlooks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss       mae \n0.3238659 0.4524673 \n```\n:::\n:::\n\n\n## Wrapping up: an end-to-end GAN example\n\nLet's walk through an end-to-end example that leverages everything you\njust learned.\n\nLet's consider:\n\n-   A generator network meant to generate 28x28x1 images.\n-   A discriminator network meant to classify 28x28x1 images into two\n    classes (\"fake\" and \"real\").\n-   One optimizer for each.\n-   A loss function to train the discriminator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n```\n:::\n\n\nHere's a feature-complete GAN class, overriding `compile()` to use its\nown signature, and implementing the entire GAN algorithm in 17 lines in\n`train_step`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n```\n:::\n\n\nLet's test-drive it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n```\n:::\n\n\nHappy training!\n\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tfdatasets_2.9.0.9000 keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           compiler_4.2.1       base64enc_0.1-3     \n [4] tools_4.2.1          zeallot_0.1.0        digest_0.6.29       \n [7] jsonlite_1.8.0       evaluate_0.15        lattice_0.20-45     \n[10] png_0.1-7            rlang_1.0.4          Matrix_1.4-1        \n[13] cli_3.3.0            yaml_2.3.5           xfun_0.31           \n[16] fastmap_1.1.0        stringr_1.4.0        knitr_1.39          \n[19] generics_0.1.3       htmlwidgets_1.5.4    vctrs_0.4.1         \n[22] rprojroot_2.0.3      grid_4.2.1           tidyselect_1.1.2    \n[25] reticulate_1.25-9000 glue_1.6.2           here_1.0.1          \n[28] R6_2.5.1             rmarkdown_2.14       purrr_0.3.4         \n[31] magrittr_2.0.3       whisker_0.4          tfruns_1.5.0        \n[34] htmltools_0.5.2      ellipsis_0.3.2       stringi_1.7.8       \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nflatbuffers==1.12\ngast==0.4.0\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npytz==2022.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-14 \nPage render time: 9 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}