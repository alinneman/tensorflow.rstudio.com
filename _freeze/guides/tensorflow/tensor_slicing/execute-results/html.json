{
  "hash": "4a208703e2abc044ff5ae3c9b901f2c0",
  "result": {
    "markdown": "---\ntitle: Tensor Slicing\ndescription: > \n  Advanced guide for working with sub-sections (slices) of Tensors.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Copyright 2020 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# Introduction to tensor slicing\n\nWhen working on ML applications such as object detection and NLP, it is\nsometimes necessary to work with sub-sections (slices) of tensors. For\nexample, if your model architecture includes routing, where one layer\nmight control which training example gets routed to the next layer. In\nthis case, you could use tensor slicing ops to split the tensors up and\nput them back together in the right order.\n\nIn NLP applications, you can use tensor slicing to perform word masking\nwhile training. For example, you can generate training data from a list\nof sentences by choosing a word index to mask in each sentence, taking\nthe word out as a label, and then replacing the chosen word with a mask\ntoken.\n\nIn this guide, you will learn how to use the TensorFlow APIs to:\n\n-   Extract slices from a tensor\n-   Insert data at specific indices in a tensor\n\nThis guide assumes familiarity with tensor indexing. Read the indexing\nsections of the\n[Tensor](https://www.tensorflow.org/guide/tensor#indexing) and\n[TensorFlow NumPy](https://www.tensorflow.org/guide/tf_numpy#indexing)\nguides before getting started with this guide.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n:::\n\n\n## Extract tensor slices\n\nPerform slicing using the `[` operator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- as_tensor(c(1, 2, 3, 4, 5, 6, 7))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nt1[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\n![](images/tf_slicing/slice_1d_1.png)\n\n::: callout-note\nUnlike base R's `[` operator, TensorFlow's `[` uses negative indexes for\nselecting starting from the end.\n\n`NULL` can be used instead of the last dimension or first, depending if\nit appears before or after the `:`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1[-3:NULL]\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([5. 6. 7.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\n![](images/tf_slicing/slice_1d_2.png)\n\nFor 2-dimensional tensors,you can use something like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt2 <- as_tensor(rbind(c(0, 1, 2, 3, 4),\n                      c(5, 6, 7, 8, 9),\n                      c(10, 11, 12, 13, 14),\n                      c(15, 16, 17, 18, 19)))\n\nt2[NULL:-1, 2:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  2.]\n [ 6.  7.]\n [11. 12.]\n [16. 17.]], shape=(4, 2), dtype=float64)\n```\n:::\n:::\n\n\n![](images/tf_slicing/slice_2d_1.png)\n\n::: callout-note\n`tf$slice` can be used instead of the `[` operator. However, not that\nwhen using functions directly from the `tf` module, dimensions and\nindexes will start from 0, unlike in R.\n\nYou also need to make sure that indexes are passed to TensorFlow with\nthe integer type, for example using the `L` suffix notation.\n:::\n\nYou can use `tf$slice` on higher dimensional tensors as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt3 <- as_tensor(array(seq(from=1, to = 31, by = 2), dim = c(2,2,4)))\ntf$slice(\n  t3,\n  begin = list(1L, 1L, 0L),\n  size = list(1L, 1L, 2L)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[[ 7. 15.]]], shape=(1, 1, 2), dtype=float64)\n```\n:::\n:::\n\n\nYou can also use `tf$strided_slice` to extract slices of tensors by\n'striding' over the tensor dimensions.\n\nUse `tf$gather` to extract specific indices from a single axis of a\ntensor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$gather(t1, indices = c(0L, 3L, 6L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 4. 7.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\n![](images/tf_slicing/slice_1d_3.png)\n\n`tf$gather` does not require indices to be evenly spaced.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalphabet <- as_tensor(strsplit(\"abcdefghijklmnopqrstuvwxyz\", \"\")[[1]])\ntf$gather(alphabet, indices = c(2L, 0L, 19L, 18L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'c' b'a' b't' b's'], shape=(4), dtype=string)\n```\n:::\n:::\n\n\n![](images/tf_slicing/gather_1.png)\n\nTo extract slices from multiple axes of a tensor, use `tf$gather_nd`.\nThis is useful when you want to gather the elements of a matrix as\nopposed to just its rows or columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt4 <- as_tensor(rbind(c(0, 5),\n                      c(1, 6),\n                      c(2, 7),\n                      c(3, 8),\n                      c(4, 9)))\n\ntf$gather_nd(t4, indices = list(list(2L), list(3L), list(0L)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2. 7.]\n [3. 8.]\n [0. 5.]], shape=(3, 2), dtype=float64)\n```\n:::\n:::\n\n\n![](images/tf_slicing/gather_2.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt5 <- array(1:18, dim = c(2,3,3))\ntf$gather_nd(t5, indices = list(c(0L, 0L, 0L), c(1L, 2L, 1L)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 1 12], shape=(2), dtype=int32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Return a list of two matrices\ntf$gather_nd(\n  t5,\n  indices = list(\n    list(c(0L, 0L), c(0L, 2L)), \n    list(c(1L, 0L), c(1L, 2L)))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 1  7 13]\n  [ 5 11 17]]\n\n [[ 2  8 14]\n  [ 6 12 18]]], shape=(2, 2, 3), dtype=int32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Return one matrix\ntf$gather_nd(\n  t5,\n  indices = list(c(0L, 0L), c(0L, 2L), c(1L, 0L), c(1L, 2L))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1  7 13]\n [ 5 11 17]\n [ 2  8 14]\n [ 6 12 18]], shape=(4, 3), dtype=int32)\n```\n:::\n:::\n\n\n## Insert data into tensors\n\nUse `tf$scatter_nd` to insert data at specific slices/indices of a\ntensor. Note that the tensor into which you insert values is\nzero-initialized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt6 <- as_tensor(list(10L))\nindices <- as_tensor(list(list(1L), list(3L), list(5L), list(7L), list(9L)))\ndata <- as_tensor(c(2, 4, 6, 8, 10))\n\ntf$scatter_nd(\n  indices = indices,\n  updates = data,\n  shape = t6\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 0.  2.  0.  4.  0.  6.  0.  8.  0. 10.], shape=(10), dtype=float64)\n```\n:::\n:::\n\n\nMethods like `tf$scatter_nd` which require zero-initialized tensors are\nsimilar to sparse tensor initializers. You can use `tf$gather_nd` and\n`tf$scatter_nd` to mimic the behavior of sparse tensor ops.\n\nConsider an example where you construct a sparse tensor using these two\nmethods in conjunction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gather values from one tensor by specifying indices\nnew_indices <- as_tensor(rbind(c(0L, 2L), c(2L, 1L), c(3L, 3L)))\nt7 <- tf$gather_nd(t2, indices = new_indices)\n```\n:::\n\n\n![](images/tf_slicing/gather_nd_sparse.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add these values into a new tensor\nt8 <- tf$scatter_nd(\n  indices = new_indices, \n  updates = t7, \n  shape = as_tensor(c(4L, 5L))\n)\nt8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float64)\n```\n:::\n:::\n\n\nThis is similar to:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt9 <- tf$SparseTensor(\n  indices = list(c(0L, 2L), c(2L, 1L), c(3L, 3L)),\n  values = c(2, 11, 18),\n  dense_shape = c(4L, 5L)\n)\nt9\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSparseTensor(indices=tf.Tensor(\n[[0 2]\n [2 1]\n [3 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([ 2. 11. 18.], shape=(3), dtype=float32), dense_shape=tf.Tensor([4 5], shape=(2), dtype=int64))\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the sparse tensor into a dense tensor\nt10 <- tf$sparse$to_dense(t9)\nt10\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float32)\n```\n:::\n:::\n\n\nTo insert data into a tensor with pre-existing values, use\n`tf$tensor_scatter_nd_add`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt11 <- as_tensor(rbind(c(2, 7, 0),\n                       c(9, 0, 1),\n                       c(0, 3, 8)))\n\n# Convert the tensor into a magic square by inserting numbers at appropriate indices\nt12 <- tf$tensor_scatter_nd_add(\n  t11,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt12\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2. 7. 6.]\n [9. 5. 1.]\n [4. 3. 8.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\nSimilarly, use `tf$tensor_scatter_nd_sub` to subtract values from a\ntensor with pre-existing values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the tensor into an identity matrix\nt13 <- tf$tensor_scatter_nd_sub(\n  t11,\n  indices = list(c(0L, 0L), c(0L, 1L), c(1L, 0L), c(1L, 1L), c(1L, 2L), c(2L, 1L), c(2L, 2L)),\n  updates = c(1, 7, 9, -1, 1, 3, 7)\n)\n\nprint(t13)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\nUse `tf$tensor_scatter_nd_min` to copy element-wise minimum values from\none tensor to another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt14 <- as_tensor(rbind(c(-2, -7, 0),\n                       c(-9, 0, 1),\n                       c(0, -3, -8)))\n\nt15 <- tf$tensor_scatter_nd_min(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(-6, -5, -4)\n)\nt15\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-2. -7. -6.]\n [-9. -5.  1.]\n [-4. -3. -8.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\nSimilarly, use `tf$tensor_scatter_nd_max` to copy element-wise maximum\nvalues from one tensor to another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt16 <- tf$tensor_scatter_nd_max(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt16\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-2. -7.  6.]\n [-9.  5.  1.]\n [ 4. -3. -8.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\n## Further reading and resources\n\nIn this guide, you learned how to use the tensor slicing ops available\nwith TensorFlow to exert finer control over the elements in your\ntensors.\n\n-   Check out the slicing ops available with TensorFlow NumPy such as\n    `tf$experimental$numpy$take_along_axis` and\n    `tf$experimental$numpy$take`.\n\n-   Also check out the [Tensor\n    guide](https://www.tensorflow.org/guide/tensor) and the [Variable\n    guide](https://www.tensorflow.org/guide/variable).\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           here_1.0.1           lattice_0.20-45     \n [4] png_0.1-7            rprojroot_2.0.3      digest_0.6.29       \n [7] grid_4.2.1           jsonlite_1.8.0       magrittr_2.0.3      \n[10] evaluate_0.15        tfruns_1.5.0         rlang_1.0.4         \n[13] stringi_1.7.8        cli_3.3.0            whisker_0.4         \n[16] Matrix_1.4-1         reticulate_1.25-9000 rmarkdown_2.14      \n[19] tools_4.2.1          stringr_1.4.0        htmlwidgets_1.5.4   \n[22] xfun_0.31            yaml_2.3.5           fastmap_1.1.0       \n[25] compiler_4.2.1       base64enc_0.1-3      htmltools_0.5.2     \n[28] knitr_1.39          \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\nbeautifulsoup4==4.11.1\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nfilelock==3.7.1\nflatbuffers==1.12\ngast==0.4.0\ngdown==4.5.1\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\nPySocks==1.7.1\npython-dateutil==2.8.2\npytz==2022.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nsoupsieve==2.3.2.post1\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-14 \nPage render time: 3 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}