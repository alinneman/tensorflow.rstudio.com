{
  "hash": "3ad974fb555b49f5d2a772e6ef800fa9",
  "result": {
    "markdown": "---\ntitle: Tensorflow Basics\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\nThis guide provides a quick overview of *TensorFlow basics*. Each\nsection of this doc is an overview of a larger topic---you can find\nlinks to full guides at the end of each section.\n\nTensorFlow is an end-to-end platform for machine learning. It supports\nthe following:\n\n-   Multidimensional-array based numeric computation (similar to\n    [Numpy](https://numpy.org)\n\n-   GPU and distributed processing\n\n-   Automatic differentiation\n\n-   Model construction, training, and export\n\n-   And more\n\n## Tensors\n\nTensorFlow operates on multidimensional arrays or *tensors* represented\nas `tensorflow.tensor` objects. Here is a two-dimensional tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([2, 3])\n```\n:::\n\n```{.r .cell-code}\nx$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n:::\n\n\nThe most important attributes of a tensor are its `shape` and `dtype`:\n\n-   `tensor$shape`: tells you the size of the tensor along each of its\n    axes.\n-   `tensor$dtype`: tells you the type of all the elements in the\n    tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as\nwell as many operations specialized for machine learning.\n\nFor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx + x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 2.  4.  6.]\n [ 8. 10. 12.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n5 * x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 5. 10. 15.]\n [20. 25. 30.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$matmul(x, t(x)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[14. 32.]\n [32. 77.]], shape=(2, 2), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$concat(list(x, x, x), axis = 0L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]], shape=(6, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$nn$softmax(x, axis = -1L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0.09003057 0.24472848 0.66524094]\n [0.09003057 0.24472848 0.66524094]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(x) # same as tf$reduce_sum(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(21.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nRunning large calculations on CPU can be slow. When properly configured,\nTensorFlow can use accelerator hardware like GPUs to execute operations\nvery quickly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTensorFlow **IS NOT** using the GPU\n```\n:::\n:::\n\n\nRefer to the [Tensor guide](tensorflow/guide/tensor.qmd) for details.\n\n## Variables\n\nNormal tensor objects are immutable. To store model weights (or other\nmutable state) in TensorFlow use a `tf$Variable`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar <- tf$Variable(c(0, 0, 0))\nvar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign(c(1, 2, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign_add(c(1, 1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>\n```\n:::\n:::\n\n\nRefer to the [Variables guide](variable$ipynb) for details.\n\n## Automatic differentiation\n\n[*Gradient descent*](https://en.wikipedia.org/wiki/Gradient_descent) and\nrelated algorithms are a cornerstone of modern machine learning.\n\nTo enable this, TensorFlow implements automatic differentiation\n(autodiff), which uses calculus to compute gradients. Typically you'll\nuse this to calculate the gradient of a model's *error* or *loss* with\nrespect to its weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(-2.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nAt `x = 1.0`, `y = f(x) = (1^2 + 2*1 - 5) = -2`.\n\nThe derivative of `y` is `y' = f'(x) = (2*x + 2) = 4`. TensorFlow can\ncalculate this automatically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(4.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThis simplified example only takes the derivative with respect to a\nsingle scalar (`x`), but TensorFlow can compute the gradient with\nrespect to any number of non-scalar tensors simultaneously.\n\nRefer to the [Autodiff guide](tensorflow/guide/autodiff.qmd) for\ndetails.\n\n## Graphs and `tf_function`\n\nWhile you can use TensorFlow interactively like any R library,\nTensorFlow also provides tools for:\n\n-   **Performance optimization**: to speed up training and inference.\n-   **Export**: so you can save your model when it's done training.\n\nThese require that you use `tf_function()` to separate your\npure-TensorFlow code from R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n```\n:::\n\n\nThe first time you run the `tf_function`, although it executes in R, it\ncaptures a complete, optimized graph representing the TensorFlow\ncomputations done within the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(1:3)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nOn subsequent calls TensorFlow only executes the optimized graph,\nskipping any non-TensorFlow steps. Below, note that `my_func` doesn't\nprint `\"Tracing.\"` since `message` is an R function, not a TensorFlow\nfunction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(10:8)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(27, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nA graph may not be reusable for inputs with a different *signature*\n(`shape` and `dtype`), so a new graph is generated instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(27.3, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThese captured graphs provide two benefits:\n\n-   In many cases they provide a significant speedup in execution\n    (though not this trivial example).\n-   You can export these graphs, using `tf$saved_model`, to run on other\n    systems like a\n    [server](https://www.tensorflow.org/tfx/serving/docker) or a [mobile\n    device](https://www.tensorflow.org/lite/guide), no Python\n    installation required.\n\nRefer to [Intro to graphs](tensorflow/guide/intro_to_graphs.qmd) for\nmore details.\n\n## Modules, layers, and models\n\n`tf$Module` is a class for managing your `tf$Variable` objects, and the\n`tf_function` objects that operate on them. The `tf$Module` class is\nnecessary to support two significant features:\n\n1.  You can save and restore the values of your variables using\n    `tf$train$Checkpoint`. This is useful during training as it is quick\n    to save and restore a model's state.\n2.  You can import and export the `tf$Variable` values *and* the\n    `tf$function` graphs using `tf$saved_model`. This allows you to run\n    your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple `tf$Module` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nSave the `Module`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n```\n:::\n\n\nThe resulting SavedModel is independent of the code that created it. You\ncan load a SavedModel from R, Python, other language bindings, or\n[TensorFlow Serving](https://www.tensorflow.org/tfx/serving/docker). You\ncan also convert it to run with [TensorFlow\nLite](https://www.tensorflow.org/lite/guide) or [TensorFlow\nJS](https://www.tensorflow.org/js/guide).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nThe `tf$keras$layers$Layer` and `tf$keras$Model` classes build on\n`tf$Module` providing additional functionality and convenience methods\nfor building, training, and saving models. Some of these are\ndemonstrated in the next section.\n\nRefer to [Intro to modules](tensorflow/guide/intro_to_modules.qmd) for\ndetails.\n\n## Training loops\n\nNow put this all together to build a basic model and train it from\nscratch.\n\nFirst, create some example data. This generates a cloud of points that\nloosely follows a quadratic curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(seq(-2, 2, length.out = 201), \"float32\")\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\nCreate a model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\nWrite a basic training loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.168\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.143\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.126\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.112\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.090\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.081\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.074\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.068\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.063\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\nThat's working, but remember that implementations of common training\nutilities are available in the `tf$keras` module. So consider using\nthose before writing your own. To start with, the `compile` and `fit`\nmethods for Keras `Model`s implement a training loop for you:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history, metrics = 'loss', method = \"base\") \n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# see ?plot.keras_training_history for more options.\n```\n:::\n\n\nRefer to [Basic training loops](basic_training_loops.qmd) and the [Keras\nguide](https://www.tensorflow.org/guide/keras) for more details.\n\n---\nformat: html\n---\n\n## Environment Details \n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_version()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\" \n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "basics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}