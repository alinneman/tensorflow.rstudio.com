{
  "hash": "f7000aca21212d4e1d645274d96682eb",
  "result": {
    "markdown": "---\ntitle: Text Classification with TF Hub\ndescription: \"Use a pretrained model from TF Hub to classify text\"\naliases:\n  - ../beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/index.html\n---\n\n\nThis tutorial classifies movie reviews as *positive* or *negative* using the text of the review.\nThis is an example of *binary*---or two-class---classification, an important and widely applicable kind of machine learning problem.\n\nThe tutorial demonstrates the basic application of transfer learning with [TensorFlow Hub](https://tfhub.dev) and Keras.\n\nIt uses the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/).\nThese are split into 25,000 reviews for training and 25,000 reviews for testing.\nThe training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews.\n\nThis notebook uses [`keras`](/guides/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow hub](https://www.tensorflow.org/hub), a library for loading trained models from [TFHub](https://tfhub.dev) in a single line of code.\nFor a more advanced text classification tutorial using Keras, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfhub)\nlibrary(keras)\n```\n:::\n\n\n## Download the IMDB dataset\n\nThe IMDB dataset is available on [imdb reviews](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) or on [TensorFlow datasets](https://www.tensorflow.org/datasets).\nThe following code downloads the IMDB dataset to your machine:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (dir.exists(\"aclImdb/\"))\n  unlink(\"aclImdb/\", recursive = TRUE)\nurl <- \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\ndataset <- get_file(\n  \"aclImdb_v1\",\n  url,\n  untar = TRUE,\n  cache_dir = '.',\n  cache_subdir = ''\n)\nunlink(\"aclImdb/train/unsup/\", recursive = TRUE)\n```\n:::\n\n\nWe can then create a TensorFlow dataset from the directory structure using the `text_dataset_from_directory()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_size <- 512\nseed <- 42\n\ntrain_data <- text_dataset_from_directory(\n  'aclImdb/train',\n  batch_size = batch_size,\n  validation_split = 0.2,\n  subset = 'training',\n  seed = seed\n)\nvalidation_data <- text_dataset_from_directory(\n  'aclImdb/train',\n  batch_size = batch_size,\n  validation_split = 0.2,\n  subset = 'validation',\n  seed = seed\n)\ntest_data <- text_dataset_from_directory(\n  'aclImdb/test',\n  batch_size = batch_size\n)\n```\n:::\n\n\n## Explore the data\n\nLet's take a moment to understand the format of the data.\nEach example is a sentence representing the movie review and a corresponding label.\nThe sentence is not preprocessed in any way.\nThe label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n\nLet's print first 10 examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch <- train_data %>%\n  reticulate::as_iterator() %>%\n  reticulate::iter_next()\n\nbatch[[1]][1]\n```\n:::\n\n\nLet's also print the first 10 labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch[[2]][1:10]\n```\n:::\n\n\n## Build the model\n\nThe neural network is created by stacking layers---this requires three main architectural decisions:\n\n-   How to represent the text?\n-   How many layers to use in the model?\n-   How many *hidden units* to use for each layer?\n\nIn this example, the input data consists of sentences.\nThe labels to predict are either 0 or 1.\n\nOne way to represent the text is to convert sentences into embeddings vectors.\nUse a pre-trained text embedding as the first layer, which will have three advantages:\n\n-   You don't have to worry about text preprocessing,\n-   Benefit from transfer learning,\n-   the embedding has a fixed size, so it's simpler to process.\n\nFor this example you use a **pre-trained text embedding model** from [TensorFlow Hub](https://tfhub.dev) called [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).\n\nThere are many other pre-trained text embeddings from TFHub that can be used in this tutorial:\n\n-   [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2) - trained with the same NNLM architecture on the same data as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with a larger embedding dimension. Larger dimensional embeddings can improve on your task but it may take longer to train your model.\n-   [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - the same as [google/nnlm-en-dim128/2](https://tfhub.dev/google/nnlm-en-dim128/2), but with additional text normalization such as removing punctuation. This can help if the text in your task contains additional characters or punctuation.\n-   [google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4) - a much larger model yielding 512 dimensional embeddings trained with a deep averaging network (DAN) encoder.\n\nAnd many more!\nFind more [text embedding models](https://tfhub.dev/s?module-type%20=%20text-embedding) on TFHub.\n\nLet's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples.\nNote that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nembedding <- \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer <- tfhub::layer_hub(handle = embedding, trainable = TRUE)\nhub_layer(batch[[1]][1:2])\n```\n:::\n\n\nLet's now build the full model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  hub_layer() %>%\n  layer_dense(16, activation = 'relu') %>%\n  layer_dense(1)\n\nsummary(model)\n```\n:::\n\n\nThe layers are stacked sequentially to build the classifier:\n\n1.  The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that you are using ([google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`. For this NNLM model, the `embedding_dimension` is 50.\n2.  This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n3.  The last layer is densely connected with a single output node.\n\nLet's compile the model.\n\n### Loss function and optimizer\n\nA model needs a loss function and an optimizer for training.\nSince this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), you'll use the `binary_crossentropy` loss function.\n\nThis isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`.\nBut, generally, `binary_crossentropy` is better for dealing with probabilities---it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n\nLater, when you are exploring regression problems (say, to predict the price of a house), you'll see how to use another loss function called mean squared error.\n\nNow, configure the model to use an optimizer and a loss function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = 'accuracy'\n)\n```\n:::\n\n\n## Train the model\n\nTrain the model for 10 epochs in mini-batches of 512 samples.\nThis is 10 iterations over all samples in the `x_train` and `y_train` tensors.\nWhile training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(\n  train_data,\n  epochs = 10,\n  validation_data = validation_data,\n  verbose <- 1\n)\n```\n:::\n\n\n## Evaluate the model\n\nAnd let's see how the model performs.\nTwo values will be returned.\nLoss (a number which represents our error, lower values are better), and accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- model %>% evaluate(test_data, verbose = 2)\nresults\n```\n:::\n\n\nThis fairly naive approach achieves an accuracy of about 87%.\nWith more advanced approaches, the model should get closer to 95%.\n\n## Further reading\n\n-   For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, see the [Text classification with preprocessed text](./text_classification.qmd) tutorial.\n-   Try out more [text-related tutorials](https://www.tensorflow.org/hub/tutorials#text-related-tutorials) using trained models from TFHub.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}