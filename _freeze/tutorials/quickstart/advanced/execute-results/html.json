{
  "hash": "a4817fe2e3767de41b41e6b60240e253",
  "result": {
    "markdown": "---\ntitle: Advanced\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Copyright 2019 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# TensorFlow 2 quickstart for experts\n\nImport TensorFlow into your program. If you haven't installed TensorFlow yet, go\nto the [installation guide](/install).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n```\n:::\n\n\nLoad and prepare the [MNIST dataset](http://yann$lecun.com/exdb/mnist/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nx_train[] <- x_train / 255\nx_test[] <-  x_test / 255 \n```\n:::\n\n\nUse TensorFlow datasets to batch and shuffle the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpand_and_cast <- function(x, y) {\n  list(\n    tf$cast(tf$expand_dims(x, axis = 2L), tf$float32),\n    y\n  )\n}\n\n\ntrain_ds <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(10000) %>% \n  dataset_batch(32)\n\ntest_ds <- tensor_slices_dataset(list(x_test, y_test)) %>% \n  dataset_batch(32)\n```\n:::\n\n\nBuild the a model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras#model_subclassing):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model <- new_model_class(\n  \"my_model\",\n  initialize = function() {\n    super()$`__init__`()\n    self$conv1 <- layer_conv_2d(filters = 32, kernel_size = 3, activation = 'relu')\n    self$flatten <- layer_flatten()\n    self$d1 <- layer_dense(units = 128, activation = 'relu')\n    self$d2 <- layer_dense(units = 10)\n  },\n  call = function(inputs) {\n    inputs %>% \n      tf$expand_dims(3L) %>% \n      self$conv1() %>% \n      self$flatten() %>% \n      self$d1() %>% \n      self$d2()\n  }\n)\n\n# Create an instance of the model\nmodel <- my_model()\n```\n:::\n\n\nChoose an optimizer and loss function for training: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_object <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n```\n:::\n\n\nSelect metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_loss <- metric_mean(name = \"train_loss\")\ntrain_accuracy <- metric_sparse_categorical_accuracy(name = \"train_accuracy\")\n\ntest_loss <- metric_mean(name = \"test_loss\")\ntest_accuracy <- metric_sparse_categorical_accuracy(name = \"test_accuracy\")\n```\n:::\n\n\nUse `tf$GradientTape` to train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_step <- function(images, labels) {\n  with(tf$GradientTape() %as% tape, {\n    # training = TRUE is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions <- model(images, training = TRUE)\n    loss <- loss_object(labels, predictions)\n  })\n  gradients <- tape$gradient(loss, model$trainable_variables)\n  optimizer$apply_gradients(zip_lists(gradients, model$trainable_variables))\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n  1\n}\n\ntrain <- tfautograph::autograph(function(train_ds) {\n  for (batch in train_ds) {\n    train_step(batch[[1]], batch[[2]])\n  }\n})\n```\n:::\n\n\nTest the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_step <- function(images, labels) {\n  # training = FALSE is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions <- model(images, training = FALSE)\n  t_loss <- loss_object(labels, predictions)\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n}\ntest <- tfautograph::autograph(function(test_ds) {\n  for (batch in test_ds) {\n    test_step(batch[[1]], batch[[2]])\n  }\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEPOCHS <- 1\nfor (epoch in seq_len(EPOCHS)) {\n  # Reset the metrics at the start of the next epoch\n  train_loss$reset_states()\n  train_accuracy$reset_states()\n  test_loss$reset_states()\n  test_accuracy$reset_states()\n  train(train_ds)\n  test(test_ds)\n  cat(sprintf('Epoch %d', epoch), \"\\n\")\n  cat(sprintf('Loss: %f', as.numeric(train_loss$result())), \"\\n\")\n  cat(sprintf('Accuracy: %f', train_accuracy$result() * 100), \"\\n\")\n  cat(sprintf('Test Loss: %f', test_loss$result()), \"\\n\")\n  cat(sprintf('Test Accuracy: %f', test_accuracy$result() * 100), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1 \nLoss: 0.140223 \nAccuracy: 95.784996 \nTest Loss: 0.069381 \nTest Accuracy: 97.879997 \n```\n:::\n:::\n\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](/tutorials).\n",
    "supporting": [
      "advanced_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}