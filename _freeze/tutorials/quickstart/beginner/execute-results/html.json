{
  "hash": "f26eff9ba5f973a6ae3a02bea540aa49",
  "result": {
    "markdown": "---\ntitle: Beginner\ndescription: \"This 'Hello, World!' example shows the Keras Sequential API and `fit`.\"\nid: 1\n---\n\n\n# TensorFlow 2 quickstart for beginners\n\nThis short introduction uses [Keras](https://www.tensorflow.org/guide/keras/overview) to:\n\n1. Load a prebuilt dataset.\n1. Build a neural network machine learning model that classifies images.\n2. Train this neural network.\n3. Evaluate the accuracy of the model.\n\n## Set up TensorFlow\n\nImport TensorFlow into your program to get started:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in reticulate::use_virtualenv(\"r-tensorflow-site\", required = TRUE) : \n  Directory ~/.virtualenvs/r-tensorflow-site is not a Python virtualenv\n```\n:::\n\n```{.r .cell-code}\nlibrary(keras)\n```\n:::\n\n\nSee the [installation guide](./install/) to learn how to correctly install TensorFlow for R.\n\n## Load a dataset\n\nLoad and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Convert the sample data from integers to floating-point numbers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nx_train[] <- x_train / 255\nx_test[] <-  x_test / 255 \n```\n:::\n\n\n## Build a machine learning model\n\nBuild a sequential model by stacking layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(28, 28)) %>% \n  layer_flatten() %>% \n  layer_dense(128, activation = \"relu\") %>% \n  layer_dropout(0.2) %>% \n  layer_dense(10)\n```\n:::\n\n\nFor each example, the model returns a vector of [logits](https://developers$google.com/machine-learning/glossary#logits) or [log-odds](https://developers$google.com/machine-learning/glossary#log-odds) scores, one for each class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(model, x_train[1:2,,])\npredictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]      [,2]      [,3]       [,4]      [,5]        [,6]\n[1,] -0.45212924 -1.000679 1.0063702 -0.2813853 0.3640309 -0.03176719\n[2,] -0.03669837 -1.229428 0.5544991 -0.1326344 0.9059005 -0.55074894\n            [,7]        [,8]      [,9]      [,10]\n[1,] -0.01737098 0.002265602 0.4170726 -1.0100819\n[2,]  0.46314085 0.023755193 0.2630941 -0.4941591\n```\n:::\n:::\n\n\nThe `tf$nn$softmax` function converts these logits to *probabilities* for each class: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$nn$softmax(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0.05908425 0.0341381  0.25403292 0.07008497 0.13363666 0.08995652\n  0.09126092 0.09307069 0.14091633 0.03381863]\n [0.08420331 0.02554661 0.15208381 0.07650058 0.21611985 0.05035932\n  0.13880548 0.08945072 0.113639   0.05329132]], shape=(2, 10), dtype=float64)\n```\n:::\n:::\n\n\nNote: It is possible to bake the `tf$nn$softmax` function into the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output. \n\nDefine a loss function for training using `loss_sparse_categorical_crossentropy()`, \nwhich takes a vector of logits and a `TRUE` index and returns a scalar loss for \neach example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n```\n:::\n\n\nThis loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\nThis untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf$math$log(1/10) ~= 2.3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_fn(y_train[1:2], predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(2.44147489276956, shape=(), dtype=float64)\n```\n:::\n:::\n\n\nBefore you start training, configure and compile the model using Keras `compile()`. \nSet the [`optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) class \nto `adam`, set the `loss` to the `loss_fn` function you defined earlier, and specify \na metric to be evaluated for the model by setting the `metrics` parameter to `accuracy`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)\n```\n:::\n\n\n## Train and evaluate your model\n\nUse the `fit()` method to adjust your model parameters and minimize the loss: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% fit(x_train, y_train, epochs = 5)\n```\n:::\n\n\nThe `evaluate()` method checks the models performance, usually on a [Validation-set](https://developers$google.com/machine-learning/glossary#validation-set) or [Test-set](https://developers$google.com/machine-learning/glossary#test-set).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(x_test,  y_test, verbose = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      loss   accuracy \n0.07559286 0.97729999 \n```\n:::\n:::\n\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\nIf you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobability_model <- keras_model_sequential() %>% \n  model() %>% \n  layer_activation_softmax()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobability_model(x_test[1:5,,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[5.05482731e-08 8.55492899e-10 7.38288236e-06 1.35267692e-04\n  8.05116390e-12 9.75205694e-09 1.18561099e-13 9.99837637e-01\n  1.85414120e-08 1.96653928e-05]\n [2.48872567e-08 5.27670607e-04 9.99467313e-01 4.85750343e-06\n  2.65540202e-19 1.64506261e-07 3.60757682e-08 1.13698737e-13\n  3.73362452e-09 6.57828216e-14]\n [1.00679961e-06 9.97797489e-01 4.69215098e-04 2.04522312e-05\n  5.64250649e-06 8.54686732e-06 2.22352446e-05 1.26194709e-03\n  4.13237256e-04 1.98122635e-07]\n [9.99778807e-01 8.29056990e-10 6.43533203e-05 8.32866021e-07\n  8.36042034e-07 4.16911098e-06 1.18370135e-05 7.52744018e-05\n  2.53316585e-08 6.39040591e-05]\n [2.04403341e-06 2.65768829e-09 1.00898387e-06 1.76633197e-08\n  9.97162521e-01 5.69798431e-08 3.26826154e-07 1.59583331e-04\n  4.31128592e-06 2.67019332e-03]], shape=(5, 10), dtype=float32)\n```\n:::\n:::\n\n\n## Conclusion\n\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the [Keras](https://www.tensorflow.org/guide/keras/overview) API.\n\nFor more examples of using Keras, check out the [tutorials](https://www.tensorflow.org/tutorials/keras/). To learn more about building models with Keras, read the [guides](https://www.tensorflow.org/guide/keras). If you want learn more about loading and preparing data, see the tutorials on [image data loading](https://www.tensorflow.org/tutorials/load_data/images) or [CSV data loading](https://www.tensorflow.org/tutorials/load_data/csv).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}