{
  "hash": "1a343dafa8e9c9909bd0f98ee5c828d5",
  "result": {
    "markdown": "##### Copyright 2018 The TensorFlow Authors.\n\n\n```{r}\n#| cellView: form\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n\n# you may not use this file except in compliance with the License.\n\n# You may obtain a copy of the License at\n\n#\n# https://www$apache$org/licenses/LICENSE-2.0\n\n#\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n\n# limitations under the License.\n\n```\n\n\n# Ragged tensors\n\n|                                                                                                                                  |                                                                                                                                                                                         |                                                                                                                                                                      |                                                                                                                                                                         |\n|----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [![](https://www$tensorflow$org/images/tf_logo_32px$png)View on TensorFlow\\$org](https://www$tensorflow$org/guide/ragged_tensor) | [![](https://www$tensorflow$org/images/colab_logo_32px$png)Run in Google Colab](https://colab$research$google$com/github/tensorflow/docs/blob/master/site/en/guide/ragged_tensor$ipynb) | [![](https://www$tensorflow$org/images/GitHub-Mark-32px$png)View source on GitHub](https://github$com/tensorflow/docs/blob/master/site/en/guide/ragged_tensor$ipynb) | [![](https://www$tensorflow$org/images/download_logo_32px$png)Download notebook](https://storage$googleapis$com/tensorflow_docs/docs/site/en/guide/ragged_tensor$ipynb) |\n\n\\^API Documentation:\\^\n[`tf$RaggedTensor`](https://www$tensorflow$org/api_docs/python/tf/RaggedTensor)\n[`tf$ragged`](https://www$tensorflow$org/api_docs/python/tf/ragged)\n\n## Setup\n\n\n```{r}\nlibrary(tensorflow)\n```\n\n\n## Overview\n\nYour data comes in many shapes; your tensors should too. *Ragged\ntensors* are the TensorFlow equivalent of nested variable-length lists.\nThey make it easy to store and process data with non-uniform shapes,\nincluding:\n\n-   Variable-length features, such as the set of actors in a movie.\n-   Batches of variable-length sequential inputs, such as sentences or\n    video clips.\n-   Hierarchical inputs, such as text documents that are subdivided into\n    sections, paragraphs, sentences, and words.\n-   Individual fields in structured inputs, such as protocol buffers.\n\n### What you can do with a ragged tensor\n\nRagged tensors are supported by more than a hundred TensorFlow\noperations, including math operations (such as `tf$add` and\n`tf$reduce_mean`), array operations (such as `tf$concat` and `tf$tile`),\nstring manipulation ops (such as `tf$strings$substr`), control flow\noperations (such as `tf$while_loop` and `tf$map_fn`), and many others:\n\n\n```{r}\ndigits <- tf$ragged$constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nwords <- tf$ragged$constant([[\"So\", \"long\"], [\"thanks\", \"for\", \"all\", \"the\", \"fish\"]])\nprint(tf$add(digits, 3))\nprint(tf$reduce_mean(digits, axis=1))\nprint(tf$concat([digits, [[5, 3]]], axis=0))\nprint(tf$tile(digits, [1, 2]))\nprint(tf$strings$substr(words, 0, 2))\nprint(tf$map_fn(tf$math$square, digits))\n```\n\n\nThere are also a number of methods and operations that are specific to\nragged tensors, including factory methods, conversion methods, and\nvalue-mapping operations. For a list of supported ops, see the\n\\^`tf$ragged` package documentation\\^.\n\nRagged tensors are supported by many TensorFlow APIs, including\n[Keras](https://www$tensorflow$org/guide/keras),\n[Datasets](https://www$tensorflow$org/guide/data),\n[tf_function](https://www$tensorflow$org/guide/function),\n[SavedModels](https://www$tensorflow$org/guide/saved_model), and\n\\[tf$Example](https://www$tensorflow\\$org/tutorials/load_data/tfrecord).\nFor more information, check the section on \\^TensorFlow APIs\\^ below.\n\nAs with normal tensors, you can use Python-style indexing to access\nspecific slices of a ragged tensor. For more information, refer to the\nsection on ^Indexing^ below.\n\n\n```{r}\nprint(digits[0])       # First row\n```\n\n```{r}\nprint(digits[:, :2])   # First two values in each row.\n```\n\n```{r}\nprint(digits[:, -2:])  # Last two values in each row.\n```\n\n\nAnd just like normal tensors, you can use Python arithmetic and\ncomparison operators to perform elementwise operations. For more\ninformation, check the section on \\^Overloaded operators\\^ below.\n\n\n```{r}\nprint(digits + 3)\n```\n\n```{r}\nprint(digits + tf$ragged$constant([[1, 2, 3, 4], [], [5, 6, 7], [8], []]))\n```\n\n\nIf you need to perform an elementwise transformation to the values of a\n`RaggedTensor`, you can use `tf$ragged$map_flat_values`, which takes a\nfunction plus one or more arguments, and applies the function to\ntransform the `RaggedTensor`'s values.\n\n\n```{r}\ntimes_two_plus_one <- lambda x: x * 2 + 1\nprint(tf$ragged$map_flat_values(times_two_plus_one, digits))\n```\n\n\nRagged tensors can be converted to nested Python `list`s and NumPy\n`array`s:\n\n\n```{r}\ndigits$to_list()\n```\n\n```{r}\ndigits$numpy()\n```\n\n\n### Constructing a ragged tensor\n\nThe simplest way to construct a ragged tensor is using\n`tf$ragged$constant`, which builds the `RaggedTensor` corresponding to a\ngiven nested Python `list` or NumPy `array`:\n\n\n```{r}\nsentences <- tf$ragged$constant([\n    [\"Let's\", \"build\", \"some\", \"ragged\", \"tensors\", \"!\"],\n    [\"We\", \"can\", \"use\", \"tf$ragged$constant\", \".\"]])\nprint(sentences)\n```\n\n```{r}\nparagraphs <- tf$ragged$constant([\n    [['I', 'have', 'a', 'cat'], ['His', 'name', 'is', 'Mat']],\n    [['Do', 'you', 'want', 'to', 'come', 'visit'], [\"I'm\", 'free', 'tomorrow']],\n])\nprint(paragraphs)\n```\n\n\nRagged tensors can also be constructed by pairing flat *values* tensors\nwith *row-partitioning* tensors indicating how those values should be\ndivided into rows, using factory classmethods such as\n`tf$RaggedTensor$from_value_rowids`, `tf$RaggedTensor$from_row_lengths`,\nand `tf$RaggedTensor$from_row_splits`.\n\n#### `tf$RaggedTensor$from_value_rowids`\n\nIf you know which row each value belongs to, then you can build a\n`RaggedTensor` using a `value_rowids` row-partitioning tensor:\n\n![value_rowids row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/value_rowids$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_value_rowids(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    value_rowids=[0, 0, 0, 0, 2, 2, 3]))\n```\n\n\n#### `tf$RaggedTensor$from_row_lengths`\n\nIf you know how long each row is, then you can use a `row_lengths`\nrow-partitioning tensor:\n\n![row_lengths row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/row_lengths$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_row_lengths(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_lengths=[4, 0, 2, 1]))\n```\n\n\n#### `tf$RaggedTensor$from_row_splits`\n\nIf you know the index where each row starts and ends, then you can use a\n`row_splits` row-partitioning tensor:\n\n![row_splits row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/row_splits$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_splits=[0, 4, 4, 6, 7]))\n```\n\n\nSee the `tf$RaggedTensor` class documentation for a full list of factory\nmethods.\n\nNote: By default, these factory methods add assertions that the row\npartition tensor is well-formed and consistent with the number of\nvalues. The `validate=FALSE` parameter can be used to skip these checks\nif you can guarantee that the inputs are well-formed and consistent.\n\n### What you can store in a ragged tensor\n\nAs with normal `Tensor`s, the values in a `RaggedTensor` must all have\nthe same type; and the values must all be at the same nesting depth (the\n*rank* of the tensor):\n\n\n```{r}\nprint(tf$ragged$constant([[\"Hi\"], [\"How\", \"are\", \"you\"]]))  # ok: type=string, rank=2\n```\n\n```{r}\nprint(tf$ragged$constant([[[1, 2], [3]], [[4, 5]]]))        # ok: type=int32, rank=3\n```\n\n```{r}\ntry:\n  tf$ragged$constant([[\"one\", \"two\"], [3, 4]])              # bad: multiple types\nexcept ValueError as exception:\n  print(exception)\n```\n\n```{r}\ntry:\n  tf$ragged$constant([\"A\", [\"B\", \"C\"]])                     # bad: multiple nesting depths\nexcept ValueError as exception:\n  print(exception)\n```\n\n\n## Example use case\n\nThe following example demonstrates how `RaggedTensor`s can be used to\nconstruct and combine unigram and bigram embeddings for a batch of\nvariable-length queries, using special markers for the beginning and end\nof each sentence. For more details on the ops used in this example,\ncheck the `tf$ragged` package documentation.\n\n\n```{r}\nqueries <- tf$ragged$constant([['Who', 'is', 'Dan', 'Smith'],\n                              ['Pause'],\n                              ['Will', 'it', 'rain', 'later', 'today']])\n\n# Create an embedding table.\n\nnum_buckets <- 1024\nembedding_size <- 4\nembedding_table <- tf$Variable(\n    tf$random$truncated_normal([num_buckets, embedding_size],\n                       stddev=1.0 / math$sqrt(embedding_size)))\n\n# Look up the embedding for each word.\n\nword_buckets <- tf$strings$to_hash_bucket_fast(queries, num_buckets)\nword_embeddings <- tf$nn$embedding_lookup(embedding_table, word_buckets)     # ①\n\n# Add markers to the beginning and end of each sentence.\n\nmarker <- tf$fill([queries$nrows(), 1], '#')\npadded <- tf$concat([marker, queries, marker], axis=1)                       # ②\n\n# Build word bigrams and look up embeddings.\n\nbigrams <- tf$strings$join([padded[:, :-1], padded[:, 1:]], separator='+')   # ③\n\nbigram_buckets <- tf$strings$to_hash_bucket_fast(bigrams, num_buckets)\nbigram_embeddings <- tf$nn$embedding_lookup(embedding_table, bigram_buckets) # ④\n\n# Find the average embedding for each sentence\n\nall_embeddings <- tf$concat([word_embeddings, bigram_embeddings], axis=1)    # ⑤\navg_embedding <- tf$reduce_mean(all_embeddings, axis=1)                      # ⑥\nprint(avg_embedding)\n```\n\n\n![Ragged tensor\nexample](https://www$tensorflow$org/images/ragged_tensors/ragged_example$png)\n\n## Ragged and uniform dimensions\n\nA \\^*ragged dimension\\^* is a dimension whose slices may have different\nlengths. For example, the inner (column) dimension of\n`rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]` is ragged, since the column\nslices (`rt[0, :]`, ..., `rt[4, :]`) have different lengths. Dimensions\nwhose slices all have the same length are called *uniform dimensions*.\n\nThe outermost dimension of a ragged tensor is always uniform, since it\nconsists of a single slice (and, therefore, there is no possibility for\ndiffering slice lengths). The remaining dimensions may be either ragged\nor uniform. For example, you may store the word embeddings for each word\nin a batch of sentences using a ragged tensor with shape\n`[num_sentences, (num_words), embedding_size]`, where the parentheses\naround `(num_words)` indicate that the dimension is ragged.\n\n![Word embeddings using a ragged\ntensor](https://www$tensorflow$org/images/ragged_tensors/sent_word_embed$png)\n\nRagged tensors may have multiple ragged dimensions. For example, you\ncould store a batch of structured text documents using a tensor with\nshape `[num_documents, (num_paragraphs), (num_sentences), (num_words)]`\n(where again parentheses are used to indicate ragged dimensions).\n\nAs with `tf$Tensor`, the \\^*rank\\^* of a ragged tensor is its total\nnumber of dimensions (including both ragged and uniform dimensions). A\n\\^*potentially ragged tensor\\^* is a value that might be either a\n`tf$Tensor` or a `tf$RaggedTensor`.\n\nWhen describing the shape of a RaggedTensor, ragged dimensions are\nconventionally indicated by enclosing them in parentheses. For example,\nas you saw above, the shape of a 3D RaggedTensor that stores word\nembeddings for each word in a batch of sentences can be written as\n`[num_sentences, (num_words), embedding_size]`.\n\nThe `RaggedTensor$shape` attribute returns a `tf$TensorShape` for a\nragged tensor where ragged dimensions have size `NULL`:\n\n\n```{r}\ntf$ragged$constant([[\"Hi\"], [\"How\", \"are\", \"you\"]]).shape\n```\n\n\nThe method `tf$RaggedTensor$bounding_shape` can be used to find a tight\nbounding shape for a given `RaggedTensor`:\n\n\n```{r}\nprint(tf$ragged$constant([[\"Hi\"], [\"How\", \"are\", \"you\"]]).bounding_shape())\n```\n\n\n## Ragged vs sparse\n\nA ragged tensor should *not* be thought of as a type of sparse tensor.\nIn particular, sparse tensors are *efficient encodings for `tf$Tensor`*\nthat model the same data in a compact format; but ragged tensor is an\n*extension to `tf$Tensor`* that models an expanded class of data. This\ndifference is crucial when defining operations:\n\n-   Applying an op to a sparse or dense tensor should always give the\n    same result.\n-   Applying an op to a ragged or sparse tensor may give different\n    results.\n\nAs an illustrative example, consider how array operations such as\n`concat`, `stack`, and `tile` are defined for ragged vs. sparse tensors.\nConcatenating ragged tensors joins each row to form a single row with\nthe combined length:\n\n![Concatenating ragged\ntensors](https://www$tensorflow$org/images/ragged_tensors/ragged_concat$png)\n\n\n```{r}\nragged_x <- tf$ragged$constant([[\"John\"], [\"a\", \"big\", \"dog\"], [\"my\", \"cat\"]])\nragged_y <- tf$ragged$constant([[\"fell\", \"asleep\"], [\"barked\"], [\"is\", \"fuzzy\"]])\nprint(tf$concat([ragged_x, ragged_y], axis=1))\n```\n\n\nHowever, concatenating sparse tensors is equivalent to concatenating the\ncorresponding dense tensors, as illustrated by the following example\n(where Ø indicates missing values):\n\n![Concatenating sparse\ntensors](https://www$tensorflow$org/images/ragged_tensors/sparse_concat$png)\n\n\n```{r}\nsparse_x <- ragged_x$to_sparse()\nsparse_y <- ragged_y$to_sparse()\nsparse_result <- tf$sparse$concat(sp_inputs=[sparse_x, sparse_y], axis=1)\nprint(tf$sparse$to_dense(sparse_result, ''))\n```\n\n\nFor another example of why this distinction is important, consider the\ndefinition of \"the mean value of each row\" for an op such as\n`tf$reduce_mean`. For a ragged tensor, the mean value for a row is the\nsum of the row's values divided by the row's width. But for a sparse\ntensor, the mean value for a row is the sum of the row's values divided\nby the sparse tensor's overall width (which is greater than or equal to\nthe width of the longest row).\n\n## TensorFlow APIs\n\n### Keras\n\n\\[tf$keras](https://www$tensorflow$org/guide/keras) is TensorFlow's high-level API for building and training deep learning models. Ragged tensors may be passed as inputs to a Keras model by setting `ragged=TRUE` on `tf$keras$Input` or `tf$keras$layers$InputLayer\\`.\nRagged tensors may also be passed between Keras layers, and returned by\nKeras models. The following example shows a toy LSTM model that is\ntrained using ragged tensors.\n\n\n```{r}\n# Task: predict whether each sentence is a question or not.\n\nsentences <- as_tensor(\n    ['What makes you think she is a witch?',\n     'She turned me into a newt.',\n     'A newt?',\n     'Well, I got better.'])\nis_question <- as_tensor([TRUE, FALSE, TRUE, FALSE])\n\n# Preprocess the input strings.\n\nhash_buckets <- 1000\nwords <- tf$strings$split(sentences, ' ')\nhashed_words <- tf$strings$to_hash_bucket_fast(words, hash_buckets)\n\n# Build the Keras model.\n\nkeras_model <- tf$keras_model_sequential([\n    tf$keras$layers$Input(shape=[NULL], dtype=tf$int64, ragged=TRUE),\n    tf$keras$layers$Embedding(hash_buckets, 16),\n    tf$keras$layers$LSTM(32, use_bias=FALSE),\n    tf$layer_dense(32),\n    tf$keras$layers$Activation(tf$nn$relu),\n    tf$layer_dense(1)\n])\n\nkeras_model %>% compile(loss='binary_crossentropy', optimizer='rmsprop')\nkeras_model %>% fit(hashed_words, is_question, epochs=5)\nprint(keras_model$predict(hashed_words))\n```\n\n\n### tf\\$Example\n\n\\[tf$Example](https://www$tensorflow$org/tutorials/load_data/tfrecord) is a standard [protobuf](https://developers$google$com/protocol-buffers/) encoding for TensorFlow data. Data encoded with `tf$Example`s often includes variable-length features.  For example, the following code defines a batch of four`tf\\$Example\\`\nmessages with different feature lengths:\n\n\n```{r}\nimport google$protobuf$text_format as pbtext\n\nbuild_tf_example <- function(s) {    }\n  return pbtext$Merge(s, tf$train$Example()).SerializeToString()\n\nexample_batch <- [\n  build_tf_example(r'''\n    features list(\n      feature {key: \"colors\" value {bytes_list {value: [\"red\", \"blue\"]) } }\n      feature list(key: \"lengths\" value {int64_list {value: [7]) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \"colors\" value {bytes_list {value: [\"orange\"]) } }\n      feature list(key: \"lengths\" value {int64_list {value: []) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \"colors\" value {bytes_list {value: [\"black\", \"yellow\"]) } }\n      feature list(key: \"lengths\" value {int64_list {value: [1, 3]) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \"colors\" value {bytes_list {value: [\"green\"]) } }\n      feature list(key: \"lengths\" value {int64_list {value: [3, 5, 2]) } } }''')]\n```\n\n\nYou can parse this encoded data using `tf$io$parse_example`, which takes\na tensor of serialized strings and a feature specification dictionary,\nand returns a dictionary mapping feature names to tensors. To read the\nvariable-length features into ragged tensors, you simply use\n`tf$io$RaggedFeature` in the feature specification dictionary:\n\n\n```{r}\nfeature_specification <- list(\n    'colors': tf$io$RaggedFeature(tf$string),\n    'lengths': tf$io$RaggedFeature(tf$int64),\n)\nfeature_tensors <- tf$io$parse_example(example_batch, feature_specification)\nfor name, value in feature_tensors$items():\n  print(\"list()=list()\".format(name, value))\n```\n\n\n`tf$io$RaggedFeature` can also be used to read features with multiple\nragged dimensions. For details, refer to the [API\ndocumentation](https://www$tensorflow$org/api_docs/python/tf/io/RaggedFeature).\n\n### Datasets\n\n\\[tf$data](https://www$tensorflow$org/guide/data) is an API that enables you to build complex input pipelines from simple, reusable pieces. Its core data structure is `tf$data\\$Dataset\\`,\nwhich represents a sequence of elements, in which each element consists\nof one or more components.\n\n\n```{r}\n# Helper function used to print datasets in the examples below.\n\nprint_dictionary_dataset <- function(dataset) {    }\n  for i, element in enumerate(dataset):\n    print(\"Element list():\".format(i))\n    for (feature_name, feature_value) in element$items():\n      print('list(:>14) = list()'.format(feature_name, feature_value))\n```\n\n\n#### Building Datasets with ragged tensors\n\nDatasets can be built from ragged tensors using the same methods that\nare used to build them from `tf$Tensor`s or NumPy `array`s, such as\n`Dataset$from_tensor_slices`:\n\n\n```{r}\ndataset <- tf$data$Dataset$from_tensor_slices(feature_tensors)\nprint_dictionary_dataset(dataset)\n```\n\n\nNote: `Dataset$from_generator` does not support ragged tensors yet, but\nsupport will be added soon.\n\n#### Batching and unbatching Datasets with ragged tensors\n\nDatasets with ragged tensors can be batched (which combines *n*\nconsecutive elements into a single elements) using the `Dataset$batch`\nmethod.\n\n\n```{r}\nbatched_dataset <- dataset$batch(2)\nprint_dictionary_dataset(batched_dataset)\n```\n\n\nConversely, a batched dataset can be transformed into a flat dataset\nusing `Dataset$unbatch`.\n\n\n```{r}\nunbatched_dataset <- batched_dataset$unbatch()\nprint_dictionary_dataset(unbatched_dataset)\n```\n\n\n#### Batching Datasets with variable-length non-ragged tensors\n\nIf you have a Dataset that contains non-ragged tensors, and tensor\nlengths vary across elements, then you can batch those non-ragged\ntensors into ragged tensors by applying the `dense_to_ragged_batch`\ntransformation:\n\n\n```{r}\nnon_ragged_dataset <- tf$data$Dataset$from_tensor_slices([1, 5, 3, 2, 8])\nnon_ragged_dataset <- non_ragged_dataset$map(tf$range)\nbatched_non_ragged_dataset <- non_ragged_dataset$apply(\n    tf$data$experimental$dense_to_ragged_batch(2))\nfor element in batched_non_ragged_dataset:\n  print(element)\n```\n\n\n#### Transforming Datasets with ragged tensors\n\nYou can also create or transform ragged tensors in Datasets using\n`Dataset$map`:\n\n\n```{r}\ntransform_lengths <- function(features) {    }\n  return list(\n      'mean_length': tf$math$reduce_mean(features['lengths']),\n      'length_ranges': tf$ragged$range(features['lengths']))\ntransformed_dataset <- dataset$map(transform_lengths)\nprint_dictionary_dataset(transformed_dataset)\n```\n\n\n### tf_function\n\n[tf_function](https://www$tensorflow$org/guide/function) is a decorator\nthat precomputes TensorFlow graphs for Python functions, which can\nsubstantially improve the performance of your TensorFlow code. Ragged\ntensors can be used transparently with `@tf_function`-decorated\nfunctions. For example, the following function works with both ragged\nand non-ragged tensors:\n\n\n```{r}\n@tf_function\nmake_palindrome <- function(x, axis) {    }\n  return tf$concat([x, tf$reverse(x, [axis])], axis)\n```\n\n```{r}\nmake_palindrome(as_tensor([[1, 2], [3, 4], [5, 6]]), axis=1)\n```\n\n```{r}\nmake_palindrome(tf$ragged$constant([[1, 2], [3], [4, 5, 6]]), axis=1)\n```\n\n\nIf you wish to explicitly specify the `input_signature` for the\n`tf_function`, then you can do so using `tf$RaggedTensorSpec`.\n\n\n```{r}\n@tf_function(\n    input_signature=[tf$RaggedTensorSpec(shape=[NULL, NULL], dtype=tf$int32)])\nmax_and_min <- function(rt) {    }\n  return (tf$math$reduce_max(rt, axis=-1), tf$math$reduce_min(rt, axis=-1))\n\nmax_and_min(tf$ragged$constant([[1, 2], [3], [4, 5, 6]]))\n```\n\n\n#### Concrete functions\n\n[Concrete\nfunctions](https://www$tensorflow$org/guide/function#obtaining_concrete_functions)\nencapsulate individual traced graphs that are built by `tf_function`.\nRagged tensors can be used transparently with concrete functions.\n\n\n```{r}\n@tf_function\nincrement <- function(x) {    }\n  return x + 1\n\nrt <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\ncf <- increment$get_concrete_function(rt)\nprint(cf(rt))\n```\n\n\n### SavedModels\n\nA [SavedModel](https://www$tensorflow$org/guide/saved_model) is a\nserialized TensorFlow program, including both weights and computation.\nIt can be built from a Keras model or from a custom model. In either\ncase, ragged tensors can be used transparently with the functions and\nmethods defined by a SavedModel.\n\n#### Example: saving a Keras model\n\n\n```{r}\nimport tempfile\n\nkeras_module_path <- tempfile$mkdtemp()\ntf$saved_model$save(keras_model, keras_module_path)\nimported_model <- tf$saved_model$load(keras_module_path)\nimported_model(hashed_words)\n```\n\n\n#### Example: saving a custom model\n\n\n```{r}\nclass CustomModule(tf$Module):\n  initialize <- function(variable_value) {    }\n    super$initialize()\n    self$v <- tf$Variable(variable_value)\n\n  @tf_function\n  grow <- function(x) {    }\n    return x * self$v\n\nmodule <- CustomModule(100.0)\n\n# Before saving a custom model, you must ensure that concrete functions are\n\n# built for each input signature that you will need.\n\nmodule$grow$get_concrete_function(tf$RaggedTensorSpec(shape=[NULL, NULL],\n                                                      dtype=tf$float32))\n\ncustom_module_path <- tempfile$mkdtemp()\ntf$saved_model$save(module, custom_module_path)\nimported_model <- tf$saved_model$load(custom_module_path)\nimported_model$grow(tf$ragged$constant([[1.0, 4.0, 3.0], [2.0]]))\n```\n\n\nNote: SavedModel\n[signatures](https://www$tensorflow$org/guide/saved_model#specifying_signatures_during_export)\nare concrete functions. As discussed in the section on Concrete\nFunctions above, ragged tensors are only handled correctly by concrete\nfunctions starting with TensorFlow 2.3. If you need to use SavedModel\nsignatures in a previous version of TensorFlow, then it's recommended\nthat you decompose the ragged tensor into its component tensors.\n\n## Overloaded operators\n\nThe `RaggedTensor` class overloads the standard Python arithmetic and\ncomparison operators, making it easy to perform basic elementwise math:\n\n\n```{r}\nx <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\ny <- tf$ragged$constant([[1, 1], [2], [3, 3, 3]])\nprint(x + y)\n```\n\n\nSince the overloaded operators perform elementwise computations, the\ninputs to all binary operations must have the same shape or be\nbroadcastable to the same shape. In the simplest broadcasting case, a\nsingle scalar is combined elementwise with each value in a ragged\ntensor:\n\n\n```{r}\nx <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\nprint(x + 3)\n```\n\n\nFor a discussion of more advanced cases, check the section on\n^Broadcasting^.\n\nRagged tensors overload the same set of operators as normal `Tensor`s:\nthe unary operators `-`, `~`, and `abs()`; and the binary operators `+`,\n`-`, `*`, `/`, `//`, `%`, `^`, `&`, `|`, `^`, `==`, `<`, `<=`, `>`, and\n`>=`.\n\n## Indexing\n\nRagged tensors support Python-style indexing, including multidimensional\nindexing and slicing. The following examples demonstrate ragged tensor\nindexing with a 2D and a 3D ragged tensor.\n\n### Indexing examples: 2D ragged tensor\n\n\n```{r}\nqueries <- tf$ragged$constant(\n    [['Who', 'is', 'George', 'Washington'],\n     ['What', 'is', 'the', 'weather', 'tomorrow'],\n     ['Goodnight']])\n```\n\n```{r}\nprint(queries[1])                   # A single query\n```\n\n```{r}\nprint(queries[1, 2])                # A single word\n```\n\n```{r}\nprint(queries[1:])                  # Everything but the first row\n```\n\n```{r}\nprint(queries[:, :3])               # The first 3 words of each query\n```\n\n```{r}\nprint(queries[:, -2:])              # The last 2 words of each query\n```\n\n\n### Indexing examples: 3D ragged tensor\n\n\n```{r}\nrt <- tf$ragged$constant([[[1, 2, 3], [4]],\n                         [[5], [], [6]],\n                         [[7]],\n                         [[8, 9], [10]]])\n```\n\n```{r}\nprint(rt[1])                        # Second row (2D RaggedTensor)\n```\n\n```{r}\nprint(rt[3, 0])                     # First element of fourth row (1D Tensor)\n```\n\n```{r}\nprint(rt[:, 1:3])                   # Items 1-3 of each row (3D RaggedTensor)\n```\n\n```{r}\nprint(rt[:, -1:])                   # Last item of each row (3D RaggedTensor)\n```\n\n\n`RaggedTensor`s support multidimensional indexing and slicing with one\nrestriction: indexing into a ragged dimension is not allowed. This case\nis problematic because the indicated value may exist in some rows but\nnot others. In such cases, it's not obvious whether you should (1) raise\nan `IndexError`; (2) use a default value; or (3) skip that value and\nreturn a tensor with fewer rows than you started with. Following the\n[guiding principles of\nPython](https://www$python$org/dev/peps/pep-0020/) (\"In the face of\nambiguity, refuse the temptation to guess\"), this operation is currently\ndisallowed.\n\n## Tensor type conversion\n\nThe `RaggedTensor` class defines methods that can be used to convert\nbetween `RaggedTensor`s and `tf$Tensor`s or `tf$SparseTensors`:\n\n\n```{r}\nragged_sentences <- tf$ragged$constant([\n    ['Hi'], ['Welcome', 'to', 'the', 'fair'], ['Have', 'fun']])\n```\n\n```{r}\n# RaggedTensor -> Tensor\n\nprint(ragged_sentences$to_tensor(default_value='', shape=[NULL, 10]))\n```\n\n```{r}\n# Tensor -> RaggedTensor\n\nx <- [[1, 3, -1, -1], [2, -1, -1, -1], [4, 5, 8, 9]]\nprint(tf$RaggedTensor$from_tensor(x, padding=-1))\n```\n\n```{r}\n#RaggedTensor -> SparseTensor\n\nprint(ragged_sentences$to_sparse())\n```\n\n```{r}\n# SparseTensor -> RaggedTensor\n\nst <- tf$SparseTensor(indices=[[0, 0], [2, 0], [2, 1]],\n                     values=['a', 'b', 'c'],\n                     dense_shape=[3, 3])\nprint(tf$RaggedTensor$from_sparse(st))\n```\n\n\n## Evaluating ragged tensors\n\nTo access the values in a ragged tensor, you can:\n\n1.  Use `tf$RaggedTensor$to_list` to convert the ragged tensor to a\n    nested Python list.\n2.  Use `tf$RaggedTensor$numpy` to convert the ragged tensor to a NumPy\n    array whose values are nested NumPy arrays.\n3.  Decompose the ragged tensor into its components, using the\n    `tf$RaggedTensor$values` and `tf$RaggedTensor$row_splits`\n    properties, or row-paritioning methods such as\n    `tf$RaggedTensor$row_lengths` and `tf$RaggedTensor$value_rowids`.\n4.  Use Python indexing to select values from the ragged tensor.\n\n\n```{r}\nrt <- tf$ragged$constant([[1, 2], [3, 4, 5], [6], [], [7]])\nprint(\"Python list:\", rt$to_list())\nprint(\"NumPy array:\", rt$numpy())\nprint(\"Values:\", rt$values$numpy())\nprint(\"Splits:\", rt$row_splits$numpy())\nprint(\"Indexed value:\", rt[1].numpy())\n```\n\n\n## Ragged Shapes\n\nThe shape of a tensor specifies the size of each axis. For example, the\nshape of `[[1, 2], [3, 4], [5, 6]]` is `[3, 2]`, since there are 3 rows\nand 2 columns. TensorFlow has two separate but related ways to describe\nshapes:\n\n-   \\^*static shape\\^*: Information about axis sizes that is known\n    statically (e.g., while tracing a `tf_function`). May be partially\n    specified.\n\n-   \\^*dynamic shape\\^*: Runtime information about the axis sizes.\n\n### Static shape\n\nA Tensor's static shape contains information about its axis sizes that\nis known at graph-construction time. For both `tf$Tensor` and\n`tf$RaggedTensor`, it is available using the `.shape` property, and is\nencoded using `tf$TensorShape`:\n\n\n```{r}\nx <- as_tensor([[1, 2], [3, 4], [5, 6]])\nx$shape  # shape of a tf$tensor\n```\n\n```{r}\nrt <- tf$ragged$constant([[1], [2, 3], [], [4]])\nrt$shape  # shape of a tf$RaggedTensor\n```\n\n\nThe static shape of a ragged dimension is always `NULL` (i\\$e.,\nunspecified). However, the inverse is not true -- if a `TensorShape`\ndimension is `NULL`, then that could indicate that the dimension is\nragged, *or* it could indicate that the dimension is uniform but that\nits size is not statically known.\n\n### Dynamic shape\n\nA tensor's dynamic shape contains information about its axis sizes that\nis known when the graph is run. It is constructed using the `tf$shape`\noperation. For `tf$Tensor`, `tf$shape` returns the shape as a 1D integer\n`Tensor`, where `tf$shape(x)[i]` is the size of axis `i`.\n\n\n```{r}\nx <- as_tensor([['a', 'b'], ['c', 'd'], ['e', 'f']])\ntf$shape(x)\n```\n\n\nHowever, a 1D `Tensor` is not expressive enough to describe the shape of\na `tf$RaggedTensor`. Instead, the dynamic shape for ragged tensors is\nencoded using a dedicated type, `tf$experimental$DynamicRaggedShape`. In\nthe following example, the `DynamicRaggedShape` returned by\n`tf$shape(rt)` indicates that the ragged tensor has 4 rows, with lengths\n1, 3, 0, and 2:\n\n\n```{r}\nrt <- tf$ragged$constant([[1], [2, 3, 4], [], [5, 6]])\nrt_shape <- tf$shape(rt)\nprint(rt_shape)\n```\n\n\n#### Dynamic shape: operations\n\n`DynamicRaggedShape`s can be used with most TensorFlow ops that expect\nshapes, including `tf$reshape`, `tf$zeros`, `tf$ones`. `tf$fill`,\n`tf$broadcast_dynamic_shape`, and `tf$broadcast_to`.\n\n\n```{r}\nprint(f\"tf$reshape(x, rt_shape) = list(tf$reshape(x, rt_shape))\")\nprint(f\"tf$zeros(rt_shape) = list(tf$zeros(rt_shape))\")\nprint(f\"tf$ones(rt_shape) = list(tf$ones(rt_shape))\")\nprint(f\"tf$fill(rt_shape, 9) = list(tf$fill(rt_shape, 'x'))\")\n```\n\n\n#### Dynamic shape: indexing and slicing\n\n`DynamicRaggedShape` can be also be indexed to get the sizes of uniform\ndimensions. For example, we can find the number of rows in a\nraggedtensor using `tf$shape(rt)[0]` (just as we would for a non-ragged\ntensor):\n\n\n```{r}\nrt_shape[0]\n```\n\n\nHowever, it is an error to use indexing to try to retrieve the size of a\nragged dimension, since it doesn't have a single size. (Since\n`RaggedTensor` keeps track of which axes are ragged, this error is only\nthrown during eager execution or when tracing a `tf_function`; it will\nnever be thrown when executing a concrete function.)\n\n\n```{r}\ntry:\n  rt_shape[1]\nexcept ValueError as e:\n  print(\"Got expected ValueError:\", e)\n```\n\n\n`DynamicRaggedShape`s can also be sliced, as long as the slice either\nbegins with axis `0`, or contains only dense dimensions.\n\n\n```{r}\nrt_shape[:1]\n```\n\n\n#### Dynamic shape: encoding\n\n`DynamicRaggedShape` is encoded using two fields:\n\n-   `inner_shape`: An integer vector giving the shape of a dense\n    `tf$Tensor`.\n-   `row_partitions`: A list of `tf$experimental$RowPartition` objects,\n    describing how the outermost dimension of that inner shape should be\n    partitioned to add ragged axes.\n\nFor more information about row partitions, see the \"RaggedTensor\nencoding\" section below, and the API docs for\n`tf$experimental$RowPartition`.\n\n#### Dynamic shape: construction\n\n`DynamicRaggedShape` is most often constructed by applying `tf$shape` to\na `RaggedTensor`, but it can also be constructed directly:\n\n\n```{r}\ntf$experimental$DynamicRaggedShape(\n    row_partitions=[tf$experimental$RowPartition$from_row_lengths([5, 3, 2])],\n    inner_shape=[10, 8])\n```\n\n\nIf the lengths of all rows are known statically,\n`DynamicRaggedShape$from_lengths` can also be used to construct a\ndynamic ragged shape. (This is mostly useful for testing and\ndemonstration code, since it's rare for the lengths of ragged dimensions\nto be known statically).\n\n\n```{r}\ntf$experimental$DynamicRaggedShape$from_lengths([4, (2, 1, 0, 8), 12])\n```\n\n\n### Broadcasting\n\nBroadcasting is the process of making tensors with different shapes have\ncompatible shapes for elementwise operations. For more background on\nbroadcasting, refer to:\n\n-   [NumPy:\n    Broadcasting](https://docs$scipy$org/doc/numpy/user/basics$broadcasting$html)\n-   `tf$broadcast_dynamic_shape`\n-   `tf$broadcast_to`\n\nThe basic steps for broadcasting two inputs `x` and `y` to have\ncompatible shapes are:\n\n1.  If `x` and `y` do not have the same number of dimensions, then add\n    outer dimensions (with size 1) until they do.\n\n2.  For each dimension where `x` and `y` have different sizes:\n\n-   If `x` or `y` have size `1` in dimension `d`, then repeat its values\n    across dimension `d` to match the other input's size.\n-   Otherwise, raise an exception (`x` and `y` are not broadcast\n    compatible).\n\nWhere the size of a tensor in a uniform dimension is a single number\n(the size of slices across that dimension); and the size of a tensor in\na ragged dimension is a list of slice lengths (for all slices across\nthat dimension).\n\n#### Broadcasting examples\n\n\n```{r}\n# x       (2D ragged):  2 x (num_rows)\n\n# y       (scalar)\n\n# result  (2D ragged):  2 x (num_rows)\n\nx <- tf$ragged$constant([[1, 2], [3]])\ny <- 3\nprint(x + y)\n```\n\n```{r}\n# x         (2d ragged):  3 x (num_rows)\n\n# y         (2d tensor):  3 x          1\n\n# Result    (2d ragged):  3 x (num_rows)\n\nx <- tf$ragged$constant(\n   [[10, 87, 12],\n    [19, 53],\n    [12, 32]])\ny <- [[1000], [2000], [3000]]\nprint(x + y)\n```\n\n```{r}\n# x      (3d ragged):  2 x (r1) x 2\n\n# y      (2d ragged):         1 x 1\n\n# Result (3d ragged):  2 x (r1) x 2\n\nx <- tf$ragged$constant(\n    [[[1, 2], [3, 4], [5, 6]],\n     [[7, 8]]],\n    ragged_rank=1)\ny <- as_tensor([[10]])\nprint(x + y)\n```\n\n```{r}\n# x      (3d ragged):  2 x (r1) x (r2) x 1\n\n# y      (1d tensor):                    3\n\n# Result (3d ragged):  2 x (r1) x (r2) x 3\n\nx <- tf$ragged$constant(\n    [\n        [\n            [[1], [2]],\n            [],\n            [[3]],\n            [[4]],\n        ],\n        [\n            [[5], [6]],\n            [[7]]\n        ]\n    ],\n    ragged_rank=2)\ny <- as_tensor([10, 20, 30])\nprint(x + y)\n```\n\n\nHere are some examples of shapes that do not broadcast:\n\n\n```{r}\n# x      (2d ragged): 3 x (r1)\n\n# y      (2d tensor): 3 x    4  # trailing dimensions do not match\n\nx <- tf$ragged$constant([[1, 2], [3, 4, 5, 6], [7]])\ny <- as_tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n```{r}\n# x      (2d ragged): 3 x (r1)\n\n# y      (2d ragged): 3 x (r2)  # ragged dimensions do not match.\n\nx <- tf$ragged$constant([[1, 2, 3], [4], [5, 6]])\ny <- tf$ragged$constant([[10, 20], [30, 40], [50]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n```{r}\n# x      (3d ragged): 3 x (r1) x 2\n\n# y      (3d ragged): 3 x (r1) x 3  # trailing dimensions do not match\n\nx <- tf$ragged$constant([[[1, 2], [3, 4], [5, 6]],\n                        [[7, 8], [9, 10]]])\ny <- tf$ragged$constant([[[1, 2, 0], [3, 4, 0], [5, 6, 0]],\n                        [[7, 8, 0], [9, 10, 0]]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n\n## RaggedTensor encoding\n\nRagged tensors are encoded using the `RaggedTensor` class. Internally,\neach `RaggedTensor` consists of:\n\n-   A `values` tensor, which concatenates the variable-length rows into\n    a flattened list.\n-   A `row_partition`, which indicates how those flattened values are\n    divided into rows.\n\n![RaggedTensor\nencoding](https://www$tensorflow$org/images/ragged_tensors/ragged_encoding_2$png)\n\nThe `row_partition` can be stored using four different encodings:\n\n-   `row_splits` is an integer vector specifying the split points\n    between rows.\n-   `value_rowids` is an integer vector specifying the row index for\n    each value.\n-   `row_lengths` is an integer vector specifying the length of each\n    row.\n-   `uniform_row_length` is an integer scalar specifying a single length\n    for all rows.\n\n![row_partition\nencodings](https://www$tensorflow$org/images/ragged_tensors/partition_encodings$png)\n\nAn integer scalar `nrows` can also be included in the `row_partition`\nencoding to account for empty trailing rows with `value_rowids` or empty\nrows with `uniform_row_length`.\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_splits=[0, 4, 4, 6, 7])\nprint(rt)\n```\n\n\nThe choice of which encoding to use for row partitions is managed\ninternally by ragged tensors to improve efficiency in some contexts. In\nparticular, some of the advantages and disadvantages of the different\nrow-partitioning schemes are:\n\n-   \\^Efficient indexing\\^: The `row_splits` encoding enables\n    constant-time indexing and slicing into ragged tensors.\n-   \\^Efficient concatenation\\^: The `row_lengths` encoding is more\n    efficient when concatenating ragged tensors, since row lengths do\n    not change when two tensors are concatenated together.\n-   \\^Small encoding size\\^: The `value_rowids` encoding is more\n    efficient when storing ragged tensors that have a large number of\n    empty rows, since the size of the tensor depends only on the total\n    number of values. On the other hand, the `row_splits` and\n    `row_lengths` encodings are more efficient when storing ragged\n    tensors with longer rows, since they require only one scalar value\n    for each row.\n-   ^Compatibility^: The `value_rowids` scheme matches the\n    [segmentation](https://www$tensorflow$org/api_docs/python/tf/math#about_segmentation)\n    format used by operations, such as `tf$segment_sum`. The\n    `row_limits` scheme matches the format used by ops such as\n    `tf$sequence_mask`.\n-   \\^Uniform dimensions\\^: As discussed below, the `uniform_row_length`\n    encoding is used to encode ragged tensors with uniform dimensions.\n\n### Multiple ragged dimensions\n\nA ragged tensor with multiple ragged dimensions is encoded by using a\nnested `RaggedTensor` for the `values` tensor. Each nested\n`RaggedTensor` adds a single ragged dimension.\n\n![Encoding of a ragged tensor with multiple ragged dimensions (rank\n2)](https://www$tensorflow$org/images/ragged_tensors/ragged_rank_2$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=tf$RaggedTensor$from_row_splits(\n        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        row_splits=[0, 3, 3, 5, 9, 10]),\n    row_splits=[0, 1, 1, 5])\nprint(rt)\nprint(\"Shape: list()\".format(rt$shape))\nprint(\"Number of partitioned dimensions: list()\".format(rt$ragged_rank))\n```\n\n\nThe factory function `tf$RaggedTensor$from_nested_row_splits` may be\nused to construct a RaggedTensor with multiple ragged dimensions\ndirectly by providing a list of `row_splits` tensors:\n\n\n```{r}\nrt <- tf$RaggedTensor$from_nested_row_splits(\n    flat_values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n    nested_row_splits=([0, 1, 1, 5], [0, 3, 3, 5, 9, 10]))\nprint(rt)\n```\n\n\n### Ragged rank and flat values\n\nA ragged tensor's \\^*ragged rank\\^* is the number of times that the\nunderlying `values` tensor has been partitioned (i\\$e. the nesting depth\nof `RaggedTensor` objects). The innermost `values` tensor is known as\nits \\^*flat_values\\^*. In the following example, `conversations` has\nragged_rank=3, and its `flat_values` is a 1D `Tensor` with 24 strings:\n\n\n```{r}\n# shape = [batch, (paragraph), (sentence), (word)]\n\nconversations <- tf$ragged$constant(\n    [[[[\"I\", \"like\", \"ragged\", \"tensors.\"]],\n      [[\"Oh\", \"yeah?\"], [\"What\", \"can\", \"you\", \"use\", \"them\", \"for?\"]],\n      [[\"Processing\", \"variable\", \"length\", \"data!\"]]],\n     [[[\"I\", \"like\", \"cheese.\"], [\"Do\", \"you?\"]],\n      [[\"Yes.\"], [\"I\", \"do.\"]]]])\nconversations$shape\n```\n\n```{r}\nassert conversations$ragged_rank == len(conversations$nested_row_splits)\nconversations$ragged_rank  # Number of partitioned dimensions.\n```\n\n```{r}\nconversations$flat_values$numpy()\n```\n\n\n### Uniform inner dimensions\n\nRagged tensors with uniform inner dimensions are encoded by using a\nmultidimensional `tf$Tensor` for the flat_values (i\\$e., the innermost\n`values`).\n\n![Encoding of ragged tensors with uniform inner\ndimensions](https://www$tensorflow$org/images/ragged_tensors/uniform_inner$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=[[1, 3], [0, 0], [1, 3], [5, 3], [3, 3], [1, 2]],\n    row_splits=[0, 3, 4, 6])\nprint(rt)\nprint(\"Shape: list()\".format(rt$shape))\nprint(\"Number of partitioned dimensions: list()\".format(rt$ragged_rank))\nprint(\"Flat values shape: list()\".format(rt$flat_values$shape))\nprint(\"Flat values:\\nlist()\".format(rt$flat_values))\n```\n\n\n### Uniform non-inner dimensions\n\nRagged tensors with uniform non-inner dimensions are encoded by\npartitioning rows with `uniform_row_length`.\n\n![Encoding of ragged tensors with uniform non-inner\ndimensions](https://www$tensorflow$org/images/ragged_tensors/uniform_outer$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_uniform_row_length(\n    values=tf$RaggedTensor$from_row_splits(\n        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        row_splits=[0, 3, 5, 9, 10]),\n    uniform_row_length=2)\nprint(rt)\nprint(\"Shape: list()\".format(rt$shape))\nprint(\"Number of partitioned dimensions: list()\".format(rt$ragged_rank))\n```\n\n",
    "supporting": [
      "ragged_tensor_files"
    ],
    "filters": [],
    "includes": {}
  }
}