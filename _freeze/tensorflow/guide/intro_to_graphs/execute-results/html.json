{
  "hash": "22e7b4b740cbcd83383a705e4deb2bf5",
  "result": {
    "markdown": "---\ntitle: Intro To_graphs\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# Introduction to graphs and tf_function\n\n## Overview\n\nThis guide goes beneath the surface of TensorFlow and Keras to\ndemonstrate how TensorFlow works. If you instead want to immediately get\nstarted with Keras, check out the [collection of Keras\nguides](https://www.tensorflow.org/guide/keras/).\n\nIn this guide, you'll learn how TensorFlow allows you to make simple\nchanges to your code to get graphs, how graphs are stored and\nrepresented, and how you can use them to accelerate your models.\n\nNote: For those of you who are only familiar with TensorFlow 1.x, this\nguide demonstrates a very different view of graphs.\n\n**This is a big-picture overview that covers how `tf_function()` allows\nyou to switch from eager execution to graph execution.** For a more\ncomplete specification of `tf_function()`, go to the [`tf_function()`\nguide](function.qmd).\n\n### What are graphs?\n\nIn the previous three guides, you ran TensorFlow **eagerly**. This means\nTensorFlow operations are executed by Python, operation by operation,\nand returning results back to Python.\n\nWhile eager execution has several unique advantages, graph execution\nenables portability outside Python and tends to offer better\nperformance. **Graph execution** means that tensor computations are\nexecuted as a *TensorFlow graph*, sometimes referred to as a `tf$Graph`\nor simply a \"graph.\"\n\n**Graphs are data structures that contain a set of `tf$Operation`\nobjects, which represent units of computation; and `tf$Tensor` objects,\nwhich represent the units of data that flow between operations.** They\nare defined in a `tf$Graph` context. Since these graphs are data\nstructures, they can be saved, run, and restored all without the\noriginal R code.\n\nThis is what a TensorFlow graph representing a two-layer neural network\nlooks like when visualized in TensorBoard.\n\n![A simple TensorFlow\ng](https://github.com/tensorflow/docs/blob/master/site/en/guide/images/intro_to_graphs/two-layer-network.png?raw=1)\n\n### The benefits of graphs\n\nWith a graph, you have a great deal of flexibility. You can use your\nTensorFlow graph in environments that don't have an R interpreter, like\nmobile applications, embedded devices, and backend servers. TensorFlow\nuses graphs as the format for [saved models](saved_model) when it\nexports them from R.\n\nGraphs are also easily optimized, allowing the compiler to do\ntransformations like:\n\n-   Statically infer the value of tensors by folding constant nodes in\n    your computation *(\"constant folding\")*.\n-   Separate sub-parts of a computation that are independent and split\n    them between threads or devices.\n-   Simplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system,\n[Grappler](./graph_optimization.qmd), to perform this and other\nspeedups.\n\nIn short, graphs are extremely useful and let your TensorFlow run\n**fast**, run **in parallel**, and run efficiently **on multiple\ndevices**.\n\nHowever, you still want to define your machine learning models (or other\ncomputations) in Python for convenience, and then automatically\nconstruct graphs when you need them.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \"%>%\")\n```\n:::\n\n\n## Taking advantage of graphs\n\nYou create and run a graph in TensorFlow by using `tf_function()`,\neither as a direct call or as a decorator. `tf_function()` takes a\nregular function as input and returns a `Function`. **A `Function` is a\ncallable that builds TensorFlow graphs from the R function. You use a\n`Function` in the same way as its R equivalent.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \"float64\", shape = c(1, 2))\ny1 <- as_tensor(2:3, \"float64\", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n```\n:::\n\n\nOn the outside, a `Function` looks like a regular function you write\nusing TensorFlow operations.\n[Underneath](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py),\nhowever, it is *very different*. A `Function` **encapsulates [several\n`tf$Graph`s behind one API](#polymorphism_one_function_many_graphs).**\nThat is how `Function` is able to give you the [benefits of graph\nexecution](#the_benefits_of_graphs), like speed and deployability.\n\n`tf_function` applies to a function *and all other functions it calls*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \"float64\", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \"float64\", shape = c(1, 2))) #%>% as.array()\n```\n:::\n\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you\nneed to define a `Placeholder` or `tf$Session()`.\n\n### Converting Python functions to graphs\n\nAny function you write with TensorFlow will contain a mixture of\nbuilt-in TF operations and R control-flow logic, such as `if-then`\nclauses, loops, `break`, `return`, `next`, and more. While TensorFlow\noperations are easily captured by a `tf$Graph`, R-specific logic needs\nto undergo an extra step in order to become part of the graph.\n`tf_function()` uses a library called {tfautograph} to evaluate the R\ncode in a special way so that it generates a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \"First branch, with graph: \", format(tf_simple_relu(as_tensor(1))), \"\\n\",\n  \"Second branch, with graph: \", format(tf_simple_relu(as_tensor(-1))), \"\\n\",\n  sep = \"\"\n)\n```\n:::\n\n\nThough it is unlikely that you will need to view graphs directly, you\ncan inspect the outputs to check the exact results. These are not easy\nto read, so no need to look too carefully!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n```\n:::\n\n\nMost of the time, `tf_function()` will work without special\nconsiderations. However, there are some caveats, and the [tf_function\nguide](./function.qmd) can help here, as well as the [tfautograph\nGetting Started\nvignette](https://t-kalinowski.github.io/tfautograph/articles/tfautograph.html)\n\n### Polymorphism: one `Function`, many graphs\n\nA `tf$Graph` is specialized to a specific type of inputs (for example,\ntensors with a specific\n[`dtype`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) or\nobjects with the same\n[`id()`](https://docs.python.org/3/library/functions.html#id%5D)) (i.e,\nthe same memory address).\n\nEach time you invoke a `Function` with a set of arguments that can't be\nhandled by any of its existing graphs (such as arguments with new\n`dtypes` or incompatible shapes), `Function` creates a new `tf$Graph`\nspecialized to those new arguments. The type specification of a\n`tf$Graph`'s inputs is known as its **input signature** or just a\n**signature**. For more information regarding when a new `tf$Graph` is\ngenerated and how that can be controlled, see the [rules of\nretracing](https://www.tensorflow.org/guide/function#rules_of_tracing).\n\nThe `Function` stores the `tf$Graph` corresponding to that signature in\na `ConcreteFunction`. **A `ConcreteFunction` is a wrapper around a\n`tf$Graph`.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_relu <- tf_function(function(x) {\n  message(\"Tracing my_relu(x) with: \", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\nmy_relu(c(1, -1))\nmy_relu(as_tensor(c(3, -3)))\n```\n:::\n\n\nIf the `Function` has already been called with that signature,\n`Function` does not create a new `tf$Graph`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n```\n:::\n\n\nBecause it's backed by multiple graphs, a `Function` is ^polymorphic^.\nThat enables it to support more input types than a single `tf$Graph`\ncould represent, as well as to optimize each `tf$Graph` for better\nperformance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())\n```\n:::\n\n\n## Using `tf_function()`\n\nSo far, you've learned how to convert a Python function into a graph\nsimply by using `tf_function()` as function wrapper. But in practice,\ngetting `tf_function` to work correctly can be tricky! In the following\nsections, you'll learn how you can make your code work as expected with\n`tf_function()`.\n\n### Graph execution vs. eager execution\n\nThe code in a `Function` can be executed both eagerly and as a graph. By\ndefault, `Function` executes its code as a graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n:::\n\n\nTo verify that your `Function`'s graph is doing the same computation as\nits equivalent Python function, you can make it execute eagerly with\n`tf$config$run_functions_eagerly(TRUE)`. This is a switch that **turns\noff `Function`'s ability to create and run graphs**, instead executing\nthe code normally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\nHowever, `Function` can behave differently under graph and eager\nexecution. The R `print()` function is one example of how these two\nmodes differ. Let's check out what happens when you insert a `print`\nstatement to your function and call it repeatedly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\"Calculating MSE!\")\n  mean((y_true - y_pred) ^ 2)\n  })\n```\n:::\n\n\nObserve what is printed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n```\n:::\n\n\nIs the output surprising? **`get_MSE` only printed once even though it\nwas called *three* times.**\n\nTo explain, the `print` statement is executed when `Function` runs the\noriginal code in order to create the graph in a process known as\n[\"tracing\"](function.qmd#tracing). **Tracing captures the TensorFlow\noperations into a graph, and `print()` is not captured in the graph.**\nThat graph is then executed for all three calls **without ever running\nthe R code again**.\n\nAs a sanity check, let's turn off graph execution to compare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\n`print` is an *R side effect*, and there are other differences that you\nshould be aware of when converting a function into a `Function`. Learn\nmore in the *Limitations* section of the [Better performance with\ntf_function](./function.qmd#limitations) guide.\n\n::: callout-note\nNote: If you would like to print values in both eager and graph\nexecution, use `tf$print()` instead.\n:::\n\n### Non-strict execution\n\nGraph execution only executes the operations necessary to produce the\nobservable effects, which includes:\n\n-   The return value of the function\n-   Documented well-known side-effects such as:\n    -   Input/output operations, like `tf$print()`\n    -   Debugging operations, such as the assert functions in\n        `tf$debugging()` (also, `stopifnot()`)\n    -   Mutations of `tf$Variable()`\n\nThis behavior is usually known as \"Non-strict execution\", and differs\nfrom eager execution, which steps through all of the program operations,\nneeded or not.\n\nIn particular, runtime error checking does not count as an observable\neffect. If an operation is skipped because it is unnecessary, it cannot\nraise any runtime errors.\n\nIn the following example, the \"unnecessary\" operation `tf$gather()` is\nskipped during graph execution, so the runtime error\n`InvalidArgumentError` is not raised as it would be in eager execution.\nDo not rely on an error being raised while executing a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n# All operations are run during eager execution so an error is raised.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n```\n:::\n\n\n### `tf_function()` best practices\n\nIt may take some time to get used to the behavior of `Function`. To get\nstarted quickly, first-time users should play around with wrapping toy\nfunctions with `tf_function()` to get experience with going from eager\nto graph execution.\n\n*Designing for `tf_function`* may be your best bet for writing\ngraph-compatible TensorFlow programs. Here are some tips:\n\n-   Toggle between eager and graph execution early and often with\n    `tf$config$run_functions_eagerly()` to pinpoint if/when the two\n    modes diverge.\n\n-   Create `tf$Variable`s outside the Python function and modify them on\n    the inside. The same goes for objects that use `tf$Variable`, like\n    `keras$layers`, `keras$Model`s and `tf$optimizers`.\n\n-   Avoid writing functions that [depend on outer Python\n    variables](function#depending_on_python_global_and_free_variables),\n    excluding `tf$Variable`s and Keras objects.\n\n-   Prefer to write functions which take tensors and other TensorFlow\n    types as input. You can pass in other object types but [be\n    careful](function#depending_on_python_objects)!\n\n-   Include as much computation as possible under a `tf_function` to\n    maximize the performance gain. For example, wrap a whole training\n    step or the entire training loop.\n\n## Seeing the speed-up\n\n`tf_function` usually improves the performance of your code, but the\namount of speed-up depends on the kind of computation you run. Small\ncomputations can be dominated by the overhead of calling a graph. You\ncan measure the difference in performance like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(bench::mark(\n  \"Eager execution\" = power(x, 100),\n  \"Graph execution\" = power_as_graph(x, 100)))\n```\n:::\n\n\n`tf_function` is commonly used to speed up training loops, and you can\nlearn more about it in [Writing a training loop from\nscratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction)\nwith Keras.\n\nNote: You can also try\n[`tf_function(jit_compile = TRUE)`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunctionjit_compiletrue)\nfor a more significant performance boost, especially if your code is\nheavy on TF control flow and uses many small tensors.\n\n### Performance and trade-offs\n\nGraphs can speed up your code, but the process of creating them has some\noverhead. For some functions, the creation of the graph takes more time\nthan the execution of the graph. **This investment is usually quickly\npaid back with the performance boost of subsequent executions, but it's\nimportant to be aware that the first few steps of any large model\ntraining can be slower due to tracing.**\n\nNo matter how large your model, you want to avoid tracing frequently.\nThe `tf_function()` guide discusses [how to set input specifications and\nuse tensor arguments](function#controlling_retracing) to avoid\nretracing. If you find you are getting unusually poor performance, it's\na good idea to check if you are retracing accidentally.\n\n## When is a `Function` tracing?\n\nTo figure out when your `Function` is tracing, add a `print` or\n`message()` statement to its code. As a rule of thumb, `Function` will\nexecute the `message` statement every time it traces.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\"Tracing!\") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\na_function_with_r_side_effect(3)\n```\n:::\n\n\nNew (non-tensor) R arguments always trigger the creation of a new graph,\nhence the extra tracing.\n\n## Next steps\n\nYou can learn more about `tf_function()` on the API reference page and\nby following the [Better performance with `tf_function`](function.qmd)\nguide.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}