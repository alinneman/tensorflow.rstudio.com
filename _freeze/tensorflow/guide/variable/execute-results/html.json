{
  "hash": "26f02aa9c295bb67233e060f975dbbbd",
  "result": {
    "markdown": "---\ntitle: Variable\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# Introduction to Variables\n\nA TensorFlow **variable** is the recommended way to represent shared,\npersistent state your program manipulates. This guide covers how to\ncreate, update, and manage instances of `tf$Variable` in TensorFlow.\n\nVariables are created and tracked via the `tf$Variable` class. A\n`tf$Variable` represents a tensor whose value can be changed by running\nops on it. Specific ops allow you to read and modify the values of this\ntensor. Higher level libraries like `tf$keras` use `tf$Variable` to\nstore model parameters.\n\n## Setup\n\nThis notebook discusses variable placement. If you want to see on what\ndevice your variables are placed, uncomment this line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)\n```\n:::\n\n\n## Create a variable\n\nTo create a variable, provide an initial value. The `tf$Variable` will\nhave the same `dtype` as the initialization value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_tensor <- as_tensor(1:4, \"float32\", shape = c(2, 2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n(my_variable <- tf$Variable(my_tensor))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(4,) dtype=bool, numpy=array([False, False, False,  True])>\n```\n:::\n\n```{.r .cell-code}\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n```\n:::\n:::\n\n\nA variable looks and acts like a tensor, and, in fact, is a data\nstructure backed by a `tf$Tensor`. Like tensors, they have a `dtype` and\na shape, and can be exported to regular R arrays.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Shape: \"); my_variable$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([2, 2])\n```\n:::\n\n```{.r .cell-code}\ncat(\"DType: \"); my_variable$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDType: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n\n```{.r .cell-code}\ncat(\"As R array: \"); str(as.array(my_variable))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAs R array: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:2, 1:2] 1 3 2 4\n```\n:::\n:::\n\n\nMost tensor operations work on variables as expected, although variables\ncannot be reshaped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage(\"A variable: \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nA variable: \n```\n:::\n\n```{.r .cell-code}\nmy_variable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Viewed as a tensor: \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nViewed as a tensor: \n```\n:::\n\n```{.r .cell-code}\nas_tensor(my_variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Index of highest value: \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nIndex of highest value: \n```\n:::\n\n```{.r .cell-code}\ntf$math$argmax(my_variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 1], shape=(2), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# This creates a new tensor; it does not reshape the variable.\nmessage(\"Copying and reshaping: \") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCopying and reshaping: \n```\n:::\n\n```{.r .cell-code}\ntf$reshape(my_variable, c(1L, 4L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n```\n:::\n:::\n\n\nAs noted above, variables are backed by tensors. You can reassign the\ntensor using `tf$Variable$assign`. Calling `assign` does not (usually)\nallocate a new tensor; instead, the existing tensor's memory is reused.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n```\n:::\n:::\n\n\nIf you use a variable like a tensor in operations, you will usually\noperate on the backing tensor.\n\nCreating new variables from existing variables duplicates the backing\ntensors. Two variables will not share the same memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([5., 6.], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# a and b are different\n\nas.array(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5 6\n```\n:::\n\n```{.r .cell-code}\nas.array(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 3\n```\n:::\n\n```{.r .cell-code}\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7 9\n```\n:::\n\n```{.r .cell-code}\nas.array(a$assign_sub(c(7,9))) # c(0, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 0\n```\n:::\n:::\n\n\n## Lifecycles, naming, and watching\n\nIn TensorFlow, `tf$Variable` instance have the same lifecycle as other R\nobjects. When there are no references to a variable it is automatically\ndeallocated (garbage-collected).\n\nVariables can also be named which can help you track and debug them. You\ncan give two variables the same name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \"Mark\")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \"Mark\")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[False False]\n [False False]], shape=(2, 2), dtype=bool)\n```\n:::\n:::\n\n\nVariable names are preserved when saving and loading models. By default,\nvariables in models will acquire unique variable names automatically, so\nyou don't need to assign them yourself unless you want to.\n\nAlthough variables are important for differentiation, some variables\nwill not need to be differentiated. You can turn off gradients for a\nvariable by setting `trainable` to false at creation. An example of a\nvariable that would not need gradients is a training step counter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(step_counter <- tf$Variable(1L, trainable = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>\n```\n:::\n:::\n\n\n## Placing variables and tensors\n\nFor better performance, TensorFlow will attempt to place tensors and\nvariables on the fastest device compatible with its `dtype`. This means\nmost variables are placed on a GPU if one is available.\n\nHowever, you can override this. In this snippet, place a float tensor\nand a variable on the CPU, even if a GPU is available. By turning on\ndevice placement logging (see above), you can see where the variable is\nplaced.\n\nNote: Although manual placement works, using [distribution\nstrategies](distributed_training.qmd) can be a more convenient and\nscalable way to optimize your computation.\n\nIf you run this notebook on different backends with and without a GPU\nyou will see different logging. *Note that logging device placement must\nbe turned on at the start of the session.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \"float32\")\n  c <- tf$matmul(a, b)\n})\n\nc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[22. 49.]\n [28. 64.]], shape=(2, 2), dtype=float32)\n```\n:::\n:::\n\n\nIt's possible to set the location of a variable or tensor on one device\nand do the computation on another device. This will introduce delay, as\ndata needs to be copied between the devices.\n\nYou might do this, however, if you had multiple GPU workers but only\nwant one copy of the variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \"float32\")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  6. 15.]\n [ 2.  8. 18.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n\nNote: Because `tf$config$set_soft_device_placement()` is turned on by\ndefault, even if you run this code on a device without a GPU, it will\nstill run. The multiplication step will happen on the CPU.\n\nFor more on distributed training, refer to the\n[guide](distributed_training$qmd).\n\n## Next steps\n\nTo understand how variables are typically used, see our guide on\n[automatic differentiation](autodiff.qmd).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}