{
  "hash": "b763cac36836bd74fbca485d65759a73",
  "result": {
    "markdown": "---\ntitle: imdb_fasttext\ndescription: Trains a FastText model on the IMDB sentiment classification task.\n---\n\nThis example demonstrates the use of fasttext for text classification\n\nBased on Joulin et al's paper: \n\"Bags of Tricks for Efficient Text Classification\"\nhttps://arxiv.org/abs/1607.01759\n\nResults on IMDB datasets with uni and bi-gram embeddings:\n Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 CPU\n Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M GPU\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(purrr)\n\n# Function Definitions ----------------------------------------------------\n\ncreate_ngram_set <- function(input_list, ngram_value = 2){\n  indices <- map(0:(length(input_list) - ngram_value), ~1:ngram_value + .x)\n  indices %>%\n    map_chr(~input_list[.x] %>% paste(collapse = \"|\")) %>%\n    unique()\n}\n\nadd_ngram <- function(sequences, token_indice, ngram_range = 2){\n  ngrams <- map(\n    sequences, \n    create_ngram_set, ngram_value = ngram_range\n  )\n  \n  seqs <- map2(sequences, ngrams, function(x, y){\n    tokens <- token_indice$token[token_indice$ngrams %in% y]  \n    c(x, tokens)\n  })\n  \n  seqs\n}\n\n\n# Parameters --------------------------------------------------------------\n\n# ngram_range = 2 will add bi-grams features\nngram_range <- 2\nmax_features <- 20000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nepochs <- 5\n\n\n# Data Preparation --------------------------------------------------------\n\n# Load data\nimdb_data <- dataset_imdb(num_words = max_features)\n\n# Train sequences\nprint(length(imdb_data$train$x))\nprint(sprintf(\"Average train sequence length: %f\", mean(map_int(imdb_data$train$x, length))))\n\n# Test sequences\nprint(length(imdb_data$test$x)) \nprint(sprintf(\"Average test sequence length: %f\", mean(map_int(imdb_data$test$x, length))))\n\nif(ngram_range > 1) {\n  \n  # Create set of unique n-gram from the training set.\n  ngrams <- imdb_data$train$x %>% \n    map(create_ngram_set) %>%\n    unlist() %>%\n    unique()\n\n  # Dictionary mapping n-gram token to a unique integer\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features\n  token_indice <- data.frame(\n    ngrams = ngrams,\n    token  = 1:length(ngrams) + (max_features), \n    stringsAsFactors = FALSE\n  )\n  \n  # max_features is the highest integer that could be found in the dataset\n  max_features <- max(token_indice$token) + 1\n  \n  # Augmenting x_train and x_test with n-grams features\n  imdb_data$train$x <- add_ngram(imdb_data$train$x, token_indice, ngram_range)\n  imdb_data$test$x <- add_ngram(imdb_data$test$x, token_indice, ngram_range)\n}\n\n# Pad sequences\nimdb_data$train$x <- pad_sequences(imdb_data$train$x, maxlen = maxlen)\nimdb_data$test$x <- pad_sequences(imdb_data$test$x, maxlen = maxlen)\n\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_embedding(\n    input_dim = max_features, output_dim = embedding_dims, \n    input_length = maxlen\n    ) %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n\n# Fitting -----------------------------------------------------------------\n\nmodel %>% fit(\n  imdb_data$train$x, imdb_data$train$y, \n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(imdb_data$test$x, imdb_data$test$y)\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}