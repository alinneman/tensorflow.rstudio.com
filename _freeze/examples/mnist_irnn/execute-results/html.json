{
  "hash": "87f49cb48ff307015606622492382f50",
  "result": {
    "markdown": "---\ntitle: mnist_irnn\ndescription: Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" by Le et al.\n---\n\nThis is a reproduction of the IRNN experiment with pixel-by-pixel sequential \nMNIST in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\"\nby Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n\narxiv:1504.00941v2 [cs.NE] 7 Apr 2015\nhttp://arxiv.org/pdf/1504.00941v2.pdf\n\nOptimizer is replaced with RMSprop which yields more stable and steady\nimprovement.\n\nReaches 0.93 train/test accuracy after 900 epochs\nThis corresponds to roughly 1687500 steps in the original paper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Data Preparation ---------------------------------------------------------------\n\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 200\nhidden_units <- 100\n\nimg_rows <- 28\nimg_cols <- 28\n\nlearning_rate <- 1e-6\nclip_norm <- 1.0\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nx_train <- array_reshape(x_train, c(nrow(x_train), img_rows * img_cols, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), img_rows * img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model ------------------------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_simple_rnn(units = hidden_units,\n                   kernel_initializer = initializer_random_normal(stddev = 0.01),\n                   recurrent_initializer = initializer_identity(gain = 1.0),\n                   activation = 'relu',\n                   input_shape = dim(x_train)[-1]) %>% \n  layer_dense(units = num_classes) %>% \n  layer_activation(activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(lr = learning_rate),\n  metrics = c('accuracy')\n)\n \n# Training & Evaluation ---------------------------------------------------------\n\ncat(\"Evaluate IRNN...\\n\")\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n  \nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('IRNN test score:', scores[[1]], '\\n')\ncat('IRNN test accuracy:', scores[[2]], '\\n')\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}