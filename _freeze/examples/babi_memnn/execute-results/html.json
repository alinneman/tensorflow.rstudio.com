{
  "hash": "c57502d0c2a37a5f797a354bef2028c9",
  "result": {
    "markdown": "---\ntitle: Trains a memory network on the bAbI dataset\ndescription: Trains a memory network on the bAbI dataset for reading comprehension.\ncategories: [nlp]\n---\n\n\nTrains a memory network on the bAbI dataset.\n\n\nReferences:\n\n- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n  http://arxiv.org/abs/1502.05698\n\n- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n  \"End-To-End Memory Networks\", http://arxiv.org/abs/1503.08895\n\nReaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\nTime per epoch: 3s on CPU (core i7).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_dfr(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n  \n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n  \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n\n# Parameters --------------------------------------------------------------\n\nchallenges <- list(\n  # QA1 with 10,000 samples\n  single_supporting_fact_10k = \"%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt\",\n  # QA2 with 10,000 samples\n  two_supporting_facts_10k = \"%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt\"\n)\n\nchallenge_type <- \"single_supporting_fact_10k\"\nchallenge <- challenges[[challenge_type]]\nmax_length <- 999999\n\n\n# Data Preparation --------------------------------------------------------\n\n# Download data\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Reading training and test data\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\n# Extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# Vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n\n# Defining the model ------------------------------------------------------\n\n# Placeholders\nsequence <- layer_input(shape = c(story_maxlen))\nquestion <- layer_input(shape = c(query_maxlen))\n\n# Encoders\n# Embed the input sequence into a sequence of vectors\nsequence_encoder_m <- keras_model_sequential()\nsequence_encoder_m %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, embedding_dim)\n\n# Embed the input into a sequence of vectors of size query_maxlen\nsequence_encoder_c <- keras_model_sequential()\nsequence_encoder_c %>%\n  layer_embedding(input_dim = vocab_size, output_dim = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, query_maxlen)\n\n# Embed the question into a sequence of vectors\nquestion_encoder <- keras_model_sequential()\nquestion_encoder %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64, \n                  input_length = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, query_maxlen, embedding_dim)\n\n# Encode input sequence and questions (which are indices)\n# to sequences of dense vectors\nsequence_encoded_m <- sequence_encoder_m(sequence)\nsequence_encoded_c <- sequence_encoder_c(sequence)\nquestion_encoded <- question_encoder(question)\n\n# Compute a 'match' between the first input vector sequence\n# and the question vector sequence\n# shape: `(samples, story_maxlen, query_maxlen)`\ndot <- layer_dot(axes = c(2,2))\nmatch <- list(sequence_encoded_m, question_encoded) %>%\n  dot() %>%\n  layer_activation(\"softmax\")\n\n# Add the match matrix with the second input vector sequence\nresponse <- list(match, sequence_encoded_c) %>%\n  layer_add() %>%\n  layer_permute(c(2,1))\n\n# Concatenate the match matrix with the question vector sequence\nanswer <- list(response, question_encoded) %>%\n  layer_concatenate() %>%\n  # The original paper uses a matrix multiplication for this reduction step.\n  # We choose to use an RNN instead.\n  layer_lstm(32) %>%\n  # One regularization layer -- more would probably be needed.\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(vocab_size) %>%\n  # We output a probability distribution over the vocabulary\n  layer_activation(\"softmax\")\n\n# Build the final model\nmodel <- keras_model(inputs = list(sequence, question), answer)\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = 32,\n  epochs = 120,\n  validation_data = list(list(test_vec$stories, test_vec$questions), test_vec$answers)\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}