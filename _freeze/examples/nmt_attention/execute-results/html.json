{
  "hash": "61d63547422b6d0a5f3141cf86b91b63",
  "result": {
    "markdown": "---\ntitle: nmt_attention\ndescription: Neural machine translation with an attention mechanism.\n---\n\nThis is the companion code to the post \n\"Attention-based Neural Machine Translation with Keras\"\non the TensorFlow for R blog.\n\nhttps://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(tibble)\n\n\n# Preprocessing -----------------------------------------------------------\n\n# Assumes you've downloaded and unzipped one of the bilingual datasets offered at\n# http://www.manythings.org/anki/ and put it into a directory \"data\"\n# This example translates English to Dutch.\n\nfilepath <- file.path(\"data\", \"nld.txt\")\n\nlines <- readLines(filepath, n = 10000)\nsentences <- str_split(lines, \"\\t\")\n\nspace_before_punct <- function(sentence) {\n  str_replace_all(sentence, \"([?.!])\", \" \\\\1\")\n}\n\nreplace_special_chars <- function(sentence) {\n  str_replace_all(sentence, \"[^a-zA-Z?.!,Â¿]+\", \" \")\n}\n\nadd_tokens <- function(sentence) {\n  paste0(\"<start> \", sentence, \" <stop>\")\n}\nadd_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)\n\npreprocess_sentence <- compose(add_tokens,\n                               str_squish,\n                               replace_special_chars,\n                               space_before_punct)\n\nword_pairs <- map(sentences, preprocess_sentence)\n\ncreate_index <- function(sentences) {\n  unique_words <- sentences %>% unlist() %>% paste(collapse = \" \") %>%\n    str_split(pattern = \" \") %>% .[[1]] %>% unique() %>% sort()\n  index <- data.frame(\n    word = unique_words,\n    index = 1:length(unique_words),\n    stringsAsFactors = FALSE\n  ) %>%\n    add_row(word = \"<pad>\",\n            index = 0,\n            .before = 1)\n  index\n}\n\nword2index <- function(word, index_df) {\n  index_df[index_df$word == word, \"index\"]\n}\nindex2word <- function(index, index_df) {\n  index_df[index_df$index == index, \"word\"]\n}\n\nsrc_index <- create_index(map(word_pairs, ~ .[[1]]))\ntarget_index <- create_index(map(word_pairs, ~ .[[2]]))\nsentence2digits <- function(sentence, index_df) {\n  map((sentence %>% str_split(pattern = \" \"))[[1]], function(word)\n    word2index(word, index_df))\n}\n\nsentlist2diglist <- function(sentence_list, index_df) {\n  map(sentence_list, function(sentence)\n    sentence2digits(sentence, index_df))\n}\n\nsrc_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)\nsrc_maxlen <- map(src_diglist, length) %>% unlist() %>% max()\nsrc_matrix <-\n  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = \"post\")\n\ntarget_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)\ntarget_maxlen <- map(target_diglist, length) %>% unlist() %>% max()\ntarget_matrix <-\n  pad_sequences(target_diglist, maxlen = target_maxlen, padding = \"post\")\n\n\n\n# Train-test-split --------------------------------------------------------\n\ntrain_indices <-\n  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)\n\nvalidation_indices <- setdiff(1:nrow(src_matrix), train_indices)\n\nx_train <- src_matrix[train_indices,]\ny_train <- target_matrix[train_indices,]\n\nx_valid <- src_matrix[validation_indices,]\ny_valid <- target_matrix[validation_indices,]\n\nbuffer_size <- nrow(x_train)\n\n# just for convenience, so we may get a glimpse at translation performance \n# during training\ntrain_sentences <- sentences[train_indices]\nvalidation_sentences <- sentences[validation_indices]\nvalidation_sample <- sample(validation_sentences, 5)\n\n\n\n# Hyperparameters / variables ---------------------------------------------\n\nbatch_size <- 32\nembedding_dim <- 64\ngru_units <- 256\n\nsrc_vocab_size <- nrow(src_index)\ntarget_vocab_size <- nrow(target_index)\n\n\n# Create datasets ---------------------------------------------------------\n\ntrain_dataset <-\n  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nvalidation_dataset <-\n  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n\n# Attention encoder -------------------------------------------------------\n\n\nattention_encoder <-\n  function(gru_units,\n           embedding_dim,\n           src_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$embedding <-\n        layer_embedding(input_dim = src_vocab_size,\n                        output_dim = embedding_dim)\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        \n        x <- self$embedding(x)\n        c(output, state) %<-% self$gru(x, initial_state = hidden)\n        \n        list(output, state)\n      }\n    })\n  }\n\n\n\n# Attention decoder -------------------------------------------------------\n\n\nattention_decoder <-\n  function(object,\n           gru_units,\n           embedding_dim,\n           target_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      self$embedding <-\n        layer_embedding(input_dim = target_vocab_size, output_dim = embedding_dim)\n      gru_units <- gru_units\n      self$fc <- layer_dense(units = target_vocab_size)\n      self$W1 <- layer_dense(units = gru_units)\n      self$W2 <- layer_dense(units = gru_units)\n      self$V <- layer_dense(units = 1L)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        encoder_output <- inputs[[3]]\n        \n        hidden_with_time_axis <- k_expand_dims(hidden, 2)\n        \n        score <-\n          self$V(k_tanh(\n            self$W1(encoder_output) + self$W2(hidden_with_time_axis)\n          ))\n        \n        attention_weights <- k_softmax(score, axis = 2)\n        \n        context_vector <- attention_weights * encoder_output\n        context_vector <- k_sum(context_vector, axis = 2)\n        \n        x <- self$embedding(x)\n        \n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)\n        \n        c(output, state) %<-% self$gru(x)\n        \n        output <- k_reshape(output, c(-1, gru_units))\n        \n        x <- self$fc(output)\n        \n        list(x, state, attention_weights)\n        \n      }\n      \n    })\n  }\n\n\n# The model ---------------------------------------------------------------\n\nencoder <- attention_encoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  src_vocab_size = src_vocab_size\n)\n\ndecoder <- attention_decoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  target_vocab_size = target_vocab_size\n)\n\noptimizer <- tf$optimizers$Adam()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- ifelse(y_true == 0L, 0, 1)\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,\n                                                   logits = y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\n\n\n# Inference / translation functions ---------------------------------------\n# they are appearing here already in the file because we want to watch how\n# the network learns\n\nevaluate <-\n  function(sentence) {\n    attention_matrix <-\n      matrix(0, nrow = target_maxlen, ncol = src_maxlen)\n    \n    sentence <- preprocess_sentence(sentence)\n    input <- sentence2digits(sentence, src_index)\n    input <-\n      pad_sequences(list(input), maxlen = src_maxlen,  padding = \"post\")\n    input <- k_constant(input)\n    \n    result <- \"\"\n    \n    hidden <- k_zeros(c(1, gru_units))\n    c(enc_output, enc_hidden) %<-% encoder(list(input, hidden))\n    \n    dec_hidden <- enc_hidden\n    dec_input <-\n      k_expand_dims(list(word2index(\"<start>\", target_index)))\n    \n    for (t in seq_len(target_maxlen - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, dec_hidden, enc_output))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t,] <- attention_weights %>% as.double()\n      \n      pred_idx <-\n        tf$compat$v1$multinomial(k_exp(preds), num_samples = 1L)[1, 1] %>% as.double()\n      pred_word <- index2word(pred_idx, target_index)\n      \n      if (pred_word == '<stop>') {\n        result <-\n          paste0(result, pred_word)\n        return (list(result, sentence, attention_matrix))\n      } else {\n        result <-\n          paste0(result, pred_word, \" \")\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    list(str_trim(result), sentence, attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           words_sentence,\n           words_result) {\n    melted <- melt(attention_matrix)\n    ggplot(data = melted, aes(\n      x = factor(Var2),\n      y = factor(Var1),\n      fill = value\n    )) +\n      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +\n      theme(axis.ticks = element_blank()) +\n      xlab(\"\") +\n      ylab(\"\") +\n      scale_x_discrete(labels = words_sentence, position = \"top\") +\n      scale_y_discrete(labels = words_result) +\n      theme(aspect.ratio = 1)\n  }\n\n\ntranslate <- function(sentence) {\n  c(result, sentence, attention_matrix) %<-% evaluate(sentence)\n  print(paste0(\"Input: \",  sentence))\n  print(paste0(\"Predicted translation: \", result))\n  attention_matrix <-\n    attention_matrix[1:length(str_split(result, \" \")[[1]]),\n                     1:length(str_split(sentence, \" \")[[1]])]\n  plot_attention(attention_matrix,\n                 str_split(sentence, \" \")[[1]],\n                 str_split(result, \" \")[[1]])\n}\n\n# Training loop -----------------------------------------------------------\n\n\nn_epochs <- 50\n\nencoder_init_hidden <- k_zeros(c(batch_size, gru_units))\n\nfor (epoch in seq_len(n_epochs)) {\n  total_loss <- 0\n  iteration <- 0\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <- 0\n    x <- batch[[1]]\n    y <- batch[[2]]\n    iteration <- iteration + 1\n\n    with(tf$GradientTape() %as% tape, {\n      c(enc_output, enc_hidden) %<-% encoder(list(x, encoder_init_hidden))\n      \n      dec_hidden <- enc_hidden\n      dec_input <-\n        k_expand_dims(rep(list(\n          word2index(\"<start>\", target_index)\n        ), batch_size))\n      \n      \n      for (t in seq_len(target_maxlen - 1)) {\n        c(preds, dec_hidden, weights) %<-%\n          decoder(list(dec_input, dec_hidden, enc_output))\n        loss <- loss + cx_loss(y[, t], preds)\n        \n        dec_input <- k_expand_dims(y[, t])\n      }\n    })\n    total_loss <-\n      total_loss + loss / k_cast_to_floatx(dim(y)[2])\n    \n    paste0(\n      \"Batch loss (epoch/batch): \",\n      epoch,\n      \"/\",\n      iteration,\n      \": \",\n      (loss / k_cast_to_floatx(dim(y)[2])) %>% as.double() %>% round(4),\n      \"\\n\"\n    ) %>% print()\n    \n    variables <- c(encoder$variables, decoder$variables)\n    gradients <- tape$gradient(loss, variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(gradients, variables)))\n    \n  })\n  \n  paste0(\n    \"Total loss (epoch): \",\n    epoch,\n    \": \",\n    (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n    \"\\n\"\n  ) %>% print()\n  \n  walk(train_sentences[1:5], function(pair)\n    translate(pair[1]))\n  walk(validation_sample, function(pair)\n    translate(pair[1]))\n}\n\n# plot a mask\nexample_sentence <- train_sentences[[1]]\ntranslate(example_sentence)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}