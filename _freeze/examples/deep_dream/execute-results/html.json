{
  "hash": "45184354e80bdea8cc680942ef65e7e5",
  "result": {
    "markdown": "---\ntitle: Deep Dream\nauthors: \n  - \"[fchollet](https://twitter.com/fchollet)\"\n  - \"[dfalbel](https://github.com/dfalbel) - R translation\"\ndate-created: 2016/01/13\ndate-last-modified: 2020/05/02\ndescription: Generating Deep Dreams with Keras.\ncategories: [generative]\naliases: \n  - ../guide/keras/examples/deep_dream/index.html\n---\n\n\n## Introduction\n\n\"Deep dream\" is an image-filtering technique which consists of taking an image\nclassification model, and running gradient ascent over an input image to\ntry to maximize the activations of specific layers (and sometimes, specific units in\nspecific layers) for this input. It produces hallucination-like visuals.\n\nIt was first introduced by Alexander Mordvintsev from Google in July 2015.\n\nProcess:\n\n- Load the original image.\n- Define a number of processing scales (\"octaves\"),\nfrom smallest to largest.\n- Resize the original image to the smallest scale.\n- For every scale, starting with(the smallest (i$e. current one), {    })\n    - Run gradient ascent\n    - Upscale image to the next scale\n    - Reinject the detail that was lost at upscaling time\n- Stop when we are back to the original size.\nTo obtain the detail lost during upscaling, we simply\ntake the original image, shrink it down, upscale it,\nand compare the result to the (resized) original image.\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nbase_image_path <- get_file(\"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nresult_prefix <- \"sky_dream\"\n\n# These are the names of the layers\n# for which we try to maximize activation,\n# as well as their weight in the final loss\n# we try to maximize.\n# You can tweak these setting to obtain new visual effects.\nlayer_settings <- list(\n  \"mixed4\" = 1.0,\n  \"mixed5\" = 1.5,\n  \"mixed6\" = 2.0,\n  \"mixed7\" = 2.5\n)\n\n# Playing with these hyperparameters will also allow you to achieve new effects\n\nstep <- 0.01  # Gradient ascent step size\nnum_octave <- 3  # Number of scales at which to run gradient ascent\noctave_scale <- 1.4  # Size ratio between scales\niterations <- 20  # Number of ascent steps per scale\nmax_loss <- 15.0\n```\n:::\n\n\nThis is our base image:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_image <- function(img) {\n  img %>%   \n    as.raster(max = 255) %>% \n    plot()\n}\n\nbase_image_path %>% \n  image_load() %>% \n  image_to_array() %>% \n  plot_image()\n```\n\n::: {.cell-output-display}\n![](deep_dream_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's set up some image preprocessing/deprocessing utilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreprocess_image <- function(image_path) {\n  # Util function to open, resize and format pictures\n  # into appropriate arrays.\n  img <- image_path %>% \n    image_load() %>% \n    image_to_array()\n  dim(img) <- c(1, dim(img))\n  inception_v3_preprocess_input(img)\n}\n\ndeprocess_image <- function(x) {\n  dim(x) <- dim(x)[-1]\n  # Undo inception v3 preprocessing\n  x <- x/2.0\n  x <- x + 0.5\n  x <- x*255.0\n  x[] <- raster::clamp(as.numeric(x), 0, 255)\n  x\n}\n```\n:::\n\n\n## Compute the Deep Dream loss\n\n\nFirst, build a feature extraction model to retrieve the activations of our target layers\ngiven an input image.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build an InceptionV3 model loaded with pre-trained ImageNet weights\nmodel <- application_inception_v3(weights = \"imagenet\", include_top = FALSE)\n\n# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\noutputs_dict <- purrr::imap(layer_settings, function(v, name) {\n  layer <- get_layer(model, name)\n  layer$output\n})\n\n# Set up a model that returns the activation values for every target layer\n# (as a dict)\n\nfeature_extractor <- keras_model(inputs = model$inputs, outputs = outputs_dict)\n```\n:::\n\n\nThe actual loss computation is very simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_loss <- function(input_image) {\n  features <- feature_extractor(input_image)\n  # Initialize the loss\n  loss <- tf$zeros(shape = shape())\n  \n  layer_settings %>% \n    purrr::imap(function(coeff, name) {\n      activation <- features[[name]]\n      scaling <- tf$reduce_prod(tf$cast(tf$shape(activation), \"float32\"))\n      # We avoid border artifacts by only involving non-border pixels in the loss.\n      coeff * tf$reduce_sum(tf$square(activation[, 3:-2, 3:-2, ])) / scaling\n    }) %>% \n    purrr::reduce(tf$add)\n}\n```\n:::\n\n\n## Set up the gradient ascent loop for one octave\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngradient_ascent_step <- tf_function(function(img, learning_rate) {\n  with(tf$GradientTape() %as% tape, {    \n    tape$watch(img)\n    loss <- compute_loss(img)\n  })\n  \n  # Compute gradients.\n  grads <- tape$gradient(loss, img)\n  # Normalize gradients.\n  grads <- grads/tf$maximum(tf$reduce_mean(tf$abs(grads)), 1e-6)\n  img <- img + learning_rate * grads\n  list(loss, img)\n})\n\n\ngradient_ascent_loop <- function(img, iterations, learning_rate, max_loss = NULL) {\n  for (i in seq_len(iterations)) {\n    c(loss, img) %<-% gradient_ascent_step(img, learning_rate)\n    if (!is.null(max_loss) && as.logical(loss > max_loss)) {\n      break\n    }\n    cat(\"... Loss value at step \", i, \": \", as.numeric(loss), \"\\n\")\n  }\n  img\n}\n```\n:::\n\n\n## Run the training loop, iterating over different octaves\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_img <- preprocess_image(base_image_path)\noriginal_shape <- dim(original_img)[2:3]\n\nsuccessive_shapes <- list(original_shape)\nfor (i in seq_len(num_octave - 1)) {\n  shape <- as.integer(original_shape / octave_scale^i)\n  successive_shapes[[i+1]] <- shape\n}\nsuccessive_shapes <- rev(successive_shapes)\n\nshrunk_original_img <- tf$image$resize(original_img, successive_shapes[[1]])\nimg <- tf$identity(original_img)  # Make a copy\nfor (i in seq_along(successive_shapes)) {\n  shape <- successive_shapes[[i]]\n  \n  cat(\"Processing octave \", i, \"with shape:\", shape, \"\\n\")\n  \n  img <- tf$image$resize(img, shape)\n  img <- gradient_ascent_loop(\n    img, iterations = iterations, learning_rate = step, max_loss = max_loss\n  )\n  upscaled_shrunk_original_img <- tf$image$resize(shrunk_original_img, shape)\n  same_size_original <- tf$image$resize(original_img, shape)\n  lost_detail <- same_size_original - upscaled_shrunk_original_img\n  \n  img <- img + lost_detail\n  shrunk_original_img <- tf$image$resize(original_img, shape)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing octave  1 with shape: 326 489 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n... Loss value at step  1 :  0.5014444 \n... Loss value at step  2 :  0.7033239 \n... Loss value at step  3 :  0.9975789 \n... Loss value at step  4 :  1.339841 \n... Loss value at step  5 :  1.69038 \n... Loss value at step  6 :  2.055832 \n... Loss value at step  7 :  2.368303 \n... Loss value at step  8 :  2.700009 \n... Loss value at step  9 :  3.018286 \n... Loss value at step  10 :  3.355762 \n... Loss value at step  11 :  3.659731 \n... Loss value at step  12 :  3.970093 \n... Loss value at step  13 :  4.239055 \n... Loss value at step  14 :  4.570164 \n... Loss value at step  15 :  4.875012 \n... Loss value at step  16 :  5.180275 \n... Loss value at step  17 :  5.479495 \n... Loss value at step  18 :  5.700634 \n... Loss value at step  19 :  6.022943 \n... Loss value at step  20 :  6.264833 \nProcessing octave  2 with shape: 457 685 \n... Loss value at step  1 :  1.214347 \n... Loss value at step  2 :  1.962548 \n... Loss value at step  3 :  2.586394 \n... Loss value at step  4 :  3.085848 \n... Loss value at step  5 :  3.57484 \n... Loss value at step  6 :  4.002796 \n... Loss value at step  7 :  4.40269 \n... Loss value at step  8 :  4.807469 \n... Loss value at step  9 :  5.220548 \n... Loss value at step  10 :  5.58031 \n... Loss value at step  11 :  5.953027 \n... Loss value at step  12 :  6.311125 \n... Loss value at step  13 :  6.666377 \n... Loss value at step  14 :  7.01564 \n... Loss value at step  15 :  7.373053 \n... Loss value at step  16 :  7.687048 \n... Loss value at step  17 :  8.022481 \n... Loss value at step  18 :  8.353153 \n... Loss value at step  19 :  8.676409 \n... Loss value at step  20 :  8.996731 \nProcessing octave  3 with shape: 640 960 \n... Loss value at step  1 :  1.34736 \n... Loss value at step  2 :  2.137553 \n... Loss value at step  3 :  2.785597 \n... Loss value at step  4 :  3.325019 \n... Loss value at step  5 :  3.836637 \n... Loss value at step  6 :  4.312602 \n... Loss value at step  7 :  4.782893 \n... Loss value at step  8 :  5.230842 \n... Loss value at step  9 :  5.646795 \n... Loss value at step  10 :  6.066389 \n... Loss value at step  11 :  6.450614 \n... Loss value at step  12 :  6.861366 \n... Loss value at step  13 :  7.217497 \n... Loss value at step  14 :  7.596985 \n... Loss value at step  15 :  7.9565 \n... Loss value at step  16 :  8.316831 \n... Loss value at step  17 :  8.604964 \n... Loss value at step  18 :  8.966898 \n... Loss value at step  19 :  9.21804 \n... Loss value at step  20 :  9.593339 \n```\n:::\n:::\n\n\nDisplay the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimg %>% \n  as.array() %>% \n  deprocess_image() %>% \n  plot_image()\n```\n\n::: {.cell-output-display}\n![](deep_dream_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "deep_dream_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}