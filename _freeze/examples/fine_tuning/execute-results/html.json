{
  "hash": "2752a984fdd8396926f9ce55d65a2da8",
  "result": {
    "markdown": "---\ntitle: fine_tuning\ndescription: Fine tuning of a image classification model.\n---\n\nIn this example we fine tune Mobile Net to better predict cats and\ndogs in photos. It also demonstrates the usage of image data generators\nfor efficient preprocessing and training.\n\nIt's preferable to run this example in a GPU.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download data -----------------------------------------------------------\n\ndownload.file(\n  \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\", \n  destfile = \"cats-dogs.zip\"\n)\n\n# Pre-processing ----------------------------------------------------------\n\nzip::unzip(\"cats-dogs.zip\", exdir = \"data-raw\")\n\n# We will organize images in the following structure:\n# data/\n#     train/\n#          Cat/\n#          Dog/\n#     validation\n#          Cat/\n#          Dog/\n#     test/\n#          images/\n#\n\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\n# some images are corrupt and we exclude them\n# this will make sure all images can be read.\nfor (im in all_imgs) {\n  out <- try(magick::image_read(im), silent = TRUE)\n  if (inherits(out, \"try-error\")) {\n    fs::file_delete(im)\n    message(\"removed image: \", im)\n  }\n}\n\n# re-list all imgs\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\nset.seed(5)\n\ntraining_imgs <- sample(all_imgs, size = length(all_imgs)/2)\nvalidation_imgs <- sample(all_imgs[!all_imgs %in% training_imgs], size = length(all_imgs)/4)         \ntesting_imgs <- all_imgs[!all_imgs %in% c(training_imgs, validation_imgs)]\n\n# create directory structure\nfs::dir_create(c(\n  \"data/train/Cat\",\n  \"data/train/Dog\",\n  \"data/validation/Cat\",\n  \"data/validation/Dog\",\n  \"data/test/images\"\n))\n\n# copy training images\nfs::file_copy(\n  path = training_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/train\", training_imgs)\n)\n\n# copy valid images\nfs::file_copy(\n  path = validation_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/validation\", validation_imgs)\n)\n\n# copy testing imgs\nfs::file_copy(\n  path = testing_imgs,\n  new_path = gsub(\"data-raw/PetImages/(Dog|Cat)/\", \"data/test/images/\\\\1\", testing_imgs)\n)\n\n# Image flow --------------------------------------------------------------\n\nlibrary(keras)\n\ntraining_image_gen <- image_data_generator(\n  rotation_range = 20,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  horizontal_flip = TRUE,\n  preprocessing_function = imagenet_preprocess_input\n)\n\nvalidation_image_gen <- image_data_generator(\n  preprocessing_function = imagenet_preprocess_input\n)\n\ntraining_image_flow <- flow_images_from_directory(\n  directory = \"data/train/\", \n  generator = training_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n)\n\nvalidation_image_flow <- flow_images_from_directory(\n  directory = \"data/validation/\", \n  generator = validation_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n  shuffle = FALSE\n)\n\n# Model -------------------------------------------------------------------\n\nmob <- application_mobilenet(include_top = FALSE, pooling = \"avg\")\nfreeze_weights(mob)\n\nmodel <- keras_model_sequential() %>% \n  mob() %>% \n  layer_dense(256, activation = \"relu\") %>% \n  layer_dropout(rate = 0.2) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 1, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# now top layers weights are fine, we can unfreeze the lower layer weights.\nunfreeze_weights(mob)\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 3, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# Generate predictions for test data --------------------------------------\n\ntest_flow <- flow_images_from_directory(\n  generator = validation_image_gen,\n  directory = \"data/test\", \n  target_size = c(224, 224),\n  class_mode = NULL,\n  shuffle = FALSE\n)\n\npredictions <- predict_generator(\n  model, \n  test_flow,\n  steps = test_flow$n/test_flow$batch_size\n)\n\nmagick::image_read(testing_imgs[1])\npredictions[1]\n\nmagick::image_read(testing_imgs[6250])\npredictions[6250]\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}