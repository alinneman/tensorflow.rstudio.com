{
  "hash": "3e237fb9bd7f6c9457c15481bdcbc3d5",
  "result": {
    "markdown": "---\ntitle: eager_image_captioning\ndescription: Generating image captions with Keras and eager execution.\n---\n\nThis is the companion code to the post \n\"Attention-based Image Captioning with Keras\"\non the TensorFlow for R blog.\n\nhttps://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nnp <- import(\"numpy\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(rjson)\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(magick)\n\nmaybecat <- function(context, x) {\n  if (debugshapes) {\n    name <- enexpr(x)\n    dims <- paste0(dim(x), collapse = \" \")\n    cat(context, \": shape of \", name, \": \", dims, \"\\n\", sep = \"\")\n  }\n}\n\ndebugshapes <- FALSE\nrestore_checkpoint <- FALSE\nsaved_features_exist <- FALSE\n\nuse_session_with_seed(7777,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\nannotation_file <- \"train2014/annotations/captions_train2014.json\"\nimage_path <- \"train2014/train2014\"\n\nannotations <- fromJSON(file = annotation_file)\n\nannot_captions <- annotations[[4]]\n# 414113\nnum_captions <- length(annot_captions)\n\nall_captions <- vector(mode = \"list\", length = num_captions)\nall_img_names <- vector(mode = \"list\", length = num_captions)\n\nfor (i in seq_len(num_captions)) {\n  caption <-\n    paste0(\"<start> \", annot_captions[[i]][[\"caption\"]], \" <end>\")\n  image_id <- annot_captions[[i]][[\"image_id\"]]\n  full_coco_image_path <-\n    sprintf(\"train2014/train2014/COCO_train2014_%012d.jpg\", image_id)\n  all_img_names[[i]] <- full_coco_image_path\n  all_captions[[i]] <- caption\n}\n\nnum_examples <- 30000\n\nif (!saved_features_exist) {\n  random_sample <- sample(1:num_captions, size = num_examples)\n  train_indices <-\n    sample(random_sample, size = length(random_sample) * 0.8)\n  validation_indices <-\n    setdiff(random_sample, train_indices)\n  saveRDS(random_sample,\n          paste0(\"random_sample_\", num_examples, \".rds\"))\n  saveRDS(train_indices,\n          paste0(\"train_indices_\", num_examples, \".rds\"))\n  saveRDS(validation_indices,\n          paste0(\"validation_indices_\", num_examples, \".rds\"))\n} else {\n  random_sample <-\n    readRDS(paste0(\"random_sample_\", num_examples, \".rds\"))\n  train_indices <-\n    readRDS(paste0(\"train_indices_\", num_examples, \".rds\"))\n  validation_indices <-\n    readRDS(paste0(\"validation_indices_\", num_examples, \".rds\"))\n}\n\nsample_captions <- all_captions[random_sample]\nsample_images <- all_img_names[random_sample]\ntrain_captions <- all_captions[train_indices]\ntrain_images <- all_img_names[train_indices]\nvalidation_captions <- all_captions[validation_indices]\nvalidation_images <- all_img_names[validation_indices]\n\n\nload_image <- function(image_path) {\n  img <- tf$read_file(image_path) %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize_images(c(299L, 299L)) %>%\n    tf$keras$applications$inception_v3$preprocess_input()\n  list(img, image_path)\n}\n\n\nimage_model <- application_inception_v3(include_top = FALSE,\n                                        weights = \"imagenet\")\n\nif (!saved_features_exist) {\n  preencode <- unique(sample_images) %>% unlist() %>% sort()\n  num_unique <- length(preencode)\n  \n  batch_size_4save <- 1\n  image_dataset <- tensor_slices_dataset(preencode) %>%\n    dataset_map(load_image) %>%\n    dataset_batch(batch_size_4save)\n  \n  save_iter <- make_iterator_one_shot(image_dataset)\n  save_count <- 0\n  \n  until_out_of_range({\n    if (save_count %% 100 == 0) {\n      cat(\"Saving feature:\", save_count, \"of\", num_unique, \"\\n\")\n    }\n    save_count <- save_count + batch_size_4save\n    batch_4save <- save_iter$get_next()\n    img <- batch_4save[[1]]\n    path <- batch_4save[[2]]\n    batch_features <- image_model(img)\n    batch_features <- tf$reshape(batch_features,\n                                 list(dim(batch_features)[1],-1L, dim(batch_features)[4]))\n    for (i in 1:dim(batch_features)[1]) {\n      p <- path[i]$numpy()$decode(\"utf-8\")\n      np$save(p,\n              batch_features[i, ,]$numpy())\n      \n    }\n    \n  })\n}\n\ntop_k <- 5000\ntokenizer <- text_tokenizer(num_words = top_k,\n                            oov_token = \"<unk>\",\n                            filters = '!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~ ')\ntokenizer$fit_on_texts(sample_captions)\ntrain_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(train_captions)\nvalidation_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(validation_captions)\ntokenizer$word_index\n\ntokenizer$word_index[\"<unk>\"]\n\ntokenizer$word_index[\"<pad>\"] <- 0\ntokenizer$word_index[\"<pad>\"]\n\nword_index_df <- data.frame(\n  word = tokenizer$word_index %>% names(),\n  index = tokenizer$word_index %>% unlist(use.names = FALSE),\n  stringsAsFactors = FALSE\n)\n\nword_index_df <- word_index_df %>% arrange(index)\n\ndecode_caption <- function(text) {\n  paste(map(text, function(number)\n    word_index_df %>%\n      filter(index == number) %>%\n      select(word) %>%\n      pull()),\n    collapse = \" \")\n}\n\ncaption_lengths <-\n  map(all_captions[1:num_examples], function(c)\n    str_split(c, \" \")[[1]] %>% length()) %>% unlist()\nfivenum(caption_lengths)\nmax_length <- fivenum(caption_lengths)[5]\n\ntrain_captions_padded <-\n  pad_sequences(\n    train_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\nvalidation_captions_padded <-\n  pad_sequences(\n    validation_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\n\nlength(train_images)\ndim(train_captions_padded)\n\nbatch_size <- 10\nbuffer_size <- num_examples\nembedding_dim <- 256\ngru_units <- 512\nvocab_size <- top_k\nfeatures_shape <- 2048\nattention_features_shape <- 64\n\ntrain_images_4checking <- train_images[c(4, 10, 30)]\ntrain_captions_4checking <- train_captions_padded[c(4, 10, 30),]\nvalidation_images_4checking <- validation_images[c(7, 10, 12)]\nvalidation_captions_4checking <-\n  validation_captions_padded[c(7, 10, 12),]\n\n\nmap_func <- function(img_name, cap) {\n  p <- paste0(img_name$decode(\"utf-8\"), \".npy\")\n  img_tensor <- np$load(p)\n  img_tensor <- tf$cast(img_tensor, tf$float32)\n  list(img_tensor, cap)\n}\n\ntrain_dataset <-\n  tensor_slices_dataset(list(train_images, train_captions_padded)) %>%\n  dataset_map(function(item1, item2)\n    tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))) %>%\n  # dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size) \n\n\ncnn_encoder <-\n  function(embedding_dim,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$fc <-\n        layer_dense(units = embedding_dim, activation = \"relu\")\n      \n      function(x, mask = NULL) {\n        # input shape: (batch_size, 64, features_shape)\n        # shape after fc: (batch_size, 64, embedding_dim)\n        maybecat(\"encoder input\", x)\n        x <- self$fc(x)\n        maybecat(\"encoder output\", x)\n        x\n      }\n    })\n  }\n\nattention_module <-\n  function(gru_units,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$W1 = layer_dense(units = gru_units)\n      self$W2 = layer_dense(units = gru_units)\n      self$V = layer_dense(units = 1)\n      \n      function(inputs, mask = NULL) {\n        features <- inputs[[1]]\n        hidden <- inputs[[2]]\n        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n        # hidden shape == (batch_size, gru_units)\n        # hidden_with_time_axis shape == (batch_size, 1, gru_units)\n        hidden_with_time_axis <- k_expand_dims(hidden, axis = 2)\n        \n        maybecat(\"attention module\", features)\n        maybecat(\"attention module\", hidden)\n        maybecat(\"attention module\", hidden_with_time_axis)\n        \n        # score shape == (batch_size, 64, 1)\n        score <-\n          self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))\n        # attention_weights shape == (batch_size, 64, 1)\n        attention_weights <- k_softmax(score, axis = 2)\n        # context_vector shape after sum == (batch_size, embedding_dim)\n        context_vector <-\n          k_sum(attention_weights * features, axis = 2)\n        \n        maybecat(\"attention module\", score)\n        maybecat(\"attention module\", attention_weights)\n        maybecat(\"attention module\", context_vector)\n        \n        list(context_vector, attention_weights)\n      }\n    })\n  }\n\nrnn_decoder <-\n  function(embedding_dim,\n           gru_units,\n           vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru_units <- gru_units\n      self$embedding <-\n        layer_embedding(input_dim = vocab_size, output_dim = embedding_dim)\n      self$gru <- if (tf$test$is_gpu_available()) {\n        layer_cudnn_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      } else {\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      }\n      \n      self$fc1 <- layer_dense(units = self$gru_units)\n      self$fc2 <- layer_dense(units = vocab_size)\n      \n      self$attention <- attention_module(self$gru_units)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        features <- inputs[[2]]\n        hidden <- inputs[[3]]\n        \n        maybecat(\"decoder\", x)\n        maybecat(\"decoder\", features)\n        maybecat(\"decoder\", hidden)\n        \n        c(context_vector, attention_weights) %<-% self$attention(list(features, hidden))\n        \n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x <- self$embedding(x)\n        \n        maybecat(\"decoder x after embedding\", x)\n        \n        # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)\n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x))\n        \n        maybecat(\"decoder x after concat\", x)\n        \n        # passing the concatenated vector to the GRU\n        c(output, state) %<-% self$gru(x)\n        \n        maybecat(\"decoder output after gru\", output)\n        maybecat(\"decoder state after gru\", state)\n        \n        # shape == (batch_size, 1, gru_units)\n        x <- self$fc1(output)\n        \n        maybecat(\"decoder output after fc1\", x)\n        \n        # x shape == (batch_size, gru_units)\n        x <- k_reshape(x, c(-1, dim(x)[[3]]))\n        \n        maybecat(\"decoder output after reshape\", x)\n        \n        # output shape == (batch_size, vocab_size)\n        x <- self$fc2(x)\n        \n        maybecat(\"decoder output after fc2\", x)\n        \n        list(x, state, attention_weights)\n        \n      }\n    })\n  }\n\n\nencoder <- cnn_encoder(embedding_dim)\ndecoder <- rnn_decoder(embedding_dim, gru_units, vocab_size)\n\noptimizer = tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- 1 - k_cast(y_true == 0L, dtype = \"float32\")\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true, logits =\n                                                     y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\nget_caption <-\n  function(image) {\n    attention_matrix <-\n      matrix(0, nrow = max_length, ncol = attention_features_shape)\n    # shape=(1, 299, 299, 3)\n    temp_input <- k_expand_dims(load_image(image)[[1]], 1)\n    # shape=(1, 8, 8, 2048),\n    img_tensor_val <- image_model(temp_input)\n    # shape=(1, 64, 2048)\n    img_tensor_val <- k_reshape(img_tensor_val,\n                                list(dim(img_tensor_val)[1],-1, dim(img_tensor_val)[4]))\n    # shape=(1, 64, 256)\n    features <- encoder(img_tensor_val)\n    \n    dec_hidden <- k_zeros(c(1, gru_units))\n    dec_input <-\n      k_expand_dims(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]))\n    \n    result <- \"\"\n    \n    for (t in seq_len(max_length - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, features, dec_hidden))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t, ] <- attention_weights %>% as.double()\n      \n      pred_idx = tf$multinomial(exp(preds), num_samples = 1)[1, 1] %>% as.double()\n      \n      pred_word <-\n        word_index_df[word_index_df$index == pred_idx, \"word\"]\n      \n      if (pred_word == \"<end>\") {\n        result <-\n          paste(result, pred_word)\n        attention_matrix <-\n          attention_matrix[1:length(str_split(result, \" \")[[1]]), , drop = FALSE]\n        return (list(str_trim(result), attention_matrix))\n      } else {\n        result <-\n          paste(result, pred_word)\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    \n    list(str_trim(result), attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           image_name,\n           result,\n           epoch) {\n    image <-\n      image_read(image_name) %>% image_scale(\"299x299!\")\n    result <- str_split(result, \" \")[[1]] %>% as.list()\n    # attention_matrix shape: nrow = max_length, ncol = attention_features_shape\n    for (i in 1:length(result)) {\n      att <- attention_matrix[i, ] %>% np$resize(tuple(8L, 8L))\n      dim(att) <- c(8, 8, 1)\n      att <- image_read(att) %>% image_scale(\"299x299\") %>%\n        image_annotate(\n          result[[i]],\n          gravity = \"northeast\",\n          size = 20,\n          color = \"white\",\n          location = \"+20+40\"\n        )\n      overlay <-\n        image_composite(att, image, operator = \"blend\", compose_args = \"30\")\n      image_write(\n        overlay,\n        paste0(\n          \"attention_plot_epoch_\",\n          epoch,\n          \"_img_\",\n          image_name %>% basename() %>% str_sub(16,-5),\n          \"_word_\",\n          i,\n          \".png\"\n        )\n      )\n    }\n  }\n\n\ncheck_sample_captions <-\n  function(epoch, mode, plot_attention) {\n    images <- switch(mode,\n                     training = train_images_4checking,\n                     validation = validation_images_4checking)\n    captions <- switch(mode,\n                       training = train_captions_4checking,\n                       validation = validation_captions_4checking)\n    cat(\"\\n\", \"Sample checks on \", mode, \" set:\", \"\\n\", sep = \"\")\n    for (i in 1:length(images)) {\n      c(result, attention_matrix) %<-% get_caption(images[[i]])\n      real_caption <-\n        decode_caption(captions[i,]) %>% str_remove_all(\" <pad>\")\n      cat(\"\\nReal caption:\",  real_caption, \"\\n\")\n      cat(\"\\nPredicted caption:\", result, \"\\n\")\n      if (plot_attention)\n        plot_attention(attention_matrix, images[[i]], result, epoch)\n    }\n    \n  }\n\ncheckpoint_dir <- \"./checkpoints_captions\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(optimizer = optimizer,\n                      encoder = encoder,\n                      decoder = decoder)\n\n\nif (restore_checkpoint) {\n  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n}\n\nnum_epochs <- 20\n\nif (!restore_checkpoint) {\n  for (epoch in seq_len(num_epochs)) {\n    cat(\"Starting epoch:\", epoch, \"\\n\")\n    total_loss <- 0\n    progress <- 0\n    train_iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      progress <- progress + 1\n      if (progress %% 10 == 0)\n        cat(\"-\")\n      \n      batch <- iterator_get_next(train_iter)\n      loss <- 0\n\n      img_tensor <- batch[[1]]\n      target_caption <- batch[[2]]\n      \n      dec_hidden <- k_zeros(c(batch_size, gru_units))\n      \n      dec_input <-\n        k_expand_dims(rep(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]), batch_size))\n      \n      with(tf$GradientTape() %as% tape, {\n        features <- encoder(img_tensor)\n        \n        for (t in seq_len(dim(target_caption)[2] - 1)) {\n          c(preds, dec_hidden, weights) %<-%\n            decoder(list(dec_input, features, dec_hidden))\n          loss <- loss + cx_loss(target_caption[, t], preds)\n          dec_input <- k_expand_dims(target_caption[, t])\n        }\n        \n      })\n      total_loss <-\n        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])\n      \n      variables <- c(encoder$variables, decoder$variables)\n      gradients <- tape$gradient(loss, variables)\n      \n      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),\n                                global_step = tf$train$get_or_create_global_step())\n    })\n    cat(paste0(\n      \"\\n\\nTotal loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n      \"\\n\"\n    ))\n    \n    \n    checkpoint$save(file_prefix = checkpoint_prefix)\n    \n    check_sample_captions(epoch, \"training\", plot_attention = FALSE)\n    check_sample_captions(epoch, \"validation\", plot_attention = FALSE)\n    \n  }\n}\n\n\nepoch <- num_epochs\ncheck_sample_captions(epoch, \"training\", plot_attention = TRUE)\ncheck_sample_captions(epoch, \"validation\", plot_attention = TRUE)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}