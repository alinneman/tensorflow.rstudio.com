{
  "hash": "88d3c0cf4d9fa5bfc8aaac89030453a3",
  "result": {
    "markdown": "---\ntitle: quora_siamese_lstm\ndescription: Classifying duplicate quesitons from Quora using Siamese Recurrent Architecture.\n---\n\nIn this tutorial we will use Keras to classify duplicated questions from Quora.\nThe dataset first appeared in the Kaggle competition \n[*Quora Question Pairs*](https://www.kaggle.com/c/quora-question-pairs).\nThe dataset consists of ~400k pairs of questions and a column indicating \nif the question pair is duplicated. \n\nOur implementation is inspired by the Siamese Recurrent Architecture, Mueller et al. [*Siamese recurrent architectures for learning sentence similarity*](https://dl.acm.org/citation.cfm?id=3016291), with small modifications like the similarity\nmeasure and the embedding layers (The original paper uses pre-trained word vectors). Using this kind\nof architecture dates back to 2005 with [Le Cun et al](https://dl.acm.org/citation.cfm?id=1068961) and is usefull for\nverification tasks. The idea is to learn a function that maps input patterns into a\ntarget space such that a similarity measure in the target space approximates\nthe “semantic” distance in the input space. \n\nAfter the competition, Quora also described their approach to this problem in \nthis [blog post](https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(keras)\nlibrary(purrr)\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 0.0001),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\n# Downloading Data --------------------------------------------------------\n\nquora_data <- get_file(\n  \"quora_duplicate_questions.tsv\",\n  \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n)\n\n\n# Pre-processing ----------------------------------------------------------\n\ndf <- read_tsv(quora_data)\n\ntokenizer <- text_tokenizer(num_words = FLAGS$vocab_size)\nfit_text_tokenizer(tokenizer, x = c(df$question1, df$question2))\n\nquestion1 <- texts_to_sequences(tokenizer, df$question1)\nquestion2 <- texts_to_sequences(tokenizer, df$question2)\n\nquestion1 <- pad_sequences(question1, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\nquestion2 <- pad_sequences(question2, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\n\n\n# Model Definition --------------------------------------------------------\n\ninput1 <- layer_input(shape = c(FLAGS$max_len_padding))\ninput2 <- layer_input(shape = c(FLAGS$max_len_padding))\n\nembedding <- layer_embedding(\n  input_dim = FLAGS$vocab_size + 2, \n  output_dim = FLAGS$embedding_size, \n  input_length = FLAGS$max_len_padding, \n  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\nseq_emb <- layer_lstm(\n  units = FLAGS$seq_embedding_size, \n  recurrent_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\n\nvector1 <- embedding(input1) %>%\n  seq_emb()\nvector2 <- embedding(input2) %>%\n  seq_emb()\n\nout <- layer_dot(list(vector1, vector2), axes = 1) %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel <- keras_model(list(input1, input2), out)\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"binary_crossentropy\", \n  metrics = list(\n    acc = metric_binary_accuracy\n  )\n)\n\n# Model Fitting -----------------------------------------------------------\n\nset.seed(1817328)\nval_sample <- sample.int(nrow(question1), size = 0.1*nrow(question1))\n\nmodel %>%\n  fit(\n    list(question1[-val_sample,], question2[-val_sample,]),\n    df$is_duplicate[-val_sample], \n    batch_size = 128, \n    epochs = 30, \n    validation_data = list(\n      list(question1[val_sample,], question2[val_sample,]), df$is_duplicate[val_sample]\n    ),\n    callbacks = list(\n      callback_early_stopping(patience = 5),\n      callback_reduce_lr_on_plateau(patience = 3)\n    )\n  )\n\nsave_model_hdf5(model, \"model-question-pairs.hdf5\", include_optimizer = TRUE)\nsave_text_tokenizer(tokenizer, \"tokenizer-question-pairs.hdf5\")\n\n\n# Prediction --------------------------------------------------------------\n# In a fresh R session:\n# Load model and tokenizer -\n\nmodel <- load_model_hdf5(\"model-question-pairs.hdf5\", compile = FALSE)\ntokenizer <- load_text_tokenizer(\"tokenizer-question-pairs.hdf5\")\n\n\npredict_question_pairs <- function(model, tokenizer, q1, q2) {\n  \n  q1 <- texts_to_sequences(tokenizer, list(q1))\n  q2 <- texts_to_sequences(tokenizer, list(q2))\n  \n  q1 <- pad_sequences(q1, 20)\n  q2 <- pad_sequences(q2, 20)\n  \n  as.numeric(predict(model, list(q1, q2)))\n}\n\n# Getting predictions\n\npredict_question_pairs(\n  model, tokenizer, \n  q1 = \"What is the main benefit of Quora?\",\n  q2 = \"What are the advantages of using Quora?\"\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}