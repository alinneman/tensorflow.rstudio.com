{
  "hash": "664a364a44942f56904195e20aa49d91",
  "result": {
    "markdown": "---\ntitle: DCGAN to generate face images\nauthors: \n  - \"[fchollet](https://twitter.com/fchollet)\"\n  - \"[dfalbel](https://github.com/dfalbel) - R translation\"\ndescription: A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images.\ncategories: [generative]\nexecute:\n  eval: false\neditor_options: \n  chunk_output_type: inline\naliases:\n  - ../guide/keras/examples/eager_dcgan/index.html\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nstop(\"STOP\")\n```\n:::\n\n\n## Prepare CelebA data\n\nWe'll use face images from the CelebA dataset, resized to 64x64.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_path <- fs::path(\"~/datasets/celeba_gan\") \n# output <- \"celeba_gan/\"\nif (!fs::dir_exists(dataset_path)) {\n  fs::dir_create(fs::path_dir(dataset_path))\n  url <- \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n  reticulate::import(\"gdown\")$download(url, output, quiet = TRUE)\n  unzip(output, exdir = fs::path_dir(output))\n}\n\ndataset_path <- fs::path(fs::path_dir(output), \"img_align_celeba\")\n```\n:::\n\n\nCreate a dataset from our folder:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- image_dataset_from_directory(\n  dataset_path, \n  image_size = c(64, 64),\n  label_mode = NULL, \n  batch_size = 32\n)\ndataset <- dataset$apply(tf$data$experimental$ignore_errors(\n    log_warning=FALSE\n))\n```\n:::\n\n\nLet's display a sample image:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  as.array() %>% \n  {.[1,,,]} %>% \n  as.raster(max = 255) %>% \n  plot()\n```\n:::\n\n\n## Create the discriminator\n\nIt maps a 64x64 image to a binary classification score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscriminator <- keras_model_sequential(name = \"discriminator\", input_shape = shape(64, 64, 3)) %>% \n  layer_conv_2d(64, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_flatten() %>% \n  layer_dropout(0.2) %>% \n  layer_dense(1, activation = \"sigmoid\")\nsummary(discriminator)\n```\n:::\n\n\n## Create the generator\n\nIt mirrors the discriminator, replacing `conv_2d` layers with\n`conv_2d_transpose` layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlatent_dim <- 128L\n\ngenerator <- keras_model_sequential(input_shape = shape(latent_dim), name = \"generator\") %>% \n  layer_dense(8 * 8 * 128) %>% \n  layer_reshape(shape(8, 8, 128)) %>% \n  layer_conv_2d_transpose(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d_transpose(256, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d_transpose(512, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(3, kernel_size = 5, padding = \"same\", activation = \"sigmoid\")\n\nsummary(generator)\n```\n:::\n\n\n## Override `train_step`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngan <- new_model_class(\n  \"gan\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super()$`__init__`()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- latent_dim\n    self$rescale <- layer_rescaling(scale = 1/255)\n  },\n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super()$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n    self$d_loss_metric <- tf$keras$metrics$Mean(name = \"d_loss\")\n    self$g_loss_metric <- keras$metrics$Mean(name = \"g_loss\")\n  },\n  metrics = mark_active(function() {\n    list(self$d_loss_metric, self$g_loss_metric)\n  }),\n  train_step = function(real_images) {\n    real_images <- self$rescale(real_images)\n    \n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <- tf$random$normal(\n      shape = reticulate::tuple(batch_size, self$latent_dim)\n    )\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <- tf$concat(list(generated_images, real_images), axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <- tf$concat(\n      list(\n        tf$ones(reticulate::tuple(batch_size, 1L)), \n        tf$zeros(reticulate::tuple(batch_size, 1L))\n      ), \n      axis = 0L\n    )\n    # Add random noise to the labels - important trick!\n    labels <- labels + 0.05 * tf$random$uniform(tf$shape(labels))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {   \n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    \n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights)\n    )\n    \n    # Sample random points in the latent space\n    random_latent_vectors <- tf$random$normal(\n      shape = reticulate::tuple(batch_size, self$latent_dim)\n    )\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(reticulate::tuple(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {   \n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(zip_lists(grads, self$generator$trainable_weights))\n    \n    # Update metrics\n    self$d_loss_metric$update_state(d_loss)\n    self$g_loss_metric$update_state(g_loss)\n    list(\n      \"d_loss\" = self$d_loss_metric$result(),\n      \"g_loss\" = self$g_loss_metric$result()\n    )\n  }\n)\n```\n:::\n\n\n## Create a callback that periodically saves generated images\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngan_monitor <- new_callback_class(\n  \"gan_monitor\",\n  initialize = function(num_img = 3, latent_dim = 128L) {\n    self$num_img <- num_img\n    self$latent_dim <- as.integer(latent_dim)\n    if (!fs::dir_exists(\"dcgan\")) fs::dir_create(\"dcgan\")\n  },\n  on_epoch_end = function(epoch, logs) {\n    random_latent_vectors <- tf$random$normal(shape = shape(self$num_img, self$latent_dim))\n    generated_images <- self$model$generator(random_latent_vectors)\n    generated_images <- tf$clip_by_value(generated_images * 255, 0, 255)\n    generated_images <- as.array(generated_images)\n    for (i in seq_len(self$num_img)) {\n      image_array_save(\n        generated_images[i,,,], \n        sprintf(\"dcgan/generated_img_%03d_%d.png\", epoch, i),\n        scale = FALSE\n      )\n    }\n  }\n)\n```\n:::\n\n\n## Train the end-to-end model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 15  # In practice, use ~100 epochs\n\ngan <- gan(discriminator = discriminator, generator = generator, latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 1e-4),\n  g_optimizer = optimizer_adam(learning_rate = 1e-4),\n  loss_fn = loss_binary_crossentropy(),\n)\n\ngan %>% fit(\n  dataset, \n  epochs = epochs, \n  callbacks = list(\n    gan_monitor(num_img = 10, latent_dim = latent_dim)\n  )\n)\n```\n:::\n\n\nSome of the last generated images around epoch 15 - each row is an\nepoch. (results keep improving after that):\n\n\n::: {.cell layout-nrow=\"15\" layout-ncol=\"10\"}\n\n```{.r .cell-code}\ngrid <- expand.grid(1:10, 0:14)\nknitr::include_graphics(sprintf(\"dcgan/generated_img_%03d_%d.png\", grid[[2]], grid[[1]]))\n```\n:::\n",
    "supporting": [
      "dcgan_overriding_train_step_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}