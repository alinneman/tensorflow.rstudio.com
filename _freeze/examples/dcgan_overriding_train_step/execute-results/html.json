{
  "hash": "9df40c7849c961f3b34e80d55a523814",
  "result": {
    "markdown": "---\ntitle: DCGAN to generate face images\nauthros: \n  - \"[fchollet](https://twitter.com/fchollet)\"\n  - \"[dfalbel](https://github.com/dfalbel) - R translation\"\ndate-created: 2019/04/29\ndate-last-modified: 2021/01/01\ndescription: A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images.\ncategories: [generative]\neditor_options: \n  chunk_output_type: inline\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Prepare CelebA data\n\nWe'll use face images from the CelebA dataset, resized to 64x64.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutput <- \"celeba_gan/data.zip\"\nif (!fs::dir_exists(\"celeba_gan\")) {\n  fs::dir_create(\"celeba_gan\")\n  url <- \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n  reticulate::import(\"gdown\")$download(url, output, quiet=TRUE)\n  unzip(output, exdir = fs::path_dir(output))\n}\n\ndataset_path <- fs::path(fs::path_dir(output), \"img_align_celeba\")\n```\n:::\n\n\nCreate a dataset from our folder:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- image_dataset_from_directory(\n  dataset_path, \n  image_size = c(64, 64),\n  label_mode = NULL, \n  batch_size = 32\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\ndataset <- dataset$apply(tf$data$experimental$ignore_errors(\n    log_warning=FALSE\n))\n```\n:::\n\n\nLet's display a sample image:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  as.array() %>% \n  {.[1,,,]} %>% \n  as.raster(max = 255) %>% \n  plot()\n```\n\n::: {.cell-output-display}\n![](dcgan_overriding_train_step_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Create the discriminator\n\nIt maps a 64x64 image to a binary classification score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscriminator <- keras_model_sequential(name = \"discriminator\", input_shape = shape(64, 64, 3)) %>% \n  layer_conv_2d(64, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_flatten() %>% \n  layer_dropout(0.2) %>% \n  layer_dense(1, activation = \"sigmoid\")\nsummary(discriminator)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"discriminator\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_2 (Conv2D)                  (None, 32, 32, 64)              3136        \n leaky_re_lu_2 (LeakyReLU)          (None, 32, 32, 64)              0           \n conv2d_1 (Conv2D)                  (None, 16, 16, 128)             131200      \n leaky_re_lu_1 (LeakyReLU)          (None, 16, 16, 128)             0           \n conv2d (Conv2D)                    (None, 8, 8, 128)               262272      \n leaky_re_lu (LeakyReLU)            (None, 8, 8, 128)               0           \n flatten (Flatten)                  (None, 8192)                    0           \n dropout (Dropout)                  (None, 8192)                    0           \n dense (Dense)                      (None, 1)                       8193        \n================================================================================\nTotal params: 404,801\nTrainable params: 404,801\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Create the generator\n\nIt mirrors the discriminator, replacing `conv_2d` layers with `conv_2d_transpose` layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlatent_dim <- 128L\n\ngenerator <- keras_model_sequential(input_shape = shape(latent_dim), name = \"generator\") %>% \n  layer_dense(8 * 8 * 128) %>% \n  layer_reshape(shape(8, 8, 128)) %>% \n  layer_conv_2d_transpose(128, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d_transpose(256, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d_transpose(512, kernel_size = 4, strides = 2, padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(3, kernel_size = 5, padding = \"same\", activation = \"sigmoid\")\n\nsummary(generator)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"generator\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 8192)                    1056768     \n reshape (Reshape)                  (None, 8, 8, 128)               0           \n conv2d_transpose_2 (Conv2DTranspos  (None, 16, 16, 128)            262272      \n e)                                                                             \n leaky_re_lu_5 (LeakyReLU)          (None, 16, 16, 128)             0           \n conv2d_transpose_1 (Conv2DTranspos  (None, 32, 32, 256)            524544      \n e)                                                                             \n leaky_re_lu_4 (LeakyReLU)          (None, 32, 32, 256)             0           \n conv2d_transpose (Conv2DTranspose)  (None, 64, 64, 512)            2097664     \n leaky_re_lu_3 (LeakyReLU)          (None, 64, 64, 512)             0           \n conv2d_3 (Conv2D)                  (None, 64, 64, 3)               38403       \n================================================================================\nTotal params: 3,979,651\nTrainable params: 3,979,651\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Override `train_step`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngan <- new_model_class(\n  \"gan\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super()$`__init__`()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- latent_dim\n    self$rescale <- layer_rescaling(scale = 1/255)\n  },\n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super()$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n    self$d_loss_metric <- tf$keras$metrics$Mean(name = \"d_loss\")\n    self$g_loss_metric <- keras$metrics$Mean(name = \"g_loss\")\n  },\n  metrics = mark_active(function() {\n    list(self$d_loss_metric, self$g_loss_metric)\n  }),\n  train_step = function(real_images) {\n    real_images <- self$rescale(real_images)\n    \n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <- tf$random$normal(\n      shape = reticulate::tuple(batch_size, self$latent_dim)\n    )\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <- tf$concat(list(generated_images, real_images), axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <- tf$concat(\n      list(\n        tf$ones(reticulate::tuple(batch_size, 1L)), \n        tf$zeros(reticulate::tuple(batch_size, 1L))\n      ), \n      axis = 0L\n    )\n    # Add random noise to the labels - important trick!\n    labels <- labels + 0.05 * tf$random$uniform(tf$shape(labels))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {   \n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    \n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights)\n    )\n    \n    # Sample random points in the latent space\n    random_latent_vectors <- tf$random$normal(\n      shape = reticulate::tuple(batch_size, self$latent_dim)\n    )\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(reticulate::tuple(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {   \n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(zip_lists(grads, self$generator$trainable_weights))\n    \n    # Update metrics\n    self$d_loss_metric$update_state(d_loss)\n    self$g_loss_metric$update_state(g_loss)\n    list(\n      \"d_loss\" = self$d_loss_metric$result(),\n      \"g_loss\" = self$g_loss_metric$result()\n    )\n  }\n)\n```\n:::\n\n\n## Create a callback that periodically saves generated images\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngan_monitor <- new_callback_class(\n  \"gan_monitor\",\n  initialize = function(num_img = 3, latent_dim = 128L) {\n    self$num_img <- num_img\n    self$latent_dim <- as.integer(latent_dim)\n    if (!fs::dir_exists(\"dcgan\")) fs::dir_create(\"dcgan\")\n  },\n  on_epoch_end = function(epoch, logs) {\n    random_latent_vectors <- tf$random$normal(shape = shape(self$num_img, self$latent_dim))\n    generated_images <- self$model$generator(random_latent_vectors)\n    generated_images <- tf$clip_by_value(generated_images * 255, 0, 255)\n    generated_images <- as.array(generated_images)\n    for (i in seq_len(self$num_img)) {\n      image_array_save(\n        generated_images[i,,,], \n        sprintf(\"dcgan/generated_img_%03d_%d.png\", epoch, i),\n        scale = FALSE\n      )\n    }\n  }\n)\n```\n:::\n\n\n## Train the end-to-end model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 15  # In practice, use ~100 epochs\n\ngan <- gan(discriminator = discriminator, generator = generator, latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 1e-4),\n  g_optimizer = optimizer_adam(learning_rate = 1e-4),\n  loss_fn = loss_binary_crossentropy(),\n)\n\ngan %>% fit(\n  dataset, \n  epochs = epochs, \n  callbacks = list(\n    gan_monitor(num_img = 10, latent_dim = latent_dim)\n  )\n)\n```\n:::\n\n\nSome of the last generated images around epoch 15 - each row is an epoch.\n(results keep improving after that):\n\n\n::: {.cell layout-nrow=\"15\" layout-ncol=\"10\"}\n\n```{.r .cell-code}\ngrid <- expand.grid(1:10, 0:14)\nknitr::include_graphics(sprintf(\"dcgan/generated_img_%03d_%d.png\", grid[[2]], grid[[1]]))\n```\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_000_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_001_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_002_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_003_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_004_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_005_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_006_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_007_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_008_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_009_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_010_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_011_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_012_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_013_10.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_1.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_2.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_3.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_4.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_5.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_6.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_7.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_8.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_9.png){width=32}\n:::\n\n::: {.cell-output-display}\n![](dcgan/generated_img_014_10.png){width=32}\n:::\n:::\n",
    "supporting": [
      "dcgan_overriding_train_step_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}