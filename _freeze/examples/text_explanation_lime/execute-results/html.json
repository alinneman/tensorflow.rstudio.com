{
  "hash": "d4350eedc9774efe3b0f68a4df2309be",
  "result": {
    "markdown": "---\ntitle: text_explanation_lime\ndescription: How to use lime to explain text data.\n---\n\nThis example shows how to use lime to explain text data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(keras)\nlibrary(tidyverse)\n\n# Download and unzip data\n\nactivity_url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip\"\ntemp <- tempfile()\ndownload.file(activity_url, temp)\nunzip(temp, \"drugLibTest_raw.tsv\")\n\n\n# Read dataset\n\ndf <- read_delim('drugLibTest_raw.tsv',delim = '\\t')\nunlink(temp)\n\n# Select only rating and text from the whole dataset\n\ndf = df %>% select(rating,commentsReview) %>% mutate(rating = if_else(rating >= 8, 0, 1))\n\n# This is our text\ntext <- df$commentsReview\n\n# And these are ratings given by customers\ny_train <- df$rating\n\n\n# text_tokenizer helps us to turn each word into integers. By selecting maximum number of features\n# we also keep the most frequent words. Additionally, by default, all punctuation is removed.\n\nmax_features <- 1000\ntokenizer <- text_tokenizer(num_words = max_features)\n\n# Then, we need to fit the tokenizer object to our text data\n\ntokenizer %>% fit_text_tokenizer(text)\n\n# Via tokenizer object you can check word indices, word counts and other interesting properties.\n\ntokenizer$word_counts \ntokenizer$word_index\n\n# Finally, we can replace words in dataset with integers\ntext_seqs <- texts_to_sequences(tokenizer, text)\n\ntext_seqs %>% head(3)\n\n# Define the parameters of the keras model\n\nmaxlen <- 15\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 64\nkernel_size <- 3\nhidden_dims <- 50\nepochs <- 15\n\n# As a final step, restrict the maximum length of all sequences and create a matrix as input for model\nx_train <- text_seqs %>% pad_sequences(maxlen = maxlen)\n\n# Lets print the first 2 rows and see that max length of first 2 sequences equals to 15\nx_train[1:2,]\n\n# Create a model\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  layer_global_max_pooling_1d() %>%\n  layer_dense(hidden_dims) %>%\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Run\nhist <- model %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split = 0.1\n  )\n\n# Understanding lime for Keras Embedding Layers\n\n# In order to explain a text with LIME, we should write a preprocess function\n# which will help to turn words into integers. Therefore, above mentioned steps \n# (how to encode a text) should be repeated BUT within a function. \n# As we already have had a tokenizer object, we can apply the same object to train/test or a new text.\n\nget_embedding_explanation <- function(text) {\n  \n  tokenizer %>% fit_text_tokenizer(text)\n  \n  text_to_seq <- texts_to_sequences(tokenizer, text)\n  sentences <- text_to_seq %>% pad_sequences(maxlen = maxlen)\n}\n\n\nlibrary(lime)\n\n# Lets choose some text (3 rows) to explain\nsentence_to_explain <- train_sentences$text[15:17]\nsentence_to_explain\n\n# You could notice that our input is just a plain text. Unlike tabular data, lime function \n# for text classification requires a preprocess fuction. Because it will help to convert a text to integers \n# with provided function. \nexplainer <- lime(sentence_to_explain, model = model, preprocess = get_embedding_explanation)\n\n# Get explanation for the first 10 words\nexplanation <- explain(sentence_to_explain, explainer, n_labels = 1, n_features = 10,n_permutations = 1e4)\n\n\n# Different graphical ways to show the same information\n\nplot_text_explanations(explanation)\n\nplot_features(explanation)\n\ninteractive_text_explanations(explainer)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}