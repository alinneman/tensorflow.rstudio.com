{
  "hash": "f5b83329907da141c703d78411684e19",
  "result": {
    "markdown": "---\ntitle: eager_styletransfer\ndescription: Neural style transfer with eager execution.\n---\n\nThis is the companion code to the post \n\"Neural style transfer with eager execution and Keras\"\non the TensorFlow for R blog.\n\nhttps://blogs.rstudio.com/tensorflow/posts/2018-09-09-eager-style-transfer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nuse_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(purrr)\nlibrary(glue)\n\nimg_shape <- c(128, 128, 3)\ncontent_path <- \"isar.jpg\"\nstyle_path <- \"The_Great_Wave_off_Kanagawa.jpg\"\n\n\nnum_iterations <- 2000\ncontent_weight <- 100\nstyle_weight <- 0.8\ntotal_variation_weight <- 0.01\n\ncontent_image <-\n  image_load(content_path, target_size = img_shape[1:2])\ncontent_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\nstyle_image <-\n  image_load(style_path, target_size = img_shape[1:2])\nstyle_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\n\nload_and_process_image <- function(path) {\n  img <- image_load(path, target_size = img_shape[1:2]) %>%\n    image_to_array() %>%\n    k_expand_dims(axis = 1) %>%\n    imagenet_preprocess_input()\n}\n\ndeprocess_image <- function(x) {\n  x <- x[1, , ,]\n  # Remove zero-center by mean pixel\n  x[, , 1] <- x[, , 1] + 103.939\n  x[, , 2] <- x[, , 2] + 116.779\n  x[, , 3] <- x[, , 3] + 123.68\n  # 'BGR'->'RGB'\n  x <- x[, , c(3, 2, 1)]\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x) / 255\n  x\n}\n\ncontent_layers <- c(\"block5_conv2\")\nstyle_layers = c(\"block1_conv1\",\n                 \"block2_conv1\",\n                 \"block3_conv1\",\n                 \"block4_conv1\",\n                 \"block5_conv1\")\nnum_content_layers <- length(content_layers)\nnum_style_layers <- length(style_layers)\n\nget_model <- function() {\n  vgg <- application_vgg19(include_top = FALSE, weights = \"imagenet\")\n  vgg$trainable <- FALSE\n  style_outputs <-\n    map(style_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  content_outputs <-\n    map(content_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  model_outputs <- c(style_outputs, content_outputs)\n  keras_model(vgg$input, model_outputs)\n}\n\ncontent_loss <- function(content_image, target) {\n  k_sum(k_square(target - content_image))\n}\n\ngram_matrix <- function(x) {\n  features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))\n  gram <- k_dot(features, k_transpose(features))\n  gram\n}\n\nstyle_loss <- function(gram_target, combination) {\n  gram_comb <- gram_matrix(combination)\n  k_sum(k_square(gram_target - gram_comb)) / (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^\n                                                2)\n}\n\ntotal_variation_loss <- function(image) {\n  y_ij  <- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]\n  y_i1j <- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]\n  y_ij1 <- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]\n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\nget_feature_representations <-\n  function(model, content_path, style_path) {\n    # dim == (1, 128, 128, 3)\n    style_image <-\n      load_and_process_image(style_path) %>% k_cast(\"float32\")\n    # dim == (1, 128, 128, 3)\n    content_image <-\n      load_and_process_image(content_path) %>% k_cast(\"float32\")\n    # dim == (2, 128, 128, 3)\n    stack_images <-\n      k_concatenate(list(style_image, content_image), axis = 1)\n    # length(model_outputs) == 6\n    # dim(model_outputs[[1]]) = (2, 128, 128, 64)\n    # dim(model_outputs[[6]]) = (2, 8, 8, 512)\n    model_outputs <- model(stack_images)\n    style_features <- model_outputs[1:num_style_layers] %>%\n      map(function(batch)\n        batch[1, , , ])\n    content_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %>%\n      map(function(batch)\n        batch[2, , , ])\n    list(style_features, content_features)\n  }\n\ncompute_loss <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    c(style_weight, content_weight) %<-% loss_weights\n    model_outputs <- model(init_image)\n    style_output_features <- model_outputs[1:num_style_layers]\n    content_output_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]\n    \n    weight_per_style_layer <- 1 / num_style_layers\n    style_score <- 0\n    # str(style_zip, max.level = 1)\n    # dim(style_zip[[5]][[1]]) == (512, 512)\n    style_zip <-\n      transpose(list(gram_style_features, style_output_features))\n    for (l in 1:length(style_zip)) {\n      # for l == 1:\n      # dim(target_style) == (64, 64)\n      # dim(comb_style) == (1, 128, 128, 64)\n      c(target_style, comb_style) %<-% style_zip[[l]]\n      style_score <-\n        style_score + weight_per_style_layer * style_loss(target_style, comb_style[1, , , ])\n    }\n    \n    weight_per_content_layer <- 1 / num_content_layers\n    content_score <- 0\n    content_zip <-\n      transpose(list(content_features, content_output_features))\n    for (l in 1:length(content_zip)) {\n      # dim(comb_content) ==  (1, 8, 8, 512)\n      # dim(target_content) == (8, 8, 512)\n      c(target_content, comb_content) %<-% content_zip[[l]]\n      content_score <-\n        content_score + weight_per_content_layer * content_loss(comb_content[1, , , ], target_content)\n    }\n    \n    variation_loss <- total_variation_loss(init_image[1, , ,])\n    style_score <- style_score * style_weight\n    content_score <- content_score * content_weight\n    variation_score <- variation_loss * total_variation_weight\n    \n    loss <- style_score + content_score + variation_score\n    list(loss, style_score, content_score, variation_score)\n  }\n\ncompute_grads <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    with(tf$GradientTape() %as% tape, {\n      scores <-\n        compute_loss(model,\n                     loss_weights,\n                     init_image,\n                     gram_style_features,\n                     content_features)\n    })\n    total_loss <- scores[[1]]\n    list(tape$gradient(total_loss, init_image), scores)\n  }\n\nrun_style_transfer <- function(content_path,\n                               style_path) {\n  model <- get_model()\n  walk(model$layers, function(layer)\n    layer$trainable = FALSE)\n  \n  c(style_features, content_features) %<-% get_feature_representations(model, content_path, style_path)\n  # dim(gram_style_features[[1]]) == (64, 64)\n  # we compute this once, in advance\n  gram_style_features <-\n    map(style_features, function(feature)\n      gram_matrix(feature))\n  \n  init_image <- load_and_process_image(content_path)\n  init_image <-\n    tf$contrib$eager$Variable(init_image, dtype = \"float32\")\n  \n  optimizer <-\n    tf$train$AdamOptimizer(learning_rate = 1,\n                           beta1 = 0.99,\n                           epsilon = 1e-1)\n  \n  c(best_loss, best_image) %<-% list(Inf, NULL)\n  loss_weights <- list(style_weight, content_weight)\n  \n  start_time <- Sys.time()\n  global_start <- Sys.time()\n  \n  norm_means <- c(103.939, 116.779, 123.68)\n  min_vals <- -norm_means\n  max_vals <- 255 - norm_means\n  \n  for (i in seq_len(num_iterations)) {\n    # dim(grads) == (1, 128, 128, 3)\n    c(grads, all_losses) %<-% compute_grads(model,\n                                            loss_weights,\n                                            init_image,\n                                            gram_style_features,\n                                            content_features)\n    c(loss, style_score, content_score, variation_score) %<-% all_losses\n    optimizer$apply_gradients(list(tuple(grads, init_image)))\n    clipped <- tf$clip_by_value(init_image, min_vals, max_vals)\n    init_image$assign(clipped)\n    \n    end_time <- Sys.time()\n    \n    if (k_cast_to_floatx(loss) < best_loss) {\n      best_loss <- k_cast_to_floatx(loss)\n      best_image <- init_image\n    }\n    \n    if (i %% 50 == 0) {\n      glue(\"Iteration: {i}\") %>% print()\n      glue(\n        \"Total loss: {k_cast_to_floatx(loss)}, style loss: {k_cast_to_floatx(style_score)},\n        content loss: {k_cast_to_floatx(content_score)}, total variation loss: {k_cast_to_floatx(variation_score)},\n        time for 1 iteration: {(Sys.time() - start_time) %>% round(2)}\"\n      ) %>% print()\n      \n      if (i %% 100 == 0) {\n        png(paste0(\"style_epoch_\", i, \".png\"))\n        plot_image <- best_image$numpy()\n        plot_image <- deprocess_image(plot_image)\n        plot(as.raster(plot_image), main = glue(\"Iteration {i}\"))\n        dev.off()\n      }\n    }\n  }\n  \n  glue(\"Total time: {Sys.time() - global_start} seconds\") %>% print()\n  list(best_image, best_loss)\n}\n\nc(best_image, best_loss) %<-% run_style_transfer(content_path, style_path)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}