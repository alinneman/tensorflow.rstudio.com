{
  "hash": "79ffba83000f662dec01e3db2e3fdfa8",
  "result": {
    "markdown": "---\ntitle: variational_autoencoder_deconv\ndescription: Demonstrates how to build a variational autoencoder with Keras using deconvolution layers.\n---\n\nThis script demonstrates how to build a variational autoencoder with Keras\nand deconvolution layers.\nReference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nK <- keras::backend()\n\n#### Parameterization ####\n\n# input image dimensions\nimg_rows <- 28L\nimg_cols <- 28L\n# color channels (1 = grayscale, 3 = RGB)\nimg_chns <- 1L\n\n# number of convolutional filters to use\nfilters <- 64L\n\n# convolution kernel size\nnum_conv <- 3L\n\nlatent_dim <- 2L\nintermediate_dim <- 128L\nepsilon_std <- 1.0\n\n# training parameters\nbatch_size <- 100L\nepochs <- 5L\n\n\n#### Model Construction ####\n\noriginal_img_size <- c(img_rows, img_cols, img_chns)\n\nx <- layer_input(shape = c(original_img_size))\n\nconv_1 <- layer_conv_2d(\n  x,\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_2 <- layer_conv_2d(\n  conv_1,\n  filters = filters,\n  kernel_size = c(2L, 2L),\n  strides = c(2L, 2L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_3 <- layer_conv_2d(\n  conv_2,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_4 <- layer_conv_2d(\n  conv_3,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nflat <- layer_flatten(conv_4)\nhidden <- layer_dense(flat, units = intermediate_dim, activation = \"relu\")\n\nz_mean <- layer_dense(hidden, units = latent_dim)\nz_log_var <- layer_dense(hidden, units = latent_dim)\n\nsampling <- function(args) {\n  z_mean <- args[, 1:(latent_dim)]\n  z_log_var <- args[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]),\n    mean = 0.,\n    stddev = epsilon_std\n  )\n  z_mean + k_exp(z_log_var) * epsilon\n}\n\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% layer_lambda(sampling)\n\noutput_shape <- c(batch_size, 14L, 14L, filters)\n\ndecoder_hidden <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_upsample <- layer_dense(units = prod(output_shape[-1]), activation = \"relu\")\n\ndecoder_reshape <- layer_reshape(target_shape = output_shape[-1])\ndecoder_deconv_1 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_2 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_3_upsample <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(3L, 3L),\n  strides = c(2L, 2L),\n  padding = \"valid\",\n  activation = \"relu\"\n)\n\ndecoder_mean_squash <- layer_conv_2d(\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"valid\",\n  activation = \"sigmoid\"\n)\n\nhidden_decoded <- decoder_hidden(z)\nup_decoded <- decoder_upsample(hidden_decoded)\nreshape_decoded <- decoder_reshape(up_decoded)\ndeconv_1_decoded <- decoder_deconv_1(reshape_decoded)\ndeconv_2_decoded <- decoder_deconv_2(deconv_1_decoded)\nx_decoded_relu <- decoder_deconv_3_upsample(deconv_2_decoded)\nx_decoded_mean_squash <- decoder_mean_squash(x_decoded_relu)\n\n# custom loss function\nvae_loss <- function(x, x_decoded_mean_squash) {\n  x <- k_flatten(x)\n  x_decoded_mean_squash <- k_flatten(x_decoded_mean_squash)\n  xent_loss <- 1.0 * img_rows * img_cols *\n    loss_binary_crossentropy(x, x_decoded_mean_squash)\n  kl_loss <- -0.5 * k_mean(1 + z_log_var - k_square(z_mean) -\n                           k_exp(z_log_var), axis = -1L)\n  k_mean(xent_loss + kl_loss)\n}\n\n## variational autoencoder\nvae <- keras_model(x, x_decoded_mean_squash)\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\nsummary(vae)\n\n## encoder: model to project inputs on the latent space\nencoder <- keras_model(x, z_mean)\n\n## build a digit generator that can sample from the learned distribution\ngen_decoder_input <- layer_input(shape = latent_dim)\ngen_hidden_decoded <- decoder_hidden(gen_decoder_input)\ngen_up_decoded <- decoder_upsample(gen_hidden_decoded)\ngen_reshape_decoded <- decoder_reshape(gen_up_decoded)\ngen_deconv_1_decoded <- decoder_deconv_1(gen_reshape_decoded)\ngen_deconv_2_decoded <- decoder_deconv_2(gen_deconv_1_decoded)\ngen_x_decoded_relu <- decoder_deconv_3_upsample(gen_deconv_2_decoded)\ngen_x_decoded_mean_squash <- decoder_mean_squash(gen_x_decoded_relu)\ngenerator <- keras_model(gen_decoder_input, gen_x_decoded_mean_squash)\n\n\n#### Data Preparation ####\n\nmnist <- dataset_mnist()\ndata <- lapply(mnist, function(m) {\n  array_reshape(m$x / 255, dim = c(dim(m$x)[1], original_img_size))\n})\nx_train <- data$train\nx_test <- data$test\n\n\n#### Model Fitting ####\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n#### Visualizations ####\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n## display a 2D plot of the digit classes in the latent space\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\nx_test_encoded %>%\n  as_data_frame() %>%\n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n## display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = digit_size))\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}