{
  "hash": "be188b57fcdcac5a7a9ec4ea5d5d46ab",
  "result": {
    "markdown": "---\ntitle: eager_pix2pix\ndescription: Image-to-image translation with Pix2Pix, using eager execution.\n---\n\nThis is the companion code to the post \n\"Image-to-image translation with Pix2Pix: An implementation using Keras and eager execution\"\non the TensorFlow for R blog.\n\nhttps://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\n\nrestore <- TRUE\n\ndata_dir <- \"facades\"\n\nbuffer_size <- 400\nbatch_size <- 1\nbatches_per_epoch <- buffer_size / batch_size\nimg_width <- 256L\nimg_height <- 256L\n\nload_image <- function(image_file, is_train) {\n\n  image <- tf$read_file(image_file)\n  image <- tf$image$decode_jpeg(image)\n  \n  w <- as.integer(k_shape(image)[2])\n  w2 <- as.integer(w / 2L)\n  real_image <- image[ , 1L:w2, ]\n  input_image <- image[ , (w2 + 1L):w, ]\n  \n  input_image <- k_cast(input_image, tf$float32)\n  real_image <- k_cast(real_image, tf$float32)\n\n  if (is_train) {\n      input_image <-\n      tf$image$resize_images(input_image,\n                             c(286L, 286L),\n                             align_corners = TRUE,\n                             method = 2)\n    real_image <- tf$image$resize_images(real_image,\n                                         c(286L, 286L),\n                                         align_corners = TRUE,\n                                         method = 2)\n    \n    stacked_image <-\n      k_stack(list(input_image, real_image), axis = 1)\n    cropped_image <-\n      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))\n    c(input_image, real_image) %<-% list(cropped_image[1, , , ], cropped_image[2, , , ])\n    \n    if (runif(1) > 0.5) {\n      input_image <- tf$image$flip_left_right(input_image)\n      real_image <- tf$image$flip_left_right(real_image)\n    }\n  } else {\n    input_image <-\n      tf$image$resize_images(\n        input_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n    real_image <-\n      tf$image$resize_images(\n        real_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n  }\n  \n  input_image <- (input_image / 127.5) - 1\n  real_image <- (real_image / 127.5) - 1\n  \n  list(input_image, real_image)\n}\n\ntrain_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"train/*.jpg\")) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"test/*.jpg\")) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\n\ndownsample <- function(filters,\n                       size,\n                       apply_batchnorm = TRUE,\n                       name = \"downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      cat(\"downsample (generator) output: \", x$shape$as_list(), \"\\n\")\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\nupsample <- function(filters,\n                     size,\n                     apply_dropout = FALSE,\n                     name = \"upsample\") {\n  keras_model_custom(name = NULL, function(self) {\n    self$apply_dropout <- apply_dropout\n    self$up_conv <- layer_conv_2d_transpose(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    if (self$apply_dropout) {\n      self$dropout <- layer_dropout(rate = 0.5)\n    }\n    function(xs,\n             mask = NULL,\n             training = TRUE) {\n      c(x1, x2) %<-% xs\n      x <- self$up_conv(x1) %>% self$batchnorm(training = training)\n      if (self$apply_dropout) {\n        x %>% self$dropout(training = training)\n      }\n      x %>% layer_activation(\"relu\")\n      concat <- k_concatenate(list(x, x2))\n      cat(\"upsample (generator) output: \", concat$shape$as_list(), \"\\n\")\n      concat\n    }\n  })\n}\n\ngenerator <- function(name = \"generator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- downsample(64, 4, apply_batchnorm = FALSE)\n    self$down2 <- downsample(128, 4)\n    self$down3 <- downsample(256, 4)\n    self$down4 <- downsample(512, 4)\n    self$down5 <- downsample(512, 4)\n    self$down6 <- downsample(512, 4)\n    self$down7 <- downsample(512, 4)\n    self$down8 <- downsample(512, 4)\n    \n    self$up1 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up2 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up3 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up4 <- upsample(512, 4)\n    self$up5 <- upsample(256, 4)\n    self$up6 <- upsample(128, 4)\n    self$up7 <- upsample(64, 4)\n    self$last <- layer_conv_2d_transpose(\n      filters = 3,\n      kernel_size = 4,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      activation = \"tanh\"\n    )\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      # x shape == (bs, 256, 256, 3)\n      x1 <-\n        x %>% self$down1(training = training)  # (bs, 128, 128, 64)\n      x2 <- self$down2(x1, training = training) # (bs, 64, 64, 128)\n      x3 <- self$down3(x2, training = training) # (bs, 32, 32, 256)\n      x4 <- self$down4(x3, training = training) # (bs, 16, 16, 512)\n      x5 <- self$down5(x4, training = training) # (bs, 8, 8, 512)\n      x6 <- self$down6(x5, training = training) # (bs, 4, 4, 512)\n      x7 <- self$down7(x6, training = training) # (bs, 2, 2, 512)\n      x8 <- self$down8(x7, training = training) # (bs, 1, 1, 512)\n\n      x9 <-\n        self$up1(list(x8, x7), training = training) # (bs, 2, 2, 1024)\n      x10 <-\n        self$up2(list(x9, x6), training = training) # (bs, 4, 4, 1024)\n      x11 <-\n        self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)\n      x12 <-\n        self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)\n      x13 <-\n        self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)\n      x14 <-\n        self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)\n      x15 <-\n        self$up7(list(x14, x1), training = training) # (bs, 128, 128, 128)\n      x16 <- self$last(x15) # (bs, 256, 256, 3)\n      cat(\"generator output: \", x16$shape$as_list(), \"\\n\")\n      x16\n    }\n  })\n}\n\n\ndisc_downsample <- function(filters,\n                            size,\n                            apply_batchnorm = TRUE,\n                            name = \"disc_downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\ndiscriminator <- function(name = \"discriminator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- disc_downsample(64, 4, FALSE)\n    self$down2 <- disc_downsample(128, 4)\n    self$down3 <- disc_downsample(256, 4)\n    # we are zero padding here with 1 because we need our shape to\n    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n    self$zero_pad1 <- layer_zero_padding_2d()\n    self$conv <- layer_conv_2d(\n      filters = 512,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    self$zero_pad2 <- layer_zero_padding_2d()\n    self$last <- layer_conv_2d(\n      filters = 1,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal()\n    )\n    \n    function(x,\n             y,\n             mask = NULL,\n             training = TRUE) {\n      x <- k_concatenate(list(x, y)) %>% # (bs, 256, 256, channels*2)\n        self$down1(training = training) %>% # (bs, 128, 128, 64)\n        self$down2(training = training) %>% # (bs, 64, 64, 128)\n        self$down3(training = training) %>% # (bs, 32, 32, 256)\n        self$zero_pad1() %>% # (bs, 34, 34, 256)\n        self$conv() %>% # (bs, 31, 31, 512)\n        self$batchnorm(training = training) %>%\n        layer_activation_leaky_relu() %>%\n        self$zero_pad2() %>% # (bs, 33, 33, 512)\n        self$last() # (bs, 30, 30, 1)\n      cat(\"discriminator output: \", x$shape$as_list(), \"\\n\")\n      x\n    }\n  })\n  \n}\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$ones_like(real_output),\n                                    logits = real_output)\n  generated_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$zeros_like(generated_output),\n                                    logits = generated_output)\n  real_loss + generated_loss\n}\n\nlambda <- 100\ngenerator_loss <-\n  function(disc_judgment, generated_output, target) {\n    gan_loss <-\n      tf$losses$sigmoid_cross_entropy(tf$ones_like(disc_judgment), disc_judgment)\n    l1_loss <- tf$reduce_mean(tf$abs(target - generated_output))\n    gan_loss + (lambda * l1_loss)\n  }\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\ngenerator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\n\ncheckpoint_dir <- \"./checkpoints_pix2pix\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer,\n    generator = generator,\n    discriminator = discriminator\n  )\n\ngenerate_images <- function(generator, input, target, id) {\n  prediction <- generator(input, training = TRUE)\n  png(paste0(\"pix2pix_\", id, \".png\"), width = 900, height = 300)\n  par(mfcol = c(1, 3))\n  par(mar = c(0, 0, 0, 0),\n      xaxs = 'i',\n      yaxs = 'i')\n  input <- input[1, , ,]$numpy() * 0.5 + 0.5\n  input[input > 1] <- 1\n  input[input < 0] <- 0\n  plot(as.raster(input, main = \"input image\"))\n  target <- target[1, , ,]$numpy() * 0.5 + 0.5\n  target[target > 1] <- 1\n  target[target < 0] <- 0\n  plot(as.raster(target, main = \"ground truth\"))\n  prediction <- prediction[1, , ,]$numpy() * 0.5 + 0.5\n  prediction[prediction > 1] <- 1\n  prediction[prediction < 0] <- 0\n  plot(as.raster(prediction, main = \"generated\"))\n  dev.off()\n}\n\ntrain <- function(dataset, num_epochs) {\n  for (epoch in 1:num_epochs) {\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      input_image <- batch[[1]]\n      target <- batch[[2]]\n      \n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          gen_output <- generator(input_image, training = TRUE)\n          disc_real_output <-\n            discriminator(input_image, target, training = TRUE)\n          disc_generated_output <-\n            discriminator(input_image, gen_output, training = TRUE)\n          gen_loss <-\n            generator_loss(disc_generated_output, gen_output, target)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n          total_loss_gen <- total_loss_gen + gen_loss\n          total_loss_disc <- total_loss_disc + disc_loss\n        })\n      })\n      generator_gradients <- gen_tape$gradient(gen_loss,\n                                               generator$variables)\n      discriminator_gradients <- disc_tape$gradient(disc_loss,\n                                                    discriminator$variables)\n      \n      generator_optimizer$apply_gradients(transpose(list(\n        generator_gradients,\n        generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(transpose(\n        list(discriminator_gradients,\n             discriminator$variables)\n      ))\n      \n    })\n    cat(\"Epoch \", epoch, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    if (epoch %% 10 == 0) {\n      test_iter <- make_iterator_one_shot(test_dataset)\n      batch <- iterator_get_next(test_iter)\n      input <- batch[[1]]\n      target <- batch[[2]]\n      generate_images(generator, input, target, paste0(\"epoch_\", i))\n    }\n    if (epoch %% 10 == 0) {\n      checkpoint$save(file_prefix = checkpoint_prefix)\n    }\n    \n  }\n}\n\nif (!restore) {\n  train(train_dataset, 200)\n} \n\n\ncheckpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n\ntest_iter <- make_iterator_one_shot(test_dataset)\ni <- 1\nuntil_out_of_range({\n  batch <- iterator_get_next(test_iter)\n  input <- batch[[1]]\n  target <- batch[[2]]\n  generate_images(generator, input, target, paste0(\"test_\", i))\n  i <- i + 1\n})\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}