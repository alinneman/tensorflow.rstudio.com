{
  "hash": "2adc2430fc3effa12944a71a76928948",
  "result": {
    "markdown": "---\ntitle: lstm_seq2seq\ndescription: This script demonstrates how to implement a basic character-level sequence-to-sequence model.\n---\n\nSequence to sequence example in Keras (character-level).\n\nThis script demonstrates how to implement a basic character-level\nsequence-to-sequence model. We apply it to translating\nshort English sentences into short French sentences,\ncharacter-by-character. Note that it is fairly unusual to\ndo character-level machine translation, as word-level\nmodels are more common in this domain.\n\n**Algorithm**\n\n- We start with input sequences from a domain (e.g. English sentences)\n    and correspding target sequences from another domain\n    (e.g. French sentences).\n- An encoder LSTM turns input sequences to 2 state vectors\n    (we keep the last LSTM state and discard the outputs).\n- A decoder LSTM is trained to turn the target sequences into\n    the same sequence but offset by one timestep in the future,\n    a training process called \"teacher forcing\" in this context.\n    Is uses as initial state the state vectors from the encoder.\n    Effectively, the decoder learns to generate `targets[t+1...]`\n    given `targets[...t]`, conditioned on the input sequence.\n- In inference mode, when we want to decode unknown input sequences, we:\n    - Encode the input sequence into state vectors\n    - Start with a target sequence of size 1\n        (just the start-of-sequence character)\n    - Feed the state vectors and 1-char target sequence\n        to the decoder to produce predictions for the next character\n    - Sample the next character using these predictions\n        (we simply use argmax).\n    - Append the sampled character to the target sequence\n    - Repeat until we generate the end-of-sequence character or we\n        hit the character limit.\n\n**Data download**\n\nEnglish to French sentence pairs.\nhttp://www.manythings.org/anki/fra-eng.zip\n\nLots of neat sentence pairs datasets can be found at:\nhttp://www.manythings.org/anki/\n\n**References**\n\n- Sequence to Sequence Learning with Neural Networks\n    https://arxiv.org/abs/1409.3215\n- Learning Phrase Representations using\n    RNN Encoder-Decoder for Statistical Machine Translation\n    https://arxiv.org/abs/1406.1078\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(data.table)\n\nbatch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 10000  # Number of samples to train on.\n\n## Path to the data txt file on disk.\ndata_path = 'fra.txt'\ntext <- fread(data_path, sep=\"\\t\", header=FALSE, nrows=num_samples)\n\n## Vectorize the data.\ninput_texts  <- text[[1]]\ntarget_texts <- paste0('\\t',text[[2]],'\\n')\ninput_texts  <- lapply( input_texts, function(s) strsplit(s, split=\"\")[[1]])\ntarget_texts <- lapply( target_texts, function(s) strsplit(s, split=\"\")[[1]])\n\ninput_characters  <- sort(unique(unlist(input_texts)))\ntarget_characters <- sort(unique(unlist(target_texts)))\nnum_encoder_tokens <- length(input_characters)\nnum_decoder_tokens <- length(target_characters)\nmax_encoder_seq_length <- max(sapply(input_texts,length))\nmax_decoder_seq_length <- max(sapply(target_texts,length))\n\ncat('Number of samples:', length(input_texts),'\\n')\ncat('Number of unique input tokens:', num_encoder_tokens,'\\n')\ncat('Number of unique output tokens:', num_decoder_tokens,'\\n')\ncat('Max sequence length for inputs:', max_encoder_seq_length,'\\n')\ncat('Max sequence length for outputs:', max_decoder_seq_length,'\\n')\n\ninput_token_index  <- 1:length(input_characters)\nnames(input_token_index) <- input_characters\ntarget_token_index <- 1:length(target_characters)\nnames(target_token_index) <- target_characters\nencoder_input_data <- array(\n  0, dim = c(length(input_texts), max_encoder_seq_length, num_encoder_tokens))\ndecoder_input_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\ndecoder_target_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\n\nfor(i in 1:length(input_texts)) {\n  d1 <- sapply( input_characters, function(x) { as.integer(x == input_texts[[i]]) })\n  encoder_input_data[i,1:nrow(d1),] <- d1\n  d2 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]]) })\n  decoder_input_data[i,1:nrow(d2),] <- d2\n  d3 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]][-1]) })\n  decoder_target_data[i,1:nrow(d3),] <- d3\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Create the model\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define an input sequence and process it.\nencoder_inputs  <- layer_input(shape=list(NULL,num_encoder_tokens))\nencoder         <- layer_lstm(units=latent_dim, return_state=TRUE)\nencoder_results <- encoder_inputs %>% encoder\n## We discard `encoder_outputs` and only keep the states.\nencoder_states  <- encoder_results[2:3]\n\n## Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs  <- layer_input(shape=list(NULL, num_decoder_tokens))\n## We set up our decoder to return full output sequences,\n## and to return internal states as well. We don't use the\n## return states in the training model, but we will use them in inference.\ndecoder_lstm    <- layer_lstm(units=latent_dim, return_sequences=TRUE,\n                              return_state=TRUE, stateful=FALSE)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense   <- layer_dense(units=num_decoder_tokens, activation='softmax')\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\n\n## Define the model that will turn\n## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel <- keras_model( inputs = list(encoder_inputs, decoder_inputs),\n                      outputs = decoder_outputs )\n\n## Compile model\nmodel %>% compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n## Run model\nmodel %>% fit( list(encoder_input_data, decoder_input_data), decoder_target_data,\n               batch_size=batch_size,\n               epochs=epochs,\n               validation_split=0.2)\n\n## Save model\nsave_model_hdf5(model,'s2s.h5')\nsave_model_weights_hdf5(model,'s2s-wt.h5')\n\n##model <- load_model_hdf5('s2s.h5')\n##load_model_weights_hdf5(model,'s2s-wt.h5')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Next: inference mode (sampling).\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Here's the drill:\n## 1) encode input and retrieve initial decoder state\n## 2) run one step of decoder with this initial state\n## and a \"start of sequence\" token as target.\n## Output will be the next target token\n## 3) Repeat with the current target token and current states\n\n## Define sampling models\nencoder_model <-  keras_model(encoder_inputs, encoder_states)\ndecoder_state_input_h <- layer_input(shape=latent_dim)\ndecoder_state_input_c <- layer_input(shape=latent_dim)\ndecoder_states_inputs <- c(decoder_state_input_h, decoder_state_input_c)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states  <- decoder_results[2:3]\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\ndecoder_model   <- keras_model(\n  inputs  = c(decoder_inputs, decoder_states_inputs),\n  outputs = c(decoder_outputs, decoder_states))\n\n## Reverse-lookup token index to decode sequences back to\n## something readable.\nreverse_input_char_index  <- as.character(input_characters)\nreverse_target_char_index <- as.character(target_characters)\n\ndecode_sequence <- function(input_seq) {\n  ## Encode the input as state vectors.\n  states_value <- predict(encoder_model, input_seq)\n  \n  ## Generate empty target sequence of length 1.\n  target_seq <- array(0, dim=c(1, 1, num_decoder_tokens))\n  ## Populate the first character of target sequence with the start character.\n  target_seq[1, 1, target_token_index['\\t']] <- 1.\n  \n  ## Sampling loop for a batch of sequences\n  ## (to simplify, here we assume a batch of size 1).\n  stop_condition = FALSE\n  decoded_sentence = ''\n  maxiter = max_decoder_seq_length\n  niter = 1\n  while (!stop_condition && niter < maxiter) {\n    \n    ## output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n    decoder_predict <- predict(decoder_model, c(list(target_seq), states_value))\n    output_tokens <- decoder_predict[[1]]\n    \n    ## Sample a token\n    sampled_token_index <- which.max(output_tokens[1, 1, ])\n    sampled_char <- reverse_target_char_index[sampled_token_index]\n    decoded_sentence <-  paste0(decoded_sentence, sampled_char)\n    decoded_sentence\n    \n    ## Exit condition: either hit max length\n    ## or find stop character.\n    if (sampled_char == '\\n' ||\n        length(decoded_sentence) > max_decoder_seq_length) {\n      stop_condition = TRUE\n    }\n    \n    ## Update the target sequence (of length 1).\n    ## target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[1, 1, ] <- 0\n    target_seq[1, 1, sampled_token_index] <- 1.\n    \n    ## Update states\n    h <- decoder_predict[[2]]\n    c <- decoder_predict[[3]]\n    states_value = list(h, c)\n    niter <- niter + 1\n  }    \n  return(decoded_sentence)\n}\n\nfor (seq_index in 1:100) {\n  ## Take one sequence (part of the training test)\n  ## for trying out decoding.\n  input_seq = encoder_input_data[seq_index,,,drop=FALSE]\n  decoded_sentence = decode_sequence(input_seq)\n  target_sentence <- gsub(\"\\t|\\n\",\"\",paste(target_texts[[seq_index]],collapse=''))\n  input_sentence  <- paste(input_texts[[seq_index]],collapse='')\n  cat('-\\n')\n  cat('Input sentence  : ', input_sentence,'\\n')\n  cat('Target sentence : ', target_sentence,'\\n')\n  cat('Decoded sentence: ', decoded_sentence,'\\n')\n}\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}