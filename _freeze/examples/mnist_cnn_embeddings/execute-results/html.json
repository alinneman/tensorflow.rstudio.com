{
  "hash": "8697d66292fd1b113563f66803bbc8d6",
  "result": {
    "markdown": "---\ntitle: mnist_cnn_embeddings\ndescription: Demonstrates how to visualize embeddings in TensorBoard.\n---\n\nThis example shows how to visualize embeddings in TensorBoard.\n\nEmbeddings in the sense used here don't necessarily refer to embedding layers.\nIn fact, features (= activations) from other hidden layers can be visualized,\nas shown in this example for a dense layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 12\n\n# Input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redefine  dimension of train/test inputs\nx_train <-\n  array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test <-\n  array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n\n# Prepare for logging embeddings --------------------------------------------------\n\nembeddings_dir <- file.path(tempdir(), 'embeddings')\nif (!file.exists(embeddings_dir))\n  dir.create(embeddings_dir)\nembeddings_metadata <- file.path(embeddings_dir, 'metadata.tsv')\n\n# we use the class names from the test set as embeddings_metadata\nreadr::write_tsv(data.frame(y_test), path = embeddings_metadata, col_names = FALSE)\n\ntensorboard_callback <- callback_tensorboard(\n  log_dir = embeddings_dir,\n  batch_size = batch_size,\n  embeddings_freq = 1,\n  # if missing or NULL all embedding layers will be monitored\n  embeddings_layer_names = list('features'),\n  # single file for all embedding layers, could also be a named list mapping\n  # layer names to file names\n  embeddings_metadata = embeddings_metadata,\n  # data to be embedded\n  embeddings_data = x_test\n)\n\n\n# Define Model -----------------------------------------------------------\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define model\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filters = 32,\n    kernel_size = c(3, 3),\n    activation = 'relu',\n    input_shape = input_shape\n  ) %>%\n  layer_conv_2d(filters = 64,\n                kernel_size = c(3, 3),\n                activation = 'relu') %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_flatten() %>%\n  # these are the embeddings (activations) we are going to visualize\n  layer_dense(units = 128, activation = 'relu', name = 'features') %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n# Launch TensorBoard\n#\n# As the model is being fit you will be able to view the embedings in the \n# Projector tab. On the left, use \"color by label\" to see the digits displayed\n# in 10 different colors. Hover over a point to see its label.\ntensorboard(embeddings_dir)\n\n# Train model\nmodel %>% fit(\n  x_train,\n  y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, y_test),\n  callbacks = list(tensorboard_callback)\n)\n\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\n\n# Output metrics\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}