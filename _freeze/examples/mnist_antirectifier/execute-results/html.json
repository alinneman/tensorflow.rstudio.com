{
  "hash": "8684745e8ae1689bfca14a27016e88a1",
  "result": {
    "markdown": "---\ntitle: mnist_antirectifier\ndescription: Demonstrates how to write custom layers for Keras\n---\n\nDemonstrates how to write custom layers for Keras.\n\nWe build a custom activation layer called 'Antirectifier', which modifies the\nshape of the tensor that passes through it. We need to specify two methods:\n`compute_output_shape` and `call`.\n\nNote that the same result can also be achieved via a Lambda layer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Data Preparation --------------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 40\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redimension\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Antirectifier Layer -----------------------------------------------------\n```\n:::\n\n\n\nThis is the combination of a sample-wise L2 normalization with the\nconcatenation of the positive part of the input with the negative part\nof the input. The result is a tensor of samples that are twice as large\nas the input samples.\n\nIt can be used in place of a ReLU.\n Input shape: 2D tensor of shape (samples, n)\n Output shape: 2D tensor of shape (samples, 2*n)\n\nWhen applying ReLU, assuming that the distribution of the previous output is\napproximately centered around 0., you are discarding half of your input. This\nis inefficient.\n\nAntirectifier allows to return all-positive outputs like ReLU, without\ndiscarding any data.\n\nTests on MNIST show that Antirectifier allows to train networks with half\nthe parameters yet with comparable classification accuracy as an equivalent\nReLU-based network.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Custom layer class\nAntirectifierLayer <- R6::R6Class(\"KerasLayer\",\n  \n  inherit = KerasLayer,\n                           \n  public = list(\n   \n    call = function(x, mask = NULL) {\n      x <- x - k_mean(x, axis = 2, keepdims = TRUE)\n      x <- k_l2_normalize(x, axis = 2)\n      pos <- k_relu(x)\n      neg <- k_relu(-x)\n      k_concatenate(c(pos, neg), axis = 2)\n      \n    },\n     \n    compute_output_shape = function(input_shape) {\n      input_shape[[2]] <- input_shape[[2]] * 2L \n      input_shape\n    }\n  )\n)\n\n# Create layer wrapper function\nlayer_antirectifier <- function(object) {\n  create_layer(AntirectifierLayer, object)\n}\n\n\n# Define & Train Model -------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 256, input_shape = c(784)) %>% \n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 256) %>%\n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile the model\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Train the model\nmodel %>% fit(x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data= list(x_test, y_test)\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}