{
  "hash": "3e87ab377842acf3f09ac30d7d8541aa",
  "result": {
    "markdown": "---\ntitle: neural_style_transfer\ndescription: Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\n---\n\nNeural style transfer with Keras.\n\nIt is preferable to run this script on a GPU, for speed.\n\nExample result: https://twitter.com/fchollet/status/686631033085677568\n\nStyle transfer consists in generating an image\nwith the same \"content\" as a base image, but with the\n\"style\" of a different picture (typically artistic). \n\nThis is achieved through the optimization of a loss function\nthat has 3 components: \"style loss\", \"content loss\",\nand \"total variation loss\":\n\n - The total variation loss imposes local spatial continuity between\n   the pixels of the combination image, giving it visual coherence.\n\n - The style loss is where the deep learning keeps in --that one is defined\n   using a deep convolutional neural network. Precisely, it consists in a sum of\n   L2 distances between the Gram matrices of the representations of\n   the base image and the style reference image, extracted from\n   different layers of a convnet (trained on ImageNet). The general idea\n   is to capture color/texture information at different spatial\n   scales (fairly large scales --defined by the depth of the layer considered).\n\n - The content loss is a L2 distance between the features of the base\n   image (extracted from a deep layer) and the features of the combination image,\n   keeping the generated image close enough to the original one.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(purrr)\nlibrary(R6)\n\n# Parameters --------------------------------------------------------------\n\nbase_image_path <- \"neural-style-base-img.png\"\nstyle_reference_image_path <- \"neural-style-style.jpg\"\niterations <- 10\n\n# these are the weights of the different loss components\ntotal_variation_weight <- 1\nstyle_weight <- 1\ncontent_weight <- 0.025\n\n# dimensions of the generated picture.\nimg <- image_load(base_image_path)\nwidth <- img$size[[1]]\nheight <- img$size[[2]]\nimg_nrows <- 400\nimg_ncols <- as.integer(width * img_nrows / height)\n\n\n# Functions ---------------------------------------------------------------\n\n# util function to open, resize and format pictures into appropriate tensors\npreprocess_image <- function(path){\n  img <- image_load(path, target_size = c(img_nrows, img_ncols)) %>%\n    image_to_array() %>%\n    array_reshape(c(1, dim(.)))\n  imagenet_preprocess_input(img)\n}\n\n# util function to convert a tensor into a valid image\n# also turn BGR into RGB.\ndeprocess_image <- function(x){\n  x <- x[1,,,]\n  # Remove zero-center by mean pixel\n  x[,,1] <- x[,,1] + 103.939\n  x[,,2] <- x[,,2] + 116.779\n  x[,,3] <- x[,,3] + 123.68\n  # BGR -> RGB\n  x <- x[,,c(3,2,1)]\n  # clip to interval 0, 255\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x)/255\n  x\n}\n\n\n# Defining the model ------------------------------------------------------\n\n# get tensor representations of our images\nbase_image <- k_variable(preprocess_image(base_image_path))\nstyle_reference_image <- k_variable(preprocess_image(style_reference_image_path))\n\n# this will contain our generated image\ncombination_image <- k_placeholder(c(1, img_nrows, img_ncols, 3))\n\n# combine the 3 images into a single Keras tensor\ninput_tensor <- k_concatenate(list(base_image, style_reference_image, \n                                   combination_image), axis = 1)\n\n# build the VGG16 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nmodel <- application_vgg16(input_tensor = input_tensor, weights = \"imagenet\", \n                           include_top = FALSE)\n\nprint(\"Model loaded.\")\n\nnms <- map_chr(model$layers, ~.x$name)\noutput_dict <- map(model$layers, ~.x$output) %>% set_names(nms)\n\n# compute the neural style loss\n# first we need to define 4 util functions\n\n# the gram matrix of an image tensor (feature-wise outer product)\n\ngram_matrix <- function(x){\n  \n  features <- x %>%\n    k_permute_dimensions(pattern = c(3, 1, 2)) %>%\n    k_batch_flatten()\n  \n  k_dot(features, k_transpose(features))\n}\n\n# the \"style loss\" is designed to maintain\n# the style of the reference image in the generated image.\n# It is based on the gram matrices (which capture style) of\n# feature maps from the style reference image\n# and from the generated image\n\nstyle_loss <- function(style, combination){\n  S <- gram_matrix(style)\n  C <- gram_matrix(combination)\n  \n  channels <- 3\n  size <- img_nrows*img_ncols\n  \n  k_sum(k_square(S - C)) / (4 * channels^2  * size^2)\n}\n\n# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\n\ncontent_loss <- function(base, combination){\n  k_sum(k_square(combination - base))\n}\n\n# the 3rd loss function, total variation loss,\n# designed to keep the generated image locally coherent\n\ntotal_variation_loss <- function(x){\n  y_ij  <- x[,1:(img_nrows - 1L), 1:(img_ncols - 1L),]\n  y_i1j <- x[,2:(img_nrows), 1:(img_ncols - 1L),]\n  y_ij1 <- x[,1:(img_nrows - 1L), 2:(img_ncols),]\n  \n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\n# combine these loss functions into a single scalar\nloss <- k_variable(0.0)\nlayer_features <- output_dict$block4_conv2\nbase_image_features <- layer_features[1,,,]\ncombination_features <- layer_features[3,,,]\n\nloss <- loss + content_weight*content_loss(base_image_features, \n                                           combination_features)\n\nfeature_layers = c('block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1')\n\nfor(layer_name in feature_layers){\n  layer_features <- output_dict[[layer_name]]\n  style_reference_features <- layer_features[2,,,]\n  combination_features <- layer_features[3,,,]\n  sl <- style_loss(style_reference_features, combination_features)\n  loss <- loss + ((style_weight / length(feature_layers)) * sl)\n}\n\nloss <- loss + (total_variation_weight * total_variation_loss(combination_image))\n\n# get the gradients of the generated image wrt the loss\ngrads <- k_gradients(loss, combination_image)[[1]]\n\nf_outputs <- k_function(list(combination_image), list(loss, grads))\n\neval_loss_and_grads <- function(image){\n  image <- array_reshape(image, c(1, img_nrows, img_ncols, 3))\n  outs <- f_outputs(list(image))\n  list(\n    loss_value = outs[[1]],\n    grad_values = array_reshape(outs[[2]], dim = length(outs[[2]]))\n  )\n}\n\n# Loss and gradients evaluator.\n# \n# This Evaluator class makes it possible\n# to compute loss and gradients in one pass\n# while retrieving them via two separate functions,\n# \"loss\" and \"grads\". This is done because scipy.optimize\n# requires separate functions for loss and gradients,\n# but computing them separately would be inefficient.\nEvaluator <- R6Class(\n  \"Evaluator\",\n  public = list(\n    \n    loss_value = NULL,\n    grad_values = NULL,\n    \n    initialize = function() {\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n    },\n    \n    loss = function(x){\n      loss_and_grad <- eval_loss_and_grads(x)\n      self$loss_value <- loss_and_grad$loss_value\n      self$grad_values <- loss_and_grad$grad_values\n      self$loss_value\n    },\n    \n    grads = function(x){\n      grad_values <- self$grad_values\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n      grad_values\n    }\n    \n  )\n)\n\nevaluator <- Evaluator$new()\n\n# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\ndms <- c(1, img_nrows, img_ncols, 3)\nx <- array(data = runif(prod(dms), min = 0, max = 255) - 128, dim = dms)\n\n# Run optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the loss\nfor(i in 1:iterations){\n\n  # Run L-BFGS\n  opt <- optim(\n    array_reshape(x, dim = length(x)), fn = evaluator$loss, gr = evaluator$grads, \n    method = \"L-BFGS-B\",\n    control = list(maxit = 15)\n  )\n  \n  # Print loss value\n  print(opt$value)\n  \n  # decode the image\n  image <- x <- opt$par\n  image <- array_reshape(image, dms)\n  \n  # plot\n  im <- deprocess_image(image)\n  plot(as.raster(im))\n}\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}