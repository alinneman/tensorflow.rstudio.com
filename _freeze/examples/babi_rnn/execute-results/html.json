{
  "hash": "2a492ae23689d87c6e11e5e2c92c51e6",
  "result": {
    "markdown": "---\ntitle: babi_rnn\ndescription: Trains a two-branch recurrent network on the bAbI dataset for reading comprehension.\n---\n\nTrains two recurrent neural networks based upon a story and a question.\nThe resulting merged vector is then queried to answer a range of bAbI tasks.\n\nThe results are comparable to those for an LSTM model provided in Weston et al.:\n\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\nhttp://arxiv.org/abs/1502.05698\n\nTask Number                  | FB LSTM Baseline | Keras QA\n---                          | ---              | ---\nQA1 - Single Supporting Fact | 50               | 100.0\nQA2 - Two Supporting Facts   | 20               | 50.0\nQA3 - Three Supporting Facts | 20               | 20.5\nQA4 - Two Arg. Relations     | 61               | 62.9\nQA5 - Three Arg. Relations   | 70               | 61.9\nQA6 - yes/No Questions       | 48               | 50.7\nQA7 - Counting               | 49               | 78.9\nQA8 - Lists/Sets             | 45               | 77.2\nQA9 - Simple Negation        | 64               | 64.0\nQA10 - Indefinite Knowledge  | 44               | 47.7\nQA11 - Basic Coreference     | 72               | 74.9\nQA12 - Conjunction           | 74               | 76.4\nQA13 - Compound Coreference  | 94               | 94.4\nQA14 - Time Reasoning        | 27               | 34.8\nQA15 - Basic Deduction       | 21               | 32.4\nQA16 - Basic Induction       | 23               | 50.6\nQA17 - Positional Reasoning  | 51               | 49.1\nQA18 - Size Reasoning        | 52               | 90.8\nQA19 - Path Finding          | 8                | 9.0\nQA20 - Agent's Motivations   | 91               | 90.7\n\nFor the resources related to the bAbI project, refer to:\n  https://research.facebook.com/researchers/1543934539189348\n\nNotes:\n  \n  - With default word, sentence, and query vector sizes, the GRU model achieves:\n  - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n  - 50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\nIn comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n\n- The task does not traditionally parse the question separately. This likely\nimproves accuracy and is a good example of merging two RNNs.\n\n- The word vector embeddings are not shared between the story and question RNNs.\n\n- See how the accuracy changes given 10,000 training samples (en-10k) instead\nof only 1000. 1000 was used in order to be comparable to the original paper.\n\n- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n\n- The length and noise (i.e. 'useless' story components) impact the ability for\nLSTMs / GRUs to provide the correct answer. Given only the supporting facts,\nthese RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\nnetworks that use attentional processes can efficiently search through this\nnoise to find the relevant statements, improving performance substantially.\nThis becomes especially obvious on QA2 and QA3, both far longer than QA1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'keras' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_df(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n\n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n    \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n\n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\nmax_length <- 99999\nembed_hidden_size <- 50\nbatch_size <- 32\nepochs <- 40\n\n# Data Preparation --------------------------------------------------------\n\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Default QA1 with 1000 samples\n# challenge = '%stasks_1-20_v1-2/en/qa1_single-supporting-fact_%s.txt'\n# QA1 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt'\n# QA2 with 1000 samples\nchallenge <- \"%stasks_1-20_v1-2/en/qa2_two-supporting-facts_%s.txt\"\n# QA2 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt'\n\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\n# extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n# Defining the model ------------------------------------------------------\n\nsentence <- layer_input(shape = c(story_maxlen), dtype = \"int32\")\nencoded_sentence <- sentence %>% \n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\nquestion <- layer_input(shape = c(query_maxlen), dtype = \"int32\")\nencoded_question <- question %>%\n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_repeat_vector(n = story_maxlen)\n\nmerged <- list(encoded_sentence, encoded_question) %>%\n  layer_add() %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\npreds <- merged %>%\n  layer_dense(units = vocab_size, activation = \"softmax\")\n\nmodel <- keras_model(inputs = list(sentence, question), outputs = preds)\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n input_2 (InputLayer)     [(None, 5)]       0        []                         \n embedding_1 (Embedding)  (None, 5, 50)     1800     ['input_2[0][0]']          \n input_1 (InputLayer)     [(None, 552)]     0        []                         \n dropout_1 (Dropout)      (None, 5, 50)     0        ['embedding_1[0][0]']      \n embedding (Embedding)    (None, 552, 50)   1800     ['input_1[0][0]']          \n lstm (LSTM)              (None, 50)        20200    ['dropout_1[0][0]']        \n dropout (Dropout)        (None, 552, 50)   0        ['embedding[0][0]']        \n repeat_vector (RepeatVec  (None, 552, 50)  0        ['lstm[0][0]']             \n tor)                                                                           \n add (Add)                (None, 552, 50)   0        ['dropout[0][0]',          \n                                                      'repeat_vector[0][0]']    \n lstm_1 (LSTM)            (None, 50)        20200    ['add[0][0]']              \n dropout_2 (Dropout)      (None, 50)        0        ['lstm_1[0][0]']           \n dense (Dense)            (None, 36)        1836     ['dropout_2[0][0]']        \n================================================================================\nTotal params: 45,836\nTrainable params: 45,836\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split=0.05\n)\n\nevaluation <- model %>% evaluate(\n  x = list(test_vec$stories, test_vec$questions),\n  y = test_vec$answers,\n  batch_size = batch_size\n)\n\nevaluation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss accuracy \n1.666561 0.318000 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}