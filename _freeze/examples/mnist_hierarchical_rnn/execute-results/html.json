{
  "hash": "a7fd7c8c45d7b7e837a11eb67c02afab",
  "result": {
    "markdown": "---\ntitle: mnist_hierarchical_rnn\ndescription: Trains a Hierarchical RNN (HRNN) to classify MNIST digits.\n---\n\nThis is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n\nHRNNs can learn across multiple levels of temporal hiearchy over a complex sequence.\nUsually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)\ninto a  sentence vector. The second recurrent layer then encodes a sequence of\nsuch vectors (encoded by the first layer) into a document vector. This\ndocument vector is considered to preserve both the word-level and\nsentence-level structure of the context.\n\nReferences:\n- [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/abs/1506.01057)\n  Encodes paragraphs and documents with HRNN.\n  Results have shown that HRNN outperforms standard RNNs and may play some role in more\n  sophisticated generation tasks like summarization or question answering.\n- [Hierarchical recurrent neural network for skeleton based action recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n  Achieved state-of-the-art results on skeleton based action recognition with 3 levels\n  of bidirectional HRNN combined with fully connected layers.\n\nIn the below MNIST example the first LSTM layer first encodes every\ncolumn of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM\nlayer encodes then these 28 column vectors of shape (28, 128) to a image vector\nrepresenting the whole image. A final dense layer is added for prediction.\n\nAfter 5 epochs: train acc: 0.9858, val acc: 0.9864\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------------------\n\n# Training parameters.\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 5\n\n# Embedding dimensions.\nrow_hidden <- 128\ncol_hidden <- 128\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Reshapes data to 4D for Hierarchical RNN.\nx_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ndim_x_train <- dim(x_train)\ncat('x_train_shape:', dim_x_train)\ncat(nrow(x_train), 'train samples')\ncat(nrow(x_test), 'test samples')\n\n# Converts class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define input dimensions\nrow <- dim_x_train[[2]]\ncol <- dim_x_train[[3]]\npixel <- dim_x_train[[4]]\n\n# Model input (4D)\ninput <- layer_input(shape = c(row, col, pixel))\n\n# Encodes a row of pixels using TimeDistributed Wrapper\nencoded_rows <- input %>% time_distributed(layer_lstm(units = row_hidden))\n\n# Encodes columns of encoded rows\nencoded_columns <- encoded_rows %>% layer_lstm(units = col_hidden)\n\n# Model output\nprediction <- encoded_columns %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Define Model ------------------------------------------------------------------------\n\nmodel <- keras_model(input, prediction)\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Training\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n\n# Evaluation\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}