{
  "hash": "64dd4d1aaeb083b7c20fac2531eaf2f4",
  "result": {
    "markdown": "---\ntitle: imdb_cnn\ndescription: Demonstrates the use of Convolution1D for text classification.\n---\n\nUse Convolution1D for text classification.\n\nOutput after 2 epochs: ~0.89 \nTime per epoch on CPU (Intel i5 2.4Ghz): 90s\nTime per epoch on GPU (Tesla K40): 10s\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Set parameters:\nmax_features <- 5000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 250\nkernel_size <- 3\nhidden_dims <- 250\nepochs <- 2\n\n\n# Data Preparation --------------------------------------------------------\n\n# Keras load all data into a list with the following structure:\n# List of 2\n# $ train:List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 0 0 1 0 0 1 0 1 0 ...\n# $ test :List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 1 1 1 1 0 0 0 1 1 ...\n#\n# The x data includes integer sequences, each integer is a word.\n# The y data includes a set of integer labels (0 or 1).\n# The num_words argument indicates that only the max_fetures most frequent\n# words will be integerized. All other will be ignored.\n# See help(dataset_imdb)\nimdb <- dataset_imdb(num_words = max_features)\n\n# Pad the sequences, so they have all the same length\n# This will convert the dataset into a matrix: each line is a review\n# and each column a word on the sequence. \n# Pad the sequences with 0 to the left.\nx_train <- imdb$train$x %>%\n  pad_sequences(maxlen = maxlen)\nx_test <- imdb$test$x %>%\n  pad_sequences(maxlen = maxlen)\n\n# Defining Model ------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>% \n  # Start off with an efficient embedding layer which maps\n  # the vocab indices into embedding_dims dimensions\n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n\n  # Add a Convolution1D, which will learn filters\n    # Word group filters of size filter_length:\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  # Apply max pooling:\n  layer_global_max_pooling_1d() %>%\n\n  # Add a vanilla hidden layer:\n  layer_dense(hidden_dims) %>%\n\n  # Apply 20% layer dropout\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n\n  # Project onto a single unit output layer, and squash it with a sigmoid\n\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile model\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Training ----------------------------------------------------------------\n\nmodel %>%\n  fit(\n    x_train, imdb$train$y,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, imdb$test$y)\n  )\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}