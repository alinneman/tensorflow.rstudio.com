{
  "hash": "f6f5f72794fdbbf2cceb28b1ee4e177b",
  "result": {
    "markdown": "---\ntitle: lstm_text_generation\ndescription: Generates text from Nietzsche's writings.\n---\n\nExample script to generate text from Nietzsche's writings.\n\nAt least 20 epochs are required before the generated text starts sounding\ncoherent.\n\nIt is recommended to run this script on GPU, as recurrent networks are quite\ncomputationally intensive.\n\nIf you try this script on new data, make sure your corpus has at least ~100k\ncharacters. ~1M is better.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tokenizers)\n\n# Parameters --------------------------------------------------------------\n\nmaxlen <- 40\n\n# Data Preparation --------------------------------------------------------\n\n# Retrieve text\npath <- get_file(\n  'nietzsche.txt', \n  origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n  )\n\n# Load, collapse, and tokenize text\ntext <- read_lines(path) %>%\n  str_to_lower() %>%\n  str_c(collapse = \"\\n\") %>%\n  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)\n\nprint(sprintf(\"corpus length: %d\", length(text)))\n\nchars <- text %>%\n  unique() %>%\n  sort()\n\nprint(sprintf(\"total chars: %d\", length(chars)))  \n\n# Cut the text in semi-redundant sequences of maxlen characters\ndataset <- map(\n  seq(1, length(text) - maxlen - 1, by = 3), \n  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])\n  )\n\ndataset <- transpose(dataset)\n\n# Vectorization\nx <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))\ny <- array(0, dim = c(length(dataset$sentece), length(chars)))\n\nfor(i in 1:length(dataset$sentece)){\n  \n  x[i,,] <- sapply(chars, function(x){\n    as.integer(x == dataset$sentece[[i]])\n  })\n  \n  y[i,] <- as.integer(chars == dataset$next_char[[i]])\n  \n}\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%\n  layer_dense(length(chars)) %>%\n  layer_activation(\"softmax\")\n\noptimizer <- optimizer_rmsprop(lr = 0.01)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = optimizer\n)\n\n# Training & Results ----------------------------------------------------\n\nsample_mod <- function(preds, temperature = 1){\n  preds <- log(preds)/temperature\n  exp_preds <- exp(preds)\n  preds <- exp_preds/sum(exp(preds))\n  \n  rmultinom(1, 1, preds) %>% \n    as.integer() %>%\n    which.max()\n}\n\non_epoch_end <- function(epoch, logs) {\n  \n  cat(sprintf(\"epoch: %02d ---------------\\n\\n\", epoch))\n  \n  for(diversity in c(0.2, 0.5, 1, 1.2)){\n    \n    cat(sprintf(\"diversity: %f ---------------\\n\\n\", diversity))\n    \n    start_index <- sample(1:(length(text) - maxlen), size = 1)\n    sentence <- text[start_index:(start_index + maxlen - 1)]\n    generated <- \"\"\n    \n    for(i in 1:400){\n      \n      x <- sapply(chars, function(x){\n        as.integer(x == sentence)\n      })\n      x <- array_reshape(x, c(1, dim(x)))\n      \n      preds <- predict(model, x)\n      next_index <- sample_mod(preds, diversity)\n      next_char <- chars[next_index]\n      \n      generated <- str_c(generated, next_char, collapse = \"\")\n      sentence <- c(sentence[-1], next_char)\n      \n    }\n    \n    cat(generated)\n    cat(\"\\n\\n\")\n    \n  }\n}\n\nprint_callback <- callback_lambda(on_epoch_end = on_epoch_end)\n\nmodel %>% fit(\n  x, y,\n  batch_size = 128,\n  epochs = 1,\n  callbacks = print_callback\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}