{
  "hash": "f56a6e8a3f900a148a7ddabd955572ba",
  "result": {
    "markdown": "---\ntitle: mnist_acgan\ndescription: Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\n---\n\nTrain an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\nMNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.\n\nYou should start to see reasonable images after ~5 epochs, and good images by\n~15 epochs. You should use a GPU, as the convolution-heavy operations are\nvery slow on the CPU. Prefer the TensorFlow backend if you plan on iterating,\nas the compilation time can be a blocker using Theano.  \n  \n| Hardware         | Backend | Time / Epoch        |\n| ---------------- | ------- | ------------------- |\n|CPU               | TF      | 3 hrs               | \n|Titan X (maxwell) | TF      | 4 min               |\n|Titan X (maxwell) | TH      | 7 min               |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(progress)\nlibrary(abind)\nk_set_image_data_format('channels_first')\n\n# Functions ---------------------------------------------------------------\n\nbuild_generator <- function(latent_size){\n  \n  # We will map a pair of (z, L), where z is a latent vector and L is a\n  # label drawn from P_c, to image space (..., 1, 28, 28)\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_dense(1024, input_shape = latent_size, activation = \"relu\") %>%\n    layer_dense(128*7*7, activation = \"relu\") %>%\n    layer_reshape(c(128, 7, 7)) %>%\n    # Upsample to (..., 14, 14)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      256, c(5,5), padding = \"same\", activation = \"relu\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Upsample to (..., 28, 28)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      128, c(5,5), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Take a channel axis reduction\n    layer_conv_2d(\n      1, c(2,2), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    )\n  \n  \n  # This is the z space commonly referred to in GAN papers\n  latent <- layer_input(shape = list(latent_size))\n  \n  # This will be our label\n  image_class <- layer_input(shape = list(1))\n  \n  # 10 classes in MNIST\n  cls <-  image_class %>%\n    layer_embedding(\n      input_dim = 10, output_dim = latent_size, \n      embeddings_initializer='glorot_normal'\n    ) %>%\n    layer_flatten()\n  \n  \n  # Hadamard product between z-space and a class conditional embedding\n  h <- layer_multiply(list(latent, cls))\n  \n  fake_image <- cnn(h)\n  \n  keras_model(list(latent, image_class), fake_image)\n}\n\nbuild_discriminator <- function(){\n  \n  # Build a relatively standard conv net, with LeakyReLUs as suggested in\n  # the reference paper\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_conv_2d(\n      32, c(3,3), padding = \"same\", strides = c(2,2),\n      input_shape = c(1, 28, 28)\n    ) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%\n    \n    layer_conv_2d(64, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(128, c(3, 3), padding = \"same\", strides = c(2,2)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(256, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_flatten()\n  \n  \n  \n  image <- layer_input(shape = c(1, 28, 28))\n  features <- cnn(image)\n  \n  # First output (name=generation) is whether or not the discriminator\n  # thinks the image that is being shown is fake, and the second output\n  # (name=auxiliary) is the class that the discriminator thinks the image\n  # belongs to.\n  fake <- features %>% \n    layer_dense(1, activation = \"sigmoid\", name = \"generation\")\n  \n  aux <- features %>%\n    layer_dense(10, activation = \"softmax\", name = \"auxiliary\")\n  \n  keras_model(image, list(fake, aux))\n}\n\n# Parameters --------------------------------------------------------------\n\n# Batch and latent size taken from the paper\nepochs <- 50\nbatch_size <- 100\nlatent_size <- 100\n\n# Adam parameters suggested in https://arxiv.org/abs/1511.06434\nadam_lr <- 0.00005 \nadam_beta_1 <- 0.5\n\n# Model Definition --------------------------------------------------------\n\n# Build the discriminator\ndiscriminator <- build_discriminator()\ndiscriminator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Build the generator\ngenerator <- build_generator(latent_size)\ngenerator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = \"binary_crossentropy\"\n)\n\nlatent <- layer_input(shape = list(latent_size))\nimage_class <- layer_input(shape = list(1), dtype = \"int32\")\n\nfake <- generator(list(latent, image_class))\n\n# Only want to be able to train generation for the combined model\nfreeze_weights(discriminator)\nresults <- discriminator(fake)\n\ncombined <- keras_model(list(latent, image_class), results)\ncombined %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Data Preparation --------------------------------------------------------\n\n# Loade mnist data, and force it to be of shape (..., 1, 28, 28) with\n# range [-1, 1]\nmnist <- dataset_mnist()\nmnist$train$x <- (mnist$train$x - 127.5)/127.5\nmnist$test$x <- (mnist$test$x - 127.5)/127.5\nmnist$train$x <- array_reshape(mnist$train$x, c(60000, 1, 28, 28))\nmnist$test$x <- array_reshape(mnist$test$x, c(10000, 1, 28, 28))\n\nnum_train <- dim(mnist$train$x)[1]\nnum_test <- dim(mnist$test$x)[1]\n\n# Training ----------------------------------------------------------------\n\nfor(epoch in 1:epochs){\n  \n  num_batches <- trunc(num_train/batch_size)\n  pb <- progress_bar$new(\n    total = num_batches, \n    format = sprintf(\"epoch %s/%s :elapsed [:bar] :percent :eta\", epoch, epochs),\n    clear = FALSE\n  )\n  \n  epoch_gen_loss <- NULL\n  epoch_disc_loss <- NULL\n  \n  possible_indexes <- 1:num_train\n  \n  for(index in 1:num_batches){\n    \n    pb$tick()\n    \n    # Generate a new batch of noise\n    noise <- runif(n = batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = batch_size, ncol = latent_size)\n    \n    # Get a batch of real images\n    batch <- sample(possible_indexes, size = batch_size)\n    possible_indexes <- possible_indexes[!possible_indexes %in% batch]\n    image_batch <- mnist$train$x[batch,,,,drop = FALSE]\n    label_batch <- mnist$train$y[batch]\n    \n    # Sample some labels from p_c\n    sampled_labels <- sample(0:9, batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Generate a batch of fake images, using the generated labels as a\n    # conditioner. We reshape the sampled labels to be\n    # (batch_size, 1) so that we can feed them into the embedding\n    # layer as a length one sequence\n    generated_images <- predict(generator, list(noise, sampled_labels))\n    \n    X <- abind(image_batch, generated_images, along = 1)\n    y <- c(rep(1L, batch_size), rep(0L, batch_size)) %>% matrix(ncol = 1)\n    aux_y <- c(label_batch, sampled_labels) %>% matrix(ncol = 1)\n    \n    # Check if the discriminator can figure itself out\n    disc_loss <- train_on_batch(\n      discriminator, x = X, \n      y = list(y, aux_y)\n    )\n    \n    epoch_disc_loss <- rbind(epoch_disc_loss, unlist(disc_loss))\n    \n    # Make new noise. Generate 2 * batch size here such that\n    # the generator optimizes over an identical number of images as the\n    # discriminator\n    noise <- runif(2*batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = 2*batch_size, ncol = latent_size)\n    sampled_labels <- sample(0:9, size = 2*batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Want to train the generator to trick the discriminator\n    # For the generator, we want all the {fake, not-fake} labels to say\n    # not-fake\n    trick <- rep(1, 2*batch_size) %>% matrix(ncol = 1)\n    \n    combined_loss <- train_on_batch(\n      combined, \n      list(noise, sampled_labels),\n      list(trick, sampled_labels)\n    )\n    \n    epoch_gen_loss <- rbind(epoch_gen_loss, unlist(combined_loss))\n    \n  }\n  \n  cat(sprintf(\"\\nTesting for epoch %02d:\", epoch))\n  \n  # Evaluate the testing loss here\n  \n  # Generate a new batch of noise\n  noise <- runif(num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = num_test, ncol = latent_size)\n  \n  # Sample some labels from p_c and generate images from them\n  sampled_labels <- sample(0:9, size = num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  generated_images <- predict(generator, list(noise, sampled_labels))\n  \n  X <- abind(mnist$test$x, generated_images, along = 1)\n  y <- c(rep(1, num_test), rep(0, num_test)) %>% matrix(ncol = 1)\n  aux_y <- c(mnist$test$y, sampled_labels) %>% matrix(ncol = 1)\n  \n  # See if the discriminator can figure itself out...\n  discriminator_test_loss <- evaluate(\n    discriminator, X, list(y, aux_y), \n    verbose = FALSE\n  ) %>% unlist()\n  \n  discriminator_train_loss <- apply(epoch_disc_loss, 2, mean)\n  \n  # Make new noise\n  noise <- runif(2*num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 2*num_test, ncol = latent_size)\n  sampled_labels <- sample(0:9, size = 2*num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  \n  trick <- rep(1, 2*num_test) %>% matrix(ncol = 1)\n  \n  generator_test_loss = combined %>% evaluate(\n    list(noise, sampled_labels),\n    list(trick, sampled_labels),\n    verbose = FALSE\n  )\n  \n  generator_train_loss <- apply(epoch_gen_loss, 2, mean)\n  \n  \n  # Generate an epoch report on performance\n  row_fmt <- \"\\n%22s : loss %4.2f | %5.2f | %5.2f\"\n  cat(sprintf(\n    row_fmt, \n    \"generator (train)\",\n    generator_train_loss[1],\n    generator_train_loss[2],\n    generator_train_loss[3]\n  ))\n  cat(sprintf(\n    row_fmt, \n    \"generator (test)\",\n    generator_test_loss[1],\n    generator_test_loss[2],\n    generator_test_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (train)\",\n    discriminator_train_loss[1],\n    discriminator_train_loss[2],\n    discriminator_train_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (test)\",\n    discriminator_test_loss[1],\n    discriminator_test_loss[2],\n    discriminator_test_loss[3]\n  ))\n  \n  cat(\"\\n\")\n  \n  # Generate some digits to display\n  noise <- runif(10*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 10, ncol = latent_size)\n  \n  sampled_labels <- 0:9 %>%\n    matrix(ncol = 1)\n  \n  # Get a batch to display\n  generated_images <- predict(\n    generator,    \n    list(noise, sampled_labels)\n  )\n  \n  img <- NULL\n  for(i in 1:10){\n    img <- cbind(img, generated_images[i,,,])\n  }\n  \n  ((img + 1)/2) %>% as.raster() %>%\n    plot()\n  \n}\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}