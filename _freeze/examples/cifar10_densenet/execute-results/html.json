{
  "hash": "443decb454120f5bd58aa51adaa50e5a",
  "result": {
    "markdown": "---\ntitle: cifar10_densenet\ndescription: Trains a DenseNet-40-12 on the CIFAR10 small images dataset.\n---\n\nIn this example we will train a DenseNet-40-12 to classify images from the \nCIFAR10 small images dataset. This takes ~125s per epoch on a NVIDIA GEFORCE 1080 Ti,\nso using a GPU is highly recommended.\n\n[DenseNet](https://arxiv.org/abs/1608.06993) is a network architecture where each \nlayer is directly connected to every other layer in a feed-forward fashion \n(within each dense block). For each layer, the feature maps of all preceding \nlayers are treated as separate inputs whereas its own feature maps are passed on as \ninputs to all subsequent layers. This connectivity pattern yields state-of-the-art \naccuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale\nILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using \nless than half the amount of parameters and roughly half the number of FLOPs.\n\nFinal accuracy on test set was 0.9351 versus 0.9300 reported on the \n[paper](https://arxiv.org/abs/1608.06993).\n\nBeside the `keras` package, you will need to install the `densenet` package.\nInstallation instructions are available [here](https://github.com/dfalbel/densenet).\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries ---------------------------------------------------------------\nlibrary(keras)\nlibrary(densenet)\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 64\nepochs <- 300\n\n# Data Preparation --------------------------------------------------------\n\n# see ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n\n# Normalisation\nfor(i in 1:3){\n  mea <- mean(cifar10$train$x[,,,i])\n  sds <- sd(cifar10$train$x[,,,i])\n  \n  cifar10$train$x[,,,i] <- (cifar10$train$x[,,,i] - mea) / sds\n  cifar10$test$x[,,,i] <- (cifar10$test$x[,,,i] - mea) / sds\n}\nx_train <- cifar10$train$x\nx_test <- cifar10$test$x\n\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n# Model Definition -------------------------------------------------------\n\ninput_img <- layer_input(shape = c(32, 32, 3))\nmodel <- application_densenet(include_top = TRUE, input_tensor = input_img, dropout_rate = 0.2)\n\nopt <- optimizer_sgd(lr = 0.1, momentum = 0.9, nesterov = TRUE)\n\nmodel %>% compile(\n  optimizer = opt,\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n# Model fitting -----------------------------------------------------------\n\n# callbacks for weights and learning rate\nlr_schedule <- function(epoch, lr) {\n  \n  if(epoch <= 150) {\n    0.1\n  } else if(epoch > 150 && epoch <= 225){\n    0.01\n  } else {\n    0.001\n  }\n\n}\n\nlr_reducer <- callback_learning_rate_scheduler(lr_schedule)\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  batch_size = batch_size, \n  epochs = epochs, \n  validation_data = list(x_test, y_test), \n  callbacks = list(\n    lr_reducer\n  )\n)\n\nplot(history)\n\nevaluate(model, x_test, y_test)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}