{
  "hash": "5781f530dd6b77a5f6c745c1a9d0f72a",
  "result": {
    "markdown": "---\ntitle: conv_lstm\ndescription: Demonstrates the use of a convolutional LSTM network.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# This script demonstrates the use of a convolutional LSTM network.\n# This network is used to predict the next frame of an artificially\n# generated movie which contains moving squares.\n\nlibrary(keras)\nlibrary(abind)\nlibrary(raster)\n\n# Function Definition -----------------------------------------------------\n\ngenerate_movies <- function(n_samples = 1200, n_frames = 15){\n  \n  rows <- 80\n  cols <- 80\n  \n  noisy_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  shifted_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  \n  n <- sample(3:8, 1)\n  \n  for(s in 1:n_samples){\n    for(i in 1:n){\n      # Initial position\n      xstart <- sample(20:60, 1)\n      ystart <- sample(20:60, 1)\n      \n      # Direction of motion\n      directionx <- sample(-1:1, 1)\n      directiony <- sample(-1:1, 1)\n      \n      # Size of the square\n      w <- sample(2:3, 1)\n      \n      x_shift <- xstart + directionx*(0:(n_frames))\n      y_shift <- ystart + directiony*(0:(n_frames))\n      \n      for(t in 1:n_frames){\n        square_x <- (x_shift[t] - w):(x_shift[t] + w)\n        square_y <- (y_shift[t] - w):(y_shift[t] + w)\n        \n        noisy_movies[s, t, square_x, square_y] <- \n          noisy_movies[s, t, square_x, square_y] + 1\n        \n        # Make it more robust by adding noise. The idea is that if \n        # during inference, the value of the pixel is not exactly \n        # one; we need to train the network to be robust and still \n        # consider it as a pixel belonging to a square.\n        if(runif(1) > 0.5){\n          noise_f <- sample(c(-1, 1), 1)\n          \n          square_x_n <- (x_shift[t] - w - 1):(x_shift[t] + w + 1)\n          square_y_n <- (y_shift[t] - w - 1):(y_shift[t] + w + 1)\n          \n          noisy_movies[s, t, square_x_n, square_y_n] <- \n            noisy_movies[s, t, square_x_n, square_y_n] + noise_f*0.1\n          \n        }\n        \n        # Shift the ground truth by 1\n        square_x_s <- (x_shift[t+1] - w):(x_shift[t+1] + w)\n        square_y_s <- (y_shift[t+1] - w):(y_shift[t+1] + w)\n        \n        shifted_movies[s, t, square_x_s, square_y_s] <- \n          shifted_movies[s, t, square_x_s, square_y_s] + 1\n      }\n    }  \n  }\n  \n  # Cut to a 40x40 window\n  noisy_movies <- noisy_movies[,,21:60, 21:60]\n  shifted_movies = shifted_movies[,,21:60, 21:60]\n  \n  noisy_movies[noisy_movies > 1] <- 1\n  shifted_movies[shifted_movies > 1] <- 1\n\n  # Add channel dimension\n  noisy_movies <- array_reshape(noisy_movies, c(dim(noisy_movies), 1))\n  shifted_movies <- array_reshape(shifted_movies, c(dim(shifted_movies), 1))\n  \n  list(\n    noisy_movies = noisy_movies,\n    shifted_movies = shifted_movies\n  )\n}\n\n\n# Data Preparation --------------------------------------------------------\n\n# Artificial data generation:\n  # Generate movies with 3 to 7 moving squares inside.\n  # The squares are of shape 1x1 or 2x2 pixels, which move linearly over time.\n  # For convenience we first create movies with bigger width and height (80x80)\n  # and at the end we select a 40x40 window.\nmovies <- generate_movies(n_samples = 1000, n_frames = 15)\nmore_movies <- generate_movies(n_samples = 200, n_frames = 15)\n\n\n# Model definition --------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>%\n\n  # Begin with 2D convolutional LSTM layer\n  layer_conv_lstm_2d(\n    input_shape = list(NULL,40,40,1), \n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", \n    return_sequences = TRUE\n  ) %>%\n  # Normalize the activations of the previous layer\n  layer_batch_normalization() %>%\n  \n  # Add 3x hidden 2D convolutions LSTM layers, with\n  # batch normalization layers between\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>% \n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  \n  # Add final 3D convolutional output layer \n  layer_conv_3d(\n    filters = 1, kernel_size = c(3,3,3),\n    activation = \"sigmoid\", \n    padding = \"same\", data_format =\"channels_last\"\n  )\n\n# Prepare model for training\nmodel %>% compile(\n  loss = \"binary_crossentropy\", \n  optimizer = \"adadelta\"\n)\n\nmodel\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  movies$noisy_movies,\n  movies$shifted_movies,\n  batch_size = 10,\n  epochs = 30, \n  validation_split = 0.05\n)\n\n\n# Visualization  ----------------------------------------------------------------\n\n# Testing the network on one movie\n# feed it with the first 7 positions and then\n# predict the new positions\n\n#Example to visualize on\nwhich <- 100\n\ntrack <- more_movies$noisy_movies[which,1:8,,,1]\ntrack <- array(track, c(1,8,40,40,1))\n\nfor (k in 1:15){\n  if (k<8){ \n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    dev.off()\n  } else {\n    \n    # And then compare the predictions to the ground truth\n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k))\n    \n    # Make Prediction\n    new_pos <- model %>% predict(track)\n   \n    # Slice the last row  \n    new_pos_loc <- new_pos[1,k,1:40,1:40,1]  \n    new_pos_loc  %>% raster() %>% plot() %>% title (main=paste0('Pred_',k))    \n    \n    # Reshape it\n    new_pos <- array(new_pos_loc, c(1,1, 40,40,1))     \n    \n    # Bind it to the earlier data\n    track <- abind(track,new_pos,along = 2)  \n    dev.off()\n  }\n} \n\n# Can also create a gif by running\nsystem(\"convert -delay 40 *.png animation.gif\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}