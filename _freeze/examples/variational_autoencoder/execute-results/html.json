{
  "hash": "6a5b30ad95a7f4ae7250f2cbddebff80",
  "result": {
    "markdown": "---\ntitle: variational_autoencoder\ndescription: Demonstrates how to build a variational autoencoder.\n---\n\nThis script demonstrates how to build a variational autoencoder with Keras.\nReference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nK <- keras::backend()\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 100L\noriginal_dim <- 784L\nlatent_dim <- 2L\nintermediate_dim <- 256L\nepochs <- 50L\nepsilon_std <- 1.0\n\n# Model definition --------------------------------------------------------\n\nx <- layer_input(shape = c(original_dim))\nh <- layer_dense(x, intermediate_dim, activation = \"relu\")\nz_mean <- layer_dense(h, latent_dim)\nz_log_var <- layer_dense(h, latent_dim)\n\nsampling <- function(arg){\n  z_mean <- arg[, 1:(latent_dim)]\n  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]), \n    mean=0.,\n    stddev=epsilon_std\n  )\n  \n  z_mean + k_exp(z_log_var/2)*epsilon\n}\n\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% \n  layer_lambda(sampling)\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_mean <- layer_dense(units = original_dim, activation = \"sigmoid\")\nh_decoded <- decoder_h(z)\nx_decoded_mean <- decoder_mean(h_decoded)\n\n# end-to-end autoencoder\nvae <- keras_model(x, x_decoded_mean)\n\n# encoder, from inputs to latent space\nencoder <- keras_model(x, z_mean)\n\n# generator, from latent space to reconstructed inputs\ndecoder_input <- layer_input(shape = latent_dim)\nh_decoded_2 <- decoder_h(decoder_input)\nx_decoded_mean_2 <- decoder_mean(h_decoded_2)\ngenerator <- keras_model(decoder_input, x_decoded_mean_2)\n\n\nvae_loss <- function(x, x_decoded_mean){\n  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)\n  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)\n  xent_loss + kl_loss\n}\n\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\n\n\n# Data preparation --------------------------------------------------------\n\nmnist <- dataset_mnist()\nx_train <- mnist$train$x/255\nx_test <- mnist$test$x/255\nx_train <- array_reshape(x_train, c(nrow(x_train), 784), order = \"F\")\nx_test <- array_reshape(x_test, c(nrow(x_test), 784), order = \"F\")\n\n\n# Model training ----------------------------------------------------------\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n# Visualizations ----------------------------------------------------------\n\nlibrary(ggplot2)\nlibrary(dplyr)\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\n\nx_test_encoded %>%\n  as_data_frame() %>% \n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n# display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = 28) )\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}