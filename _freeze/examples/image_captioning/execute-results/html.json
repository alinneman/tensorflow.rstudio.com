{
  "hash": "fe172839a8171667670c8dfd2e1b614f",
  "result": {
    "markdown": "---\ntitle: Image Captioning\nauthors: \n  - \"[A_K_Nain](https://twitter.com/A_K_Nain)\"\n  - \"[dfalbel](https://github.com/dfalbel) - R translation\"\ndescription: Implement an image captioning model using a CNN and a Transformer.\naliases:\n  - ../guide/keras/examples/eager_image_captioning/index.html\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n```\n:::\n\n\n## Download the dataset\n\n\nWe will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n8,000 images, that are each paired with five different captions.\n\n::: {.cell}\n\n```{.r .cell-code}\nflickr_images <- get_file(\n  \"fickr8k.zip\",\n  \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n)\n\nflickr_text <- get_file(\n  \"flickr9k_text.zip\",\n  \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n)\n\n\nif (!fs::dir_exists(fs::path(fs::path_dir(flickr_text), \"Flicker8k_Dataset\"))) {\n  unzip(flickr_images, exdir = fs::path_dir(flickr_images))\n  unzip(flickr_text, exdir = fs::path_dir(flickr_text))  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Path to the images\nIMAGES_PATH <- \"Flicker8k_Dataset\"\n\n# Desired image dimensions\nIMAGE_SIZE <- shape(299, 299)\n\n# Vocabulary size\nVOCAB_SIZE <- 10000\n\n# Fixed length allowed for any sequence\nSEQ_LENGTH <- 25\n\n# Dimension for the image embeddings and token embeddings\nEMBED_DIM <- 512\n\n# Per-layer units in the feed-forward network\nFF_DIM <- 512\n\n# Other training parameters\n\nBATCH_SIZE <- 64\nEPOCHS <- 30\nAUTOTUNE <- tf$data$AUTOTUNE\n```\n:::\n\n\n\n## Preparing the dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncaptions <- fs::path(fs::path_dir(flickr_text), \"Flickr8k.token.txt\") %>% \n  readr::read_delim(\n    col_names = c(\"img\", \"caption\"),\n    delim = \"\\t\"\n  ) %>% \n  tidyr::separate(img, into = c(\"img\", \"caption_id\"), sep = \"#\") %>% \n  dplyr::select(img, caption) %>% \n  dplyr::group_by(img) %>% \n  dplyr::summarise(caption = list(caption)) %>% \n  dplyr::mutate(img = fs::path(fs::path_dir(flickr_text), \"Flicker8k_Dataset\", img))\n\n\ntrain <- fs::path(fs::path_dir(flickr_text), \"Flickr_8k.trainImages.txt\") %>% \n  readr::read_lines() \n  \nvalid <- fs::path(fs::path_dir(flickr_text), \"Flickr_8k.devImages.txt\") %>% \n  readr::read_lines()\n\ntest <- fs::path(fs::path_dir(flickr_text), \"Flickr_8k.testImages.txt\") %>% \n  readr::read_lines()\n\n\ntrain_data <- captions %>% \n  dplyr::filter(fs::path_file(img) %in% train)\n\nvalid_data <- captions %>% \n  dplyr::filter(fs::path_file(img) %in% test)\n\ndplyr::n_distinct(train_data$img)\ndplyr::n_distinct(valid_data$img)\n```\n:::\n\n\n## Vectorizing the text data\n\n\nWe'll use the `text_vectorization` layer to vectorize the text data,\nthat is to say, to turn the\noriginal strings into integer sequences where each integer represents the index of\na word in a vocabulary. We will use a custom string standardization scheme\n(strip punctuation characters except `<` and `>`) and the default\nsplitting scheme (split on whitespace).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunctuation <- c(\"!\", \"\\\\\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \n\"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"=\", \"?\", \"@\", \"[\", \n\"\\\\\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\")\nre <- reticulate::import(\"re\")\npunctuation_group <- punctuation %>% \n  sapply(re$escape) %>% \n  paste0(collapse = \"\") %>% \n  sprintf(\"[%s]\", .)\n\ncustom_standardization <- function(input_string) {\n  lowercase <- tf$strings$lower(input_string)\n  tf$strings$regex_replace(lowercase, punctuation_group, \"\")\n}\n\nvectorization <- layer_text_vectorization(\n    max_tokens = VOCAB_SIZE,\n    output_mode = \"int\",\n    output_sequence_length = SEQ_LENGTH,\n    standardize = custom_standardization,\n)\nvectorization %>% adapt(unlist(train_data$caption))\n\n# Data augmentation for image data\n\nimage_augmentation <- keras_model_sequential() %>% \n  layer_random_flip(\"horizontal\") %>% \n  layer_random_rotation(0.2) %>% \n  layer_random_contrast(0.3)\n```\n:::\n\n\n\n## Building a TensorFlow dataset pipeline for training\n\nWe will generate pairs of images and corresponding captions using a `tf$data$Dataset` object.\nThe pipeline consists of two steps:\n\n1. Read the image from the disk\n2. Tokenize all the five captions corresponding to the image\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecode_and_resize <- function(img_path) {\n  img_path %>% \n    tf$io$read_file() %>% \n    tf$image$decode_jpeg(channels = 3) %>% \n    tf$image$resize(IMAGE_SIZE) %>% \n    tf$image$convert_image_dtype(tf$float32)\n}\n\n\nprocess_input <- function(img_path, captions) {\n  reticulate::tuple(\n    decode_and_resize(img_path), \n    vectorization(captions)\n  )\n}\n\nmake_dataset <- function(data) {\n  data %>% unname() %>% \n    tensor_slices_dataset() %>% \n    dataset_shuffle(nrow(data)) %>% \n    dataset_map(process_input, num_parallel_calls = AUTOTUNE) %>% \n    dataset_batch(BATCH_SIZE) %>% \n    dataset_prefetch(AUTOTUNE)\n}\n\n\n# Pass the list of images and the list of corresponding captions\ntrain_dataset <- make_dataset(train_data)\nvalid_dataset <- make_dataset(valid_data)\n```\n:::\n\n\n\n## Building the model\n\nOur image captioning architecture consists of three models:\n\n1. A CNN: used to extract the image features\n2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n                    based encoder that generates a new representation of the inputs\n3. A TransformerDecoder: This model takes the encoder output and the text data\n                    (sequences) as inputs and tries to learn to generate the caption.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_cnn_model <- function() {\n  base_model <- application_efficientnet_b0(\n    input_shape = c(IMAGE_SIZE, 3),\n    include_top = FALSE,\n    weights = \"imagenet\"\n  )\n  # We freeze our feature extractor\n  base_model$trainable <- FALSE\n  base_model_out <- base_model$output %>% \n    layer_reshape(target_shape = c(-1, tail(dim(base_model$output), 1)))\n  keras_model(base_model$input, base_model_out)\n}\n\n\ntransformer_encoder_block <- new_layer_class(\n  \"transformer_encoder_block\", \n  initialize = function(embed_dim, dense_dim, num_heads, ...) {\n    super()$`__init__`(...)\n    self$embed_dim <- embed_dim\n    self$dense_dim <- dense_dim\n    self$num_heads <- num_heads\n    self$attention_1 <- layer_multi_head_attention(\n      num_heads = num_heads, key_dim = embed_dim, dropout = 0.0\n    )\n    self$layernorm_1 <- layer_normalization()\n    self$layernorm_2 <- layer_normalization()\n    self$dense_1 <- layer_dense(units = embed_dim, activation = \"relu\")\n  },\n  call = function(inputs, training, mask = NULL) {\n    inputs <- self$layernorm_1(inputs)\n    inputs <- self$dense_1(inputs)\n    \n    attention_output_1 <- self$attention_1(\n      query = inputs,\n      value = inputs,\n      key = inputs,\n      attention_mask = NULL,\n      training = training,\n    )\n    out_1 <- self$layernorm_2(inputs + attention_output_1)\n  \n    out_1\n  }\n)\n\npositional_embedding <- new_layer_class(\n  \"positional_embedding\",\n  initialize = function(sequence_length, vocab_size, embed_dim, ...) {\n    super()$`__init__`(...)\n    self$token_embeddings <- layer_embedding(\n      input_dim = vocab_size, output_dim = embed_dim\n    )\n    self$position_embeddings <- layer_embedding(\n      input_dim = sequence_length, output_dim = embed_dim\n    )\n    self$sequence_length <- sequence_length\n    self$vocab_size <- vocab_size\n    self$embed_dim <- embed_dim\n    self$embed_scale <- tf$math$sqrt(tf$cast(embed_dim, tf$float32))\n  },\n  call = function(inputs) {\n    length <- tail(dim(inputs), 1)\n    positions <- tf$range(start = 0L, limit = length, delta = 1L)\n    embedded_tokens <- self$token_embeddings(inputs)\n    embedded_tokens <- embedded_tokens * self$embed_scale\n    embedded_positions <- self$position_embeddings(positions)\n    embedded_tokens + embedded_positions\n  },\n  compute_mask = function(inputs, mask) {\n    tf$math$not_equal(inputs, 0L)\n  }\n)\n\ntransformer_decoder_block <- new_layer_class(\n  \"transformer_decoder_block\",\n  initialize = function(embed_dim, ff_dim, num_heads, ...) {\n    super()$`__init__`(...)\n    self$embed_dim <- embed_dim\n    self$ff_dim <- ff_dim\n    self$num_heads <- num_heads\n    self$attention_1 <- layer_multi_head_attention(\n      num_heads = num_heads, key_dim = embed_dim, dropout = 0.1\n    )\n    self$attention_2 <- layer_multi_head_attention(\n      num_heads = num_heads, key_dim = embed_dim, dropout = 0.1\n    )\n    self$ffn_layer_1 <- layer_dense(units = ff_dim, activation = \"relu\")\n    self$ffn_layer_2 <- layer_dense(units = embed_dim)\n    \n    self$layernorm_1 <- layer_normalization()\n    self$layernorm_2 <- layer_normalization()\n    self$layernorm_3 <- layer_normalization()\n    \n    self$embedding <- positional_embedding(\n      embed_dim = EMBED_DIM, sequence_length = SEQ_LENGTH, vocab_size = VOCAB_SIZE\n    )\n    self$out <- layer_dense(units = VOCAB_SIZE, activation = \"softmax\")\n    \n    self$dropout_1 <- layer_dropout(rate = 0.3)\n    self$dropout_2 <- layer_dropout(rate = 0.5)\n    self$supports_masking <- TRUE\n  },\n  call = function(inputs, encoder_outputs, training, mask = NULL) {\n    inputs <- self$embedding(inputs)\n    causal_mask <- self$get_causal_attention_mask(inputs)\n    \n    if(!is.null(mask)) {\n      padding_mask <- tf$cast(mask[, , tf$newaxis], dtype = tf$int32)\n      combined_mask <- tf$cast(mask[, tf$newaxis, ], dtype = tf$int32)\n      combined_mask <- tf$minimum(combined_mask, causal_mask)\n    }\n    \n    attention_output_1 <- self$attention_1(\n      query = inputs,\n      value = inputs,\n      key = inputs,\n      attention_mask = combined_mask,\n      training = training,\n    )\n    out_1 <- self$layernorm_1(inputs + attention_output_1)\n    \n    attention_output_2 <- self$attention_2(\n      query = out_1,\n      value = encoder_outputs,\n      key = encoder_outputs,\n      attention_mask = padding_mask,\n      training = training,\n    )\n    out_2 <- self$layernorm_2(out_1 + attention_output_2)\n    \n    ffn_out <- self$ffn_layer_1(out_2)\n    ffn_out <- self$dropout_1(ffn_out, training = training)\n    ffn_out <- self$ffn_layer_2(ffn_out)\n    \n    ffn_out <- self$layernorm_3(ffn_out + out_2, training = training)\n    ffn_out <- self$dropout_2(ffn_out, training = training)\n    preds <- self$out(ffn_out)\n    preds\n  },\n  get_causal_attention_mask = function(inputs) {\n    input_shape <- tf$shape(inputs)\n    batch_size <- input_shape[1]\n    sequence_length <- input_shape[2]\n    i <- tf$range(sequence_length)[, tf$newaxis]\n    j <- tf$range(sequence_length)\n    mask <- tf$cast(i >= j, dtype = \"int32\")\n    mask <- tf$reshape(mask, list(1L, input_shape[2], input_shape[2]))\n    mult <- tf$concat(list(\n      tf$expand_dims(batch_size, -1L), \n      as_tensor(c(1L, 1L), dtype = tf$int32)\n    ), axis = 0L)\n    tf$tile(mask, mult)\n  }\n)\n\nimage_captioning_model <- new_model_class(\n  \"image_captioning_model\",\n  initialize = function(cnn_model, encoder, decoder, num_captions_per_image = 5,\n                        image_aug = NULL) {\n    super()$`__init__`()\n    self$cnn_model <- cnn_model\n    self$encoder <- encoder\n    self$decoder <- decoder\n    self$loss_tracker <- metric_mean(name = \"loss\")\n    self$acc_tracker <- metric_mean(name = \"accuracy\")\n    self$num_captions_per_image <- num_captions_per_image\n    self$image_aug <- image_aug\n  },\n  calculate_loss = function(y_true, y_pred, mask) {\n    loss <- self$loss(y_true, y_pred)\n    mask <- tf$cast(mask, dtype = loss$dtype)\n    loss <- loss* mask\n    tf$reduce_sum(loss) / tf$reduce_sum(mask)\n  },\n  calculate_accuracy = function(y_true, y_pred, mask) {\n    accuracy <- tf$equal(y_true, tf$argmax(y_pred, axis = 2L))\n    accuracy <- tf$math$logical_and(mask, accuracy)\n    accuracy <- tf$cast(accuracy, dtype = tf$float32)\n    mask <- tf$cast(mask, dtype = tf$float32)\n    tf$reduce_sum(accuracy) / tf$reduce_sum(mask)\n  },\n  .compute_caption_loss_and_acc = function(img_embed, batch_seq, training = TRUE) {\n    encoder_out <- self$encoder(img_embed, training = training)\n    batch_seq_inp <- batch_seq[, NULL:-2]\n    batch_seq_true <- batch_seq[, 2:NULL]\n    mask <- tf$math$not_equal(batch_seq_true, 0L)\n    batch_seq_pred <- self$decoder(\n      batch_seq_inp, encoder_out, training = training, mask = mask\n    )\n    loss <- self$calculate_loss(batch_seq_true, batch_seq_pred, mask)\n    acc <- self$calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n    list(loss, acc)\n  },\n  train_step = function(batch_data) {\n    batch_img <- batch_data[[1]] \n    batch_seq <- batch_data[[2]]\n    batch_loss <- 0\n    batch_acc <- 0\n    \n    if (!is.null(self$image_aug)){\n      batch_img <- self$image_aug(batch_img)\n    }\n    \n    \n    # 1. Get image embeddings\n    img_embed <- self$cnn_model(batch_img)\n    \n    # 2. Pass each of the five captions one by one to the decoder\n    # along with the encoder outputs and compute the loss as well as accuracy\n    # for each caption.\n    for (i in seq_len(self$num_captions_per_image)) {\n      with(tf$GradientTape() %as% tape, {    \n        c(loss, acc) %<-% self$.compute_caption_loss_and_acc(\n          img_embed, batch_seq[, i, ], training = TRUE\n        )\n        \n        # 3. Update loss and accuracy\n        batch_loss <- batch_loss + loss \n        batch_acc <- batch_acc + acc    \n      })\n      \n      # 4. Get the list of all the trainable weights\n      train_vars <- c(self$encoder$trainable_variables,\n                      self$decoder$trainable_variables)\n      \n      # 5. Get the gradients\n      grads <- tape$gradient(loss, train_vars)\n      \n      # 6. Update the trainable weights\n      self$optimizer$apply_gradients(zip_lists(grads, train_vars))\n    }\n    \n    # 7. Update the trackers\n    batch_acc <- batch_acc/self$num_captions_per_image\n    self$loss_tracker$update_state(batch_loss)\n    self$acc_tracker$update_state(batch_acc)\n    \n    # 8. Return the loss and accuracy values\n    list(\n      loss = self$loss_tracker$result(), \n      acc = self$acc_tracker$result()\n    )\n  },\n  test_step = function(batch_data) {\n    batch_img <- batch_data[[1]] \n    batch_seq <- batch_data[[2]]\n    batch_loss <- 0\n    batch_acc <- 0\n\n    # 1. Get image embeddings\n    img_embed <- self$cnn_model(batch_img)\n    \n    # 2. Pass each of the five captions one by one to the decoder\n    # along with the encoder outputs and compute the loss as well as accuracy\n    # for each caption.\n    for (i in seq_len(self$num_captions_per_image)) {\n      with(tf$GradientTape() %as% tape, {    \n        c(loss, acc) %<-% self$.compute_caption_loss_and_acc(\n          img_embed, batch_seq[, i, ], training = TRUE\n        )\n        \n        # 3. Update loss and accuracy\n        batch_loss <- batch_loss + loss \n        batch_acc <- batch_acc + acc    \n      })\n    }\n    \n    batch_acc <- batch_acc / self$num_captions_per_image\n    \n    # 4. Update the trackers\n    self$loss_tracker$update_state(batch_loss)\n    self$acc_tracker$update_state(batch_acc)\n    \n    # 5. Return the loss and accuracy values\n    list(\n      \"loss\" = self$loss_tracker$result(), \n      \"acc\" = self$acc_tracker$result()\n    )\n  },\n  metrics = mark_active(function() {\n    # We need to list our metrics here so the `reset_states()` can be\n    # called automatically.\n    list(self$loss_tracker, self$acc_tracker)\n  })\n)\n\n\ncnn_model <- get_cnn_model()\nencoder <- transformer_encoder_block(embed_dim = EMBED_DIM, dense_dim = FF_DIM, num_heads = 1)\ndecoder <- transformer_decoder_block(embed_dim = EMBED_DIM, ff_dim = FF_DIM, num_heads = 2)\ncaption_model <- image_captioning_model(\n  cnn_model = cnn_model,\n  encoder = encoder,\n  decoder = decoder,\n  image_aug = image_augmentation\n)\n```\n:::\n\n\n\n## Model training\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the loss function\ncross_entropy <- loss_sparse_categorical_crossentropy(\n    from_logits = FALSE, reduction = \"none\"\n)\n\n# EarlyStopping criteria\nearly_stopping <- callback_early_stopping(patience = 3, restore_best_weights = TRUE)\n\n\n# Learning Rate Scheduler for the optimizer\nlr_schedule <- new_learning_rate_schedule_class(\n  \"lr_schedule\",\n  initialize = function(post_warmup_learning_rate, warmup_steps) {\n    super()$`__init__`()\n    self$post_warmup_learning_rate <- post_warmup_learning_rate\n    self$warmup_steps <- warmup_steps\n  },\n  call = function(step) {\n    global_step <- tf$cast(step, tf$float32)\n    warmup_steps <- tf$cast(self$warmup_steps, tf$float32)\n    warmup_progress <- global_step / warmup_steps\n    warmup_learning_rate <- self$post_warmup_learning_rate * warmup_progress\n    tf$cond(\n      global_step < warmup_steps,\n      function() warmup_learning_rate,\n      function() self$post_warmup_learning_rate\n    )\n  }\n)\n\n\n\n# Create a learning rate schedule\nnum_train_steps <- length(train_dataset) * EPOCHS\nnum_warmup_steps <- num_train_steps %/% 15\nlr <- lr_schedule(post_warmup_learning_rate = 1e-4, warmup_steps = num_warmup_steps)\n\n# Compile the model\ncaption_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = lr), \n  loss = cross_entropy\n)\n\n# Fit the model\ncaption_model %>% fit(\n  train_dataset,\n  epochs = EPOCHS,\n  validation_data = valid_dataset,\n  callbacks = list(early_stopping)\n)\n```\n:::\n\n\n\n## Check sample predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvocab <- get_vocabulary(vectorization)\nmax_decoded_sentence_length <- SEQ_LENGTH - 1\nvalid_images <- valid_data$img\n\n\ngenerate_caption <- function() {\n  # Select a random image from the validation dataset\n  sample_img <- sample(valid_images, 1)\n  \n  # Read the image from the disk\n  sample_img <- decode_and_resize(sample_img)\n  img <- as.array(tf$clip_by_value(sample_img, 0, 255))\n  img %>% as.raster(max = 255) %>% plot()\n  \n  # Pass the image to the CNN\n  img <- tf$expand_dims(sample_img, 0L)\n  img <- caption_model$cnn_model(img)\n  \n  # Pass the image features to the Transformer encoder\n  encoded_img <- caption_model$encoder(img, training = FALSE)\n  \n  # Generate the caption using the Transformer decoder\n  decoded_caption <- \"<start> \"\n  for (i in seq_len(max_decoded_sentence_length)) {\n    tokenized_caption <- vectorization(list(decoded_caption))\n    mask <- tf$math$not_equal(tokenized_caption, 0L)\n    predictions <- caption_model$decoder(\n      tokenized_caption, encoded_img, training = FALSE, mask = mask\n    )\n    sampled_token_index <- tf$argmax(predictions[1, i, ])\n    sampled_token <- vocab[as.integer(sampled_token_index) + 1]\n    \n    if (sampled_token == \" <end>\") {\n      break\n    }\n\n    decoded_caption <- paste(decoded_caption, sampled_token, sep = \" \")\n  }\n    \n  cat(\"Predicted Caption: \", decoded_caption)\n}\n\n# Check predictions for a few samples\n\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\n```\n:::\n\n\n## End Notes\n\nWe saw that the model starts to generate reasonable captions after a few epochs. To keep\nthis example easily runnable, we have trained it with a few constraints, like a minimal\nnumber of attention heads. To improve the predictions, you can try changing these training\nsettings and find a good model for your use case.\n",
    "supporting": [
      "image_captioning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}