{
  "hash": "085081d32211d504988bb08672f2f09c",
  "result": {
    "markdown": "---\ntitle: Simple CNN on CIFAR10 dataset\ndescription: Trains a simple deep CNN on the CIFAR10 small images dataset.\ncategories: [cv]\naliases:\n  - ../guide/keras/examples/cifar10_cnn/index.html\n---\n\nTrain a simple deep CNN on the CIFAR10 small images dataset.\n \nIt gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs,\nthough it is still underfitting at that point.\n\nIf doing data augmentation you may try increasing the number of filters in convolutions\nand in dense layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 32\nepochs <- 50\ndata_augmentation <- FALSE\n\n\n# Data Preparation --------------------------------------------------------\n\n# See ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Feature scale RGB values in test and train inputs  \nx_train <- cifar10$train$x/255\nx_test <- cifar10$test$x/255\ny_train <- cifar10$train$y\ny_test <- cifar10$test$y\n\n\n# Defining Model ----------------------------------------------------------\n\n# Initialize sequential model\nmodel <- keras_model_sequential()\n\n\nif (data_augmentation) {\n  data_augmentation = keras_model_sequential() %>% \n    layer_random_flip(\"horizontal\") %>% \n    layer_random_rotation(0.2)\n  \n  model <- model %>% \n    data_augmentation()\n}\n\nmodel <- model %>%\n  # Start with hidden 2D convolutional layer being fed 32x32 pixel images\n  layer_conv_2d(\n    filter = 16, kernel_size = c(3,3), padding = \"same\", \n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation_leaky_relu(0.1) %>% \n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu(0.1) %>% \n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\") %>%\n  layer_activation_leaky_relu(0.1) %>% \n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu(0.1) %>% \n\n  # Use max pooling once more\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # Flatten max filtered output into feature vector \n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(256) %>%\n  layer_activation_leaky_relu(0.1) %>% \n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10 unit output layer\n  layer_dense(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in layer_conv_2d(., filter = 64, kernel_size = c(3, 3)): partial\nargument match of 'filter' to 'filters'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in layer_conv_2d(., filter = 32, kernel_size = c(3, 3), padding =\n\"same\"): partial argument match of 'filter' to 'filters'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in layer_conv_2d(., filter = 32, kernel_size = c(3, 3)): partial\nargument match of 'filter' to 'filters'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in layer_conv_2d(., filter = 16, kernel_size = c(3, 3), padding =\n\"same\", : partial argument match of 'filter' to 'filters'\n```\n:::\n\n```{.r .cell-code}\nopt <- optimizer_adamax(learning_rate = learning_rate_schedule_exponential_decay(\n  initial_learning_rate = 5e-3, \n  decay_rate = 0.96, \n  decay_steps = 1500, \n  staircase = TRUE\n))\n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = opt,\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, y_test),\n  shuffle = TRUE\n)\n\nmodel %>% evaluate(x_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.6197287 0.8200000 \n```\n:::\n:::\n",
    "supporting": [
      "cifar10_cnn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}