{
  "hash": "7f934d5590c1aeee205fb2e9a45babc2",
  "result": {
    "markdown": "---\ntitle: mnist_tfrecord\ndescription: MNIST dataset with TFRecords, the standard TensorFlow data format.\n---\n\nMNIST dataset with TFRecords, the standard TensorFlow data format.\n\nTFRecord is a data format supported throughout TensorFlow. This example\ndemonstrates how to load TFRecord data using Input Tensors. Input Tensors\ndiffer from the normal Keras workflow because instead of fitting to data\nloaded into a a numpy array, data is supplied via a special tensor that reads\ndata from nodes that are wired directly into model graph with the\n`layer_input(tensor=input_tensor)` parameter.\n\nThere are several advantages to using Input Tensors. First, if a dataset is\nalready in TFRecord format you can load and train on that data directly in\nKeras. Second, extended backend API capabilities such as TensorFlow data\naugmentation is easy to integrate directly into your Keras training scripts\nvia input tensors. Third, TensorFlow implements several data APIs for\nTFRecords, some of which provide significantly faster training performance\nthan numpy arrays can provide because they run via the C++ backend. Please\nnote that this example is tailored for brevity and clarity and not to\ndemonstrate performance or augmentation capabilities.\n\nInput Tensors also have important disadvantages. In particular, Input Tensors\nare fixed at model construction because rewiring networks is not yet\nsupported. For this reason, changing the data input source means model\nweights must be saved and the model rebuilt from scratch to connect the new\ninput data. validation cannot currently be performed as training progresses,\nand must be performed after training completes. This example demonstrates how\nto train with input tensors, save the model weights, and then evaluate the\nmodel using the standard Keras API.\n\nGets to ~99.1% validation accuracy after 5 epochs (there is still a lot of margin\nfor parameter tuning).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tensorflow)\n\nif (k_backend() != 'tensorflow') {\n  stop('This example can only run with the ',\n       'TensorFlow backend, ',\n       'because it requires TFRecords, which ',\n       'are not supported on other platforms.')\n}\n\n# Define Model -------------------------------------------------------------------\n\ncnn_layers <- function(x_train_input) {\n  x_train_input %>% \n    layer_conv_2d(filters = 32, kernel_size = c(3,3), \n                  activation = 'relu', padding = 'valid') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_flatten() %>% \n    layer_dense(units = 512, activation = 'relu') %>% \n    layer_dropout(rate = 0.5) %>% \n    layer_dense(units = classes, activation = 'softmax', name = 'x_train_out')\n}\n\nsess <- k_get_session()\n\n# Data Preparation --------------------------------------------------------------\n\nbatch_size <- 128L\nbatch_shape = list(batch_size, 28L, 28L, 1L)\nsteps_per_epoch <- 469L\nepochs <- 5L\nclasses <- 10L\n\n# The capacity variable controls the maximum queue size\n# allowed when prefetching data for training.\ncapacity <- 10000L\n\n# min_after_dequeue is the minimum number elements in the queue\n# after a dequeue, which ensures sufficient mixing of elements.\nmin_after_dequeue <- 3000L\n\n# If `enqueue_many` is `FALSE`, `tensors` is assumed to represent a\n# single example.  An input tensor with shape `(x, y, z)` will be output\n# as a tensor with shape `(batch_size, x, y, z)`.\n#\n# If `enqueue_many` is `TRUE`, `tensors` is assumed to represent a\n# batch of examples, where the first dimension is indexed by example,\n# and all members of `tensors` should have the same size in the\n# first dimension.  If an input tensor has shape `(*, x, y, z)`, the\n# output will have shape `(batch_size, x, y, z)`.\nenqueue_many <- TRUE\n\n# mnist dataset from tf contrib\nmnist <- tf$contrib$learn$datasets$mnist\ndata <- mnist$load_mnist()\n\ntrain_data <- tf$train$shuffle_batch(\n  tensors = list(data$train$images, data$train$labels),\n  batch_size = batch_size,\n  capacity = capacity,\n  min_after_dequeue = min_after_dequeue,\n  enqueue_many = enqueue_many,\n  num_threads = 8L\n)\nx_train_batch <- train_data[[1]]\ny_train_batch <- train_data[[2]]\n\nx_train_batch <- tf$cast(x_train_batch, tf$float32)\nx_train_batch <- tf$reshape(x_train_batch, shape = batch_shape)\n\ny_train_batch <- tf$cast(y_train_batch, tf$int32)\ny_train_batch <- tf$one_hot(y_train_batch, classes)\n\nx_batch_shape <- x_train_batch$get_shape()$as_list()\ny_batch_shape = y_train_batch$get_shape()$as_list()\n\nx_train_input <- layer_input(tensor = x_train_batch, batch_shape = x_batch_shape)\nx_train_out <- cnn_layers(x_train_input)\n\n# Training & Evaluation ---------------------------------------------------------\n\ntrain_model = keras_model(inputs = x_train_input, outputs = x_train_out)\n\n# Pass the target tensor `y_train_batch` to `compile`\n# via the `target_tensors` keyword argument:\ntrain_model %>% compile(\n  optimizer = optimizer_rmsprop(lr = 2e-3, decay = 1e-5),\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy'),\n  target_tensors = y_train_batch\n)\n\nsummary(train_model)\n\n# Fit the model using data from the TFRecord data tensors.\ncoord <- tf$train$Coordinator()\nthreads = tf$train$start_queue_runners(sess, coord)\n\ntrain_model %>% fit(\n  epochs = epochs,\n  steps_per_epoch = steps_per_epoch\n)\n\n# Save the model weights.\ntrain_model %>% save_model_weights_hdf5('saved_wt.h5')\n\n# Clean up the TF session.\ncoord$request_stop()\ncoord$join(threads)\nk_clear_session()\n\n# Second Session to test loading trained model without tensors\nx_test <- data$validation$images\nx_test <- array_reshape(x_test, dim = c(nrow(x_test), 28, 28, 1))\ny_test <- data$validation$labels\nx_test_inp <- layer_input(shape = dim(x_test)[-1])\ntest_out <- cnn_layers(x_test_inp)\ntest_model <- keras_model(inputs = x_test_inp, outputs = test_out)\ntest_model %>% load_model_weights_hdf5('saved_wt.h5')\ntest_model %>% compile(\n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy')\n)\nsummary(test_model)\n\nresult <- test_model %>% evaluate(x_test, to_categorical(y_test, classes))\ncat(sprintf('\\nTest accuracy: %f', result$acc))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}