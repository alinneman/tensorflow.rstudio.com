{
  "hash": "010ce255894c3e3ee0b8b9679f05066d",
  "result": {
    "markdown": "---\ntitle: vq_vae\ndescription: Discrete Representation Learning with VQ-VAE and TensorFlow Probability.\n---\n\nThis is the companion code to the post\n\"Discrete Representation Learning with VQ-VAE and TensorFlow Probability\"\non the TensorFlow for R blog.\n\nhttps://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nuse_session_with_seed(7778,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\nlibrary(curry)\n\nmoving_averages <- tf$python$training$moving_averages\n\n\n# Utilities --------------------------------------------------------\n\nvisualize_images <-\n  function(dataset,\n           epoch,\n           reconstructed_images,\n           random_images) {\n    write_png(dataset, epoch, \"reconstruction\", reconstructed_images)\n    write_png(dataset, epoch, \"random\", random_images)\n    \n  }\n\nwrite_png <- function(dataset, epoch, desc, images) {\n  png(paste0(dataset, \"_epoch_\", epoch, \"_\", desc, \".png\"))\n  par(mfcol = c(8, 8))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:64) {\n    img <- images[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n  \n}\n\n\n# Setup and preprocessing -------------------------------------------------\n\nnp <- import(\"numpy\")\n\n# download from: https://github.com/rois-codh/kmnist\nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 64\nnum_examples_to_generate <- batch_size\n\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n# test\niter <- make_iterator_one_shot(train_dataset)\nbatch <-  iterator_get_next(iter)\nbatch %>% dim()\n\n# Params ------------------------------------------------------------------\n\nlearning_rate <- 0.001\nlatent_size <- 1\nnum_codes <- 64L\ncode_size <- 16L\nbase_depth <- 32\nactivation <- \"elu\"\nbeta <- 0.25\ndecay <- 0.99\ninput_shape <- c(28, 28, 1)\n\n# Models -------------------------------------------------------------------\n\ndefault_conv <-\n  set_defaults(layer_conv_2d, list(padding = \"same\", activation = activation))\ndefault_deconv <-\n  set_defaults(layer_conv_2d_transpose,\n               list(padding = \"same\", activation = activation))\n\n# Encoder ------------------------------------------------------------------\n\nencoder_model <- function(name = NULL,\n                          code_size) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <- default_conv(filters = base_depth, kernel_size = 5)\n    self$conv2 <-\n      default_conv(filters = base_depth,\n                   kernel_size = 5,\n                   strides = 2)\n    self$conv3 <-\n      default_conv(filters = 2 * base_depth, kernel_size = 5)\n    self$conv4 <-\n      default_conv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$conv5 <-\n      default_conv(\n        filters = 4 * latent_size,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = latent_size * code_size)\n    self$reshape <-\n      layer_reshape(target_shape = c(latent_size, code_size))\n    \n    function (x, mask = NULL) {\n      x %>%\n        # output shape:  7 28 28 32\n        self$conv1() %>%\n        # output shape:  7 14 14 32\n        self$conv2() %>%\n        # output shape:  7 14 14 64\n        self$conv3() %>%\n        # output shape:  7 7 7 64\n        self$conv4() %>%\n        # output shape:  7 1 1 4\n        self$conv5() %>%\n        # output shape:  7 4\n        self$flatten() %>%\n        # output shape:  7 16\n        self$dense() %>%\n        # output shape:  7 1 16\n        self$reshape()\n    }\n    \n  })\n}\n\n\n# Decoder ------------------------------------------------------------------\n\ndecoder_model <- function(name = NULL,\n                          input_size,\n                          output_shape) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$reshape1 <- layer_reshape(target_shape = c(1, 1, input_size))\n    self$deconv1 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$deconv2 <-\n      default_deconv(filters = 2 * base_depth, kernel_size = 5)\n    self$deconv3 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$deconv4 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$deconv5 <-\n      default_deconv(filters = base_depth,\n                     kernel_size = 5,\n                     strides = 2)\n    self$deconv6 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$conv1 <-\n      default_conv(filters = output_shape[3],\n                   kernel_size = 5,\n                   activation = \"linear\")\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        # output shape:  7 1 1 16\n        self$reshape1() %>%\n        # output shape:  7 7 7 64\n        self$deconv1() %>%\n        # output shape:  7 7 7 64\n        self$deconv2() %>%\n        # output shape:  7 14 14 64\n        self$deconv3() %>%\n        # output shape:  7 14 14 32\n        self$deconv4() %>%\n        # output shape:  7 28 28 32\n        self$deconv5() %>%\n        # output shape:  7 28 28 32\n        self$deconv6() %>%\n        # output shape:  7 28 28 1\n        self$conv1()\n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = length(output_shape))\n    }\n  })\n}\n\n# Vector quantizer -------------------------------------------------------------------\n\nvector_quantizer_model <- \n  function(name = NULL, num_codes, code_size) {\n    \n    keras_model_custom(name = name, function(self) {\n      self$num_codes <- num_codes\n      self$code_size <- code_size\n      self$codebook <- tf$get_variable(\"codebook\",\n                                       shape = c(num_codes, code_size),\n                                       dtype = tf$float32)\n      self$ema_count <- tf$get_variable(\n        name = \"ema_count\",\n        shape = c(num_codes),\n        initializer = tf$constant_initializer(0),\n        trainable = FALSE\n      )\n      self$ema_means = tf$get_variable(\n        name = \"ema_means\",\n        initializer = self$codebook$initialized_value(),\n        trainable = FALSE\n      )\n      \n      function (x, mask = NULL) {\n\n        # bs * 1 * num_codes\n        distances <- tf$norm(tf$expand_dims(x, axis = 2L) -\n                               tf$reshape(self$codebook,\n                                          c(\n                                            1L, 1L, self$num_codes, self$code_size\n                                          )),\n                             axis = 3L)\n        \n        # bs * 1\n        assignments <- tf$argmin(distances, axis = 2L)\n        \n        # bs * 1 * num_codes\n        one_hot_assignments <-\n          tf$one_hot(assignments, depth = self$num_codes)\n        \n        # bs * 1 * code_size\n        nearest_codebook_entries <- tf$reduce_sum(\n          tf$expand_dims(one_hot_assignments,-1L) * # bs, 1, 64, 1\n            tf$reshape(self$codebook, c(\n              1L, 1L, self$num_codes, self$code_size\n            )),\n          axis = 2L # 1, 1, 64, 16\n        )\n        \n        list(nearest_codebook_entries, one_hot_assignments)\n      }\n    })\n  }\n\n\n# Update codebook ------------------------------------------------------\n\nupdate_ema <- function(vector_quantizer,\n                       one_hot_assignments,\n                       codes,\n                       decay) {\n  # shape = 64\n  updated_ema_count <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_count,\n    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # 64 * 16\n  updated_ema_means <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_means,\n    # selects all assigned values (masking out the others) and sums them up over the batch\n    # (will be divided by count later)\n    tf$reduce_sum(\n      tf$expand_dims(codes, 2L) *\n        tf$expand_dims(one_hot_assignments, 3L),\n      axis = c(0L, 1L)\n    ),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # Add small value to avoid dividing by zero\n  updated_ema_count <- updated_ema_count + 1e-5\n  updated_ema_means <-\n    updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)\n  \n  tf$assign(vector_quantizer$codebook, updated_ema_means)\n}\n\n\n# Training setup -----------------------------------------------------------\n\nencoder <- encoder_model(code_size = code_size)\ndecoder <- decoder_model(input_size = latent_size * code_size,\n                         output_shape = input_shape)\n\nvector_quantizer <-\n  vector_quantizer_model(num_codes = num_codes, code_size = code_size)\n\noptimizer <- tf$train$AdamOptimizer(learning_rate = learning_rate)\n\ncheckpoint_dir <- \"./vq_vae_checkpoints\"\n\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    optimizer = optimizer,\n    encoder = encoder,\n    decoder = decoder,\n    vector_quantizer_model = vector_quantizer\n  )\n\ncheckpoint$save(file_prefix = checkpoint_prefix)\n\n# Training loop -----------------------------------------------------------\n\nnum_epochs <- 20\n\nfor (epoch in seq_len(num_epochs)) {\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  reconstruction_loss_total <- 0\n  commitment_loss_total <- 0\n  prior_loss_total <- 0\n  \n  until_out_of_range({\n    \n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      codes <- encoder(x)\n      c(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n      codes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\n      decoder_distribution <- decoder(codes_straight_through)\n      \n      reconstruction_loss <-\n        -tf$reduce_mean(decoder_distribution$log_prob(x))\n      \n      commitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n      \n      prior_dist <- tfd$Multinomial(total_count = 1,\n                                    logits = tf$zeros(c(latent_size, num_codes)))\n      prior_loss <- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))\n      \n      loss <-\n        reconstruction_loss + beta * commitment_loss + prior_loss\n      \n    })\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    update_ema(vector_quantizer,\n               one_hot_assignments,\n               codes,\n               decay)\n    \n    total_loss <- total_loss + loss\n    reconstruction_loss_total <-\n      reconstruction_loss_total + reconstruction_loss\n    commitment_loss_total <- commitment_loss_total + commitment_loss\n    prior_loss_total <- prior_loss_total + prior_loss\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Loss (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss)/trunc(buffer_size/batch_size)) %>% round(4)} loss\",\n      \"  {(as.numeric(reconstruction_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} reconstruction_loss\",\n      \"  {(as.numeric(commitment_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} commitment_loss\",\n      \"  {(as.numeric(prior_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} prior_loss\",\n      \n    ),\n    \"\\n\"\n  )\n  \n  # display example images (choose your frequency)\n  if (TRUE) {\n    reconstructed_images <- decoder_distribution$mean()\n    # (64, 1, 16)\n    prior_samples <- tf$reduce_sum(\n      # selects one of the codes (masking out 63 of 64 codes)\n      # (bs, 1, 64, 1)\n      tf$expand_dims(prior_dist$sample(num_examples_to_generate),-1L) *\n        # (1, 1, 64, 16)\n        tf$reshape(vector_quantizer$codebook,\n                   c(1L, 1L, num_codes, code_size)),\n      axis = 2L\n    )\n    decoded_distribution_given_random_prior <-\n      decoder(prior_samples)\n    random_images <- decoded_distribution_given_random_prior$mean()\n    visualize_images(\"k\", epoch, reconstructed_images, random_images)\n  }\n}\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}