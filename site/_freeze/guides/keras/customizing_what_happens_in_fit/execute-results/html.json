{
  "hash": "700dc92b7c243ecbcbe4fc5842d501ed",
  "result": {
    "markdown": "---\ntitle: Customizing what happens in `fit()`\nauthor: \n  - name: Francois Chollet\n    url: https://twitter.com/fchollet\n  - name: Tomasz Kalinowski\n    url: https://github.com/t-kalinowski\nexecute:\n  eval: true\n---\n\n\n## Introduction\n\nWhen you're doing supervised learning, you can use `fit()` and\neverything works smoothly.\n\nWhen you need to write your own training loop from scratch, you can use\nthe `GradientTape` and take control of every little detail.\n\nBut what if you need a custom training algorithm, but you still want to\nbenefit from the convenient features of `fit()`, such as callbacks,\nbuilt-in distribution support, or step fusing?\n\nA core principle of Keras is **progressive disclosure of complexity**.\nYou should always be able to get into lower-level workflows in a gradual\nway. You shouldn't fall off a cliff if the high-level functionality\ndoesn't exactly match your use case. You should be able to gain more\ncontrol over the small details while retaining a commensurate amount of\nhigh-level convenience.\n\nWhen you need to customize what `fit()` does, you should **override the\ntraining step function of the `Model` class**. This is the function that\nis called by `fit()` for every batch of data. You will then be able to\ncall `fit()` as usual -- and it will be running your own learning\nalgorithm.\n\nNote that this pattern does not prevent you from building models with\nthe Functional API. You can do this whether you're building `Sequential`\nmodels, Functional API models, or subclassed models.\n\nLet's see how that works.\n\n## Setup\n\nRequires TensorFlow 2.2 or later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tensorflow' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(keras)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'keras' was built under R version 4.1.2\n```\n:::\n:::\n\n\n## A first simple example\n\nLet's start from a simple example:\n\n-   We create a new model class by calling `new_model_class()`.\n-   We just override the method `train_step(data)`.\n-   We return a dictionary mapping metric names (including the loss) to\n    their current value.\n\nThe input argument `data` is what gets passed to fit as training data:\n\n-   If you pass arrays, by calling `fit(x, y, ...)`, then `data` will be\n    the tuple `(x, y)`\n-   If you pass a `tf$data$Dataset`, by calling `fit(dataset, ...)`,\n    then `data` will be what gets yielded by `dataset` at each batch.\n\nIn the body of the `train_step` method, we implement a regular training\nupdate, similar to what you are already familiar with. Importantly, **we\ncompute the loss via `self$compiled_loss`**, which wraps the loss(es)\nfunction(s) that were passed to `compile()`.\n\nSimilarly, we call `self$compiled_metrics$update_state(y, y_pred)` to\nupdate the state of the metrics that were passed in `compile()`, and we\nquery results from `self$metrics` at the end to retrieve their current\nvalue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nLet's try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)\n```\n:::\n\n\n## Going lower-level\n\nNaturally, you could just skip passing a loss function in `compile()`,\nand instead do everything *manually* in `train_step`. Likewise for\nmetrics.\n\nHere's a lower-level example, that only uses `compile()` to configure\nthe optimizer:\n\n-   We start by creating `Metric` instances to track our loss and a MAE\n    score.\n-   We implement a custom `train_step()` that updates the state of these\n    metrics (by calling `update_state()` on them), then query them (via\n    `result()`) to return their current average value, to be displayed\n    by the progress bar and to be pass to any callback.\n-   Note that we would need to call `reset_states()` on our metrics\n    between each epoch! Otherwise calling `result()` would return an\n    average since the start of training, whereas we usually work with\n    per-epoch averages. Thankfully, the framework can do that for us:\n    just list any metric you want to reset in the `metrics` property of\n    the model. The model will call `reset_states()` on any object listed\n    here at the beginning of each `fit()` epoch or at the beginning of a\n    call to `evaluate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)\n```\n:::\n\n\n## Supporting `sample_weight` & `class_weight`\n\nYou may have noticed that our first basic example didn't make any\nmention of sample weighting. If you want to support the `fit()`\narguments `sample_weight` and `class_weight`, you'd simply do the\nfollowing:\n\n-   Unpack `sample_weight` from the `data` argument\n-   Pass it to `compiled_loss` & `compiled_metrics` (of course, you\n    could also just apply it manually if you don't rely on `compile()`\n    for losses & metrics)\n-   That's it. That's the list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)\n```\n:::\n\n\n## Providing your own evaluation step\n\nWhat if you want to do the same for calls to `model$evaluate()`? Then\nyou would override `test_step` in exactly the same way. Here's what it\nlooks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss       mae \n0.6081660 0.6558823 \n```\n:::\n:::\n\n\n## Wrapping up: an end-to-end GAN example\n\nLet's walk through an end-to-end example that leverages everything you\njust learned.\n\nLet's consider:\n\n-   A generator network meant to generate 28x28x1 images.\n-   A discriminator network meant to classify 28x28x1 images into two\n    classes (\"fake\" and \"real\").\n-   One optimizer for each.\n-   A loss function to train the discriminator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n```\n:::\n\n\nHere's a feature-complete GAN class, overriding `compile()` to use its\nown signature, and implementing the entire GAN algorithm in 17 lines in\n`train_step`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n```\n:::\n\n\nLet's test-drive it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n```\n:::\n\n\nHappy training!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}