[
  {
    "objectID": "deploy/docker.html",
    "href": "deploy/docker.html",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using TensorFlow serving.\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\nUsing the Docker container is a an easy way to test the API locally and then deploy it to any cloud provider."
  },
  {
    "objectID": "deploy/docker.html#building-the-model",
    "href": "deploy/docker.html#building-the-model",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.03585155 0.98879999 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/docker.html#running-locally",
    "href": "deploy/docker.html#running-locally",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Running locally",
    "text": "Running locally\nYou can run the tensorflow/serving Docker image locally using the great stevedore package. For example:\n\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside `/models`\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n\nNow we have initialized the container serving the model. You can see the container logs with:\n\ncontainer$logs()\n\nNow you can make POST requests no the following endpoint : http://localhost:8501/v1/models/model/versions/1:predict. The input data must be passed in a special format 0 - see the format definition here, which may seem unnatural for R users. Here is an example:\n\ninstances <- purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %>% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nThis is how you can serve TensorFlow models with TF serving locally. Additionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\nWhen done, you can stop the container with:\n\ncontainer$stop()"
  },
  {
    "objectID": "deploy/docker.html#deploying-to-google-cloud-run",
    "href": "deploy/docker.html#deploying-to-google-cloud-run",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Deploying to Google Cloud Run",
    "text": "Deploying to Google Cloud Run\nTHe first thing you need to do is to follow the section Before you begin in this page.\nNow let’s create a Dockerfile that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\nHere’s an example - create a file called Dockerfile in the same root folder as your SavedModel and paste the following:\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\", \"--rest_api_port=8080\"]\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io. Run the following in your terminal:\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\nYou can get your PROJECT-ID by running:\ngcloud config get-value project\nNext, we can create the service in Google Cloud Run using:\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly you will get a url like https://cnn-mnist-ld4lzfalyq-ue.a.run.app which you can now use to make requests to your model. For example:\n\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API."
  },
  {
    "objectID": "deploy/index.html",
    "href": "deploy/index.html",
    "title": "Overview",
    "section": "",
    "text": "Plumber API: Create a REST API using Plumber to deploy your TensorFlow model. With Plumber you will still depend on having an R runtime which be useful when you want to make the data pre-processing in R.\nShiny: Create a Shiny app that uses a TensorFlow model to generate outputs.\nTensorFlow Serving: This is the most performant way of deploying TensorFlow models since it’s based only inn the TensorFlow serving C++ server. With TF serving you don’t depend on an R runtime, so all pre-processing must be done in the TensorFlow graph.\nRStudio Connect: RStudio Connect makes it easy to deploy TensorFlow models and uses TensorFlow serving in the backend.\n\nThere are many other options to deploy TensorFlow models built with R that are not covered in this section. For example:\n\nDeploy it using a Python runtime.\nDeploy using a JavaScript runtime.\nDeploy to a mobile phone app using TensorFlow Lite.\nDeploy to a iOS app using Apple’s Core ML tool.\nUse plumber and Docker to deploy your TensorFlow model (by T-Mobile)."
  },
  {
    "objectID": "deploy/plumber.html",
    "href": "deploy/plumber.html",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using a plumber API.\nIn this example we will build an endpoint that takes POST requests sending images containing handwritten digits and returning the predicted number."
  },
  {
    "objectID": "deploy/plumber.html#building-the-model",
    "href": "deploy/plumber.html#building-the-model",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. W We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.02827194 0.99059999 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/plumber.html#plumber-api",
    "href": "deploy/plumber.html#plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Plumber API",
    "text": "Plumber API\nA plumber API is defined by a .R file with a few annotations. Here’s is how we can write our api.R file:\n\nlibrary(keras)\n\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n#* Predicts the number in an image\n#* @param enc a base64  encoded 28x28 image\n#* @post /cnn-mnist\nfunction(enc) {\n  # decode and read the jpeg image\n  img <- jpeg::readJPEG(source = base64enc::base64decode(enc))\n  \n  # reshape\n  img <- img %>% \n    array_reshape(., dim = c(1, dim(.), 1))\n  \n  # make the prediction\n  predict_classes(model, img)\n}\n\nMake sure to have the your SavedModel in the same folder as api.R and call:\n\np <- plumber::plumb(\"api.R\")\np$run(port = 8000)\n\nYou can now make requests to the http://lcoalhost:8000/cnn-minist/ endpoint. For example, let’s verify we can make a POST request to the API sending the first image from the test set:\n\nimg <- mnist$test$x[1,,,]\nmnist$test$y[1]\n\n[1] 7\n\n\nFirst let’s encode the image:\n\nencoded_img <- img %>% \n  jpeg::writeJPEG() %>% \n  base64enc::base64encode()\nencoded_img\n\n[1] \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APHIYJbmZYYInllc4VEUszH2A6064tbi0k8q5glgcfwyIVP5GoqKsWGoXel3sd7YXD29zESUljOGXIwf0JrpLf4o+MLeNUbVftAU5BuIUlPTplhn/wDVXQ+PNcvk8D6bpmtPbz6vqDi9lCwqhtocYRRtHU8k9+orzKius8CeHrS/nutd1pWGiaQnm3GBnznyNsQ+p/w4zWL4h1u48Ra7d6rcDa1w+VQdEUcKo+gAFZtFbsfjDVIPB7+FoRBHYyymWVlT95JyDgknGMgdADx1rCor/9k=\"\n\n\n\nreq <- httr::POST(\"http://localhost:8000/cnn-mnist\",\n           body = list(enc = encoded_img), \n           encode = \"json\")\nhttr::content(req)\n\n[[1]]\n[1] 7\nYou can also access the Swagger interface by accessing http://127.0.0.1:8000/swagger/ and paste the encoded string in the UI to visualize the result."
  },
  {
    "objectID": "deploy/plumber.html#more-advanced-models",
    "href": "deploy/plumber.html#more-advanced-models",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/plumber.html#hosting-the-plumber-api",
    "href": "deploy/plumber.html#hosting-the-plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Hosting the plumber API",
    "text": "Hosting the plumber API\nPlumber is very flexible and allows multiple hosting options. See the plumber Hostinng documentation for more information."
  },
  {
    "objectID": "deploy/rsconnect.html",
    "href": "deploy/rsconnect.html",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model to RStudio Connect. RStudio Connect uses TensorFlow Serving for performance but makes it much easier for R users to manage their deployment."
  },
  {
    "objectID": "deploy/rsconnect.html#building-the-model",
    "href": "deploy/rsconnect.html#building-the-model",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y, verbose = 0)\n\n      loss   accuracy \n0.03402501 0.98860002 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "href": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Deployiong to RStudio Connect",
    "text": "Deployiong to RStudio Connect\nOnce the model is saved to the SavedModel format, the model can be deployed with a single line of code:\n\nrsconnect::deployTFModel(\"cnn-mnist/\")\n\nWhen the deployment is complete you will be redirected to your browser with some instructions on how to call the REST endpoint:"
  },
  {
    "objectID": "deploy/shiny.html",
    "href": "deploy/shiny.html",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model inside a Shiny app. We will build a model that can classify handwritten digits in images, then we will build a Shiny app that let’s you upload an image and get predictions from this model."
  },
  {
    "objectID": "deploy/shiny.html#building-the-model",
    "href": "deploy/shiny.html#building-the-model",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.03422958 0.98920000 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/shiny.html#shiny-app",
    "href": "deploy/shiny.html#shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Shiny app",
    "text": "Shiny app\nA simple shiny app can be define in an app.R file with a few conventions. Here’s how we can structure our Shiny app.\n\nlibrary(shiny)\nlibrary(keras)\n\n# Load the model\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n# Define the UI\nui <- fluidPage(\n  # App title ----\n  titlePanel(\"Hello TensorFlow!\"),\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n      # Input: File upload\n      fileInput(\"image_path\", label = \"Input a JPEG image\")\n    ),\n    # Main panel for displaying outputs ----\n    mainPanel(\n      # Output: Histogram ----\n      textOutput(outputId = \"prediction\"),\n      plotOutput(outputId = \"image\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram ----\nserver <- function(input, output) {\n  \n  image <- reactive({\n    req(input$image_path)\n    jpeg::readJPEG(input$image_path$datapath)\n  })\n  \n  output$prediction <- renderText({\n    \n    img <- image() %>% \n      array_reshape(., dim = c(1, dim(.), 1))\n    \n    paste0(\"The predicted class number is \", predict_classes(model, img))\n  })\n  \n  output$image <- renderPlot({\n    plot(as.raster(image()))\n  })\n  \n}\n\nshinyApp(ui, server)\n\nThis app can be used locally or deployed using any Shiny deployment option. If you are deploying to RStudio Connect or Shinnyapps.io, don’t forget to set the RETICULATE_PYTHON environment variable so rsconnect can detect what python packages are required to reproduce your local environment. See the F.A.Q. for more information.\n\nYou can see a live version of this app here. Note that to keep the code simple, it will only accept JPEG images with 28x28 pixels. You can download this file if you want to try the app."
  },
  {
    "objectID": "deploy/shiny.html#more-advanced-models",
    "href": "deploy/shiny.html#more-advanced-models",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/shiny.html#hosting-the-shiny-app",
    "href": "deploy/shiny.html#hosting-the-shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Hosting the shiny app",
    "text": "Hosting the shiny app\nThis Shiny app can be hosted in any server using the Shiny Server. If you are managing the complete infrastructure, make sure that you have Python and all required Python packages installed in the server.\nIf you are using Shinyapps.io or RStudio Connect the dependencies will be infered when deploying the app. In this case, don’t forget to set the RETICULATE_PYTHON environment variable.\nYou can find more examples of using reticulate in RStudio products here and learn more about Python in RStudio Connect best practices here."
  },
  {
    "objectID": "examples/addition_rnn.html",
    "href": "examples/addition_rnn.html",
    "title": "addition_rnn",
    "section": "",
    "text": "Input: “535+61”\nOutput: “596”\nPadding is handled by using a repeated sentinel character (space)\nInput may optionally be reversed, shown to increase performance in many tasks in: “Learning to Execute” http://arxiv.org/abs/1410.4615 and “Sequence to Sequence Learning with Neural Networks” http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf Theoretically it introduces shorter term dependencies between source and target.\nTwo digits reversed: One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\nThree digits reversed: One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\nFour digits reversed: One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\nFive digits reversed: One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(stringi)\n\n# Function Definitions ----------------------------------------------------\n\n# Creates the char table and sorts them.\nlearn_encoding <- function(chars){\n  sort(chars)\n}\n\n# Encode from a character sequence to a one hot integer representation.\n# > encode(\"22+22\", char_table)\n# [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# +    0    1    0    0    0    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\nencode <- function(char, char_table){\n  strsplit(char, \"\") %>%\n    unlist() %>%\n    sapply(function(x){\n      as.numeric(x == char_table)\n    }) %>% \n    t()\n}\n\n# Decode the one hot representation/probabilities representation\n# to their character output.\ndecode <- function(x, char_table){\n  apply(x,1, function(y){\n    char_table[which.max(y)]\n  }) %>% paste0(collapse = \"\")\n}\n\n# Returns a list of questions and expected answers.\ngenerate_data <- function(size, digits, invert = TRUE){\n  \n  max_num <- as.integer(paste0(rep(9, digits), collapse = \"\"))\n  \n  # generate integers for both sides of question\n  x <- sample(1:max_num, size = size, replace = TRUE)\n  y <- sample(1:max_num, size = size, replace = TRUE)\n  \n  # make left side always smaller than right side\n  left_side <- ifelse(x <= y, x, y)\n  right_side <- ifelse(x >= y, x, y)\n  \n  results <- left_side + right_side\n  \n  # pad with spaces on the right\n  questions <- paste0(left_side, \"+\", right_side)\n  questions <- stri_pad(questions, width = 2*digits+1, \n                        side = \"right\", pad = \" \")\n  if(invert){\n    questions <- stri_reverse(questions)\n  }\n  # pad with spaces on the left\n  results <- stri_pad(results, width = digits + 1, \n                      side = \"left\", pad = \" \")\n  \n  list(\n    questions = questions,\n    results = results\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\n# Parameters for the model and dataset\nTRAINING_SIZE <- 50000\nDIGITS <- 2\n\n# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n# int is DIGITS\nMAXLEN <- DIGITS + 1 + DIGITS\n\n# All the numbers, plus sign and space for padding\ncharset <- c(0:9, \"+\", \" \")\nchar_table <- learn_encoding(charset)\n\n\n# Data Preparation --------------------------------------------------------\n\n# Generate Data\nexamples <- generate_data(size = TRAINING_SIZE, digits = DIGITS)\n\n# Vectorization\nx <- array(0, dim = c(length(examples$questions), MAXLEN, length(char_table)))\ny <- array(0, dim = c(length(examples$questions), DIGITS + 1, length(char_table)))\n\nfor(i in 1:TRAINING_SIZE){\n  x[i,,] <- encode(examples$questions[i], char_table)\n  y[i,,] <- encode(examples$results[i], char_table)\n}\n\n# Shuffle\nindices <- sample(1:TRAINING_SIZE, size = TRAINING_SIZE)\nx <- x[indices,,]\ny <- y[indices,,]\n\n\n# Explicitly set apart 10% for validation data that we never train over\nsplit_at <- trunc(TRAINING_SIZE/10)\nx_val <- x[1:split_at,,]\ny_val <- y[1:split_at,,]\nx_train <- x[(split_at + 1):TRAINING_SIZE,,]\ny_train <- y[(split_at + 1):TRAINING_SIZE,,]\n\nprint('Training Data:')\n\n[1] \"Training Data:\"\n\nprint(dim(x_train))\n\n[1] 45000     5    12\n\nprint(dim(y_train))\n\n[1] 45000     3    12\n\nprint('Validation Data:')\n\n[1] \"Validation Data:\"\n\nprint(dim(x_val))\n\n[1] 5000    5   12\n\nprint(dim(y_val))\n\n[1] 5000    3   12\n\n# Training ----------------------------------------------------------------\n\nHIDDEN_SIZE <- 128\nBATCH_SIZE <- 128\nLAYERS <- 1\n\n# Initialize sequential model\nmodel <- keras_model_sequential() \n\nLoaded Tensorflow version 2.9.1\n\nmodel %>%\n  # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n  # Note: In a situation where your input sequences have a variable length,\n  # use input_shape=(None, num_feature).\n  layer_lstm(HIDDEN_SIZE, input_shape=c(MAXLEN, length(char_table))) %>%\n  # As the decoder RNN's input, repeatedly provide with the last hidden state of\n  # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n  # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n  layer_repeat_vector(DIGITS + 1)\n\n# The decoder RNN could be multiple layers stacked or a single layer.\n# By setting return_sequences to True, return not only the last output but\n# all the outputs so far in the form of (num_samples, timesteps,\n# output_dim). This is necessary as TimeDistributed in the below expects\n# the first dimension to be the timesteps.\nfor(i in 1:LAYERS)\n  model %>% layer_lstm(HIDDEN_SIZE, return_sequences = TRUE)\n\nmodel %>% \n  # Apply a dense layer to the every temporal slice of an input. For each of step\n  # of the output sequence, decide which character should be chosen.\n  time_distributed(layer_dense(units = length(char_table))) %>%\n  layer_activation(\"softmax\")\n\n# Compiling the model\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = \"adam\", \n  metrics = \"accuracy\"\n)\n\n# Get the model summary\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n lstm (LSTM)                        (None, 128)                     72192       \n repeat_vector (RepeatVector)       (None, 3, 128)                  0           \n lstm_1 (LSTM)                      (None, 3, 128)                  131584      \n time_distributed (TimeDistributed)  (None, 3, 12)                  1548        \n activation (Activation)            (None, 3, 12)                   0           \n================================================================================\nTotal params: 205,324\nTrainable params: 205,324\nNon-trainable params: 0\n________________________________________________________________________________\n\n# Fitting loop\nmodel %>% fit( \n  x = x_train, \n  y = y_train, \n  batch_size = BATCH_SIZE, \n  epochs = 70,\n  validation_data = list(x_val, y_val)\n)\n\n# Predict for a new observation\nnew_obs <- encode(\"55+22\", char_table) %>%\n  array(dim = c(1,5,12))\nresult <- predict(model, new_obs)\nresult <- result[1,,]\ndecode(result, char_table)\n\n[1] \" 77\""
  },
  {
    "objectID": "examples/babi_memnn.html",
    "href": "examples/babi_memnn.html",
    "title": "babi_memnn",
    "section": "",
    "text": "References:\n\nJason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks”, http://arxiv.org/abs/1502.05698\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, “End-To-End Memory Networks”, http://arxiv.org/abs/1503.08895\n\nReaches 98.6% accuracy on task ‘single_supporting_fact_10k’ after 120 epochs. Time per epoch: 3s on CPU (core i7).\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.2\n\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\n\nWarning: package 'tibble' was built under R version 4.1.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_df(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n  \n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n  \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n\n# Parameters --------------------------------------------------------------\n\nchallenges <- list(\n  # QA1 with 10,000 samples\n  single_supporting_fact_10k = \"%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt\",\n  # QA2 with 10,000 samples\n  two_supporting_facts_10k = \"%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt\"\n)\n\nchallenge_type <- \"single_supporting_fact_10k\"\nchallenge <- challenges[[challenge_type]]\nmax_length <- 999999\n\n\n# Data Preparation --------------------------------------------------------\n\n# Download data\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\n\nLoaded Tensorflow version 2.9.1\n\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Reading training and test data\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n\n# Extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# Vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n\n# Defining the model ------------------------------------------------------\n\n# Placeholders\nsequence <- layer_input(shape = c(story_maxlen))\nquestion <- layer_input(shape = c(query_maxlen))\n\n# Encoders\n# Embed the input sequence into a sequence of vectors\nsequence_encoder_m <- keras_model_sequential()\nsequence_encoder_m %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, embedding_dim)\n\n# Embed the input into a sequence of vectors of size query_maxlen\nsequence_encoder_c <- keras_model_sequential()\nsequence_encoder_c %>%\n  layer_embedding(input_dim = vocab_size, output = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, query_maxlen)\n\n# Embed the question into a sequence of vectors\nquestion_encoder <- keras_model_sequential()\nquestion_encoder %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64, \n                  input_length = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, query_maxlen, embedding_dim)\n\n# Encode input sequence and questions (which are indices)\n# to sequences of dense vectors\nsequence_encoded_m <- sequence_encoder_m(sequence)\nsequence_encoded_c <- sequence_encoder_c(sequence)\nquestion_encoded <- question_encoder(question)\n\n# Compute a 'match' between the first input vector sequence\n# and the question vector sequence\n# shape: `(samples, story_maxlen, query_maxlen)`\nmatch <- list(sequence_encoded_m, question_encoded) %>%\n  layer_dot(axes = c(2,2)) %>%\n  layer_activation(\"softmax\")\n\n# Add the match matrix with the second input vector sequence\nresponse <- list(match, sequence_encoded_c) %>%\n  layer_add() %>%\n  layer_permute(c(2,1))\n\n# Concatenate the match matrix with the question vector sequence\nanswer <- list(response, question_encoded) %>%\n  layer_concatenate() %>%\n  # The original paper uses a matrix multiplication for this reduction step.\n  # We choose to use an RNN instead.\n  layer_lstm(32) %>%\n  # One regularization layer -- more would probably be needed.\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(vocab_size) %>%\n  # We output a probability distribution over the vocabulary\n  layer_activation(\"softmax\")\n\n# Build the final model\nmodel <- keras_model(inputs = list(sequence, question), answer)\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = 32,\n  epochs = 120,\n  validation_data = list(list(test_vec$stories, test_vec$questions), test_vec$answers)\n)"
  },
  {
    "objectID": "examples/babi_rnn.html",
    "href": "examples/babi_rnn.html",
    "title": "babi_rnn",
    "section": "",
    "text": "The results are comparable to those for an LSTM model provided in Weston et al.: “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks” http://arxiv.org/abs/1502.05698\n\n\n\nTask Number\nFB LSTM Baseline\nKeras QA\n\n\n\n\nQA1 - Single Supporting Fact\n50\n100.0\n\n\nQA2 - Two Supporting Facts\n20\n50.0\n\n\nQA3 - Three Supporting Facts\n20\n20.5\n\n\nQA4 - Two Arg. Relations\n61\n62.9\n\n\nQA5 - Three Arg. Relations\n70\n61.9\n\n\nQA6 - yes/No Questions\n48\n50.7\n\n\nQA7 - Counting\n49\n78.9\n\n\nQA8 - Lists/Sets\n45\n77.2\n\n\nQA9 - Simple Negation\n64\n64.0\n\n\nQA10 - Indefinite Knowledge\n44\n47.7\n\n\nQA11 - Basic Coreference\n72\n74.9\n\n\nQA12 - Conjunction\n74\n76.4\n\n\nQA13 - Compound Coreference\n94\n94.4\n\n\nQA14 - Time Reasoning\n27\n34.8\n\n\nQA15 - Basic Deduction\n21\n32.4\n\n\nQA16 - Basic Induction\n23\n50.6\n\n\nQA17 - Positional Reasoning\n51\n49.1\n\n\nQA18 - Size Reasoning\n52\n90.8\n\n\nQA19 - Path Finding\n8\n9.0\n\n\nQA20 - Agent’s Motivations\n91\n90.7\n\n\n\nFor the resources related to the bAbI project, refer to: https://research.facebook.com/researchers/1543934539189348\nNotes:\n\nWith default word, sentence, and query vector sizes, the GRU model achieves:\n100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU) In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\nThe task does not traditionally parse the question separately. This likely improves accuracy and is a good example of merging two RNNs.\nThe word vector embeddings are not shared between the story and question RNNs.\nSee how the accuracy changes given 10,000 training samples (en-10k) instead of only 1000. 1000 was used in order to be comparable to the original paper.\nExperiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\nThe length and noise (i.e. ‘useless’ story components) impact the ability for LSTMs / GRUs to provide the correct answer. Given only the supporting facts, these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural networks that use attentional processes can efficiently search through this noise to find the relevant statements, improving performance substantially. This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.2\n\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\n\nWarning: package 'tibble' was built under R version 4.1.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_df(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n\n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n    \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n\n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\nmax_length <- 99999\nembed_hidden_size <- 50\nbatch_size <- 32\nepochs <- 40\n\n# Data Preparation --------------------------------------------------------\n\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\n\nLoaded Tensorflow version 2.9.1\n\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Default QA1 with 1000 samples\n# challenge = '%stasks_1-20_v1-2/en/qa1_single-supporting-fact_%s.txt'\n# QA1 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt'\n# QA2 with 1000 samples\nchallenge <- \"%stasks_1-20_v1-2/en/qa2_two-supporting-facts_%s.txt\"\n# QA2 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt'\n\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n`summarise()` has grouped output by 'story_id', 'nid', 'question'. You can\noverride using the `.groups` argument.\n\n# extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n# Defining the model ------------------------------------------------------\n\nsentence <- layer_input(shape = c(story_maxlen), dtype = \"int32\")\nencoded_sentence <- sentence %>% \n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\nquestion <- layer_input(shape = c(query_maxlen), dtype = \"int32\")\nencoded_question <- question %>%\n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_repeat_vector(n = story_maxlen)\n\nmerged <- list(encoded_sentence, encoded_question) %>%\n  layer_add() %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\npreds <- merged %>%\n  layer_dense(units = vocab_size, activation = \"softmax\")\n\nmodel <- keras_model(inputs = list(sentence, question), outputs = preds)\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nmodel\n\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n input_2 (InputLayer)     [(None, 5)]       0        []                         \n embedding_1 (Embedding)  (None, 5, 50)     1800     ['input_2[0][0]']          \n input_1 (InputLayer)     [(None, 552)]     0        []                         \n dropout_1 (Dropout)      (None, 5, 50)     0        ['embedding_1[0][0]']      \n embedding (Embedding)    (None, 552, 50)   1800     ['input_1[0][0]']          \n lstm (LSTM)              (None, 50)        20200    ['dropout_1[0][0]']        \n dropout (Dropout)        (None, 552, 50)   0        ['embedding[0][0]']        \n repeat_vector (RepeatVec  (None, 552, 50)  0        ['lstm[0][0]']             \n tor)                                                                           \n add (Add)                (None, 552, 50)   0        ['dropout[0][0]',          \n                                                      'repeat_vector[0][0]']    \n lstm_1 (LSTM)            (None, 50)        20200    ['add[0][0]']              \n dropout_2 (Dropout)      (None, 50)        0        ['lstm_1[0][0]']           \n dense (Dense)            (None, 36)        1836     ['dropout_2[0][0]']        \n================================================================================\nTotal params: 45,836\nTrainable params: 45,836\nNon-trainable params: 0\n________________________________________________________________________________\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split=0.05\n)\n\nevaluation <- model %>% evaluate(\n  x = list(test_vec$stories, test_vec$questions),\n  y = test_vec$answers,\n  batch_size = batch_size\n)\n\nevaluation\n\n    loss accuracy \n1.666561 0.318000"
  },
  {
    "objectID": "examples/cifar10_cnn.html",
    "href": "examples/cifar10_cnn.html",
    "title": "cifar10_cnn",
    "section": "",
    "text": "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs, though it is still underfitting at that point.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 32\nepochs <- 200\ndata_augmentation <- TRUE\n\n\n# Data Preparation --------------------------------------------------------\n\n# See ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n\nLoaded Tensorflow version 2.9.1\n\n# Feature scale RGB values in test and train inputs  \nx_train <- cifar10$train$x/255\nx_test <- cifar10$test$x/255\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n\n# Defining Model ----------------------------------------------------------\n\n# Initialize sequential model\nmodel <- keras_model_sequential()\n\nmodel %>%\n \n  # Start with hidden 2D convolutional layer being fed 32x32 pixel images\n  layer_conv_2d(\n    filter = 32, kernel_size = c(3,3), padding = \"same\", \n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation(\"relu\") %>%\n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling once more\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # Flatten max filtered output into feature vector \n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(512) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10 unit output layer\n  layer_dense(10) %>%\n  layer_activation(\"softmax\")\n\nopt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)\n\nWarning in backcompat_fix_rename_lr_to_learning_rate(...): the `lr` argument has\nbeen renamed to `learning_rate`.\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = opt,\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nif(!data_augmentation){\n  \n  model %>% fit(\n    x_train, y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, y_test),\n    shuffle = TRUE\n  )\n  \n} else {\n  \n  datagen <- image_data_generator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    horizontal_flip = TRUE\n  )\n  \n  datagen %>% fit_image_data_generator(x_train)\n  \n  model %>% fit_generator(\n    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),\n    steps_per_epoch = as.integer(50000/batch_size), \n    epochs = epochs, \n    validation_data = list(x_test, y_test)\n  )\n  \n}\n\nWarning in fit_generator(., flow_images_from_data(x_train, y_train, datagen, :\n`fit_generator` is deprecated. Use `fit` instead, it now accept generators."
  },
  {
    "objectID": "examples/cifar10_densenet.html",
    "href": "examples/cifar10_densenet.html",
    "title": "cifar10_densenet",
    "section": "",
    "text": "DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each dense block). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using less than half the amount of parameters and roughly half the number of FLOPs.\nFinal accuracy on test set was 0.9351 versus 0.9300 reported on the paper.\nBeside the keras package, you will need to install the densenet package. Installation instructions are available here.\n\n# Libraries ---------------------------------------------------------------\nlibrary(keras)\nlibrary(densenet)\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 64\nepochs <- 300\n\n# Data Preparation --------------------------------------------------------\n\n# see ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n\n# Normalisation\nfor(i in 1:3){\n  mea <- mean(cifar10$train$x[,,,i])\n  sds <- sd(cifar10$train$x[,,,i])\n  \n  cifar10$train$x[,,,i] <- (cifar10$train$x[,,,i] - mea) / sds\n  cifar10$test$x[,,,i] <- (cifar10$test$x[,,,i] - mea) / sds\n}\nx_train <- cifar10$train$x\nx_test <- cifar10$test$x\n\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n# Model Definition -------------------------------------------------------\n\ninput_img <- layer_input(shape = c(32, 32, 3))\nmodel <- application_densenet(include_top = TRUE, input_tensor = input_img, dropout_rate = 0.2)\n\nopt <- optimizer_sgd(lr = 0.1, momentum = 0.9, nesterov = TRUE)\n\nmodel %>% compile(\n  optimizer = opt,\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n# Model fitting -----------------------------------------------------------\n\n# callbacks for weights and learning rate\nlr_schedule <- function(epoch, lr) {\n  \n  if(epoch <= 150) {\n    0.1\n  } else if(epoch > 150 && epoch <= 225){\n    0.01\n  } else {\n    0.001\n  }\n\n}\n\nlr_reducer <- callback_learning_rate_scheduler(lr_schedule)\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  batch_size = batch_size, \n  epochs = epochs, \n  validation_data = list(x_test, y_test), \n  callbacks = list(\n    lr_reducer\n  )\n)\n\nplot(history)\n\nevaluate(model, x_test, y_test)"
  },
  {
    "objectID": "examples/conv_lstm.html",
    "href": "examples/conv_lstm.html",
    "title": "conv_lstm",
    "section": "",
    "text": "# This script demonstrates the use of a convolutional LSTM network.\n# This network is used to predict the next frame of an artificially\n# generated movie which contains moving squares.\n\nlibrary(keras)\nlibrary(abind)\nlibrary(raster)\n\n# Function Definition -----------------------------------------------------\n\ngenerate_movies <- function(n_samples = 1200, n_frames = 15){\n  \n  rows <- 80\n  cols <- 80\n  \n  noisy_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  shifted_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  \n  n <- sample(3:8, 1)\n  \n  for(s in 1:n_samples){\n    for(i in 1:n){\n      # Initial position\n      xstart <- sample(20:60, 1)\n      ystart <- sample(20:60, 1)\n      \n      # Direction of motion\n      directionx <- sample(-1:1, 1)\n      directiony <- sample(-1:1, 1)\n      \n      # Size of the square\n      w <- sample(2:3, 1)\n      \n      x_shift <- xstart + directionx*(0:(n_frames))\n      y_shift <- ystart + directiony*(0:(n_frames))\n      \n      for(t in 1:n_frames){\n        square_x <- (x_shift[t] - w):(x_shift[t] + w)\n        square_y <- (y_shift[t] - w):(y_shift[t] + w)\n        \n        noisy_movies[s, t, square_x, square_y] <- \n          noisy_movies[s, t, square_x, square_y] + 1\n        \n        # Make it more robust by adding noise. The idea is that if \n        # during inference, the value of the pixel is not exactly \n        # one; we need to train the network to be robust and still \n        # consider it as a pixel belonging to a square.\n        if(runif(1) > 0.5){\n          noise_f <- sample(c(-1, 1), 1)\n          \n          square_x_n <- (x_shift[t] - w - 1):(x_shift[t] + w + 1)\n          square_y_n <- (y_shift[t] - w - 1):(y_shift[t] + w + 1)\n          \n          noisy_movies[s, t, square_x_n, square_y_n] <- \n            noisy_movies[s, t, square_x_n, square_y_n] + noise_f*0.1\n          \n        }\n        \n        # Shift the ground truth by 1\n        square_x_s <- (x_shift[t+1] - w):(x_shift[t+1] + w)\n        square_y_s <- (y_shift[t+1] - w):(y_shift[t+1] + w)\n        \n        shifted_movies[s, t, square_x_s, square_y_s] <- \n          shifted_movies[s, t, square_x_s, square_y_s] + 1\n      }\n    }  \n  }\n  \n  # Cut to a 40x40 window\n  noisy_movies <- noisy_movies[,,21:60, 21:60]\n  shifted_movies = shifted_movies[,,21:60, 21:60]\n  \n  noisy_movies[noisy_movies > 1] <- 1\n  shifted_movies[shifted_movies > 1] <- 1\n\n  # Add channel dimension\n  noisy_movies <- array_reshape(noisy_movies, c(dim(noisy_movies), 1))\n  shifted_movies <- array_reshape(shifted_movies, c(dim(shifted_movies), 1))\n  \n  list(\n    noisy_movies = noisy_movies,\n    shifted_movies = shifted_movies\n  )\n}\n\n\n# Data Preparation --------------------------------------------------------\n\n# Artificial data generation:\n  # Generate movies with 3 to 7 moving squares inside.\n  # The squares are of shape 1x1 or 2x2 pixels, which move linearly over time.\n  # For convenience we first create movies with bigger width and height (80x80)\n  # and at the end we select a 40x40 window.\nmovies <- generate_movies(n_samples = 1000, n_frames = 15)\nmore_movies <- generate_movies(n_samples = 200, n_frames = 15)\n\n\n# Model definition --------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>%\n\n  # Begin with 2D convolutional LSTM layer\n  layer_conv_lstm_2d(\n    input_shape = list(NULL,40,40,1), \n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", \n    return_sequences = TRUE\n  ) %>%\n  # Normalize the activations of the previous layer\n  layer_batch_normalization() %>%\n  \n  # Add 3x hidden 2D convolutions LSTM layers, with\n  # batch normalization layers between\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>% \n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  \n  # Add final 3D convolutional output layer \n  layer_conv_3d(\n    filters = 1, kernel_size = c(3,3,3),\n    activation = \"sigmoid\", \n    padding = \"same\", data_format =\"channels_last\"\n  )\n\n# Prepare model for training\nmodel %>% compile(\n  loss = \"binary_crossentropy\", \n  optimizer = \"adadelta\"\n)\n\nmodel\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  movies$noisy_movies,\n  movies$shifted_movies,\n  batch_size = 10,\n  epochs = 30, \n  validation_split = 0.05\n)\n\n\n# Visualization  ----------------------------------------------------------------\n\n# Testing the network on one movie\n# feed it with the first 7 positions and then\n# predict the new positions\n\n#Example to visualize on\nwhich <- 100\n\ntrack <- more_movies$noisy_movies[which,1:8,,,1]\ntrack <- array(track, c(1,8,40,40,1))\n\nfor (k in 1:15){\n  if (k<8){ \n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    dev.off()\n  } else {\n    \n    # And then compare the predictions to the ground truth\n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k))\n    \n    # Make Prediction\n    new_pos <- model %>% predict(track)\n   \n    # Slice the last row  \n    new_pos_loc <- new_pos[1,k,1:40,1:40,1]  \n    new_pos_loc  %>% raster() %>% plot() %>% title (main=paste0('Pred_',k))    \n    \n    # Reshape it\n    new_pos <- array(new_pos_loc, c(1,1, 40,40,1))     \n    \n    # Bind it to the earlier data\n    track <- abind(track,new_pos,along = 2)  \n    dev.off()\n  }\n} \n\n# Can also create a gif by running\nsystem(\"convert -delay 40 *.png animation.gif\")"
  },
  {
    "objectID": "examples/deep_dream.html",
    "href": "examples/deep_dream.html",
    "title": "deep_dream",
    "section": "",
    "text": "library(keras)\n\n\n# Utility functions -------------------------------------------------------\n\n# Util function to open, resize, and format pictures into tensors that Inception V3 can process\npreprocess_image <- function(image_path) {\n  image_load(image_path) %>%\n    image_to_array() %>%\n    array_reshape(dim = c(1, dim(.))) %>%\n    inception_v3_preprocess_input()\n}\n\n# Util function to convert a tensor into a valid image\ndeprocess_image <- function(img) {\n  img <- array_reshape(img, dim = c(dim(img)[[2]], dim(img)[[3]], 3))\n  # Undoes preprocessing that was performed by `imagenet_preprocess_input`\n  img <- img / 2\n  img <- img + 0.5\n  img <- img * 255\n  \n  dims <- dim(img)\n  img <- pmax(0, pmin(img, 255))\n  dim(img) <- dims\n  img\n}\n\nresize_img <- function(img, size) {\n  image_array_resize(img, size[[1]], size[[2]])\n}\n\nsave_img <- function(img, fname) {\n  img <- deprocess_image(img)\n  image_array_save(img, fname)\n}\n\n\n# Model  ----------------------------------------------\n\n# You won't be training the model, so this command disables all training-specific operations.\nk_set_learning_phase(0)\n\n# Builds the Inception V3 network, without its convolutional base. The model will be loaded with pretrained ImageNet weights.\nmodel <- application_inception_v3(weights = \"imagenet\",\n                                  include_top = FALSE)\n\n# Named list mapping layer names to a coefficient quantifying how much the layer's activation contributes to the loss you'll seek to maximize. Note that the layer names are hardcoded in the built-in Inception V3 application. You can list all layer names using `summary(model)`.\nlayer_contributions <- list(\n  mixed2 = 0.2,\n  mixed3 = 3,\n  mixed4 = 2,\n  mixed5 = 1.5\n)\n\n# You'll define the loss by adding layer contributions to this scalar variable\nloss <- k_variable(0)\nfor (layer_name in names(layer_contributions)) {\n  coeff <- layer_contributions[[layer_name]]\n  # Retrieves the layer's output\n  activation <- get_layer(model, layer_name)$output\n  scaling <- k_prod(k_cast(k_shape(activation), \"float32\"))\n  # Retrieves the layer's output\n  loss <- loss + (coeff * k_sum(k_square(activation)) / scaling)\n}\n\n# Retrieves the layer's output\ndream <- model$input\n\n# Computes the gradients of the dream with regard to the loss\ngrads <- k_gradients(loss, dream)[[1]]\n\n# Normalizes the gradients (important trick)\ngrads <- grads / k_maximum(k_mean(k_abs(grads)), 1e-7)\n\noutputs <- list(loss, grads)\n\n# Sets up a Keras function to retrieve the value of the loss and gradients, given an input image\nfetch_loss_and_grads <- k_function(list(dream), outputs)\n\neval_loss_and_grads <- function(x) {\n  outs <- fetch_loss_and_grads(list(x))\n  loss_value <- outs[[1]]\n  grad_values <- outs[[2]]\n  list(loss_value, grad_values)\n}\n\n\n# Run gradient ascent -----------------------------------------------------\n\n# This function runs gradient ascent for a number of iterations.\ngradient_ascent <-\n  function(x, iterations, step, max_loss = NULL) {\n    for (i in 1:iterations) {\n      c(loss_value, grad_values) %<-% eval_loss_and_grads(x)\n      if (!is.null(max_loss) && loss_value > max_loss)\n        break\n      cat(\"...Loss value at\", i, \":\", loss_value, \"\\n\")\n      x <- x + (step * grad_values)\n    }\n    x\n  }\n\n# Playing with these hyperparameters will let you achieve new effects.\n# Gradient ascent step size\nstep <- 0.01\n# Number of scales at which to run gradient ascent\nnum_octave <- 3\n# Size ratio between scales\noctave_scale <- 1.4\n# Number of ascent steps to run at each scale\niterations <- 20\n# If the loss grows larger than 10, we will interrupt the gradient-ascent process to avoid ugly artifacts.\nmax_loss <- 10\n\n# Fill this with the path to the image you want to use.\nbase_image_path <- \"/tmp/mypic.jpg\"\n\n# Loads the base image into an array\nimg <-\n  preprocess_image(base_image_path)\n\n# Prepares a list of shape tuples defining the different scales at which to run gradient ascent\noriginal_shape <- dim(img)[-1]\nsuccessive_shapes <-\n  list(original_shape)\nfor (i in 1:num_octave) {\n  shape <- as.integer(original_shape / (octave_scale ^ i))\n  successive_shapes[[length(successive_shapes) + 1]] <-\n    shape\n}\n# Reverses the list of shapes so they're in increasing order\nsuccessive_shapes <-\n  rev(successive_shapes)\n\noriginal_img <- img\n#  Resizes the array of the image to the smallest scale\nshrunk_original_img <-\n  resize_img(img, successive_shapes[[1]])\n\nfor (shape in successive_shapes) {\n  cat(\"Processing image shape\", shape, \"\\n\")\n  # Scales up the dream image\n  img <- resize_img(img, shape)\n  # Runs gradient ascent, altering the dream\n  img <- gradient_ascent(img,\n                         iterations = iterations,\n                         step = step,\n                         max_loss = max_loss)\n  # Scales up the smaller version of the original image: it will be pixellated\n  upscaled_shrunk_original_img <-\n    resize_img(shrunk_original_img, shape)\n  # Computes the high-quality version of the original image at this size\n  same_size_original <-\n    resize_img(original_img, shape)\n  # The difference between the two is the detail that was lost when scaling up\n  lost_detail <-\n    same_size_original - upscaled_shrunk_original_img\n  # Reinjects lost detail into the dream\n  img <- img + lost_detail\n  shrunk_original_img <-\n    resize_img(original_img, shape)\n  save_img(img, fname = sprintf(\"dream_at_scale_%s.png\",\n                                paste(shape, collapse = \"x\")))\n}"
  },
  {
    "objectID": "examples/eager_dcgan.html",
    "href": "examples/eager_dcgan.html",
    "title": "eager_dcgan",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nuse_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\n\nmnist <- dataset_mnist()\nc(train_images, train_labels) %<-% mnist$train\n\ntrain_images <- train_images %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- (train_images - 127.5) / 127.5\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- (buffer_size / batch_size) %>% round()\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      \n      self$conv1 <-\n        layer_conv_2d_transpose(\n          filters = 64,\n          kernel_size = c(5, 5),\n          strides = c(1, 1),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm2 <- layer_batch_normalization()\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      \n      self$conv2 <-\n        layer_conv_2d_transpose(\n          filters = 32,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm3 <- layer_batch_normalization()\n      self$leaky_relu3 <- layer_activation_leaky_relu()\n      \n      self$conv3 <-\n        layer_conv_2d_transpose(\n          filters = 1,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE,\n          activation = \"tanh\"\n        )\n      \n      function(inputs,\n               mask = NULL,\n               training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          self$leaky_relu1() %>%\n          k_reshape(shape = c(-1, 7, 7, 64)) %>%\n          \n          self$conv1() %>%\n          self$batchnorm2(training = training) %>%\n          self$leaky_relu2() %>%\n          \n          self$conv2() %>%\n          self$batchnorm3(training = training) %>%\n          self$leaky_relu3() %>%\n          \n          self$conv3()\n      }\n    })\n  }\n\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$conv1 <- layer_conv_2d(\n        filters = 64,\n        kernel_size = c(5, 5),\n        strides = c(2, 2),\n        padding = \"same\"\n      )\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$dropout <- layer_dropout(rate = 0.3)\n      \n      self$conv2 <-\n        layer_conv_2d(\n          filters = 128,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\"\n        )\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$flatten <- layer_flatten()\n      self$fc1 <- layer_dense(units = 1)\n      \n      function(inputs,\n               mask = NULL,\n               training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          self$dropout(training = training) %>%\n          self$conv2() %>%\n          self$leaky_relu2() %>%\n          self$flatten() %>%\n          self$fc1()\n        \n      }\n    })\n  }\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = k_ones_like(real_output),\n                                    logits = real_output)\n  generated_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = k_zeros_like(generated_output),\n                                    logits = generated_output)\n  real_loss + generated_loss\n}\n\ngenerator_loss <- function(generated_output) {\n  tf$losses$sigmoid_cross_entropy(tf$ones_like(generated_output), generated_output)\n}\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(1e-4)\ngenerator_optimizer <- tf$train$AdamOptimizer(1e-4)\n\nnum_epochs <- 150\nnoise_dim <- 100\nnum_examples_to_generate <- 25L\n\nrandom_vector_for_generation <-\n  k_random_normal(c(num_examples_to_generate,\n                    noise_dim))\n\ngenerate_and_save_images <- function(model, epoch, test_input) {\n  predictions <- model(test_input, training = FALSE)\n  png(paste0(\"images_epoch_\", epoch, \".png\"))\n  par(mfcol = c(5, 5))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:25) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\ntrain <- function(dataset, epochs, noise_dim) {\n  for (epoch in seq_len(num_epochs)) {\n    start <- Sys.time()\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      noise <- k_random_normal(c(batch_size, noise_dim))\n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          generated_images <- generator(noise)\n          disc_real_output <- discriminator(batch, training = TRUE)\n          disc_generated_output <-\n            discriminator(generated_images, training = TRUE)\n          gen_loss <- generator_loss(disc_generated_output)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n        })\n      })\n      \n      gradients_of_generator <-\n        gen_tape$gradient(gen_loss, generator$variables)\n      gradients_of_discriminator <-\n        disc_tape$gradient(disc_loss, discriminator$variables)\n      \n      generator_optimizer$apply_gradients(purrr::transpose(list(\n        gradients_of_generator, generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_discriminator, discriminator$variables)\n      ))\n      \n      total_loss_gen <- total_loss_gen + gen_loss\n      total_loss_disc <- total_loss_disc + disc_loss\n      \n    })\n    \n    cat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    if (epoch %% 10 == 0)\n      generate_and_save_images(generator,\n                               epoch,\n                               random_vector_for_generation)\n    \n  }\n}\n\ntrain(train_dataset, num_epochs, noise_dim)"
  },
  {
    "objectID": "examples/eager_image_captioning.html",
    "href": "examples/eager_image_captioning.html",
    "title": "eager_image_captioning",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nnp <- import(\"numpy\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(rjson)\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(magick)\n\nmaybecat <- function(context, x) {\n  if (debugshapes) {\n    name <- enexpr(x)\n    dims <- paste0(dim(x), collapse = \" \")\n    cat(context, \": shape of \", name, \": \", dims, \"\\n\", sep = \"\")\n  }\n}\n\ndebugshapes <- FALSE\nrestore_checkpoint <- FALSE\nsaved_features_exist <- FALSE\n\nuse_session_with_seed(7777,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\nannotation_file <- \"train2014/annotations/captions_train2014.json\"\nimage_path <- \"train2014/train2014\"\n\nannotations <- fromJSON(file = annotation_file)\n\nannot_captions <- annotations[[4]]\n# 414113\nnum_captions <- length(annot_captions)\n\nall_captions <- vector(mode = \"list\", length = num_captions)\nall_img_names <- vector(mode = \"list\", length = num_captions)\n\nfor (i in seq_len(num_captions)) {\n  caption <-\n    paste0(\"<start> \", annot_captions[[i]][[\"caption\"]], \" <end>\")\n  image_id <- annot_captions[[i]][[\"image_id\"]]\n  full_coco_image_path <-\n    sprintf(\"train2014/train2014/COCO_train2014_%012d.jpg\", image_id)\n  all_img_names[[i]] <- full_coco_image_path\n  all_captions[[i]] <- caption\n}\n\nnum_examples <- 30000\n\nif (!saved_features_exist) {\n  random_sample <- sample(1:num_captions, size = num_examples)\n  train_indices <-\n    sample(random_sample, size = length(random_sample) * 0.8)\n  validation_indices <-\n    setdiff(random_sample, train_indices)\n  saveRDS(random_sample,\n          paste0(\"random_sample_\", num_examples, \".rds\"))\n  saveRDS(train_indices,\n          paste0(\"train_indices_\", num_examples, \".rds\"))\n  saveRDS(validation_indices,\n          paste0(\"validation_indices_\", num_examples, \".rds\"))\n} else {\n  random_sample <-\n    readRDS(paste0(\"random_sample_\", num_examples, \".rds\"))\n  train_indices <-\n    readRDS(paste0(\"train_indices_\", num_examples, \".rds\"))\n  validation_indices <-\n    readRDS(paste0(\"validation_indices_\", num_examples, \".rds\"))\n}\n\nsample_captions <- all_captions[random_sample]\nsample_images <- all_img_names[random_sample]\ntrain_captions <- all_captions[train_indices]\ntrain_images <- all_img_names[train_indices]\nvalidation_captions <- all_captions[validation_indices]\nvalidation_images <- all_img_names[validation_indices]\n\n\nload_image <- function(image_path) {\n  img <- tf$read_file(image_path) %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize_images(c(299L, 299L)) %>%\n    tf$keras$applications$inception_v3$preprocess_input()\n  list(img, image_path)\n}\n\n\nimage_model <- application_inception_v3(include_top = FALSE,\n                                        weights = \"imagenet\")\n\nif (!saved_features_exist) {\n  preencode <- unique(sample_images) %>% unlist() %>% sort()\n  num_unique <- length(preencode)\n  \n  batch_size_4save <- 1\n  image_dataset <- tensor_slices_dataset(preencode) %>%\n    dataset_map(load_image) %>%\n    dataset_batch(batch_size_4save)\n  \n  save_iter <- make_iterator_one_shot(image_dataset)\n  save_count <- 0\n  \n  until_out_of_range({\n    if (save_count %% 100 == 0) {\n      cat(\"Saving feature:\", save_count, \"of\", num_unique, \"\\n\")\n    }\n    save_count <- save_count + batch_size_4save\n    batch_4save <- save_iter$get_next()\n    img <- batch_4save[[1]]\n    path <- batch_4save[[2]]\n    batch_features <- image_model(img)\n    batch_features <- tf$reshape(batch_features,\n                                 list(dim(batch_features)[1],-1L, dim(batch_features)[4]))\n    for (i in 1:dim(batch_features)[1]) {\n      p <- path[i]$numpy()$decode(\"utf-8\")\n      np$save(p,\n              batch_features[i, ,]$numpy())\n      \n    }\n    \n  })\n}\n\ntop_k <- 5000\ntokenizer <- text_tokenizer(num_words = top_k,\n                            oov_token = \"<unk>\",\n                            filters = '!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~ ')\ntokenizer$fit_on_texts(sample_captions)\ntrain_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(train_captions)\nvalidation_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(validation_captions)\ntokenizer$word_index\n\ntokenizer$word_index[\"<unk>\"]\n\ntokenizer$word_index[\"<pad>\"] <- 0\ntokenizer$word_index[\"<pad>\"]\n\nword_index_df <- data.frame(\n  word = tokenizer$word_index %>% names(),\n  index = tokenizer$word_index %>% unlist(use.names = FALSE),\n  stringsAsFactors = FALSE\n)\n\nword_index_df <- word_index_df %>% arrange(index)\n\ndecode_caption <- function(text) {\n  paste(map(text, function(number)\n    word_index_df %>%\n      filter(index == number) %>%\n      select(word) %>%\n      pull()),\n    collapse = \" \")\n}\n\ncaption_lengths <-\n  map(all_captions[1:num_examples], function(c)\n    str_split(c, \" \")[[1]] %>% length()) %>% unlist()\nfivenum(caption_lengths)\nmax_length <- fivenum(caption_lengths)[5]\n\ntrain_captions_padded <-\n  pad_sequences(\n    train_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\nvalidation_captions_padded <-\n  pad_sequences(\n    validation_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\n\nlength(train_images)\ndim(train_captions_padded)\n\nbatch_size <- 10\nbuffer_size <- num_examples\nembedding_dim <- 256\ngru_units <- 512\nvocab_size <- top_k\nfeatures_shape <- 2048\nattention_features_shape <- 64\n\ntrain_images_4checking <- train_images[c(4, 10, 30)]\ntrain_captions_4checking <- train_captions_padded[c(4, 10, 30),]\nvalidation_images_4checking <- validation_images[c(7, 10, 12)]\nvalidation_captions_4checking <-\n  validation_captions_padded[c(7, 10, 12),]\n\n\nmap_func <- function(img_name, cap) {\n  p <- paste0(img_name$decode(\"utf-8\"), \".npy\")\n  img_tensor <- np$load(p)\n  img_tensor <- tf$cast(img_tensor, tf$float32)\n  list(img_tensor, cap)\n}\n\ntrain_dataset <-\n  tensor_slices_dataset(list(train_images, train_captions_padded)) %>%\n  dataset_map(function(item1, item2)\n    tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))) %>%\n  # dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size) \n\n\ncnn_encoder <-\n  function(embedding_dim,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$fc <-\n        layer_dense(units = embedding_dim, activation = \"relu\")\n      \n      function(x, mask = NULL) {\n        # input shape: (batch_size, 64, features_shape)\n        # shape after fc: (batch_size, 64, embedding_dim)\n        maybecat(\"encoder input\", x)\n        x <- self$fc(x)\n        maybecat(\"encoder output\", x)\n        x\n      }\n    })\n  }\n\nattention_module <-\n  function(gru_units,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$W1 = layer_dense(units = gru_units)\n      self$W2 = layer_dense(units = gru_units)\n      self$V = layer_dense(units = 1)\n      \n      function(inputs, mask = NULL) {\n        features <- inputs[[1]]\n        hidden <- inputs[[2]]\n        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n        # hidden shape == (batch_size, gru_units)\n        # hidden_with_time_axis shape == (batch_size, 1, gru_units)\n        hidden_with_time_axis <- k_expand_dims(hidden, axis = 2)\n        \n        maybecat(\"attention module\", features)\n        maybecat(\"attention module\", hidden)\n        maybecat(\"attention module\", hidden_with_time_axis)\n        \n        # score shape == (batch_size, 64, 1)\n        score <-\n          self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))\n        # attention_weights shape == (batch_size, 64, 1)\n        attention_weights <- k_softmax(score, axis = 2)\n        # context_vector shape after sum == (batch_size, embedding_dim)\n        context_vector <-\n          k_sum(attention_weights * features, axis = 2)\n        \n        maybecat(\"attention module\", score)\n        maybecat(\"attention module\", attention_weights)\n        maybecat(\"attention module\", context_vector)\n        \n        list(context_vector, attention_weights)\n      }\n    })\n  }\n\nrnn_decoder <-\n  function(embedding_dim,\n           gru_units,\n           vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru_units <- gru_units\n      self$embedding <-\n        layer_embedding(input_dim = vocab_size, output_dim = embedding_dim)\n      self$gru <- if (tf$test$is_gpu_available()) {\n        layer_cudnn_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      } else {\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      }\n      \n      self$fc1 <- layer_dense(units = self$gru_units)\n      self$fc2 <- layer_dense(units = vocab_size)\n      \n      self$attention <- attention_module(self$gru_units)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        features <- inputs[[2]]\n        hidden <- inputs[[3]]\n        \n        maybecat(\"decoder\", x)\n        maybecat(\"decoder\", features)\n        maybecat(\"decoder\", hidden)\n        \n        c(context_vector, attention_weights) %<-% self$attention(list(features, hidden))\n        \n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x <- self$embedding(x)\n        \n        maybecat(\"decoder x after embedding\", x)\n        \n        # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)\n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x))\n        \n        maybecat(\"decoder x after concat\", x)\n        \n        # passing the concatenated vector to the GRU\n        c(output, state) %<-% self$gru(x)\n        \n        maybecat(\"decoder output after gru\", output)\n        maybecat(\"decoder state after gru\", state)\n        \n        # shape == (batch_size, 1, gru_units)\n        x <- self$fc1(output)\n        \n        maybecat(\"decoder output after fc1\", x)\n        \n        # x shape == (batch_size, gru_units)\n        x <- k_reshape(x, c(-1, dim(x)[[3]]))\n        \n        maybecat(\"decoder output after reshape\", x)\n        \n        # output shape == (batch_size, vocab_size)\n        x <- self$fc2(x)\n        \n        maybecat(\"decoder output after fc2\", x)\n        \n        list(x, state, attention_weights)\n        \n      }\n    })\n  }\n\n\nencoder <- cnn_encoder(embedding_dim)\ndecoder <- rnn_decoder(embedding_dim, gru_units, vocab_size)\n\noptimizer = tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- 1 - k_cast(y_true == 0L, dtype = \"float32\")\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true, logits =\n                                                     y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\nget_caption <-\n  function(image) {\n    attention_matrix <-\n      matrix(0, nrow = max_length, ncol = attention_features_shape)\n    # shape=(1, 299, 299, 3)\n    temp_input <- k_expand_dims(load_image(image)[[1]], 1)\n    # shape=(1, 8, 8, 2048),\n    img_tensor_val <- image_model(temp_input)\n    # shape=(1, 64, 2048)\n    img_tensor_val <- k_reshape(img_tensor_val,\n                                list(dim(img_tensor_val)[1],-1, dim(img_tensor_val)[4]))\n    # shape=(1, 64, 256)\n    features <- encoder(img_tensor_val)\n    \n    dec_hidden <- k_zeros(c(1, gru_units))\n    dec_input <-\n      k_expand_dims(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]))\n    \n    result <- \"\"\n    \n    for (t in seq_len(max_length - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, features, dec_hidden))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t, ] <- attention_weights %>% as.double()\n      \n      pred_idx = tf$multinomial(exp(preds), num_samples = 1)[1, 1] %>% as.double()\n      \n      pred_word <-\n        word_index_df[word_index_df$index == pred_idx, \"word\"]\n      \n      if (pred_word == \"<end>\") {\n        result <-\n          paste(result, pred_word)\n        attention_matrix <-\n          attention_matrix[1:length(str_split(result, \" \")[[1]]), , drop = FALSE]\n        return (list(str_trim(result), attention_matrix))\n      } else {\n        result <-\n          paste(result, pred_word)\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    \n    list(str_trim(result), attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           image_name,\n           result,\n           epoch) {\n    image <-\n      image_read(image_name) %>% image_scale(\"299x299!\")\n    result <- str_split(result, \" \")[[1]] %>% as.list()\n    # attention_matrix shape: nrow = max_length, ncol = attention_features_shape\n    for (i in 1:length(result)) {\n      att <- attention_matrix[i, ] %>% np$resize(tuple(8L, 8L))\n      dim(att) <- c(8, 8, 1)\n      att <- image_read(att) %>% image_scale(\"299x299\") %>%\n        image_annotate(\n          result[[i]],\n          gravity = \"northeast\",\n          size = 20,\n          color = \"white\",\n          location = \"+20+40\"\n        )\n      overlay <-\n        image_composite(att, image, operator = \"blend\", compose_args = \"30\")\n      image_write(\n        overlay,\n        paste0(\n          \"attention_plot_epoch_\",\n          epoch,\n          \"_img_\",\n          image_name %>% basename() %>% str_sub(16,-5),\n          \"_word_\",\n          i,\n          \".png\"\n        )\n      )\n    }\n  }\n\n\ncheck_sample_captions <-\n  function(epoch, mode, plot_attention) {\n    images <- switch(mode,\n                     training = train_images_4checking,\n                     validation = validation_images_4checking)\n    captions <- switch(mode,\n                       training = train_captions_4checking,\n                       validation = validation_captions_4checking)\n    cat(\"\\n\", \"Sample checks on \", mode, \" set:\", \"\\n\", sep = \"\")\n    for (i in 1:length(images)) {\n      c(result, attention_matrix) %<-% get_caption(images[[i]])\n      real_caption <-\n        decode_caption(captions[i,]) %>% str_remove_all(\" <pad>\")\n      cat(\"\\nReal caption:\",  real_caption, \"\\n\")\n      cat(\"\\nPredicted caption:\", result, \"\\n\")\n      if (plot_attention)\n        plot_attention(attention_matrix, images[[i]], result, epoch)\n    }\n    \n  }\n\ncheckpoint_dir <- \"./checkpoints_captions\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(optimizer = optimizer,\n                      encoder = encoder,\n                      decoder = decoder)\n\n\nif (restore_checkpoint) {\n  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n}\n\nnum_epochs <- 20\n\nif (!restore_checkpoint) {\n  for (epoch in seq_len(num_epochs)) {\n    cat(\"Starting epoch:\", epoch, \"\\n\")\n    total_loss <- 0\n    progress <- 0\n    train_iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      progress <- progress + 1\n      if (progress %% 10 == 0)\n        cat(\"-\")\n      \n      batch <- iterator_get_next(train_iter)\n      loss <- 0\n\n      img_tensor <- batch[[1]]\n      target_caption <- batch[[2]]\n      \n      dec_hidden <- k_zeros(c(batch_size, gru_units))\n      \n      dec_input <-\n        k_expand_dims(rep(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]), batch_size))\n      \n      with(tf$GradientTape() %as% tape, {\n        features <- encoder(img_tensor)\n        \n        for (t in seq_len(dim(target_caption)[2] - 1)) {\n          c(preds, dec_hidden, weights) %<-%\n            decoder(list(dec_input, features, dec_hidden))\n          loss <- loss + cx_loss(target_caption[, t], preds)\n          dec_input <- k_expand_dims(target_caption[, t])\n        }\n        \n      })\n      total_loss <-\n        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])\n      \n      variables <- c(encoder$variables, decoder$variables)\n      gradients <- tape$gradient(loss, variables)\n      \n      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),\n                                global_step = tf$train$get_or_create_global_step())\n    })\n    cat(paste0(\n      \"\\n\\nTotal loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n      \"\\n\"\n    ))\n    \n    \n    checkpoint$save(file_prefix = checkpoint_prefix)\n    \n    check_sample_captions(epoch, \"training\", plot_attention = FALSE)\n    check_sample_captions(epoch, \"validation\", plot_attention = FALSE)\n    \n  }\n}\n\n\nepoch <- num_epochs\ncheck_sample_captions(epoch, \"training\", plot_attention = TRUE)\ncheck_sample_captions(epoch, \"validation\", plot_attention = TRUE)"
  },
  {
    "objectID": "examples/eager_pix2pix.html",
    "href": "examples/eager_pix2pix.html",
    "title": "eager_pix2pix",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\n\nrestore <- TRUE\n\ndata_dir <- \"facades\"\n\nbuffer_size <- 400\nbatch_size <- 1\nbatches_per_epoch <- buffer_size / batch_size\nimg_width <- 256L\nimg_height <- 256L\n\nload_image <- function(image_file, is_train) {\n\n  image <- tf$read_file(image_file)\n  image <- tf$image$decode_jpeg(image)\n  \n  w <- as.integer(k_shape(image)[2])\n  w2 <- as.integer(w / 2L)\n  real_image <- image[ , 1L:w2, ]\n  input_image <- image[ , (w2 + 1L):w, ]\n  \n  input_image <- k_cast(input_image, tf$float32)\n  real_image <- k_cast(real_image, tf$float32)\n\n  if (is_train) {\n      input_image <-\n      tf$image$resize_images(input_image,\n                             c(286L, 286L),\n                             align_corners = TRUE,\n                             method = 2)\n    real_image <- tf$image$resize_images(real_image,\n                                         c(286L, 286L),\n                                         align_corners = TRUE,\n                                         method = 2)\n    \n    stacked_image <-\n      k_stack(list(input_image, real_image), axis = 1)\n    cropped_image <-\n      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))\n    c(input_image, real_image) %<-% list(cropped_image[1, , , ], cropped_image[2, , , ])\n    \n    if (runif(1) > 0.5) {\n      input_image <- tf$image$flip_left_right(input_image)\n      real_image <- tf$image$flip_left_right(real_image)\n    }\n  } else {\n    input_image <-\n      tf$image$resize_images(\n        input_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n    real_image <-\n      tf$image$resize_images(\n        real_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n  }\n  \n  input_image <- (input_image / 127.5) - 1\n  real_image <- (real_image / 127.5) - 1\n  \n  list(input_image, real_image)\n}\n\ntrain_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"train/*.jpg\")) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"test/*.jpg\")) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\n\ndownsample <- function(filters,\n                       size,\n                       apply_batchnorm = TRUE,\n                       name = \"downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      cat(\"downsample (generator) output: \", x$shape$as_list(), \"\\n\")\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\nupsample <- function(filters,\n                     size,\n                     apply_dropout = FALSE,\n                     name = \"upsample\") {\n  keras_model_custom(name = NULL, function(self) {\n    self$apply_dropout <- apply_dropout\n    self$up_conv <- layer_conv_2d_transpose(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    if (self$apply_dropout) {\n      self$dropout <- layer_dropout(rate = 0.5)\n    }\n    function(xs,\n             mask = NULL,\n             training = TRUE) {\n      c(x1, x2) %<-% xs\n      x <- self$up_conv(x1) %>% self$batchnorm(training = training)\n      if (self$apply_dropout) {\n        x %>% self$dropout(training = training)\n      }\n      x %>% layer_activation(\"relu\")\n      concat <- k_concatenate(list(x, x2))\n      cat(\"upsample (generator) output: \", concat$shape$as_list(), \"\\n\")\n      concat\n    }\n  })\n}\n\ngenerator <- function(name = \"generator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- downsample(64, 4, apply_batchnorm = FALSE)\n    self$down2 <- downsample(128, 4)\n    self$down3 <- downsample(256, 4)\n    self$down4 <- downsample(512, 4)\n    self$down5 <- downsample(512, 4)\n    self$down6 <- downsample(512, 4)\n    self$down7 <- downsample(512, 4)\n    self$down8 <- downsample(512, 4)\n    \n    self$up1 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up2 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up3 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up4 <- upsample(512, 4)\n    self$up5 <- upsample(256, 4)\n    self$up6 <- upsample(128, 4)\n    self$up7 <- upsample(64, 4)\n    self$last <- layer_conv_2d_transpose(\n      filters = 3,\n      kernel_size = 4,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      activation = \"tanh\"\n    )\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      # x shape == (bs, 256, 256, 3)\n      x1 <-\n        x %>% self$down1(training = training)  # (bs, 128, 128, 64)\n      x2 <- self$down2(x1, training = training) # (bs, 64, 64, 128)\n      x3 <- self$down3(x2, training = training) # (bs, 32, 32, 256)\n      x4 <- self$down4(x3, training = training) # (bs, 16, 16, 512)\n      x5 <- self$down5(x4, training = training) # (bs, 8, 8, 512)\n      x6 <- self$down6(x5, training = training) # (bs, 4, 4, 512)\n      x7 <- self$down7(x6, training = training) # (bs, 2, 2, 512)\n      x8 <- self$down8(x7, training = training) # (bs, 1, 1, 512)\n\n      x9 <-\n        self$up1(list(x8, x7), training = training) # (bs, 2, 2, 1024)\n      x10 <-\n        self$up2(list(x9, x6), training = training) # (bs, 4, 4, 1024)\n      x11 <-\n        self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)\n      x12 <-\n        self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)\n      x13 <-\n        self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)\n      x14 <-\n        self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)\n      x15 <-\n        self$up7(list(x14, x1), training = training) # (bs, 128, 128, 128)\n      x16 <- self$last(x15) # (bs, 256, 256, 3)\n      cat(\"generator output: \", x16$shape$as_list(), \"\\n\")\n      x16\n    }\n  })\n}\n\n\ndisc_downsample <- function(filters,\n                            size,\n                            apply_batchnorm = TRUE,\n                            name = \"disc_downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\ndiscriminator <- function(name = \"discriminator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- disc_downsample(64, 4, FALSE)\n    self$down2 <- disc_downsample(128, 4)\n    self$down3 <- disc_downsample(256, 4)\n    # we are zero padding here with 1 because we need our shape to\n    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n    self$zero_pad1 <- layer_zero_padding_2d()\n    self$conv <- layer_conv_2d(\n      filters = 512,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    self$zero_pad2 <- layer_zero_padding_2d()\n    self$last <- layer_conv_2d(\n      filters = 1,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal()\n    )\n    \n    function(x,\n             y,\n             mask = NULL,\n             training = TRUE) {\n      x <- k_concatenate(list(x, y)) %>% # (bs, 256, 256, channels*2)\n        self$down1(training = training) %>% # (bs, 128, 128, 64)\n        self$down2(training = training) %>% # (bs, 64, 64, 128)\n        self$down3(training = training) %>% # (bs, 32, 32, 256)\n        self$zero_pad1() %>% # (bs, 34, 34, 256)\n        self$conv() %>% # (bs, 31, 31, 512)\n        self$batchnorm(training = training) %>%\n        layer_activation_leaky_relu() %>%\n        self$zero_pad2() %>% # (bs, 33, 33, 512)\n        self$last() # (bs, 30, 30, 1)\n      cat(\"discriminator output: \", x$shape$as_list(), \"\\n\")\n      x\n    }\n  })\n  \n}\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$ones_like(real_output),\n                                    logits = real_output)\n  generated_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$zeros_like(generated_output),\n                                    logits = generated_output)\n  real_loss + generated_loss\n}\n\nlambda <- 100\ngenerator_loss <-\n  function(disc_judgment, generated_output, target) {\n    gan_loss <-\n      tf$losses$sigmoid_cross_entropy(tf$ones_like(disc_judgment), disc_judgment)\n    l1_loss <- tf$reduce_mean(tf$abs(target - generated_output))\n    gan_loss + (lambda * l1_loss)\n  }\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\ngenerator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\n\ncheckpoint_dir <- \"./checkpoints_pix2pix\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer,\n    generator = generator,\n    discriminator = discriminator\n  )\n\ngenerate_images <- function(generator, input, target, id) {\n  prediction <- generator(input, training = TRUE)\n  png(paste0(\"pix2pix_\", id, \".png\"), width = 900, height = 300)\n  par(mfcol = c(1, 3))\n  par(mar = c(0, 0, 0, 0),\n      xaxs = 'i',\n      yaxs = 'i')\n  input <- input[1, , ,]$numpy() * 0.5 + 0.5\n  input[input > 1] <- 1\n  input[input < 0] <- 0\n  plot(as.raster(input, main = \"input image\"))\n  target <- target[1, , ,]$numpy() * 0.5 + 0.5\n  target[target > 1] <- 1\n  target[target < 0] <- 0\n  plot(as.raster(target, main = \"ground truth\"))\n  prediction <- prediction[1, , ,]$numpy() * 0.5 + 0.5\n  prediction[prediction > 1] <- 1\n  prediction[prediction < 0] <- 0\n  plot(as.raster(prediction, main = \"generated\"))\n  dev.off()\n}\n\ntrain <- function(dataset, num_epochs) {\n  for (epoch in 1:num_epochs) {\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      input_image <- batch[[1]]\n      target <- batch[[2]]\n      \n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          gen_output <- generator(input_image, training = TRUE)\n          disc_real_output <-\n            discriminator(input_image, target, training = TRUE)\n          disc_generated_output <-\n            discriminator(input_image, gen_output, training = TRUE)\n          gen_loss <-\n            generator_loss(disc_generated_output, gen_output, target)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n          total_loss_gen <- total_loss_gen + gen_loss\n          total_loss_disc <- total_loss_disc + disc_loss\n        })\n      })\n      generator_gradients <- gen_tape$gradient(gen_loss,\n                                               generator$variables)\n      discriminator_gradients <- disc_tape$gradient(disc_loss,\n                                                    discriminator$variables)\n      \n      generator_optimizer$apply_gradients(transpose(list(\n        generator_gradients,\n        generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(transpose(\n        list(discriminator_gradients,\n             discriminator$variables)\n      ))\n      \n    })\n    cat(\"Epoch \", epoch, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    if (epoch %% 10 == 0) {\n      test_iter <- make_iterator_one_shot(test_dataset)\n      batch <- iterator_get_next(test_iter)\n      input <- batch[[1]]\n      target <- batch[[2]]\n      generate_images(generator, input, target, paste0(\"epoch_\", i))\n    }\n    if (epoch %% 10 == 0) {\n      checkpoint$save(file_prefix = checkpoint_prefix)\n    }\n    \n  }\n}\n\nif (!restore) {\n  train(train_dataset, 200)\n} \n\n\ncheckpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n\ntest_iter <- make_iterator_one_shot(test_dataset)\ni <- 1\nuntil_out_of_range({\n  batch <- iterator_get_next(test_iter)\n  input <- batch[[1]]\n  target <- batch[[2]]\n  generate_images(generator, input, target, paste0(\"test_\", i))\n  i <- i + 1\n})"
  },
  {
    "objectID": "examples/eager_styletransfer.html",
    "href": "examples/eager_styletransfer.html",
    "title": "eager_styletransfer",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-09-eager-style-transfer\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nuse_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(purrr)\nlibrary(glue)\n\nimg_shape <- c(128, 128, 3)\ncontent_path <- \"isar.jpg\"\nstyle_path <- \"The_Great_Wave_off_Kanagawa.jpg\"\n\n\nnum_iterations <- 2000\ncontent_weight <- 100\nstyle_weight <- 0.8\ntotal_variation_weight <- 0.01\n\ncontent_image <-\n  image_load(content_path, target_size = img_shape[1:2])\ncontent_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\nstyle_image <-\n  image_load(style_path, target_size = img_shape[1:2])\nstyle_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\n\nload_and_process_image <- function(path) {\n  img <- image_load(path, target_size = img_shape[1:2]) %>%\n    image_to_array() %>%\n    k_expand_dims(axis = 1) %>%\n    imagenet_preprocess_input()\n}\n\ndeprocess_image <- function(x) {\n  x <- x[1, , ,]\n  # Remove zero-center by mean pixel\n  x[, , 1] <- x[, , 1] + 103.939\n  x[, , 2] <- x[, , 2] + 116.779\n  x[, , 3] <- x[, , 3] + 123.68\n  # 'BGR'->'RGB'\n  x <- x[, , c(3, 2, 1)]\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x) / 255\n  x\n}\n\ncontent_layers <- c(\"block5_conv2\")\nstyle_layers = c(\"block1_conv1\",\n                 \"block2_conv1\",\n                 \"block3_conv1\",\n                 \"block4_conv1\",\n                 \"block5_conv1\")\nnum_content_layers <- length(content_layers)\nnum_style_layers <- length(style_layers)\n\nget_model <- function() {\n  vgg <- application_vgg19(include_top = FALSE, weights = \"imagenet\")\n  vgg$trainable <- FALSE\n  style_outputs <-\n    map(style_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  content_outputs <-\n    map(content_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  model_outputs <- c(style_outputs, content_outputs)\n  keras_model(vgg$input, model_outputs)\n}\n\ncontent_loss <- function(content_image, target) {\n  k_sum(k_square(target - content_image))\n}\n\ngram_matrix <- function(x) {\n  features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))\n  gram <- k_dot(features, k_transpose(features))\n  gram\n}\n\nstyle_loss <- function(gram_target, combination) {\n  gram_comb <- gram_matrix(combination)\n  k_sum(k_square(gram_target - gram_comb)) / (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^\n                                                2)\n}\n\ntotal_variation_loss <- function(image) {\n  y_ij  <- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]\n  y_i1j <- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]\n  y_ij1 <- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]\n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\nget_feature_representations <-\n  function(model, content_path, style_path) {\n    # dim == (1, 128, 128, 3)\n    style_image <-\n      load_and_process_image(style_path) %>% k_cast(\"float32\")\n    # dim == (1, 128, 128, 3)\n    content_image <-\n      load_and_process_image(content_path) %>% k_cast(\"float32\")\n    # dim == (2, 128, 128, 3)\n    stack_images <-\n      k_concatenate(list(style_image, content_image), axis = 1)\n    # length(model_outputs) == 6\n    # dim(model_outputs[[1]]) = (2, 128, 128, 64)\n    # dim(model_outputs[[6]]) = (2, 8, 8, 512)\n    model_outputs <- model(stack_images)\n    style_features <- model_outputs[1:num_style_layers] %>%\n      map(function(batch)\n        batch[1, , , ])\n    content_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %>%\n      map(function(batch)\n        batch[2, , , ])\n    list(style_features, content_features)\n  }\n\ncompute_loss <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    c(style_weight, content_weight) %<-% loss_weights\n    model_outputs <- model(init_image)\n    style_output_features <- model_outputs[1:num_style_layers]\n    content_output_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]\n    \n    weight_per_style_layer <- 1 / num_style_layers\n    style_score <- 0\n    # str(style_zip, max.level = 1)\n    # dim(style_zip[[5]][[1]]) == (512, 512)\n    style_zip <-\n      transpose(list(gram_style_features, style_output_features))\n    for (l in 1:length(style_zip)) {\n      # for l == 1:\n      # dim(target_style) == (64, 64)\n      # dim(comb_style) == (1, 128, 128, 64)\n      c(target_style, comb_style) %<-% style_zip[[l]]\n      style_score <-\n        style_score + weight_per_style_layer * style_loss(target_style, comb_style[1, , , ])\n    }\n    \n    weight_per_content_layer <- 1 / num_content_layers\n    content_score <- 0\n    content_zip <-\n      transpose(list(content_features, content_output_features))\n    for (l in 1:length(content_zip)) {\n      # dim(comb_content) ==  (1, 8, 8, 512)\n      # dim(target_content) == (8, 8, 512)\n      c(target_content, comb_content) %<-% content_zip[[l]]\n      content_score <-\n        content_score + weight_per_content_layer * content_loss(comb_content[1, , , ], target_content)\n    }\n    \n    variation_loss <- total_variation_loss(init_image[1, , ,])\n    style_score <- style_score * style_weight\n    content_score <- content_score * content_weight\n    variation_score <- variation_loss * total_variation_weight\n    \n    loss <- style_score + content_score + variation_score\n    list(loss, style_score, content_score, variation_score)\n  }\n\ncompute_grads <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    with(tf$GradientTape() %as% tape, {\n      scores <-\n        compute_loss(model,\n                     loss_weights,\n                     init_image,\n                     gram_style_features,\n                     content_features)\n    })\n    total_loss <- scores[[1]]\n    list(tape$gradient(total_loss, init_image), scores)\n  }\n\nrun_style_transfer <- function(content_path,\n                               style_path) {\n  model <- get_model()\n  walk(model$layers, function(layer)\n    layer$trainable = FALSE)\n  \n  c(style_features, content_features) %<-% get_feature_representations(model, content_path, style_path)\n  # dim(gram_style_features[[1]]) == (64, 64)\n  # we compute this once, in advance\n  gram_style_features <-\n    map(style_features, function(feature)\n      gram_matrix(feature))\n  \n  init_image <- load_and_process_image(content_path)\n  init_image <-\n    tf$contrib$eager$Variable(init_image, dtype = \"float32\")\n  \n  optimizer <-\n    tf$train$AdamOptimizer(learning_rate = 1,\n                           beta1 = 0.99,\n                           epsilon = 1e-1)\n  \n  c(best_loss, best_image) %<-% list(Inf, NULL)\n  loss_weights <- list(style_weight, content_weight)\n  \n  start_time <- Sys.time()\n  global_start <- Sys.time()\n  \n  norm_means <- c(103.939, 116.779, 123.68)\n  min_vals <- -norm_means\n  max_vals <- 255 - norm_means\n  \n  for (i in seq_len(num_iterations)) {\n    # dim(grads) == (1, 128, 128, 3)\n    c(grads, all_losses) %<-% compute_grads(model,\n                                            loss_weights,\n                                            init_image,\n                                            gram_style_features,\n                                            content_features)\n    c(loss, style_score, content_score, variation_score) %<-% all_losses\n    optimizer$apply_gradients(list(tuple(grads, init_image)))\n    clipped <- tf$clip_by_value(init_image, min_vals, max_vals)\n    init_image$assign(clipped)\n    \n    end_time <- Sys.time()\n    \n    if (k_cast_to_floatx(loss) < best_loss) {\n      best_loss <- k_cast_to_floatx(loss)\n      best_image <- init_image\n    }\n    \n    if (i %% 50 == 0) {\n      glue(\"Iteration: {i}\") %>% print()\n      glue(\n        \"Total loss: {k_cast_to_floatx(loss)}, style loss: {k_cast_to_floatx(style_score)},\n        content loss: {k_cast_to_floatx(content_score)}, total variation loss: {k_cast_to_floatx(variation_score)},\n        time for 1 iteration: {(Sys.time() - start_time) %>% round(2)}\"\n      ) %>% print()\n      \n      if (i %% 100 == 0) {\n        png(paste0(\"style_epoch_\", i, \".png\"))\n        plot_image <- best_image$numpy()\n        plot_image <- deprocess_image(plot_image)\n        plot(as.raster(plot_image), main = glue(\"Iteration {i}\"))\n        dev.off()\n      }\n    }\n  }\n  \n  glue(\"Total time: {Sys.time() - global_start} seconds\") %>% print()\n  list(best_image, best_loss)\n}\n\nc(best_image, best_loss) %<-% run_style_transfer(content_path, style_path)"
  },
  {
    "objectID": "examples/fine_tuning.html",
    "href": "examples/fine_tuning.html",
    "title": "fine_tuning",
    "section": "",
    "text": "It’s preferable to run this example in a GPU.\n\n# Download data -----------------------------------------------------------\n\ndownload.file(\n  \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\", \n  destfile = \"cats-dogs.zip\"\n)\n\n# Pre-processing ----------------------------------------------------------\n\nzip::unzip(\"cats-dogs.zip\", exdir = \"data-raw\")\n\n# We will organize images in the following structure:\n# data/\n#     train/\n#          Cat/\n#          Dog/\n#     validation\n#          Cat/\n#          Dog/\n#     test/\n#          images/\n#\n\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\n# some images are corrupt and we exclude them\n# this will make sure all images can be read.\nfor (im in all_imgs) {\n  out <- try(magick::image_read(im), silent = TRUE)\n  if (inherits(out, \"try-error\")) {\n    fs::file_delete(im)\n    message(\"removed image: \", im)\n  }\n}\n\n# re-list all imgs\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\nset.seed(5)\n\ntraining_imgs <- sample(all_imgs, size = length(all_imgs)/2)\nvalidation_imgs <- sample(all_imgs[!all_imgs %in% training_imgs], size = length(all_imgs)/4)         \ntesting_imgs <- all_imgs[!all_imgs %in% c(training_imgs, validation_imgs)]\n\n# create directory structure\nfs::dir_create(c(\n  \"data/train/Cat\",\n  \"data/train/Dog\",\n  \"data/validation/Cat\",\n  \"data/validation/Dog\",\n  \"data/test/images\"\n))\n\n# copy training images\nfs::file_copy(\n  path = training_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/train\", training_imgs)\n)\n\n# copy valid images\nfs::file_copy(\n  path = validation_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/validation\", validation_imgs)\n)\n\n# copy testing imgs\nfs::file_copy(\n  path = testing_imgs,\n  new_path = gsub(\"data-raw/PetImages/(Dog|Cat)/\", \"data/test/images/\\\\1\", testing_imgs)\n)\n\n# Image flow --------------------------------------------------------------\n\nlibrary(keras)\n\ntraining_image_gen <- image_data_generator(\n  rotation_range = 20,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  horizontal_flip = TRUE,\n  preprocessing_function = imagenet_preprocess_input\n)\n\nvalidation_image_gen <- image_data_generator(\n  preprocessing_function = imagenet_preprocess_input\n)\n\ntraining_image_flow <- flow_images_from_directory(\n  directory = \"data/train/\", \n  generator = training_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n)\n\nvalidation_image_flow <- flow_images_from_directory(\n  directory = \"data/validation/\", \n  generator = validation_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n  shuffle = FALSE\n)\n\n# Model -------------------------------------------------------------------\n\nmob <- application_mobilenet(include_top = FALSE, pooling = \"avg\")\nfreeze_weights(mob)\n\nmodel <- keras_model_sequential() %>% \n  mob() %>% \n  layer_dense(256, activation = \"relu\") %>% \n  layer_dropout(rate = 0.2) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 1, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# now top layers weights are fine, we can unfreeze the lower layer weights.\nunfreeze_weights(mob)\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 3, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# Generate predictions for test data --------------------------------------\n\ntest_flow <- flow_images_from_directory(\n  generator = validation_image_gen,\n  directory = \"data/test\", \n  target_size = c(224, 224),\n  class_mode = NULL,\n  shuffle = FALSE\n)\n\npredictions <- predict_generator(\n  model, \n  test_flow,\n  steps = test_flow$n/test_flow$batch_size\n)\n\nmagick::image_read(testing_imgs[1])\npredictions[1]\n\nmagick::image_read(testing_imgs[6250])\npredictions[6250]"
  },
  {
    "objectID": "examples/imdb_bidirectional_lstm.html",
    "href": "examples/imdb_bidirectional_lstm.html",
    "title": "imdb_bidirectional_lstm",
    "section": "",
    "text": "Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s.\n\nlibrary(keras)\n\n# Define maximum number of input features\nmax_features <- 20000\n\n# Cut texts after this number of words\n# (among top max_features most common words)\nmaxlen <- 100\n\nbatch_size <- 32\n\n# Load imdb dataset \ncat('Loading data...\\n')\nimdb <- dataset_imdb(num_words = max_features)\n\n# Define training and test sets\nx_train <- imdb$train$x\ny_train <- imdb$train$y\nx_test <- imdb$test$x\ny_test <- imdb$test$y\n\n# Output lengths of testing and training sets\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\ncat('Pad sequences (samples x time)\\n')\n\n# Pad training and test inputs\nx_train <- pad_sequences(x_train, maxlen = maxlen)\nx_test <- pad_sequences(x_test, maxlen = maxlen)\n\n# Output dimensions of training and test inputs\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\n# Initialize model\nmodel <- keras_model_sequential()\nmodel %>%\n  # Creates dense embedding layer; outputs 3D tensor\n  # with shape (batch_size, sequence_length, output_dim)\n  layer_embedding(input_dim = max_features, \n                  output_dim = 128, \n                  input_length = maxlen) %>% \n  bidirectional(layer_lstm(units = 64)) %>%\n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\n# Try using different optimizers and different optimizer configs\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\n# Train model over four epochs\ncat('Train...\\n')\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = 4,\n  validation_data = list(x_test, y_test)\n)"
  },
  {
    "objectID": "examples/imdb_cnn.html",
    "href": "examples/imdb_cnn.html",
    "title": "imdb_cnn",
    "section": "",
    "text": "Output after 2 epochs: ~0.89 Time per epoch on CPU (Intel i5 2.4Ghz): 90s Time per epoch on GPU (Tesla K40): 10s\n\nlibrary(keras)\n\n# Set parameters:\nmax_features <- 5000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 250\nkernel_size <- 3\nhidden_dims <- 250\nepochs <- 2\n\n\n# Data Preparation --------------------------------------------------------\n\n# Keras load all data into a list with the following structure:\n# List of 2\n# $ train:List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 0 0 1 0 0 1 0 1 0 ...\n# $ test :List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 1 1 1 1 0 0 0 1 1 ...\n#\n# The x data includes integer sequences, each integer is a word.\n# The y data includes a set of integer labels (0 or 1).\n# The num_words argument indicates that only the max_fetures most frequent\n# words will be integerized. All other will be ignored.\n# See help(dataset_imdb)\nimdb <- dataset_imdb(num_words = max_features)\n\n# Pad the sequences, so they have all the same length\n# This will convert the dataset into a matrix: each line is a review\n# and each column a word on the sequence. \n# Pad the sequences with 0 to the left.\nx_train <- imdb$train$x %>%\n  pad_sequences(maxlen = maxlen)\nx_test <- imdb$test$x %>%\n  pad_sequences(maxlen = maxlen)\n\n# Defining Model ------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>% \n  # Start off with an efficient embedding layer which maps\n  # the vocab indices into embedding_dims dimensions\n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n\n  # Add a Convolution1D, which will learn filters\n    # Word group filters of size filter_length:\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  # Apply max pooling:\n  layer_global_max_pooling_1d() %>%\n\n  # Add a vanilla hidden layer:\n  layer_dense(hidden_dims) %>%\n\n  # Apply 20% layer dropout\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n\n  # Project onto a single unit output layer, and squash it with a sigmoid\n\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile model\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Training ----------------------------------------------------------------\n\nmodel %>%\n  fit(\n    x_train, imdb$train$y,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, imdb$test$y)\n  )"
  },
  {
    "objectID": "examples/imdb_cnn_lstm.html",
    "href": "examples/imdb_cnn_lstm.html",
    "title": "imdb_cnn_lstm",
    "section": "",
    "text": "Achieves 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n\nlibrary(keras)\n\n# Parameters --------------------------------------------------------------\n\n# Embedding\nmax_features = 20000\nmaxlen = 100\nembedding_size = 128\n\n# Convolution\nkernel_size = 5\nfilters = 64\npool_size = 4\n\n# LSTM\nlstm_output_size = 70\n\n# Training\nbatch_size = 30\nepochs = 2\n\n# Data Preparation --------------------------------------------------------\n\n# The x data includes integer sequences, each integer is a word\n# The y data includes a set of integer labels (0 or 1)\n# The num_words argument indicates that only the max_fetures most frequent\n# words will be integerized. All other will be ignored.\n# See help(dataset_imdb)\nimdb <- dataset_imdb(num_words = max_features)\n# Keras load all data into a list with the following structure:\nstr(imdb)\n\n# Pad the sequences to the same length\n  # This will convert our dataset into a matrix: each line is a review\n  # and each column a word on the sequence\n# We pad the sequences with 0s to the left\nx_train <- imdb$train$x %>%\n  pad_sequences(maxlen = maxlen)\nx_test <- imdb$test$x %>%\n  pad_sequences(maxlen = maxlen)\n\n# Defining Model ------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_embedding(max_features, embedding_size, input_length = maxlen) %>%\n  layer_dropout(0.25) %>%\n  layer_conv_1d(\n    filters, \n    kernel_size, \n    padding = \"valid\",\n    activation = \"relu\",\n    strides = 1\n  ) %>%\n  layer_max_pooling_1d(pool_size) %>%\n  layer_lstm(lstm_output_size) %>%\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x_train, imdb$train$y,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, imdb$test$y)\n)"
  },
  {
    "objectID": "examples/imdb_fasttext.html",
    "href": "examples/imdb_fasttext.html",
    "title": "imdb_fasttext",
    "section": "",
    "text": "Based on Joulin et al’s paper: “Bags of Tricks for Efficient Text Classification” https://arxiv.org/abs/1607.01759\nResults on IMDB datasets with uni and bi-gram embeddings: Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 CPU Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M GPU\n\nlibrary(keras)\nlibrary(purrr)\n\n# Function Definitions ----------------------------------------------------\n\ncreate_ngram_set <- function(input_list, ngram_value = 2){\n  indices <- map(0:(length(input_list) - ngram_value), ~1:ngram_value + .x)\n  indices %>%\n    map_chr(~input_list[.x] %>% paste(collapse = \"|\")) %>%\n    unique()\n}\n\nadd_ngram <- function(sequences, token_indice, ngram_range = 2){\n  ngrams <- map(\n    sequences, \n    create_ngram_set, ngram_value = ngram_range\n  )\n  \n  seqs <- map2(sequences, ngrams, function(x, y){\n    tokens <- token_indice$token[token_indice$ngrams %in% y]  \n    c(x, tokens)\n  })\n  \n  seqs\n}\n\n\n# Parameters --------------------------------------------------------------\n\n# ngram_range = 2 will add bi-grams features\nngram_range <- 2\nmax_features <- 20000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nepochs <- 5\n\n\n# Data Preparation --------------------------------------------------------\n\n# Load data\nimdb_data <- dataset_imdb(num_words = max_features)\n\n# Train sequences\nprint(length(imdb_data$train$x))\nprint(sprintf(\"Average train sequence length: %f\", mean(map_int(imdb_data$train$x, length))))\n\n# Test sequences\nprint(length(imdb_data$test$x)) \nprint(sprintf(\"Average test sequence length: %f\", mean(map_int(imdb_data$test$x, length))))\n\nif(ngram_range > 1) {\n  \n  # Create set of unique n-gram from the training set.\n  ngrams <- imdb_data$train$x %>% \n    map(create_ngram_set) %>%\n    unlist() %>%\n    unique()\n\n  # Dictionary mapping n-gram token to a unique integer\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features\n  token_indice <- data.frame(\n    ngrams = ngrams,\n    token  = 1:length(ngrams) + (max_features), \n    stringsAsFactors = FALSE\n  )\n  \n  # max_features is the highest integer that could be found in the dataset\n  max_features <- max(token_indice$token) + 1\n  \n  # Augmenting x_train and x_test with n-grams features\n  imdb_data$train$x <- add_ngram(imdb_data$train$x, token_indice, ngram_range)\n  imdb_data$test$x <- add_ngram(imdb_data$test$x, token_indice, ngram_range)\n}\n\n# Pad sequences\nimdb_data$train$x <- pad_sequences(imdb_data$train$x, maxlen = maxlen)\nimdb_data$test$x <- pad_sequences(imdb_data$test$x, maxlen = maxlen)\n\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_embedding(\n    input_dim = max_features, output_dim = embedding_dims, \n    input_length = maxlen\n    ) %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n\n# Fitting -----------------------------------------------------------------\n\nmodel %>% fit(\n  imdb_data$train$x, imdb_data$train$y, \n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(imdb_data$test$x, imdb_data$test$y)\n)"
  },
  {
    "objectID": "examples/imdb_lstm.html",
    "href": "examples/imdb_lstm.html",
    "title": "imdb_lstm",
    "section": "",
    "text": "The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg.\nNotes: - RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won’t converge. - LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc.\n\nlibrary(keras)\n\nmax_features <- 20000\nbatch_size <- 32\n\n# Cut texts after this number of words (among top max_features most common words)\nmaxlen <- 80  \n\ncat('Loading data...\\n')\nimdb <- dataset_imdb(num_words = max_features)\nx_train <- imdb$train$x\ny_train <- imdb$train$y\nx_test <- imdb$test$x\ny_test <- imdb$test$y\n\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\ncat('Pad sequences (samples x time)\\n')\nx_train <- pad_sequences(x_train, maxlen = maxlen)\nx_test <- pad_sequences(x_test, maxlen = maxlen)\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\ncat('Build model...\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_embedding(input_dim = max_features, output_dim = 128) %>% \n  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\n# Try using different optimizers and different optimizer configs\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\ncat('Train...\\n')\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = 15,\n  validation_data = list(x_test, y_test)\n)\n\nscores <- model %>% evaluate(\n  x_test, y_test,\n  batch_size = batch_size\n)\n\ncat('Test score:', scores[[1]])\ncat('Test accuracy', scores[[2]])"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Keras Examples",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\naddition_rnn\n\n\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\n\n\n\n\nbabi_memnn\n\n\nTrains a memory network on the bAbI dataset for reading comprehension.\n\n\n\n\nbabi_rnn\n\n\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\n\n\n\n\ncifar10_cnn\n\n\nTrains a simple deep CNN on the CIFAR10 small images dataset.\n\n\n\n\ncifar10_densenet\n\n\nTrains a DenseNet-40-12 on the CIFAR10 small images dataset.\n\n\n\n\nconv_lstm\n\n\nDemonstrates the use of a convolutional LSTM network.\n\n\n\n\ndeep_dream\n\n\nDeep Dreams in Keras.\n\n\n\n\neager_dcgan\n\n\nGenerating digits with generative adversarial networks and eager execution.\n\n\n\n\neager_image_captioning\n\n\nGenerating image captions with Keras and eager execution.\n\n\n\n\neager_pix2pix\n\n\nImage-to-image translation with Pix2Pix, using eager execution.\n\n\n\n\neager_styletransfer\n\n\nNeural style transfer with eager execution.\n\n\n\n\nfine_tuning\n\n\nFine tuning of a image classification model.\n\n\n\n\nimdb_bidirectional_lstm\n\n\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\n\n\n\n\nimdb_cnn\n\n\nDemonstrates the use of Convolution1D for text classification.\n\n\n\n\nimdb_cnn_lstm\n\n\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\n\n\n\n\nimdb_fasttext\n\n\nTrains a FastText model on the IMDB sentiment classification task.\n\n\n\n\nimdb_lstm\n\n\nTrains a LSTM on the IMDB sentiment classification task.\n\n\n\n\nlstm_seq2seq\n\n\nThis script demonstrates how to implement a basic character-level sequence-to-sequence model.\n\n\n\n\nlstm_text_generation\n\n\nGenerates text from Nietzsche’s writings.\n\n\n\n\nmnist_acgan\n\n\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\n\n\n\n\nmnist_antirectifier\n\n\nDemonstrates how to write custom layers for Keras\n\n\n\n\nmnist_cnn\n\n\nTrains a simple convnet on the MNIST dataset.\n\n\n\n\nmnist_cnn_embeddings\n\n\nDemonstrates how to visualize embeddings in TensorBoard.\n\n\n\n\nmnist_hierarchical_rnn\n\n\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\n\n\n\n\nmnist_irnn\n\n\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\n\n\n\n\nmnist_mlp\n\n\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\n\n\n\n\nmnist_tfrecord\n\n\nMNIST dataset with TFRecords, the standard TensorFlow data format.\n\n\n\n\nmnist_transfer_cnn\n\n\nTransfer learning toy example.\n\n\n\n\nneural_style_transfer\n\n\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\n\n\n\n\nnmt_attention\n\n\nNeural machine translation with an attention mechanism.\n\n\n\n\nquora_siamese_lstm\n\n\nClassifying duplicate quesitons from Quora using Siamese Recurrent Architecture.\n\n\n\n\nreuters_mlp\n\n\nTrains and evaluatea a simple MLP on the Reuters newswire topic classification task.\n\n\n\n\nstateful_lstm\n\n\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\n\n\n\n\ntext_explanation_lime\n\n\nHow to use lime to explain text data.\n\n\n\n\ntfprob_vae\n\n\nA variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST.\n\n\n\n\nvariational_autoencoder\n\n\nDemonstrates how to build a variational autoencoder.\n\n\n\n\nvariational_autoencoder_deconv\n\n\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\n\n\n\n\nvq_vae\n\n\nDiscrete Representation Learning with VQ-VAE and TensorFlow Probability.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples/lstm_seq2seq.html",
    "href": "examples/lstm_seq2seq.html",
    "title": "lstm_seq2seq",
    "section": "",
    "text": "This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain.\nAlgorithm\n\nWe start with input sequences from a domain (e.g. English sentences) and correspding target sequences from another domain (e.g. French sentences).\nAn encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).\nA decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called “teacher forcing” in this context. Is uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\nIn inference mode, when we want to decode unknown input sequences, we:\n\nEncode the input sequence into state vectors\nStart with a target sequence of size 1 (just the start-of-sequence character)\nFeed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\nSample the next character using these predictions (we simply use argmax).\nAppend the sampled character to the target sequence\nRepeat until we generate the end-of-sequence character or we hit the character limit.\n\n\nData download\nEnglish to French sentence pairs. http://www.manythings.org/anki/fra-eng.zip\nLots of neat sentence pairs datasets can be found at: http://www.manythings.org/anki/\nReferences\n\nSequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078\n\n\nlibrary(keras)\nlibrary(data.table)\n\nbatch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 10000  # Number of samples to train on.\n\n## Path to the data txt file on disk.\ndata_path = 'fra.txt'\ntext <- fread(data_path, sep=\"\\t\", header=FALSE, nrows=num_samples)\n\n## Vectorize the data.\ninput_texts  <- text[[1]]\ntarget_texts <- paste0('\\t',text[[2]],'\\n')\ninput_texts  <- lapply( input_texts, function(s) strsplit(s, split=\"\")[[1]])\ntarget_texts <- lapply( target_texts, function(s) strsplit(s, split=\"\")[[1]])\n\ninput_characters  <- sort(unique(unlist(input_texts)))\ntarget_characters <- sort(unique(unlist(target_texts)))\nnum_encoder_tokens <- length(input_characters)\nnum_decoder_tokens <- length(target_characters)\nmax_encoder_seq_length <- max(sapply(input_texts,length))\nmax_decoder_seq_length <- max(sapply(target_texts,length))\n\ncat('Number of samples:', length(input_texts),'\\n')\ncat('Number of unique input tokens:', num_encoder_tokens,'\\n')\ncat('Number of unique output tokens:', num_decoder_tokens,'\\n')\ncat('Max sequence length for inputs:', max_encoder_seq_length,'\\n')\ncat('Max sequence length for outputs:', max_decoder_seq_length,'\\n')\n\ninput_token_index  <- 1:length(input_characters)\nnames(input_token_index) <- input_characters\ntarget_token_index <- 1:length(target_characters)\nnames(target_token_index) <- target_characters\nencoder_input_data <- array(\n  0, dim = c(length(input_texts), max_encoder_seq_length, num_encoder_tokens))\ndecoder_input_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\ndecoder_target_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\n\nfor(i in 1:length(input_texts)) {\n  d1 <- sapply( input_characters, function(x) { as.integer(x == input_texts[[i]]) })\n  encoder_input_data[i,1:nrow(d1),] <- d1\n  d2 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]]) })\n  decoder_input_data[i,1:nrow(d2),] <- d2\n  d3 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]][-1]) })\n  decoder_target_data[i,1:nrow(d3),] <- d3\n}\n\n\n## Create the model\n\n\n## Define an input sequence and process it.\nencoder_inputs  <- layer_input(shape=list(NULL,num_encoder_tokens))\nencoder         <- layer_lstm(units=latent_dim, return_state=TRUE)\nencoder_results <- encoder_inputs %>% encoder\n## We discard `encoder_outputs` and only keep the states.\nencoder_states  <- encoder_results[2:3]\n\n## Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs  <- layer_input(shape=list(NULL, num_decoder_tokens))\n## We set up our decoder to return full output sequences,\n## and to return internal states as well. We don't use the\n## return states in the training model, but we will use them in inference.\ndecoder_lstm    <- layer_lstm(units=latent_dim, return_sequences=TRUE,\n                              return_state=TRUE, stateful=FALSE)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense   <- layer_dense(units=num_decoder_tokens, activation='softmax')\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\n\n## Define the model that will turn\n## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel <- keras_model( inputs = list(encoder_inputs, decoder_inputs),\n                      outputs = decoder_outputs )\n\n## Compile model\nmodel %>% compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n## Run model\nmodel %>% fit( list(encoder_input_data, decoder_input_data), decoder_target_data,\n               batch_size=batch_size,\n               epochs=epochs,\n               validation_split=0.2)\n\n## Save model\nsave_model_hdf5(model,'s2s.h5')\nsave_model_weights_hdf5(model,'s2s-wt.h5')\n\n##model <- load_model_hdf5('s2s.h5')\n##load_model_weights_hdf5(model,'s2s-wt.h5')\n\n\n## Next: inference mode (sampling).\n\n\n## Here's the drill:\n## 1) encode input and retrieve initial decoder state\n## 2) run one step of decoder with this initial state\n## and a \"start of sequence\" token as target.\n## Output will be the next target token\n## 3) Repeat with the current target token and current states\n\n## Define sampling models\nencoder_model <-  keras_model(encoder_inputs, encoder_states)\ndecoder_state_input_h <- layer_input(shape=latent_dim)\ndecoder_state_input_c <- layer_input(shape=latent_dim)\ndecoder_states_inputs <- c(decoder_state_input_h, decoder_state_input_c)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states  <- decoder_results[2:3]\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\ndecoder_model   <- keras_model(\n  inputs  = c(decoder_inputs, decoder_states_inputs),\n  outputs = c(decoder_outputs, decoder_states))\n\n## Reverse-lookup token index to decode sequences back to\n## something readable.\nreverse_input_char_index  <- as.character(input_characters)\nreverse_target_char_index <- as.character(target_characters)\n\ndecode_sequence <- function(input_seq) {\n  ## Encode the input as state vectors.\n  states_value <- predict(encoder_model, input_seq)\n  \n  ## Generate empty target sequence of length 1.\n  target_seq <- array(0, dim=c(1, 1, num_decoder_tokens))\n  ## Populate the first character of target sequence with the start character.\n  target_seq[1, 1, target_token_index['\\t']] <- 1.\n  \n  ## Sampling loop for a batch of sequences\n  ## (to simplify, here we assume a batch of size 1).\n  stop_condition = FALSE\n  decoded_sentence = ''\n  maxiter = max_decoder_seq_length\n  niter = 1\n  while (!stop_condition && niter < maxiter) {\n    \n    ## output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n    decoder_predict <- predict(decoder_model, c(list(target_seq), states_value))\n    output_tokens <- decoder_predict[[1]]\n    \n    ## Sample a token\n    sampled_token_index <- which.max(output_tokens[1, 1, ])\n    sampled_char <- reverse_target_char_index[sampled_token_index]\n    decoded_sentence <-  paste0(decoded_sentence, sampled_char)\n    decoded_sentence\n    \n    ## Exit condition: either hit max length\n    ## or find stop character.\n    if (sampled_char == '\\n' ||\n        length(decoded_sentence) > max_decoder_seq_length) {\n      stop_condition = TRUE\n    }\n    \n    ## Update the target sequence (of length 1).\n    ## target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[1, 1, ] <- 0\n    target_seq[1, 1, sampled_token_index] <- 1.\n    \n    ## Update states\n    h <- decoder_predict[[2]]\n    c <- decoder_predict[[3]]\n    states_value = list(h, c)\n    niter <- niter + 1\n  }    \n  return(decoded_sentence)\n}\n\nfor (seq_index in 1:100) {\n  ## Take one sequence (part of the training test)\n  ## for trying out decoding.\n  input_seq = encoder_input_data[seq_index,,,drop=FALSE]\n  decoded_sentence = decode_sequence(input_seq)\n  target_sentence <- gsub(\"\\t|\\n\",\"\",paste(target_texts[[seq_index]],collapse=''))\n  input_sentence  <- paste(input_texts[[seq_index]],collapse='')\n  cat('-\\n')\n  cat('Input sentence  : ', input_sentence,'\\n')\n  cat('Target sentence : ', target_sentence,'\\n')\n  cat('Decoded sentence: ', decoded_sentence,'\\n')\n}"
  },
  {
    "objectID": "examples/lstm_text_generation.html",
    "href": "examples/lstm_text_generation.html",
    "title": "lstm_text_generation",
    "section": "",
    "text": "At least 20 epochs are required before the generated text starts sounding coherent.\nIt is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.\nIf you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better.\n\nlibrary(keras)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tokenizers)\n\n# Parameters --------------------------------------------------------------\n\nmaxlen <- 40\n\n# Data Preparation --------------------------------------------------------\n\n# Retrieve text\npath <- get_file(\n  'nietzsche.txt', \n  origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n  )\n\n# Load, collapse, and tokenize text\ntext <- read_lines(path) %>%\n  str_to_lower() %>%\n  str_c(collapse = \"\\n\") %>%\n  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)\n\nprint(sprintf(\"corpus length: %d\", length(text)))\n\nchars <- text %>%\n  unique() %>%\n  sort()\n\nprint(sprintf(\"total chars: %d\", length(chars)))  \n\n# Cut the text in semi-redundant sequences of maxlen characters\ndataset <- map(\n  seq(1, length(text) - maxlen - 1, by = 3), \n  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])\n  )\n\ndataset <- transpose(dataset)\n\n# Vectorization\nx <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))\ny <- array(0, dim = c(length(dataset$sentece), length(chars)))\n\nfor(i in 1:length(dataset$sentece)){\n  \n  x[i,,] <- sapply(chars, function(x){\n    as.integer(x == dataset$sentece[[i]])\n  })\n  \n  y[i,] <- as.integer(chars == dataset$next_char[[i]])\n  \n}\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%\n  layer_dense(length(chars)) %>%\n  layer_activation(\"softmax\")\n\noptimizer <- optimizer_rmsprop(lr = 0.01)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = optimizer\n)\n\n# Training & Results ----------------------------------------------------\n\nsample_mod <- function(preds, temperature = 1){\n  preds <- log(preds)/temperature\n  exp_preds <- exp(preds)\n  preds <- exp_preds/sum(exp(preds))\n  \n  rmultinom(1, 1, preds) %>% \n    as.integer() %>%\n    which.max()\n}\n\non_epoch_end <- function(epoch, logs) {\n  \n  cat(sprintf(\"epoch: %02d ---------------\\n\\n\", epoch))\n  \n  for(diversity in c(0.2, 0.5, 1, 1.2)){\n    \n    cat(sprintf(\"diversity: %f ---------------\\n\\n\", diversity))\n    \n    start_index <- sample(1:(length(text) - maxlen), size = 1)\n    sentence <- text[start_index:(start_index + maxlen - 1)]\n    generated <- \"\"\n    \n    for(i in 1:400){\n      \n      x <- sapply(chars, function(x){\n        as.integer(x == sentence)\n      })\n      x <- array_reshape(x, c(1, dim(x)))\n      \n      preds <- predict(model, x)\n      next_index <- sample_mod(preds, diversity)\n      next_char <- chars[next_index]\n      \n      generated <- str_c(generated, next_char, collapse = \"\")\n      sentence <- c(sentence[-1], next_char)\n      \n    }\n    \n    cat(generated)\n    cat(\"\\n\\n\")\n    \n  }\n}\n\nprint_callback <- callback_lambda(on_epoch_end = on_epoch_end)\n\nmodel %>% fit(\n  x, y,\n  batch_size = 128,\n  epochs = 1,\n  callbacks = print_callback\n)"
  },
  {
    "objectID": "examples/mnist_acgan.html",
    "href": "examples/mnist_acgan.html",
    "title": "mnist_acgan",
    "section": "",
    "text": "You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano.\n\n\n\nHardware\nBackend\nTime / Epoch\n\n\n\n\nCPU\nTF\n3 hrs\n\n\nTitan X (maxwell)\nTF\n4 min\n\n\nTitan X (maxwell)\nTH\n7 min\n\n\n\n\nlibrary(keras)\nlibrary(progress)\nlibrary(abind)\nk_set_image_data_format('channels_first')\n\n# Functions ---------------------------------------------------------------\n\nbuild_generator <- function(latent_size){\n  \n  # We will map a pair of (z, L), where z is a latent vector and L is a\n  # label drawn from P_c, to image space (..., 1, 28, 28)\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_dense(1024, input_shape = latent_size, activation = \"relu\") %>%\n    layer_dense(128*7*7, activation = \"relu\") %>%\n    layer_reshape(c(128, 7, 7)) %>%\n    # Upsample to (..., 14, 14)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      256, c(5,5), padding = \"same\", activation = \"relu\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Upsample to (..., 28, 28)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      128, c(5,5), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Take a channel axis reduction\n    layer_conv_2d(\n      1, c(2,2), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    )\n  \n  \n  # This is the z space commonly referred to in GAN papers\n  latent <- layer_input(shape = list(latent_size))\n  \n  # This will be our label\n  image_class <- layer_input(shape = list(1))\n  \n  # 10 classes in MNIST\n  cls <-  image_class %>%\n    layer_embedding(\n      input_dim = 10, output_dim = latent_size, \n      embeddings_initializer='glorot_normal'\n    ) %>%\n    layer_flatten()\n  \n  \n  # Hadamard product between z-space and a class conditional embedding\n  h <- layer_multiply(list(latent, cls))\n  \n  fake_image <- cnn(h)\n  \n  keras_model(list(latent, image_class), fake_image)\n}\n\nbuild_discriminator <- function(){\n  \n  # Build a relatively standard conv net, with LeakyReLUs as suggested in\n  # the reference paper\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_conv_2d(\n      32, c(3,3), padding = \"same\", strides = c(2,2),\n      input_shape = c(1, 28, 28)\n    ) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%\n    \n    layer_conv_2d(64, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(128, c(3, 3), padding = \"same\", strides = c(2,2)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(256, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_flatten()\n  \n  \n  \n  image <- layer_input(shape = c(1, 28, 28))\n  features <- cnn(image)\n  \n  # First output (name=generation) is whether or not the discriminator\n  # thinks the image that is being shown is fake, and the second output\n  # (name=auxiliary) is the class that the discriminator thinks the image\n  # belongs to.\n  fake <- features %>% \n    layer_dense(1, activation = \"sigmoid\", name = \"generation\")\n  \n  aux <- features %>%\n    layer_dense(10, activation = \"softmax\", name = \"auxiliary\")\n  \n  keras_model(image, list(fake, aux))\n}\n\n# Parameters --------------------------------------------------------------\n\n# Batch and latent size taken from the paper\nepochs <- 50\nbatch_size <- 100\nlatent_size <- 100\n\n# Adam parameters suggested in https://arxiv.org/abs/1511.06434\nadam_lr <- 0.00005 \nadam_beta_1 <- 0.5\n\n# Model Definition --------------------------------------------------------\n\n# Build the discriminator\ndiscriminator <- build_discriminator()\ndiscriminator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Build the generator\ngenerator <- build_generator(latent_size)\ngenerator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = \"binary_crossentropy\"\n)\n\nlatent <- layer_input(shape = list(latent_size))\nimage_class <- layer_input(shape = list(1), dtype = \"int32\")\n\nfake <- generator(list(latent, image_class))\n\n# Only want to be able to train generation for the combined model\nfreeze_weights(discriminator)\nresults <- discriminator(fake)\n\ncombined <- keras_model(list(latent, image_class), results)\ncombined %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Data Preparation --------------------------------------------------------\n\n# Loade mnist data, and force it to be of shape (..., 1, 28, 28) with\n# range [-1, 1]\nmnist <- dataset_mnist()\nmnist$train$x <- (mnist$train$x - 127.5)/127.5\nmnist$test$x <- (mnist$test$x - 127.5)/127.5\nmnist$train$x <- array_reshape(mnist$train$x, c(60000, 1, 28, 28))\nmnist$test$x <- array_reshape(mnist$test$x, c(10000, 1, 28, 28))\n\nnum_train <- dim(mnist$train$x)[1]\nnum_test <- dim(mnist$test$x)[1]\n\n# Training ----------------------------------------------------------------\n\nfor(epoch in 1:epochs){\n  \n  num_batches <- trunc(num_train/batch_size)\n  pb <- progress_bar$new(\n    total = num_batches, \n    format = sprintf(\"epoch %s/%s :elapsed [:bar] :percent :eta\", epoch, epochs),\n    clear = FALSE\n  )\n  \n  epoch_gen_loss <- NULL\n  epoch_disc_loss <- NULL\n  \n  possible_indexes <- 1:num_train\n  \n  for(index in 1:num_batches){\n    \n    pb$tick()\n    \n    # Generate a new batch of noise\n    noise <- runif(n = batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = batch_size, ncol = latent_size)\n    \n    # Get a batch of real images\n    batch <- sample(possible_indexes, size = batch_size)\n    possible_indexes <- possible_indexes[!possible_indexes %in% batch]\n    image_batch <- mnist$train$x[batch,,,,drop = FALSE]\n    label_batch <- mnist$train$y[batch]\n    \n    # Sample some labels from p_c\n    sampled_labels <- sample(0:9, batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Generate a batch of fake images, using the generated labels as a\n    # conditioner. We reshape the sampled labels to be\n    # (batch_size, 1) so that we can feed them into the embedding\n    # layer as a length one sequence\n    generated_images <- predict(generator, list(noise, sampled_labels))\n    \n    X <- abind(image_batch, generated_images, along = 1)\n    y <- c(rep(1L, batch_size), rep(0L, batch_size)) %>% matrix(ncol = 1)\n    aux_y <- c(label_batch, sampled_labels) %>% matrix(ncol = 1)\n    \n    # Check if the discriminator can figure itself out\n    disc_loss <- train_on_batch(\n      discriminator, x = X, \n      y = list(y, aux_y)\n    )\n    \n    epoch_disc_loss <- rbind(epoch_disc_loss, unlist(disc_loss))\n    \n    # Make new noise. Generate 2 * batch size here such that\n    # the generator optimizes over an identical number of images as the\n    # discriminator\n    noise <- runif(2*batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = 2*batch_size, ncol = latent_size)\n    sampled_labels <- sample(0:9, size = 2*batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Want to train the generator to trick the discriminator\n    # For the generator, we want all the {fake, not-fake} labels to say\n    # not-fake\n    trick <- rep(1, 2*batch_size) %>% matrix(ncol = 1)\n    \n    combined_loss <- train_on_batch(\n      combined, \n      list(noise, sampled_labels),\n      list(trick, sampled_labels)\n    )\n    \n    epoch_gen_loss <- rbind(epoch_gen_loss, unlist(combined_loss))\n    \n  }\n  \n  cat(sprintf(\"\\nTesting for epoch %02d:\", epoch))\n  \n  # Evaluate the testing loss here\n  \n  # Generate a new batch of noise\n  noise <- runif(num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = num_test, ncol = latent_size)\n  \n  # Sample some labels from p_c and generate images from them\n  sampled_labels <- sample(0:9, size = num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  generated_images <- predict(generator, list(noise, sampled_labels))\n  \n  X <- abind(mnist$test$x, generated_images, along = 1)\n  y <- c(rep(1, num_test), rep(0, num_test)) %>% matrix(ncol = 1)\n  aux_y <- c(mnist$test$y, sampled_labels) %>% matrix(ncol = 1)\n  \n  # See if the discriminator can figure itself out...\n  discriminator_test_loss <- evaluate(\n    discriminator, X, list(y, aux_y), \n    verbose = FALSE\n  ) %>% unlist()\n  \n  discriminator_train_loss <- apply(epoch_disc_loss, 2, mean)\n  \n  # Make new noise\n  noise <- runif(2*num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 2*num_test, ncol = latent_size)\n  sampled_labels <- sample(0:9, size = 2*num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  \n  trick <- rep(1, 2*num_test) %>% matrix(ncol = 1)\n  \n  generator_test_loss = combined %>% evaluate(\n    list(noise, sampled_labels),\n    list(trick, sampled_labels),\n    verbose = FALSE\n  )\n  \n  generator_train_loss <- apply(epoch_gen_loss, 2, mean)\n  \n  \n  # Generate an epoch report on performance\n  row_fmt <- \"\\n%22s : loss %4.2f | %5.2f | %5.2f\"\n  cat(sprintf(\n    row_fmt, \n    \"generator (train)\",\n    generator_train_loss[1],\n    generator_train_loss[2],\n    generator_train_loss[3]\n  ))\n  cat(sprintf(\n    row_fmt, \n    \"generator (test)\",\n    generator_test_loss[1],\n    generator_test_loss[2],\n    generator_test_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (train)\",\n    discriminator_train_loss[1],\n    discriminator_train_loss[2],\n    discriminator_train_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (test)\",\n    discriminator_test_loss[1],\n    discriminator_test_loss[2],\n    discriminator_test_loss[3]\n  ))\n  \n  cat(\"\\n\")\n  \n  # Generate some digits to display\n  noise <- runif(10*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 10, ncol = latent_size)\n  \n  sampled_labels <- 0:9 %>%\n    matrix(ncol = 1)\n  \n  # Get a batch to display\n  generated_images <- predict(\n    generator,    \n    list(noise, sampled_labels)\n  )\n  \n  img <- NULL\n  for(i in 1:10){\n    img <- cbind(img, generated_images[i,,,])\n  }\n  \n  ((img + 1)/2) %>% as.raster() %>%\n    plot()\n  \n}"
  },
  {
    "objectID": "examples/mnist_antirectifier.html",
    "href": "examples/mnist_antirectifier.html",
    "title": "mnist_antirectifier",
    "section": "",
    "text": "We build a custom activation layer called ‘Antirectifier’, which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call.\nNote that the same result can also be achieved via a Lambda layer.\n\nlibrary(keras)\n\n# Data Preparation --------------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 40\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redimension\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Antirectifier Layer -----------------------------------------------------\n\nThis is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input. The result is a tensor of samples that are twice as large as the input samples.\nIt can be used in place of a ReLU. Input shape: 2D tensor of shape (samples, n) Output shape: 2D tensor of shape (samples, 2*n)\nWhen applying ReLU, assuming that the distribution of the previous output is approximately centered around 0., you are discarding half of your input. This is inefficient.\nAntirectifier allows to return all-positive outputs like ReLU, without discarding any data.\nTests on MNIST show that Antirectifier allows to train networks with half the parameters yet with comparable classification accuracy as an equivalent ReLU-based network.\n\n# Custom layer class\nAntirectifierLayer <- R6::R6Class(\"KerasLayer\",\n  \n  inherit = KerasLayer,\n                           \n  public = list(\n   \n    call = function(x, mask = NULL) {\n      x <- x - k_mean(x, axis = 2, keepdims = TRUE)\n      x <- k_l2_normalize(x, axis = 2)\n      pos <- k_relu(x)\n      neg <- k_relu(-x)\n      k_concatenate(c(pos, neg), axis = 2)\n      \n    },\n     \n    compute_output_shape = function(input_shape) {\n      input_shape[[2]] <- input_shape[[2]] * 2L \n      input_shape\n    }\n  )\n)\n\n# Create layer wrapper function\nlayer_antirectifier <- function(object) {\n  create_layer(AntirectifierLayer, object)\n}\n\n\n# Define & Train Model -------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 256, input_shape = c(784)) %>% \n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 256) %>%\n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile the model\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Train the model\nmodel %>% fit(x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data= list(x_test, y_test)\n)"
  },
  {
    "objectID": "examples/mnist_cnn.html",
    "href": "examples/mnist_cnn.html",
    "title": "mnist_cnn",
    "section": "",
    "text": "Gets to 99.25% test accuracy after 12 epochs Note: There is still a large margin for parameter tuning\n16 seconds per epoch on a GRID K520 GPU.\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 12\n\n# Input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redefine  dimension of train/test inputs\nx_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model -----------------------------------------------------------\n\n# Define model\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n                input_shape = input_shape) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n# Train model\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split = 0.2\n)\n\n\n\n\nscores <- model %>% evaluate(\n  x_test, y_test, verbose = 0\n)\n\n# Output metrics\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_cnn_embeddings.html",
    "href": "examples/mnist_cnn_embeddings.html",
    "title": "mnist_cnn_embeddings",
    "section": "",
    "text": "Embeddings in the sense used here don’t necessarily refer to embedding layers. In fact, features (= activations) from other hidden layers can be visualized, as shown in this example for a dense layer.\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 12\n\n# Input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redefine  dimension of train/test inputs\nx_train <-\n  array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test <-\n  array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n\n# Prepare for logging embeddings --------------------------------------------------\n\nembeddings_dir <- file.path(tempdir(), 'embeddings')\nif (!file.exists(embeddings_dir))\n  dir.create(embeddings_dir)\nembeddings_metadata <- file.path(embeddings_dir, 'metadata.tsv')\n\n# we use the class names from the test set as embeddings_metadata\nreadr::write_tsv(data.frame(y_test), path = embeddings_metadata, col_names = FALSE)\n\ntensorboard_callback <- callback_tensorboard(\n  log_dir = embeddings_dir,\n  batch_size = batch_size,\n  embeddings_freq = 1,\n  # if missing or NULL all embedding layers will be monitored\n  embeddings_layer_names = list('features'),\n  # single file for all embedding layers, could also be a named list mapping\n  # layer names to file names\n  embeddings_metadata = embeddings_metadata,\n  # data to be embedded\n  embeddings_data = x_test\n)\n\n\n# Define Model -----------------------------------------------------------\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define model\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filters = 32,\n    kernel_size = c(3, 3),\n    activation = 'relu',\n    input_shape = input_shape\n  ) %>%\n  layer_conv_2d(filters = 64,\n                kernel_size = c(3, 3),\n                activation = 'relu') %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_flatten() %>%\n  # these are the embeddings (activations) we are going to visualize\n  layer_dense(units = 128, activation = 'relu', name = 'features') %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n# Launch TensorBoard\n#\n# As the model is being fit you will be able to view the embedings in the \n# Projector tab. On the left, use \"color by label\" to see the digits displayed\n# in 10 different colors. Hover over a point to see its label.\ntensorboard(embeddings_dir)\n\n# Train model\nmodel %>% fit(\n  x_train,\n  y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, y_test),\n  callbacks = list(tensorboard_callback)\n)\n\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\n\n# Output metrics\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_hierarchical_rnn.html",
    "href": "examples/mnist_hierarchical_rnn.html",
    "title": "mnist_hierarchical_rnn",
    "section": "",
    "text": "HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence. Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors) into a sentence vector. The second recurrent layer then encodes a sequence of such vectors (encoded by the first layer) into a document vector. This document vector is considered to preserve both the word-level and sentence-level structure of the context.\nReferences: - A Hierarchical Neural Autoencoder for Paragraphs and Documents Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. - Hierarchical recurrent neural network for skeleton based action recognition Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers.\nIn the below MNIST example the first LSTM layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final dense layer is added for prediction.\nAfter 5 epochs: train acc: 0.9858, val acc: 0.9864\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------------------\n\n# Training parameters.\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 5\n\n# Embedding dimensions.\nrow_hidden <- 128\ncol_hidden <- 128\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Reshapes data to 4D for Hierarchical RNN.\nx_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ndim_x_train <- dim(x_train)\ncat('x_train_shape:', dim_x_train)\ncat(nrow(x_train), 'train samples')\ncat(nrow(x_test), 'test samples')\n\n# Converts class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define input dimensions\nrow <- dim_x_train[[2]]\ncol <- dim_x_train[[3]]\npixel <- dim_x_train[[4]]\n\n# Model input (4D)\ninput <- layer_input(shape = c(row, col, pixel))\n\n# Encodes a row of pixels using TimeDistributed Wrapper\nencoded_rows <- input %>% time_distributed(layer_lstm(units = row_hidden))\n\n# Encodes columns of encoded rows\nencoded_columns <- encoded_rows %>% layer_lstm(units = col_hidden)\n\n# Model output\nprediction <- encoded_columns %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Define Model ------------------------------------------------------------------------\n\nmodel <- keras_model(input, prediction)\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Training\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n\n# Evaluation\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_irnn.html",
    "href": "examples/mnist_irnn.html",
    "title": "mnist_irnn",
    "section": "",
    "text": "arxiv:1504.00941v2 [cs.NE] 7 Apr 2015 http://arxiv.org/pdf/1504.00941v2.pdf\nOptimizer is replaced with RMSprop which yields more stable and steady improvement.\nReaches 0.93 train/test accuracy after 900 epochs This corresponds to roughly 1687500 steps in the original paper.\n\nlibrary(keras)\n\n# Data Preparation ---------------------------------------------------------------\n\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 200\nhidden_units <- 100\n\nimg_rows <- 28\nimg_cols <- 28\n\nlearning_rate <- 1e-6\nclip_norm <- 1.0\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nx_train <- array_reshape(x_train, c(nrow(x_train), img_rows * img_cols, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), img_rows * img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model ------------------------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_simple_rnn(units = hidden_units,\n                   kernel_initializer = initializer_random_normal(stddev = 0.01),\n                   recurrent_initializer = initializer_identity(gain = 1.0),\n                   activation = 'relu',\n                   input_shape = dim(x_train)[-1]) %>% \n  layer_dense(units = num_classes) %>% \n  layer_activation(activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(lr = learning_rate),\n  metrics = c('accuracy')\n)\n \n# Training & Evaluation ---------------------------------------------------------\n\ncat(\"Evaluate IRNN...\\n\")\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n  \nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('IRNN test score:', scores[[1]], '\\n')\ncat('IRNN test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_mlp.html",
    "href": "examples/mnist_mlp.html",
    "title": "mnist_mlp",
    "section": "",
    "text": "Gets to 98.40% test accuracy after 20 epochs (there is a lot of margin for parameter tuning). 2 seconds per epoch on a K520 GPU.\n\nlibrary(keras)\n\n# Data Preparation ---------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 30\n\n# The data, shuffled and split between train and test sets\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model --------------------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nsummary(model)\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n# Training & Evaluation ----------------------------------------------------\n\n# Fit model to data\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_split = 0.2\n)\n\nplot(history)\n  \nscore <- model %>% evaluate(\n  x_test, y_test,\n  verbose = 0\n)\n\n# Output metrics\ncat('Test loss:', score[[1]], '\\n')\ncat('Test accuracy:', score[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_tfrecord.html",
    "href": "examples/mnist_tfrecord.html",
    "title": "mnist_tfrecord",
    "section": "",
    "text": "TFRecord is a data format supported throughout TensorFlow. This example demonstrates how to load TFRecord data using Input Tensors. Input Tensors differ from the normal Keras workflow because instead of fitting to data loaded into a a numpy array, data is supplied via a special tensor that reads data from nodes that are wired directly into model graph with the layer_input(tensor=input_tensor) parameter.\nThere are several advantages to using Input Tensors. First, if a dataset is already in TFRecord format you can load and train on that data directly in Keras. Second, extended backend API capabilities such as TensorFlow data augmentation is easy to integrate directly into your Keras training scripts via input tensors. Third, TensorFlow implements several data APIs for TFRecords, some of which provide significantly faster training performance than numpy arrays can provide because they run via the C++ backend. Please note that this example is tailored for brevity and clarity and not to demonstrate performance or augmentation capabilities.\nInput Tensors also have important disadvantages. In particular, Input Tensors are fixed at model construction because rewiring networks is not yet supported. For this reason, changing the data input source means model weights must be saved and the model rebuilt from scratch to connect the new input data. validation cannot currently be performed as training progresses, and must be performed after training completes. This example demonstrates how to train with input tensors, save the model weights, and then evaluate the model using the standard Keras API.\nGets to ~99.1% validation accuracy after 5 epochs (there is still a lot of margin for parameter tuning).\n\nlibrary(keras)\nlibrary(tensorflow)\n\nif (k_backend() != 'tensorflow') {\n  stop('This example can only run with the ',\n       'TensorFlow backend, ',\n       'because it requires TFRecords, which ',\n       'are not supported on other platforms.')\n}\n\n# Define Model -------------------------------------------------------------------\n\ncnn_layers <- function(x_train_input) {\n  x_train_input %>% \n    layer_conv_2d(filters = 32, kernel_size = c(3,3), \n                  activation = 'relu', padding = 'valid') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_flatten() %>% \n    layer_dense(units = 512, activation = 'relu') %>% \n    layer_dropout(rate = 0.5) %>% \n    layer_dense(units = classes, activation = 'softmax', name = 'x_train_out')\n}\n\nsess <- k_get_session()\n\n# Data Preparation --------------------------------------------------------------\n\nbatch_size <- 128L\nbatch_shape = list(batch_size, 28L, 28L, 1L)\nsteps_per_epoch <- 469L\nepochs <- 5L\nclasses <- 10L\n\n# The capacity variable controls the maximum queue size\n# allowed when prefetching data for training.\ncapacity <- 10000L\n\n# min_after_dequeue is the minimum number elements in the queue\n# after a dequeue, which ensures sufficient mixing of elements.\nmin_after_dequeue <- 3000L\n\n# If `enqueue_many` is `FALSE`, `tensors` is assumed to represent a\n# single example.  An input tensor with shape `(x, y, z)` will be output\n# as a tensor with shape `(batch_size, x, y, z)`.\n#\n# If `enqueue_many` is `TRUE`, `tensors` is assumed to represent a\n# batch of examples, where the first dimension is indexed by example,\n# and all members of `tensors` should have the same size in the\n# first dimension.  If an input tensor has shape `(*, x, y, z)`, the\n# output will have shape `(batch_size, x, y, z)`.\nenqueue_many <- TRUE\n\n# mnist dataset from tf contrib\nmnist <- tf$contrib$learn$datasets$mnist\ndata <- mnist$load_mnist()\n\ntrain_data <- tf$train$shuffle_batch(\n  tensors = list(data$train$images, data$train$labels),\n  batch_size = batch_size,\n  capacity = capacity,\n  min_after_dequeue = min_after_dequeue,\n  enqueue_many = enqueue_many,\n  num_threads = 8L\n)\nx_train_batch <- train_data[[1]]\ny_train_batch <- train_data[[2]]\n\nx_train_batch <- tf$cast(x_train_batch, tf$float32)\nx_train_batch <- tf$reshape(x_train_batch, shape = batch_shape)\n\ny_train_batch <- tf$cast(y_train_batch, tf$int32)\ny_train_batch <- tf$one_hot(y_train_batch, classes)\n\nx_batch_shape <- x_train_batch$get_shape()$as_list()\ny_batch_shape = y_train_batch$get_shape()$as_list()\n\nx_train_input <- layer_input(tensor = x_train_batch, batch_shape = x_batch_shape)\nx_train_out <- cnn_layers(x_train_input)\n\n# Training & Evaluation ---------------------------------------------------------\n\ntrain_model = keras_model(inputs = x_train_input, outputs = x_train_out)\n\n# Pass the target tensor `y_train_batch` to `compile`\n# via the `target_tensors` keyword argument:\ntrain_model %>% compile(\n  optimizer = optimizer_rmsprop(lr = 2e-3, decay = 1e-5),\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy'),\n  target_tensors = y_train_batch\n)\n\nsummary(train_model)\n\n# Fit the model using data from the TFRecord data tensors.\ncoord <- tf$train$Coordinator()\nthreads = tf$train$start_queue_runners(sess, coord)\n\ntrain_model %>% fit(\n  epochs = epochs,\n  steps_per_epoch = steps_per_epoch\n)\n\n# Save the model weights.\ntrain_model %>% save_model_weights_hdf5('saved_wt.h5')\n\n# Clean up the TF session.\ncoord$request_stop()\ncoord$join(threads)\nk_clear_session()\n\n# Second Session to test loading trained model without tensors\nx_test <- data$validation$images\nx_test <- array_reshape(x_test, dim = c(nrow(x_test), 28, 28, 1))\ny_test <- data$validation$labels\nx_test_inp <- layer_input(shape = dim(x_test)[-1])\ntest_out <- cnn_layers(x_test_inp)\ntest_model <- keras_model(inputs = x_test_inp, outputs = test_out)\ntest_model %>% load_model_weights_hdf5('saved_wt.h5')\ntest_model %>% compile(\n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy')\n)\nsummary(test_model)\n\nresult <- test_model %>% evaluate(x_test, to_categorical(y_test, classes))\ncat(sprintf('\\nTest accuracy: %f', result$acc))"
  },
  {
    "objectID": "examples/mnist_transfer_cnn.html",
    "href": "examples/mnist_transfer_cnn.html",
    "title": "mnist_transfer_cnn",
    "section": "",
    "text": "Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\nFreeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].\n\n\nlibrary(keras)\n\nnow <- Sys.time()\n\nbatch_size <- 128\nnum_classes <- 5\nepochs <- 5\n\n# input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# number of convolutional filters to use\nfilters <- 32\n\n# size of pooling area for max pooling\npool_size <- 2\n\n# convolution kernel size\nkernel_size <- c(3, 3)\n\n# input shape\ninput_shape <- c(img_rows, img_cols, 1)\n\n# the data, shuffled and split between train and test sets\ndata <- dataset_mnist()\nx_train <- data$train$x\ny_train <- data$train$y\nx_test <- data$test$x\ny_test <- data$test$y\n\n# create two datasets one with digits below 5 and one with 5 and above\nx_train_lt5 <- x_train[y_train < 5]\ny_train_lt5 <- y_train[y_train < 5]\nx_test_lt5 <- x_test[y_test < 5]\ny_test_lt5 <- y_test[y_test < 5]\n\nx_train_gte5 <- x_train[y_train >= 5]\ny_train_gte5 <- y_train[y_train >= 5] - 5\nx_test_gte5 <- x_test[y_test >= 5]\ny_test_gte5 <- y_test[y_test >= 5] - 5\n\n# define two groups of layers: feature (convolutions) and classification (dense)\nfeature_layers <- \n  layer_conv_2d(filters = filters, kernel_size = kernel_size, \n                input_shape = input_shape) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_conv_2d(filters = filters, kernel_size = kernel_size) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = pool_size) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten()\n  \n\n\n# feature_layers = [\n#   Conv2D(filters, kernel_size,\n#          padding='valid',\n#          input_shape=input_shape),\n#   Activation('relu'),\n#   Conv2D(filters, kernel_size),\n#   Activation('relu'),\n#   MaxPooling2D(pool_size=pool_size),\n#   Dropout(0.25),\n#   Flatten(),\n#   ]\n# \n# classification_layers = [\n#   Dense(128),\n#   Activation('relu'),\n#   Dropout(0.5),\n#   Dense(num_classes),\n#   Activation('softmax')\n#   ]"
  },
  {
    "objectID": "examples/neural_style_transfer.html",
    "href": "examples/neural_style_transfer.html",
    "title": "neural_style_transfer",
    "section": "",
    "text": "It is preferable to run this script on a GPU, for speed.\nExample result: https://twitter.com/fchollet/status/686631033085677568\nStyle transfer consists in generating an image with the same “content” as a base image, but with the “style” of a different picture (typically artistic).\nThis is achieved through the optimization of a loss function that has 3 components: “style loss”, “content loss”, and “total variation loss”:\n\nThe total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence.\nThe style loss is where the deep learning keeps in –that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales –defined by the depth of the layer considered).\nThe content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one.\n\n\nlibrary(keras)\nlibrary(purrr)\nlibrary(R6)\n\n# Parameters --------------------------------------------------------------\n\nbase_image_path <- \"neural-style-base-img.png\"\nstyle_reference_image_path <- \"neural-style-style.jpg\"\niterations <- 10\n\n# these are the weights of the different loss components\ntotal_variation_weight <- 1\nstyle_weight <- 1\ncontent_weight <- 0.025\n\n# dimensions of the generated picture.\nimg <- image_load(base_image_path)\nwidth <- img$size[[1]]\nheight <- img$size[[2]]\nimg_nrows <- 400\nimg_ncols <- as.integer(width * img_nrows / height)\n\n\n# Functions ---------------------------------------------------------------\n\n# util function to open, resize and format pictures into appropriate tensors\npreprocess_image <- function(path){\n  img <- image_load(path, target_size = c(img_nrows, img_ncols)) %>%\n    image_to_array() %>%\n    array_reshape(c(1, dim(.)))\n  imagenet_preprocess_input(img)\n}\n\n# util function to convert a tensor into a valid image\n# also turn BGR into RGB.\ndeprocess_image <- function(x){\n  x <- x[1,,,]\n  # Remove zero-center by mean pixel\n  x[,,1] <- x[,,1] + 103.939\n  x[,,2] <- x[,,2] + 116.779\n  x[,,3] <- x[,,3] + 123.68\n  # BGR -> RGB\n  x <- x[,,c(3,2,1)]\n  # clip to interval 0, 255\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x)/255\n  x\n}\n\n\n# Defining the model ------------------------------------------------------\n\n# get tensor representations of our images\nbase_image <- k_variable(preprocess_image(base_image_path))\nstyle_reference_image <- k_variable(preprocess_image(style_reference_image_path))\n\n# this will contain our generated image\ncombination_image <- k_placeholder(c(1, img_nrows, img_ncols, 3))\n\n# combine the 3 images into a single Keras tensor\ninput_tensor <- k_concatenate(list(base_image, style_reference_image, \n                                   combination_image), axis = 1)\n\n# build the VGG16 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nmodel <- application_vgg16(input_tensor = input_tensor, weights = \"imagenet\", \n                           include_top = FALSE)\n\nprint(\"Model loaded.\")\n\nnms <- map_chr(model$layers, ~.x$name)\noutput_dict <- map(model$layers, ~.x$output) %>% set_names(nms)\n\n# compute the neural style loss\n# first we need to define 4 util functions\n\n# the gram matrix of an image tensor (feature-wise outer product)\n\ngram_matrix <- function(x){\n  \n  features <- x %>%\n    k_permute_dimensions(pattern = c(3, 1, 2)) %>%\n    k_batch_flatten()\n  \n  k_dot(features, k_transpose(features))\n}\n\n# the \"style loss\" is designed to maintain\n# the style of the reference image in the generated image.\n# It is based on the gram matrices (which capture style) of\n# feature maps from the style reference image\n# and from the generated image\n\nstyle_loss <- function(style, combination){\n  S <- gram_matrix(style)\n  C <- gram_matrix(combination)\n  \n  channels <- 3\n  size <- img_nrows*img_ncols\n  \n  k_sum(k_square(S - C)) / (4 * channels^2  * size^2)\n}\n\n# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\n\ncontent_loss <- function(base, combination){\n  k_sum(k_square(combination - base))\n}\n\n# the 3rd loss function, total variation loss,\n# designed to keep the generated image locally coherent\n\ntotal_variation_loss <- function(x){\n  y_ij  <- x[,1:(img_nrows - 1L), 1:(img_ncols - 1L),]\n  y_i1j <- x[,2:(img_nrows), 1:(img_ncols - 1L),]\n  y_ij1 <- x[,1:(img_nrows - 1L), 2:(img_ncols),]\n  \n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\n# combine these loss functions into a single scalar\nloss <- k_variable(0.0)\nlayer_features <- output_dict$block4_conv2\nbase_image_features <- layer_features[1,,,]\ncombination_features <- layer_features[3,,,]\n\nloss <- loss + content_weight*content_loss(base_image_features, \n                                           combination_features)\n\nfeature_layers = c('block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1')\n\nfor(layer_name in feature_layers){\n  layer_features <- output_dict[[layer_name]]\n  style_reference_features <- layer_features[2,,,]\n  combination_features <- layer_features[3,,,]\n  sl <- style_loss(style_reference_features, combination_features)\n  loss <- loss + ((style_weight / length(feature_layers)) * sl)\n}\n\nloss <- loss + (total_variation_weight * total_variation_loss(combination_image))\n\n# get the gradients of the generated image wrt the loss\ngrads <- k_gradients(loss, combination_image)[[1]]\n\nf_outputs <- k_function(list(combination_image), list(loss, grads))\n\neval_loss_and_grads <- function(image){\n  image <- array_reshape(image, c(1, img_nrows, img_ncols, 3))\n  outs <- f_outputs(list(image))\n  list(\n    loss_value = outs[[1]],\n    grad_values = array_reshape(outs[[2]], dim = length(outs[[2]]))\n  )\n}\n\n# Loss and gradients evaluator.\n# \n# This Evaluator class makes it possible\n# to compute loss and gradients in one pass\n# while retrieving them via two separate functions,\n# \"loss\" and \"grads\". This is done because scipy.optimize\n# requires separate functions for loss and gradients,\n# but computing them separately would be inefficient.\nEvaluator <- R6Class(\n  \"Evaluator\",\n  public = list(\n    \n    loss_value = NULL,\n    grad_values = NULL,\n    \n    initialize = function() {\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n    },\n    \n    loss = function(x){\n      loss_and_grad <- eval_loss_and_grads(x)\n      self$loss_value <- loss_and_grad$loss_value\n      self$grad_values <- loss_and_grad$grad_values\n      self$loss_value\n    },\n    \n    grads = function(x){\n      grad_values <- self$grad_values\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n      grad_values\n    }\n    \n  )\n)\n\nevaluator <- Evaluator$new()\n\n# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\ndms <- c(1, img_nrows, img_ncols, 3)\nx <- array(data = runif(prod(dms), min = 0, max = 255) - 128, dim = dms)\n\n# Run optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the loss\nfor(i in 1:iterations){\n\n  # Run L-BFGS\n  opt <- optim(\n    array_reshape(x, dim = length(x)), fn = evaluator$loss, gr = evaluator$grads, \n    method = \"L-BFGS-B\",\n    control = list(maxit = 15)\n  )\n  \n  # Print loss value\n  print(opt$value)\n  \n  # decode the image\n  image <- x <- opt$par\n  image <- array_reshape(image, dms)\n  \n  # plot\n  im <- deprocess_image(image)\n  plot(as.raster(im))\n}"
  },
  {
    "objectID": "examples/nmt_attention.html",
    "href": "examples/nmt_attention.html",
    "title": "nmt_attention",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(tibble)\n\n\n# Preprocessing -----------------------------------------------------------\n\n# Assumes you've downloaded and unzipped one of the bilingual datasets offered at\n# http://www.manythings.org/anki/ and put it into a directory \"data\"\n# This example translates English to Dutch.\n\nfilepath <- file.path(\"data\", \"nld.txt\")\n\nlines <- readLines(filepath, n = 10000)\nsentences <- str_split(lines, \"\\t\")\n\nspace_before_punct <- function(sentence) {\n  str_replace_all(sentence, \"([?.!])\", \" \\\\1\")\n}\n\nreplace_special_chars <- function(sentence) {\n  str_replace_all(sentence, \"[^a-zA-Z?.!,¿]+\", \" \")\n}\n\nadd_tokens <- function(sentence) {\n  paste0(\"<start> \", sentence, \" <stop>\")\n}\nadd_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)\n\npreprocess_sentence <- compose(add_tokens,\n                               str_squish,\n                               replace_special_chars,\n                               space_before_punct)\n\nword_pairs <- map(sentences, preprocess_sentence)\n\ncreate_index <- function(sentences) {\n  unique_words <- sentences %>% unlist() %>% paste(collapse = \" \") %>%\n    str_split(pattern = \" \") %>% .[[1]] %>% unique() %>% sort()\n  index <- data.frame(\n    word = unique_words,\n    index = 1:length(unique_words),\n    stringsAsFactors = FALSE\n  ) %>%\n    add_row(word = \"<pad>\",\n            index = 0,\n            .before = 1)\n  index\n}\n\nword2index <- function(word, index_df) {\n  index_df[index_df$word == word, \"index\"]\n}\nindex2word <- function(index, index_df) {\n  index_df[index_df$index == index, \"word\"]\n}\n\nsrc_index <- create_index(map(word_pairs, ~ .[[1]]))\ntarget_index <- create_index(map(word_pairs, ~ .[[2]]))\nsentence2digits <- function(sentence, index_df) {\n  map((sentence %>% str_split(pattern = \" \"))[[1]], function(word)\n    word2index(word, index_df))\n}\n\nsentlist2diglist <- function(sentence_list, index_df) {\n  map(sentence_list, function(sentence)\n    sentence2digits(sentence, index_df))\n}\n\nsrc_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)\nsrc_maxlen <- map(src_diglist, length) %>% unlist() %>% max()\nsrc_matrix <-\n  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = \"post\")\n\ntarget_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)\ntarget_maxlen <- map(target_diglist, length) %>% unlist() %>% max()\ntarget_matrix <-\n  pad_sequences(target_diglist, maxlen = target_maxlen, padding = \"post\")\n\n\n\n# Train-test-split --------------------------------------------------------\n\ntrain_indices <-\n  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)\n\nvalidation_indices <- setdiff(1:nrow(src_matrix), train_indices)\n\nx_train <- src_matrix[train_indices,]\ny_train <- target_matrix[train_indices,]\n\nx_valid <- src_matrix[validation_indices,]\ny_valid <- target_matrix[validation_indices,]\n\nbuffer_size <- nrow(x_train)\n\n# just for convenience, so we may get a glimpse at translation performance \n# during training\ntrain_sentences <- sentences[train_indices]\nvalidation_sentences <- sentences[validation_indices]\nvalidation_sample <- sample(validation_sentences, 5)\n\n\n\n# Hyperparameters / variables ---------------------------------------------\n\nbatch_size <- 32\nembedding_dim <- 64\ngru_units <- 256\n\nsrc_vocab_size <- nrow(src_index)\ntarget_vocab_size <- nrow(target_index)\n\n\n# Create datasets ---------------------------------------------------------\n\ntrain_dataset <-\n  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nvalidation_dataset <-\n  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n\n# Attention encoder -------------------------------------------------------\n\n\nattention_encoder <-\n  function(gru_units,\n           embedding_dim,\n           src_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$embedding <-\n        layer_embedding(input_dim = src_vocab_size,\n                        output_dim = embedding_dim)\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        \n        x <- self$embedding(x)\n        c(output, state) %<-% self$gru(x, initial_state = hidden)\n        \n        list(output, state)\n      }\n    })\n  }\n\n\n\n# Attention decoder -------------------------------------------------------\n\n\nattention_decoder <-\n  function(object,\n           gru_units,\n           embedding_dim,\n           target_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      self$embedding <-\n        layer_embedding(input_dim = target_vocab_size, output_dim = embedding_dim)\n      gru_units <- gru_units\n      self$fc <- layer_dense(units = target_vocab_size)\n      self$W1 <- layer_dense(units = gru_units)\n      self$W2 <- layer_dense(units = gru_units)\n      self$V <- layer_dense(units = 1L)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        encoder_output <- inputs[[3]]\n        \n        hidden_with_time_axis <- k_expand_dims(hidden, 2)\n        \n        score <-\n          self$V(k_tanh(\n            self$W1(encoder_output) + self$W2(hidden_with_time_axis)\n          ))\n        \n        attention_weights <- k_softmax(score, axis = 2)\n        \n        context_vector <- attention_weights * encoder_output\n        context_vector <- k_sum(context_vector, axis = 2)\n        \n        x <- self$embedding(x)\n        \n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)\n        \n        c(output, state) %<-% self$gru(x)\n        \n        output <- k_reshape(output, c(-1, gru_units))\n        \n        x <- self$fc(output)\n        \n        list(x, state, attention_weights)\n        \n      }\n      \n    })\n  }\n\n\n# The model ---------------------------------------------------------------\n\nencoder <- attention_encoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  src_vocab_size = src_vocab_size\n)\n\ndecoder <- attention_decoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  target_vocab_size = target_vocab_size\n)\n\noptimizer <- tf$optimizers$Adam()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- ifelse(y_true == 0L, 0, 1)\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,\n                                                   logits = y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\n\n\n# Inference / translation functions ---------------------------------------\n# they are appearing here already in the file because we want to watch how\n# the network learns\n\nevaluate <-\n  function(sentence) {\n    attention_matrix <-\n      matrix(0, nrow = target_maxlen, ncol = src_maxlen)\n    \n    sentence <- preprocess_sentence(sentence)\n    input <- sentence2digits(sentence, src_index)\n    input <-\n      pad_sequences(list(input), maxlen = src_maxlen,  padding = \"post\")\n    input <- k_constant(input)\n    \n    result <- \"\"\n    \n    hidden <- k_zeros(c(1, gru_units))\n    c(enc_output, enc_hidden) %<-% encoder(list(input, hidden))\n    \n    dec_hidden <- enc_hidden\n    dec_input <-\n      k_expand_dims(list(word2index(\"<start>\", target_index)))\n    \n    for (t in seq_len(target_maxlen - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, dec_hidden, enc_output))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t,] <- attention_weights %>% as.double()\n      \n      pred_idx <-\n        tf$compat$v1$multinomial(k_exp(preds), num_samples = 1L)[1, 1] %>% as.double()\n      pred_word <- index2word(pred_idx, target_index)\n      \n      if (pred_word == '<stop>') {\n        result <-\n          paste0(result, pred_word)\n        return (list(result, sentence, attention_matrix))\n      } else {\n        result <-\n          paste0(result, pred_word, \" \")\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    list(str_trim(result), sentence, attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           words_sentence,\n           words_result) {\n    melted <- melt(attention_matrix)\n    ggplot(data = melted, aes(\n      x = factor(Var2),\n      y = factor(Var1),\n      fill = value\n    )) +\n      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +\n      theme(axis.ticks = element_blank()) +\n      xlab(\"\") +\n      ylab(\"\") +\n      scale_x_discrete(labels = words_sentence, position = \"top\") +\n      scale_y_discrete(labels = words_result) +\n      theme(aspect.ratio = 1)\n  }\n\n\ntranslate <- function(sentence) {\n  c(result, sentence, attention_matrix) %<-% evaluate(sentence)\n  print(paste0(\"Input: \",  sentence))\n  print(paste0(\"Predicted translation: \", result))\n  attention_matrix <-\n    attention_matrix[1:length(str_split(result, \" \")[[1]]),\n                     1:length(str_split(sentence, \" \")[[1]])]\n  plot_attention(attention_matrix,\n                 str_split(sentence, \" \")[[1]],\n                 str_split(result, \" \")[[1]])\n}\n\n# Training loop -----------------------------------------------------------\n\n\nn_epochs <- 50\n\nencoder_init_hidden <- k_zeros(c(batch_size, gru_units))\n\nfor (epoch in seq_len(n_epochs)) {\n  total_loss <- 0\n  iteration <- 0\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <- 0\n    x <- batch[[1]]\n    y <- batch[[2]]\n    iteration <- iteration + 1\n\n    with(tf$GradientTape() %as% tape, {\n      c(enc_output, enc_hidden) %<-% encoder(list(x, encoder_init_hidden))\n      \n      dec_hidden <- enc_hidden\n      dec_input <-\n        k_expand_dims(rep(list(\n          word2index(\"<start>\", target_index)\n        ), batch_size))\n      \n      \n      for (t in seq_len(target_maxlen - 1)) {\n        c(preds, dec_hidden, weights) %<-%\n          decoder(list(dec_input, dec_hidden, enc_output))\n        loss <- loss + cx_loss(y[, t], preds)\n        \n        dec_input <- k_expand_dims(y[, t])\n      }\n    })\n    total_loss <-\n      total_loss + loss / k_cast_to_floatx(dim(y)[2])\n    \n    paste0(\n      \"Batch loss (epoch/batch): \",\n      epoch,\n      \"/\",\n      iteration,\n      \": \",\n      (loss / k_cast_to_floatx(dim(y)[2])) %>% as.double() %>% round(4),\n      \"\\n\"\n    ) %>% print()\n    \n    variables <- c(encoder$variables, decoder$variables)\n    gradients <- tape$gradient(loss, variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(gradients, variables)))\n    \n  })\n  \n  paste0(\n    \"Total loss (epoch): \",\n    epoch,\n    \": \",\n    (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n    \"\\n\"\n  ) %>% print()\n  \n  walk(train_sentences[1:5], function(pair)\n    translate(pair[1]))\n  walk(validation_sample, function(pair)\n    translate(pair[1]))\n}\n\n# plot a mask\nexample_sentence <- train_sentences[[1]]\ntranslate(example_sentence)"
  },
  {
    "objectID": "examples/quora_siamese_lstm.html",
    "href": "examples/quora_siamese_lstm.html",
    "title": "quora_siamese_lstm",
    "section": "",
    "text": "Our implementation is inspired by the Siamese Recurrent Architecture, Mueller et al. Siamese recurrent architectures for learning sentence similarity, with small modifications like the similarity measure and the embedding layers (The original paper uses pre-trained word vectors). Using this kind of architecture dates back to 2005 with Le Cun et al and is usefull for verification tasks. The idea is to learn a function that maps input patterns into a target space such that a similarity measure in the target space approximates the “semantic” distance in the input space.\nAfter the competition, Quora also described their approach to this problem in this blog post.\n\nlibrary(readr)\nlibrary(keras)\nlibrary(purrr)\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 0.0001),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\n# Downloading Data --------------------------------------------------------\n\nquora_data <- get_file(\n  \"quora_duplicate_questions.tsv\",\n  \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n)\n\n\n# Pre-processing ----------------------------------------------------------\n\ndf <- read_tsv(quora_data)\n\ntokenizer <- text_tokenizer(num_words = FLAGS$vocab_size)\nfit_text_tokenizer(tokenizer, x = c(df$question1, df$question2))\n\nquestion1 <- texts_to_sequences(tokenizer, df$question1)\nquestion2 <- texts_to_sequences(tokenizer, df$question2)\n\nquestion1 <- pad_sequences(question1, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\nquestion2 <- pad_sequences(question2, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\n\n\n# Model Definition --------------------------------------------------------\n\ninput1 <- layer_input(shape = c(FLAGS$max_len_padding))\ninput2 <- layer_input(shape = c(FLAGS$max_len_padding))\n\nembedding <- layer_embedding(\n  input_dim = FLAGS$vocab_size + 2, \n  output_dim = FLAGS$embedding_size, \n  input_length = FLAGS$max_len_padding, \n  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\nseq_emb <- layer_lstm(\n  units = FLAGS$seq_embedding_size, \n  recurrent_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\n\nvector1 <- embedding(input1) %>%\n  seq_emb()\nvector2 <- embedding(input2) %>%\n  seq_emb()\n\nout <- layer_dot(list(vector1, vector2), axes = 1) %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel <- keras_model(list(input1, input2), out)\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"binary_crossentropy\", \n  metrics = list(\n    acc = metric_binary_accuracy\n  )\n)\n\n# Model Fitting -----------------------------------------------------------\n\nset.seed(1817328)\nval_sample <- sample.int(nrow(question1), size = 0.1*nrow(question1))\n\nmodel %>%\n  fit(\n    list(question1[-val_sample,], question2[-val_sample,]),\n    df$is_duplicate[-val_sample], \n    batch_size = 128, \n    epochs = 30, \n    validation_data = list(\n      list(question1[val_sample,], question2[val_sample,]), df$is_duplicate[val_sample]\n    ),\n    callbacks = list(\n      callback_early_stopping(patience = 5),\n      callback_reduce_lr_on_plateau(patience = 3)\n    )\n  )\n\nsave_model_hdf5(model, \"model-question-pairs.hdf5\", include_optimizer = TRUE)\nsave_text_tokenizer(tokenizer, \"tokenizer-question-pairs.hdf5\")\n\n\n# Prediction --------------------------------------------------------------\n# In a fresh R session:\n# Load model and tokenizer -\n\nmodel <- load_model_hdf5(\"model-question-pairs.hdf5\", compile = FALSE)\ntokenizer <- load_text_tokenizer(\"tokenizer-question-pairs.hdf5\")\n\n\npredict_question_pairs <- function(model, tokenizer, q1, q2) {\n  \n  q1 <- texts_to_sequences(tokenizer, list(q1))\n  q2 <- texts_to_sequences(tokenizer, list(q2))\n  \n  q1 <- pad_sequences(q1, 20)\n  q2 <- pad_sequences(q2, 20)\n  \n  as.numeric(predict(model, list(q1, q2)))\n}\n\n# Getting predictions\n\npredict_question_pairs(\n  model, tokenizer, \n  q1 = \"What is the main benefit of Quora?\",\n  q2 = \"What are the advantages of using Quora?\"\n)"
  },
  {
    "objectID": "examples/reuters_mlp.html",
    "href": "examples/reuters_mlp.html",
    "title": "reuters_mlp",
    "section": "",
    "text": "library(keras)\n\nmax_words <- 1000\nbatch_size <- 32\nepochs <- 5\n\ncat('Loading data...\\n')\nreuters <- dataset_reuters(num_words = max_words, test_split = 0.2)\nx_train <- reuters$train$x\ny_train <- reuters$train$y\nx_test <- reuters$test$x\ny_test <- reuters$test$y\n\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\nnum_classes <- max(y_train) + 1\ncat(num_classes, '\\n')\n\ncat('Vectorizing sequence data...\\n')\n\ntokenizer <- text_tokenizer(num_words = max_words)\nx_train <- sequences_to_matrix(tokenizer, x_train, mode = 'binary')\nx_test <- sequences_to_matrix(tokenizer, x_test, mode = 'binary')\n\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\ncat('Convert class vector to binary class matrix',\n    '(for use with categorical_crossentropy)\\n')\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\ncat('y_train shape:', dim(y_train), '\\n')\ncat('y_test shape:', dim(y_test), '\\n')\n\ncat('Building model...\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 512, input_shape = c(max_words)) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = num_classes) %>% \n  layer_activation(activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_split = 0.1\n)\n\nscore <- model %>% evaluate(\n  x_test, y_test,\n  batch_size = batch_size,\n  verbose = 1\n)\n\ncat('Test score:', score[[1]], '\\n')\ncat('Test accuracy', score[[2]], '\\n')"
  },
  {
    "objectID": "examples/stateful_lstm.html",
    "href": "examples/stateful_lstm.html",
    "title": "stateful_lstm",
    "section": "",
    "text": "library(keras)\n\n# since we are using stateful rnn tsteps can be set to 1\ntsteps <- 1\nbatch_size <- 25\nepochs <- 25\n# number of elements ahead that are used to make the prediction\nlahead <- 1\n\n# Generates an absolute cosine time series with the amplitude exponentially decreasing\n# Arguments:\n#   amp: amplitude of the cosine function\n#   period: period of the cosine function\n#   x0: initial x of the time series\n#   xn: final x of the time series\n#   step: step of the time series discretization\n#   k: exponential rate\ngen_cosine_amp <- function(amp = 100, period = 1000, x0 = 0, xn = 50000, step = 1, k = 0.0001) {\n  n <- (xn-x0) * step\n  cos <- array(data = numeric(n), dim = c(n, 1, 1))\n  for (i in 1:length(cos)) {\n    idx <- x0 + i * step\n    cos[[i, 1, 1]] <- amp * cos(2 * pi * idx / period)\n    cos[[i, 1, 1]] <- cos[[i, 1, 1]] * exp(-k * idx)\n  }\n  cos\n}\n\ncat('Generating Data...\\n')\ncos <- gen_cosine_amp()\ncat('Input shape:', dim(cos), '\\n')\n\nexpected_output <- array(data = numeric(length(cos)), dim = c(length(cos), 1))\nfor (i in 1:(length(cos) - lahead)) {\n  expected_output[[i, 1]] <- mean(cos[(i + 1):(i + lahead)])\n}\n\ncat('Output shape:', dim(expected_output), '\\n')\n\ncat('Creating model:\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_lstm(units = 50, input_shape = c(tsteps, 1), batch_size = batch_size,\n             return_sequences = TRUE, stateful = TRUE) %>% \n  layer_lstm(units = 50, return_sequences = FALSE, stateful = TRUE) %>% \n  layer_dense(units = 1)\nmodel %>% compile(loss = 'mse', optimizer = 'rmsprop')\n\ncat('Training\\n')\nfor (i in 1:epochs) {\n  model %>% fit(cos, expected_output, batch_size = batch_size,\n                epochs = 1, verbose = 1, shuffle = FALSE)\n            \n  model %>% reset_states()\n}\n\ncat('Predicting\\n')\npredicted_output <- model %>% predict(cos, batch_size = batch_size)\n\ncat('Plotting Results\\n')\nop <- par(mfrow=c(2,1))\nplot(expected_output, xlab = '')\ntitle(\"Expected\")\nplot(predicted_output, xlab = '')\ntitle(\"Predicted\")\npar(op)"
  },
  {
    "objectID": "examples/text_explanation_lime.html",
    "href": "examples/text_explanation_lime.html",
    "title": "text_explanation_lime",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(keras)\nlibrary(tidyverse)\n\n# Download and unzip data\n\nactivity_url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip\"\ntemp <- tempfile()\ndownload.file(activity_url, temp)\nunzip(temp, \"drugLibTest_raw.tsv\")\n\n\n# Read dataset\n\ndf <- read_delim('drugLibTest_raw.tsv',delim = '\\t')\nunlink(temp)\n\n# Select only rating and text from the whole dataset\n\ndf = df %>% select(rating,commentsReview) %>% mutate(rating = if_else(rating >= 8, 0, 1))\n\n# This is our text\ntext <- df$commentsReview\n\n# And these are ratings given by customers\ny_train <- df$rating\n\n\n# text_tokenizer helps us to turn each word into integers. By selecting maximum number of features\n# we also keep the most frequent words. Additionally, by default, all punctuation is removed.\n\nmax_features <- 1000\ntokenizer <- text_tokenizer(num_words = max_features)\n\n# Then, we need to fit the tokenizer object to our text data\n\ntokenizer %>% fit_text_tokenizer(text)\n\n# Via tokenizer object you can check word indices, word counts and other interesting properties.\n\ntokenizer$word_counts \ntokenizer$word_index\n\n# Finally, we can replace words in dataset with integers\ntext_seqs <- texts_to_sequences(tokenizer, text)\n\ntext_seqs %>% head(3)\n\n# Define the parameters of the keras model\n\nmaxlen <- 15\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 64\nkernel_size <- 3\nhidden_dims <- 50\nepochs <- 15\n\n# As a final step, restrict the maximum length of all sequences and create a matrix as input for model\nx_train <- text_seqs %>% pad_sequences(maxlen = maxlen)\n\n# Lets print the first 2 rows and see that max length of first 2 sequences equals to 15\nx_train[1:2,]\n\n# Create a model\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  layer_global_max_pooling_1d() %>%\n  layer_dense(hidden_dims) %>%\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Run\nhist <- model %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split = 0.1\n  )\n\n# Understanding lime for Keras Embedding Layers\n\n# In order to explain a text with LIME, we should write a preprocess function\n# which will help to turn words into integers. Therefore, above mentioned steps \n# (how to encode a text) should be repeated BUT within a function. \n# As we already have had a tokenizer object, we can apply the same object to train/test or a new text.\n\nget_embedding_explanation <- function(text) {\n  \n  tokenizer %>% fit_text_tokenizer(text)\n  \n  text_to_seq <- texts_to_sequences(tokenizer, text)\n  sentences <- text_to_seq %>% pad_sequences(maxlen = maxlen)\n}\n\n\nlibrary(lime)\n\n# Lets choose some text (3 rows) to explain\nsentence_to_explain <- train_sentences$text[15:17]\nsentence_to_explain\n\n# You could notice that our input is just a plain text. Unlike tabular data, lime function \n# for text classification requires a preprocess fuction. Because it will help to convert a text to integers \n# with provided function. \nexplainer <- lime(sentence_to_explain, model = model, preprocess = get_embedding_explanation)\n\n# Get explanation for the first 10 words\nexplanation <- explain(sentence_to_explain, explainer, n_labels = 1, n_features = 10,n_permutations = 1e4)\n\n\n# Different graphical ways to show the same information\n\nplot_text_explanations(explanation)\n\nplot_features(explanation)\n\ninteractive_text_explanations(explainer)"
  },
  {
    "objectID": "examples/tfprob_vae.html",
    "href": "examples/tfprob_vae.html",
    "title": "tfprob_vae",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n\n\n# Utilities --------------------------------------------------------\n\nnum_examples_to_generate <- 64L\n\ngenerate_random <- function(epoch) {\n  decoder_likelihood <-\n    decoder(latent_prior$sample(num_examples_to_generate))\n  predictions <- decoder_likelihood$mean()\n  # change path according to your preferences\n  png(file.path(\"/tmp\", paste0(\"random_epoch_\", epoch, \".png\")))\n  par(mfcol = c(8, 8))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:64) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\nshow_grid <- function(epoch) {\n  # change path according to your preferences\n  png(file.path(\"/tmp\", paste0(\"grid_epoch_\", epoch, \".png\")))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  n <- 16\n  img_size <- 28\n  grid_x <- seq(-4, 4, length.out = n)\n  grid_y <- seq(-4, 4, length.out = n)\n  rows <- NULL\n  for (i in 1:length(grid_x)) {\n    column <- NULL\n    for (j in 1:length(grid_y)) {\n      z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n      decoder_likelihood <- decoder(k_cast(z_sample, k_floatx()))\n      column <-\n        rbind(column,\n              (decoder_likelihood$mean() %>% as.numeric()) %>% matrix(ncol = img_size))\n    }\n    rows <- cbind(rows, column)\n  }\n  rows %>% as.raster() %>% plot()\n  dev.off()\n}\n\n\n# Setup and preprocessing -------------------------------------------------\n\nnp <- import(\"numpy\")\n\n# assume data have been downloaded from https://github.com/rois-codh/kmnist\n# and stored in /tmp\nkuzushiji <- np$load(\"/tmp/kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\n\n# Params ------------------------------------------------------------------\n\nlatent_dim <- 2\nmixture_components <- 16\n\n\n# Model -------------------------------------------------------------------\n\n# Encoder ------------------------------------------------------------------\n\nencoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense()\n      tfd$MultivariateNormalDiag(loc = x[, 1:latent_dim],\n                                 scale_diag = tf$nn$softplus(x[, (latent_dim + 1):(2 * latent_dim)] + 1e-5))\n    }\n  })\n}\n\n\n# Decoder ------------------------------------------------------------------\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = 3L)\n      \n    }\n  })\n}\n\n# Learnable Prior -------------------------------------------------------------------\n\nlearnable_prior_model <-\n  function(name = NULL, latent_dim, mixture_components) {\n    \n    keras_model_custom(name = name, function(self) {\n      self$loc <-\n        tf$get_variable(\n          name = \"loc\",\n          shape = list(mixture_components, latent_dim),\n          dtype = tf$float32\n        )\n      self$raw_scale_diag <- tf$get_variable(\n        name = \"raw_scale_diag\",\n        shape = c(mixture_components, latent_dim),\n        dtype = tf$float32\n      )\n      self$mixture_logits <-\n        tf$get_variable(\n          name = \"mixture_logits\",\n          shape = c(mixture_components),\n          dtype = tf$float32\n        )\n      \n      function (x, mask = NULL) {\n        tfd$MixtureSameFamily(\n          components_distribution = tfd$MultivariateNormalDiag(\n            loc = self$loc,\n            scale_diag = tf$nn$softplus(self$raw_scale_diag)\n          ),\n          mixture_distribution = tfd$Categorical(logits = self$mixture_logits)\n        )\n      }\n    })\n  }\n\n\n# Loss and optimizer ------------------------------------------------------\n\ncompute_kl_loss <-\n  function(latent_prior,\n           approx_posterior,\n           approx_posterior_sample) {\n    kl_div <- approx_posterior$log_prob(approx_posterior_sample) - latent_prior$log_prob(approx_posterior_sample)\n    avg_kl_div <- tf$reduce_mean(kl_div)\n    avg_kl_div\n  }\n\n\nglobal_step <- tf$train$get_or_create_global_step()\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n\n# Training loop -----------------------------------------------------------\n\nnum_epochs <- 50\n\nencoder <- encoder_model()\ndecoder <- decoder_model()\nlatent_prior_model <-\n  learnable_prior_model(latent_dim = latent_dim, mixture_components = mixture_components)\n\n# change this according to your preferences\ncheckpoint_dir <- \"/tmp/checkpoints\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    optimizer = optimizer,\n    global_step = global_step,\n    encoder = encoder,\n    decoder = decoder,\n    latent_prior_model = latent_prior_model\n  )\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      \n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      latent_prior <- latent_prior_model(NULL)\n      \n      kl_loss <-\n        compute_kl_loss(latent_prior,\n                        approx_posterior,\n                        approx_posterior_sample)\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    prior_gradients <-\n      tape$gradient(loss, latent_prior_model$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      prior_gradients, latent_prior_model$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n})\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n  \n  if (TRUE) {\n    generate_random(epoch)\n    show_grid(epoch)\n  }\n}"
  },
  {
    "objectID": "examples/variational_autoencoder.html",
    "href": "examples/variational_autoencoder.html",
    "title": "variational_autoencoder",
    "section": "",
    "text": "library(keras)\nK <- keras::backend()\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 100L\noriginal_dim <- 784L\nlatent_dim <- 2L\nintermediate_dim <- 256L\nepochs <- 50L\nepsilon_std <- 1.0\n\n# Model definition --------------------------------------------------------\n\nx <- layer_input(shape = c(original_dim))\nh <- layer_dense(x, intermediate_dim, activation = \"relu\")\nz_mean <- layer_dense(h, latent_dim)\nz_log_var <- layer_dense(h, latent_dim)\n\nsampling <- function(arg){\n  z_mean <- arg[, 1:(latent_dim)]\n  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]), \n    mean=0.,\n    stddev=epsilon_std\n  )\n  \n  z_mean + k_exp(z_log_var/2)*epsilon\n}\n\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% \n  layer_lambda(sampling)\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_mean <- layer_dense(units = original_dim, activation = \"sigmoid\")\nh_decoded <- decoder_h(z)\nx_decoded_mean <- decoder_mean(h_decoded)\n\n# end-to-end autoencoder\nvae <- keras_model(x, x_decoded_mean)\n\n# encoder, from inputs to latent space\nencoder <- keras_model(x, z_mean)\n\n# generator, from latent space to reconstructed inputs\ndecoder_input <- layer_input(shape = latent_dim)\nh_decoded_2 <- decoder_h(decoder_input)\nx_decoded_mean_2 <- decoder_mean(h_decoded_2)\ngenerator <- keras_model(decoder_input, x_decoded_mean_2)\n\n\nvae_loss <- function(x, x_decoded_mean){\n  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)\n  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)\n  xent_loss + kl_loss\n}\n\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\n\n\n# Data preparation --------------------------------------------------------\n\nmnist <- dataset_mnist()\nx_train <- mnist$train$x/255\nx_test <- mnist$test$x/255\nx_train <- array_reshape(x_train, c(nrow(x_train), 784), order = \"F\")\nx_test <- array_reshape(x_test, c(nrow(x_test), 784), order = \"F\")\n\n\n# Model training ----------------------------------------------------------\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n# Visualizations ----------------------------------------------------------\n\nlibrary(ggplot2)\nlibrary(dplyr)\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\n\nx_test_encoded %>%\n  as_data_frame() %>% \n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n# display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = 28) )\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()"
  },
  {
    "objectID": "examples/variational_autoencoder_deconv.html",
    "href": "examples/variational_autoencoder_deconv.html",
    "title": "variational_autoencoder_deconv",
    "section": "",
    "text": "library(keras)\nK <- keras::backend()\n\n#### Parameterization ####\n\n# input image dimensions\nimg_rows <- 28L\nimg_cols <- 28L\n# color channels (1 = grayscale, 3 = RGB)\nimg_chns <- 1L\n\n# number of convolutional filters to use\nfilters <- 64L\n\n# convolution kernel size\nnum_conv <- 3L\n\nlatent_dim <- 2L\nintermediate_dim <- 128L\nepsilon_std <- 1.0\n\n# training parameters\nbatch_size <- 100L\nepochs <- 5L\n\n\n#### Model Construction ####\n\noriginal_img_size <- c(img_rows, img_cols, img_chns)\n\nx <- layer_input(shape = c(original_img_size))\n\nconv_1 <- layer_conv_2d(\n  x,\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_2 <- layer_conv_2d(\n  conv_1,\n  filters = filters,\n  kernel_size = c(2L, 2L),\n  strides = c(2L, 2L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_3 <- layer_conv_2d(\n  conv_2,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_4 <- layer_conv_2d(\n  conv_3,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nflat <- layer_flatten(conv_4)\nhidden <- layer_dense(flat, units = intermediate_dim, activation = \"relu\")\n\nz_mean <- layer_dense(hidden, units = latent_dim)\nz_log_var <- layer_dense(hidden, units = latent_dim)\n\nsampling <- function(args) {\n  z_mean <- args[, 1:(latent_dim)]\n  z_log_var <- args[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]),\n    mean = 0.,\n    stddev = epsilon_std\n  )\n  z_mean + k_exp(z_log_var) * epsilon\n}\n\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% layer_lambda(sampling)\n\noutput_shape <- c(batch_size, 14L, 14L, filters)\n\ndecoder_hidden <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_upsample <- layer_dense(units = prod(output_shape[-1]), activation = \"relu\")\n\ndecoder_reshape <- layer_reshape(target_shape = output_shape[-1])\ndecoder_deconv_1 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_2 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_3_upsample <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(3L, 3L),\n  strides = c(2L, 2L),\n  padding = \"valid\",\n  activation = \"relu\"\n)\n\ndecoder_mean_squash <- layer_conv_2d(\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"valid\",\n  activation = \"sigmoid\"\n)\n\nhidden_decoded <- decoder_hidden(z)\nup_decoded <- decoder_upsample(hidden_decoded)\nreshape_decoded <- decoder_reshape(up_decoded)\ndeconv_1_decoded <- decoder_deconv_1(reshape_decoded)\ndeconv_2_decoded <- decoder_deconv_2(deconv_1_decoded)\nx_decoded_relu <- decoder_deconv_3_upsample(deconv_2_decoded)\nx_decoded_mean_squash <- decoder_mean_squash(x_decoded_relu)\n\n# custom loss function\nvae_loss <- function(x, x_decoded_mean_squash) {\n  x <- k_flatten(x)\n  x_decoded_mean_squash <- k_flatten(x_decoded_mean_squash)\n  xent_loss <- 1.0 * img_rows * img_cols *\n    loss_binary_crossentropy(x, x_decoded_mean_squash)\n  kl_loss <- -0.5 * k_mean(1 + z_log_var - k_square(z_mean) -\n                           k_exp(z_log_var), axis = -1L)\n  k_mean(xent_loss + kl_loss)\n}\n\n## variational autoencoder\nvae <- keras_model(x, x_decoded_mean_squash)\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\nsummary(vae)\n\n## encoder: model to project inputs on the latent space\nencoder <- keras_model(x, z_mean)\n\n## build a digit generator that can sample from the learned distribution\ngen_decoder_input <- layer_input(shape = latent_dim)\ngen_hidden_decoded <- decoder_hidden(gen_decoder_input)\ngen_up_decoded <- decoder_upsample(gen_hidden_decoded)\ngen_reshape_decoded <- decoder_reshape(gen_up_decoded)\ngen_deconv_1_decoded <- decoder_deconv_1(gen_reshape_decoded)\ngen_deconv_2_decoded <- decoder_deconv_2(gen_deconv_1_decoded)\ngen_x_decoded_relu <- decoder_deconv_3_upsample(gen_deconv_2_decoded)\ngen_x_decoded_mean_squash <- decoder_mean_squash(gen_x_decoded_relu)\ngenerator <- keras_model(gen_decoder_input, gen_x_decoded_mean_squash)\n\n\n#### Data Preparation ####\n\nmnist <- dataset_mnist()\ndata <- lapply(mnist, function(m) {\n  array_reshape(m$x / 255, dim = c(dim(m$x)[1], original_img_size))\n})\nx_train <- data$train\nx_test <- data$test\n\n\n#### Model Fitting ####\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n#### Visualizations ####\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n## display a 2D plot of the digit classes in the latent space\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\nx_test_encoded %>%\n  as_data_frame() %>%\n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n## display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = digit_size))\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()"
  },
  {
    "objectID": "examples/vq_vae.html",
    "href": "examples/vq_vae.html",
    "title": "vq_vae",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nuse_session_with_seed(7778,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\nlibrary(curry)\n\nmoving_averages <- tf$python$training$moving_averages\n\n\n# Utilities --------------------------------------------------------\n\nvisualize_images <-\n  function(dataset,\n           epoch,\n           reconstructed_images,\n           random_images) {\n    write_png(dataset, epoch, \"reconstruction\", reconstructed_images)\n    write_png(dataset, epoch, \"random\", random_images)\n    \n  }\n\nwrite_png <- function(dataset, epoch, desc, images) {\n  png(paste0(dataset, \"_epoch_\", epoch, \"_\", desc, \".png\"))\n  par(mfcol = c(8, 8))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:64) {\n    img <- images[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n  \n}\n\n\n# Setup and preprocessing -------------------------------------------------\n\nnp <- import(\"numpy\")\n\n# download from: https://github.com/rois-codh/kmnist\nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 64\nnum_examples_to_generate <- batch_size\n\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n# test\niter <- make_iterator_one_shot(train_dataset)\nbatch <-  iterator_get_next(iter)\nbatch %>% dim()\n\n# Params ------------------------------------------------------------------\n\nlearning_rate <- 0.001\nlatent_size <- 1\nnum_codes <- 64L\ncode_size <- 16L\nbase_depth <- 32\nactivation <- \"elu\"\nbeta <- 0.25\ndecay <- 0.99\ninput_shape <- c(28, 28, 1)\n\n# Models -------------------------------------------------------------------\n\ndefault_conv <-\n  set_defaults(layer_conv_2d, list(padding = \"same\", activation = activation))\ndefault_deconv <-\n  set_defaults(layer_conv_2d_transpose,\n               list(padding = \"same\", activation = activation))\n\n# Encoder ------------------------------------------------------------------\n\nencoder_model <- function(name = NULL,\n                          code_size) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <- default_conv(filters = base_depth, kernel_size = 5)\n    self$conv2 <-\n      default_conv(filters = base_depth,\n                   kernel_size = 5,\n                   strides = 2)\n    self$conv3 <-\n      default_conv(filters = 2 * base_depth, kernel_size = 5)\n    self$conv4 <-\n      default_conv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$conv5 <-\n      default_conv(\n        filters = 4 * latent_size,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = latent_size * code_size)\n    self$reshape <-\n      layer_reshape(target_shape = c(latent_size, code_size))\n    \n    function (x, mask = NULL) {\n      x %>%\n        # output shape:  7 28 28 32\n        self$conv1() %>%\n        # output shape:  7 14 14 32\n        self$conv2() %>%\n        # output shape:  7 14 14 64\n        self$conv3() %>%\n        # output shape:  7 7 7 64\n        self$conv4() %>%\n        # output shape:  7 1 1 4\n        self$conv5() %>%\n        # output shape:  7 4\n        self$flatten() %>%\n        # output shape:  7 16\n        self$dense() %>%\n        # output shape:  7 1 16\n        self$reshape()\n    }\n    \n  })\n}\n\n\n# Decoder ------------------------------------------------------------------\n\ndecoder_model <- function(name = NULL,\n                          input_size,\n                          output_shape) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$reshape1 <- layer_reshape(target_shape = c(1, 1, input_size))\n    self$deconv1 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$deconv2 <-\n      default_deconv(filters = 2 * base_depth, kernel_size = 5)\n    self$deconv3 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$deconv4 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$deconv5 <-\n      default_deconv(filters = base_depth,\n                     kernel_size = 5,\n                     strides = 2)\n    self$deconv6 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$conv1 <-\n      default_conv(filters = output_shape[3],\n                   kernel_size = 5,\n                   activation = \"linear\")\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        # output shape:  7 1 1 16\n        self$reshape1() %>%\n        # output shape:  7 7 7 64\n        self$deconv1() %>%\n        # output shape:  7 7 7 64\n        self$deconv2() %>%\n        # output shape:  7 14 14 64\n        self$deconv3() %>%\n        # output shape:  7 14 14 32\n        self$deconv4() %>%\n        # output shape:  7 28 28 32\n        self$deconv5() %>%\n        # output shape:  7 28 28 32\n        self$deconv6() %>%\n        # output shape:  7 28 28 1\n        self$conv1()\n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = length(output_shape))\n    }\n  })\n}\n\n# Vector quantizer -------------------------------------------------------------------\n\nvector_quantizer_model <- \n  function(name = NULL, num_codes, code_size) {\n    \n    keras_model_custom(name = name, function(self) {\n      self$num_codes <- num_codes\n      self$code_size <- code_size\n      self$codebook <- tf$get_variable(\"codebook\",\n                                       shape = c(num_codes, code_size),\n                                       dtype = tf$float32)\n      self$ema_count <- tf$get_variable(\n        name = \"ema_count\",\n        shape = c(num_codes),\n        initializer = tf$constant_initializer(0),\n        trainable = FALSE\n      )\n      self$ema_means = tf$get_variable(\n        name = \"ema_means\",\n        initializer = self$codebook$initialized_value(),\n        trainable = FALSE\n      )\n      \n      function (x, mask = NULL) {\n\n        # bs * 1 * num_codes\n        distances <- tf$norm(tf$expand_dims(x, axis = 2L) -\n                               tf$reshape(self$codebook,\n                                          c(\n                                            1L, 1L, self$num_codes, self$code_size\n                                          )),\n                             axis = 3L)\n        \n        # bs * 1\n        assignments <- tf$argmin(distances, axis = 2L)\n        \n        # bs * 1 * num_codes\n        one_hot_assignments <-\n          tf$one_hot(assignments, depth = self$num_codes)\n        \n        # bs * 1 * code_size\n        nearest_codebook_entries <- tf$reduce_sum(\n          tf$expand_dims(one_hot_assignments,-1L) * # bs, 1, 64, 1\n            tf$reshape(self$codebook, c(\n              1L, 1L, self$num_codes, self$code_size\n            )),\n          axis = 2L # 1, 1, 64, 16\n        )\n        \n        list(nearest_codebook_entries, one_hot_assignments)\n      }\n    })\n  }\n\n\n# Update codebook ------------------------------------------------------\n\nupdate_ema <- function(vector_quantizer,\n                       one_hot_assignments,\n                       codes,\n                       decay) {\n  # shape = 64\n  updated_ema_count <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_count,\n    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # 64 * 16\n  updated_ema_means <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_means,\n    # selects all assigned values (masking out the others) and sums them up over the batch\n    # (will be divided by count later)\n    tf$reduce_sum(\n      tf$expand_dims(codes, 2L) *\n        tf$expand_dims(one_hot_assignments, 3L),\n      axis = c(0L, 1L)\n    ),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # Add small value to avoid dividing by zero\n  updated_ema_count <- updated_ema_count + 1e-5\n  updated_ema_means <-\n    updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)\n  \n  tf$assign(vector_quantizer$codebook, updated_ema_means)\n}\n\n\n# Training setup -----------------------------------------------------------\n\nencoder <- encoder_model(code_size = code_size)\ndecoder <- decoder_model(input_size = latent_size * code_size,\n                         output_shape = input_shape)\n\nvector_quantizer <-\n  vector_quantizer_model(num_codes = num_codes, code_size = code_size)\n\noptimizer <- tf$train$AdamOptimizer(learning_rate = learning_rate)\n\ncheckpoint_dir <- \"./vq_vae_checkpoints\"\n\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    optimizer = optimizer,\n    encoder = encoder,\n    decoder = decoder,\n    vector_quantizer_model = vector_quantizer\n  )\n\ncheckpoint$save(file_prefix = checkpoint_prefix)\n\n# Training loop -----------------------------------------------------------\n\nnum_epochs <- 20\n\nfor (epoch in seq_len(num_epochs)) {\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  reconstruction_loss_total <- 0\n  commitment_loss_total <- 0\n  prior_loss_total <- 0\n  \n  until_out_of_range({\n    \n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      codes <- encoder(x)\n      c(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n      codes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\n      decoder_distribution <- decoder(codes_straight_through)\n      \n      reconstruction_loss <-\n        -tf$reduce_mean(decoder_distribution$log_prob(x))\n      \n      commitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n      \n      prior_dist <- tfd$Multinomial(total_count = 1,\n                                    logits = tf$zeros(c(latent_size, num_codes)))\n      prior_loss <- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))\n      \n      loss <-\n        reconstruction_loss + beta * commitment_loss + prior_loss\n      \n    })\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    update_ema(vector_quantizer,\n               one_hot_assignments,\n               codes,\n               decay)\n    \n    total_loss <- total_loss + loss\n    reconstruction_loss_total <-\n      reconstruction_loss_total + reconstruction_loss\n    commitment_loss_total <- commitment_loss_total + commitment_loss\n    prior_loss_total <- prior_loss_total + prior_loss\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Loss (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss)/trunc(buffer_size/batch_size)) %>% round(4)} loss\",\n      \"  {(as.numeric(reconstruction_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} reconstruction_loss\",\n      \"  {(as.numeric(commitment_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} commitment_loss\",\n      \"  {(as.numeric(prior_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} prior_loss\",\n      \n    ),\n    \"\\n\"\n  )\n  \n  # display example images (choose your frequency)\n  if (TRUE) {\n    reconstructed_images <- decoder_distribution$mean()\n    # (64, 1, 16)\n    prior_samples <- tf$reduce_sum(\n      # selects one of the codes (masking out 63 of 64 codes)\n      # (bs, 1, 64, 1)\n      tf$expand_dims(prior_dist$sample(num_examples_to_generate),-1L) *\n        # (1, 1, 64, 16)\n        tf$reshape(vector_quantizer$codebook,\n                   c(1L, 1L, num_codes, code_size)),\n      axis = 2L\n    )\n    decoded_distribution_given_random_prior <-\n      decoder(prior_samples)\n    random_images <- decoded_distribution_given_random_prior$mean()\n    visualize_images(\"k\", epoch, reconstructed_images, random_images)\n  }\n}"
  },
  {
    "objectID": "guides/data/data.html",
    "href": "guides/data/data.html",
    "title": "Build TensorFlow input pipelines",
    "section": "",
    "text": "The tfdatasets API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The tf$data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\nThe tfdatasets API introduces a TensorFlow Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\nThere are two distinct ways to create a dataset:"
  },
  {
    "objectID": "guides/data/data.html#basic-mechanics",
    "href": "guides/data/data.html#basic-mechanics",
    "title": "Build TensorFlow input pipelines",
    "section": "Basic mechanics",
    "text": "Basic mechanics\nTo create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tensors_dataset() or tensor_slices_dataset(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tfrecord_dataset().\nOnce you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the Dataset object. For example, you can apply per-element transformations such as dataset_map(), and multi-element transformations such as dataset_batch(). See the documentation for tfdatasets for a complete list of transformations.\nThe Dataset object is a iterable. This makes it possible to consume its elements using a the coro::loop:\n\ndataset <- tensor_slices_dataset(c(8, 3, 0, 8, 2, 1))\n\nLoaded Tensorflow version 2.9.1\n\ndataset\n\n<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.float32, name=None)>\n\n\n\ncoro::loop(for(elem in dataset) {\n  print(as.numeric(elem))\n})\n\n[1] 8\n[1] 3\n[1] 0\n[1] 8\n[1] 2\n[1] 1\n\n\nOr by explicitly creating a Python iterator using iter and consuming its elements using next:\n\nit <- reticulate::as_iterator(dataset)\nprint(reticulate::iter_next(it))\n\ntf.Tensor(8.0, shape=(), dtype=float32)\n\n\nAlternatively, dataset elements can be consumed using the reduce transformation, which reduces all elements to produce a single result. The following example illustrates how to use the reduce transformation to compute the sum of a dataset of integers.\n\ndataset %>% \n  dataset_reduce(0, function(state, value) state + value)\n\ntf.Tensor(22.0, shape=(), dtype=float32)\n\n\n\nDataset structure\nA dataset produces a sequence of elements, where each element is the same (nested) structure of components. Individual components of the structure can be of any type representable by tf$TypeSpec, including tf$Tensor, tf$sparse$SparseTensor, tf$RaggedTensor, tf$TensorArray, or tf$data$Dataset.\nThe constructs that can be used to express the (nested) structure of elements include tuple, dict, NamedTuple, and OrderedDict. In particular, list is not a valid construct for expressing the structure of dataset elements. This is because early tfdataset users felt strongly about list inputs (e.g. passed to tensors_dataset()) being automatically packed as tensors and list outputs (e.g. return values of user-defined functions) being coerced into a tuple. As a consequence, if you would like a list input to be treated as a structure, you need to convert it into tuple and if you would like a list output to be a single component, then you need to explicitly pack it using tf$stack.\nThe dataset_element_spec property allows you to inspect the type of each element component. The property returns a nested structure of tf$TypeSpec objects, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components. For example:\n\ndataset1 <- tensor_slices_dataset(tf$random$uniform(shape(4, 10)))\ndataset1$element_spec\n\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\n\n\ndataset2 <- tensor_slices_dataset(reticulate::tuple(\n  tf$random$uniform(shape(4)),\n  tf$random$uniform(shape(4, 100), maxval = 100L, dtype = tf$int32)\n))\n\ndataset2$element_spec\n\n[[1]]\nTensorSpec(shape=(), dtype=tf.float32, name=None)\n\n[[2]]\nTensorSpec(shape=(100,), dtype=tf.int32, name=None)\n\n\n\ndataset3 <- zip_datasets(dataset1, dataset2)\ndataset3$element_spec\n\n[[1]]\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\n[[2]]\n[[2]][[1]]\nTensorSpec(shape=(), dtype=tf.float32, name=None)\n\n[[2]][[2]]\nTensorSpec(shape=(100,), dtype=tf.int32, name=None)\n\n\n\n# Dataset containing a sparse tensor.\nsparse <- tf$SparseTensor(\n  indices = list(c(0L,0L), c(1L,2L)), \n  values = c(1,2),\n  dense_shape = shape(3,4)\n)\ndataset4 <- tensors_dataset(sparse)\ndataset4$element_spec\n\nSparseTensorSpec(TensorShape([3, 4]), tf.float32)\n\n\n\n# Use value_type to see the type of value represented by the element spec\ndataset4$element_spec$value_type\n\n<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n\n\nThe Dataset transformations support datasets of any structure. When using the dataset_map(), and dataset_filter() transformations, which apply a function to each element, the element structure determines the arguments of the function:\n\ndataset1 <- tensor_slices_dataset(\n    tf$random$uniform(shape(4, 10), minval = 1L, maxval = 10L, dtype = tf$int32))\ndataset1\n\n<TensorSliceDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int32, name=None)>\n\n\n\ndataset1 %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\n\ntf.Tensor([1 1 9 7 2 3 3 2 8 3], shape=(10), dtype=int32)\n\n\n\ndataset2 <- tensor_slices_dataset(reticulate::tuple(\n  tf$random$uniform(shape(4)),\n  tf$random$uniform(shape(4, 100), maxval = 100L, dtype = tf$int32)\n))\ndataset2\n\n<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n\n\n\ndataset3 <- zip_datasets(dataset1, dataset2)\ndataset3\n\n<ZipDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))>\n\n\n\ndataset3 %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nList of 2\n $ :<tf.Tensor: shape=(10), dtype=int32, numpy=array([1, 1, 9, 7, 2, 3, 3, 2, 8, 3], dtype=int32)>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(), dtype=float32, numpy=0.33178997>\n  ..$ :<tf.Tensor: shape=(100), dtype=int32, numpy=…>"
  },
  {
    "objectID": "guides/data/data.html#reading-input-data",
    "href": "guides/data/data.html#reading-input-data",
    "title": "Build TensorFlow input pipelines",
    "section": "Reading input data",
    "text": "Reading input data\n\nConsuming R arrays\nSee Loading NumPy arrays for more examples.\nIf all of your input data fits in memory, the simplest way to create a Dataset from them is to convert them to tf$Tensor objects and use tensor_slices_dataset().\n\nc(train, test) %<-% dataset_fashion_mnist()\n\n\ntrain[[1]][] <- train[[1]]/255\ndataset <- tensor_slices_dataset(train)\ndataset\n\n<TensorSliceDataset element_spec={'x': TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), 'y': TensorSpec(shape=(), dtype=tf.int32, name=None)}>\n\n\nNote: The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf$constant() operations. This works well for a small dataset, but wastes memory—because the contents of the array will be copied multiple times—and can run into the 2GB limit for the tf$GraphDef protocol buffer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsuming TFRecord data\nSee Loading TFRecords for an end-to-end example.\nThe tfdatasets API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tfrecord_dataset() class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.\nHere is an example using the test file from the French Street Name Signs (FSNS).\n\n# Creates a dataset that reads all of the examples from two files.\nfsns_test_file <- get_file(\n  \"fsns.tfrec\", \n  \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\"\n)\n\nThe filenames argument to the tfrecord_dataset() initializer can either be a string, a list of strings, or a tf$Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:\n\ndataset <- tfrecord_dataset(filenames = list(fsns_test_file))\ndataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n\nMany TensorFlow projects use serialized tf$train$Example records in their TFRecord files. These need to be decoded before they can be inspected:\n\nraw_example <- dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nparsed <- tf$train$Example$FromString(raw_example$numpy())\nparsed$features$feature['image/text']\n\nbytes_list {\n  value: \"Rue Perreyon\"\n}\n\n\n\n\nConsuming text data\nSee Loading Text for an end to end example.\nMany datasets are distributed as one or more text files. The text_line_dataset() provides an easy way to extract lines from one or more text files. Given one or more filenames, a text_line_dataset() will produce one string-valued element per line of those files.\n\ndirectory_url <- 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\nfile_names <- c('cowper.txt', 'derby.txt', 'butler.txt')\n\nfile_paths <- lapply(file_names, function(file_name) {\n get_file(file_name, paste0(directory_url, file_name)) \n})\n\n\ndataset <- text_line_dataset(file_paths)\n\nHere are the first few lines of the first file:\n\ndataset %>% \n  dataset_take(5) %>% \n  coro::collect(5) %>% \n  str()\n\nList of 5\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'His wrath pernicious, who ten thousand woes'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"Caused to Achaia's host, sent many a soul\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Illustrious into Ades premature,'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'And Heroes gave (so stood the will of Jove)'>\n\n\nTo alternate lines between files use dataset_interleave(). This makes it easier to shuffle files together. Here are the first, second and third lines from each translation:\n\nfiles_ds <- tensor_slices_dataset(unlist(file_paths))\nlines_ds <- files_ds %>% \n  dataset_interleave(text_line_dataset, cycle_length = 3)\n\nlines_ds %>% \n  coro::collect(9) %>% \n  str()\n\nList of 9\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'His wrath pernicious, who ten thousand woes'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'The vengeance, deep and deadly; whence to Greece'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'countless ills upon the Achaeans. Many a brave soul did it send'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"Caused to Achaia's host, sent many a soul\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Unnumbered ills arose; which many a soul'>\n  [list output truncated]\n\n\nBy default, a text_line_dataset() yields every line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the dataset_skip() or dataset_filter() transformations. Here, you skip the first line, then filter to find only survivors.\n\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\ntitanic_lines <- text_line_dataset(titanic_file)\n\n\ntitanic_lines %>% \n  coro::collect(10) %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,35.0,1,0,53.1,First,C,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'>\n  [list output truncated]\n\n\n\nsurvived <- function(line) {\n  tf$not_equal(tf$strings$substr(line, 0L, 1L), \"0\")\n}\n\nsurvivors <- titanic_lines %>% \n  dataset_skip(1) %>% \n  dataset_filter(survived)\n\n\nsurvivors %>% \n  coro::collect(10) %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,35.0,1,0,53.1,First,C,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'>\n  [list output truncated]\n\n\n\n\nConsuming CSV data\nSee Loading CSV Files, and Loading Data Frames for more examples.\nThe CSV file format is a popular format for storing tabular data in plain text.\nFor example:\n\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\n\n\ndf <- readr::read_csv(titanic_file)\n\nRows: 627 Columns: 10\n── Column specification ────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): sex, class, deck, embark_town, alone\ndbl (5): survived, age, n_siblings_spouses, parch, fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df)\n\n# A tibble: 6 × 10\n  survived sex      age n_siblings_spou… parch  fare class deck  embark_town\n     <dbl> <chr>  <dbl>            <dbl> <dbl> <dbl> <chr> <chr> <chr>      \n1        0 male      22                1     0  7.25 Third unkn… Southampton\n2        1 female    38                1     0 71.3  First C     Cherbourg  \n3        1 female    26                0     0  7.92 Third unkn… Southampton\n4        1 female    35                1     0 53.1  First C     Southampton\n5        0 male      28                0     0  8.46 Third unkn… Queenstown \n6        0 male       2                3     1 21.1  Third unkn… Southampton\n# … with 1 more variable: alone <chr>\n\n\nIf your data fits in memory the same tensor_slices_dataset() method works on lists, allowing this data to be easily imported:\n\ntitanic_slices <- tensor_slices_dataset(df)\n\ntitanic_slices %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nList of 10\n $ survived          :<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n $ sex               :<tf.Tensor: shape=(), dtype=string, numpy=b'male'>\n $ age               :<tf.Tensor: shape=(), dtype=float32, numpy=22.0>\n $ n_siblings_spouses:<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n $ parch             :<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n $ fare              :<tf.Tensor: shape=(), dtype=float32, numpy=7.25>\n $ class             :<tf.Tensor: shape=(), dtype=string, numpy=b'Third'>\n $ deck              :<tf.Tensor: shape=(), dtype=string, numpy=b'unknown'>\n  [list output truncated]\n\n\nA more scalable approach is to load from disk as necessary.\nThe tfdatasets package provides methods to extract records from one or more CSV files that comply with RFC 4180.\nThe experimental$make_csv_dataset function is the high level interface for reading sets of csv files. It supports column type inference and many other features, like batching and shuffling, to make usage simple.\n\ntitanic_batches <- tf$data$experimental$make_csv_dataset(\n  titanic_file, \n  batch_size = 4L,\n  label_name = \"survived\"\n)\n\n\ntitanic_batches %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nList of 2\n $ :OrderedDict([('sex', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'male', b'female', b'male', b'male'], dtype=object)>), ('age', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([56., 28., 28., 40.], dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>), ('parch', <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>), ('fare', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([26.55, 33.  ,  0.  ,  0.  ], dtype=float32)>), ('class', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'First', b'Second', b'Second', b'First'], dtype=object)>), ('deck', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'unknown', b'unknown', b'unknown', b'B'], dtype=object)>), ('embark_town', <tf.Tensor: shape=(4,), dtype=string, numpy=\narray([b'Southampton', b'Southampton', b'Southampton', b'Southampton'],\n      dtype=object)>), ('alone', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'y', b'y', b'y', b'y'], dtype=object)>)])\n $ :<tf.Tensor: shape=(4), dtype=int32, numpy=array([0, 1, 0, 0], dtype=int32)>\n\n\nYou can use the select_columns argument if you only need a subset of columns.\n\ntitanic_batches <- tf$data$experimental$make_csv_dataset(\n  titanic_file, \n  batch_size = 4L,\n  label_name = \"survived\", \n  select_columns = c('class', 'fare', 'survived')\n)\n\n\ntitanic_batches %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nList of 2\n $ :OrderedDict([('fare', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([69.55  , 51.8625,  7.225 , 16.1   ], dtype=float32)>), ('class', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'Third', b'First', b'Third', b'Third'], dtype=object)>)])\n $ :<tf.Tensor: shape=(4), dtype=int32, numpy=array([0, 1, 0, 0], dtype=int32)>\n\n\nThere is also a lower-level tf$experimental$CsvDataset class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column.\n\ntitanic_types  = list(tf$int32, tf$string, tf$float32, tf$int32, tf$int32, tf$float32, tf$string, tf$string, tf$string, tf$string)\ndataset <- tf$data$experimental$CsvDataset(titanic_file, titanic_types , header = TRUE)\n\ndataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=0>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'male'>\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=22.0>\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=1>\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=0>\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=7.25>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Third'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'unknown'>\n  [list output truncated]\n\n\nIf some columns are empty, this low-level interface allows you to provide default values instead of column types.\n\nwriteLines(con = \"missing.csv\", text = \n\"1,2,3,4\n,2,3,4\n1,,3,4\n1,2,,4\n1,2,3,\n,,,\"\n)\n\n\n# Creates a dataset that reads all of the records from two CSV files, each with\n# four float columns which may have missing values.\n\nrecord_defaults <- c(999,999,999,999)\ndataset <- tf$data$experimental$CsvDataset(\"missing.csv\", record_defaults)\ndataset <- dataset %>% \n  dataset_map(function(...) tf$stack(list(...)))\ndataset\n\n<MapDataset element_spec=TensorSpec(shape=(4,), dtype=tf.float32, name=None)>\n\n\n\ndataset %>% \n  coro::collect() %>% \n  str()\n\nList of 6\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([999.,   2.,   3.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1., 999.,   3.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1.,   2., 999.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1.,   2.,   3., 999.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([999., 999., 999., 999.], dtype=float32)>\n\n\nBy default, a CsvDataset yields every column of every line of the file, which may not be desirable, for example if the file starts with a header line that should be ignored, or if some columns are not required in the input. These lines and fields can be removed with the header and select_cols arguments respectively.\n\n# Creates a dataset that reads all of the records from two CSV files with\n# headers, extracting float data from columns 2 and 4.\nrecord_defaults <- c(999, 999) # Only provide defaults for the selected columns\ndataset <- tf$data$experimental$CsvDataset(\n  \"missing.csv\", \n  record_defaults, \n  select_cols = c(1L, 3L)\n)\ndataset <- dataset %>% \n  dataset_map(function(...) tf$stack(list(...)))\ndataset\n\n<MapDataset element_spec=TensorSpec(shape=(2,), dtype=tf.float32, name=None)>\n\n\n\ndataset %>% \n  coro::collect() %>% \n  str()\n\nList of 6\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([999.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([  2., 999.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([999., 999.], dtype=float32)>\n\n\n\n\nConsuming sets of files\nThere are many datasets distributed as a set of files, where each file is an example.\n\nflowers_root <- get_file(\n  'flower_photos',\n  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n  untar = TRUE\n)\n\nNote: these images are licensed CC-BY, see LICENSE.txt for details.\nThe root directory contains a directory for each class:\n\nfs::dir_ls(flowers_root)\n\n/Users/dfalbel/.keras/datasets/flower_photos/LICENSE.txt\n/Users/dfalbel/.keras/datasets/flower_photos/daisy\n/Users/dfalbel/.keras/datasets/flower_photos/dandelion\n/Users/dfalbel/.keras/datasets/flower_photos/roses\n/Users/dfalbel/.keras/datasets/flower_photos/sunflowers\n/Users/dfalbel/.keras/datasets/flower_photos/tulips\n\n\nThe files in each class directory are examples:\n\nlist_ds <- file_list_dataset(fs::path(flowers_root, \"*\", \"*\"))\nlist_ds %>% \n  dataset_take(5) %>% \n  coro::collect() %>% \n  str()\n\nList of 5\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/dfalbel/.keras/datasets/flower_photos/roses/15738649506_2b4c2fd933_m.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/dfalbel/.keras/datasets/flower_photos/tulips/14487712670_aebe715525_m.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/dfalbel/.keras/datasets/flower_photos/dandelion/18001393975_2a6acaabd8.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/dfalbel/.keras/datasets/flower_photos/dandelion/15644450971_6a28298454_n.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/dfalbel/.keras/datasets/flower_photos/tulips/18270448366_d5676dec64_z.jpg'>\n\n\nRead the data using the tf$io$read_file function and extract the label from the path, returning (image, label) pairs:\n\nprocess_path <- function(file_path) {\n  label <- tf$strings$split(file_path, \"/\")[-2]\n  list(\n    tf$io$read_file(file_path), \n    label\n  )\n}\n\nlabeled_ds <- list_ds %>% \n  dataset_map(process_path)\n\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n\n\n\nel <- labeled_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nel[[1]]$shape\n\nTensorShape([])\n\nel[[2]]\n\ntf.Tensor(b'dandelion', shape=(), dtype=string)"
  },
  {
    "objectID": "guides/data/data.html#batching-dataset-elements",
    "href": "guides/data/data.html#batching-dataset-elements",
    "title": "Build TensorFlow input pipelines",
    "section": "Batching dataset elements",
    "text": "Batching dataset elements\n\nSimple batching\nThe simplest form of batching stacks n consecutive elements of a dataset into a single element. The dataset_batch() transformation does exactly this, with the same constraints as the tf$stack() operator, applied to each component of the elements: ie. for each component i, all elements must have a tensor of the exact same shape.\n\ninc_dataset <- range_dataset(to = 100)\ndec_dataset <- range_dataset(0, -100, -1)\ndataset <- zip_datasets(inc_dataset, dec_dataset)\n\nbatched_dataset <- dataset %>% \n  dataset_batch(4)\n\nbatched_dataset %>% \n  coro::collect(4) %>% \n  str()\n\nList of 4\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([0, 1, 2, 3])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ 0, -1, -2, -3])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([4, 5, 6, 7])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([-4, -5, -6, -7])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ 8,  9, 10, 11])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ -8,  -9, -10, -11])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([12, 13, 14, 15])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([-12, -13, -14, -15])>\n\n\nWhile tfdatasets tries to propagate shape information, the default settings of dataset_batch result in an unknown batch size because the last batch may not be full. Note the NULLs in the shape:\n\nbatched_dataset\n\n<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n\n\nUse the drop_remainder argument to ignore that last batch, and get full shape propagation:\n\nbatched_dataset <- dataset %>% dataset_batch(7, drop_remainder = TRUE)\nbatched_dataset\n\n<BatchDataset element_spec=(TensorSpec(shape=(7,), dtype=tf.int64, name=None), TensorSpec(shape=(7,), dtype=tf.int64, name=None))>\n\n\n\n\nBatching tensors with padding\nThe above recipe works for tensors that all have the same size. However, many models (e.g. sequence models) work with input data that can have varying size (e.g. sequences of different lengths). To handle this case, the dataset_padded_batch() transformation enables you to batch tensors of different shape by specifying one or more dimensions in which they may be padded.\n\ndataset <- range_dataset(to = 100)\ndataset <- dataset %>% \n  dataset_map(function(x) tf$fill(list(tf$cast(x, tf$int32)), x)) %>% \n  dataset_padded_batch(4, padded_shapes = shape(NULL))\n\ndataset %>% \n  coro::collect(2) %>% \n  str()\n\nList of 2\n $ :<tf.Tensor: shape=(4, 3), dtype=int64, numpy=…>\n $ :<tf.Tensor: shape=(4, 7), dtype=int64, numpy=…>\n\n\nThe dataset_padded_batch() transformation allows you to set different padding for each dimension of each component, and it may be variable-length (signified by NULL in the example above) or constant-length. It is also possible to override the padding value, which defaults to 0."
  },
  {
    "objectID": "guides/data/data.html#training-workflows",
    "href": "guides/data/data.html#training-workflows",
    "title": "Build TensorFlow input pipelines",
    "section": "Training workflows",
    "text": "Training workflows\n\nProcessing multiple epochs\nThe tfdatasets API offers two main ways to process multiple epochs of the same data.\nThe simplest way to iterate over a dataset in multiple epochs is to use the dataset_repeat() transformation. First, create a dataset of titanic data:\n\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\ntitanic_lines <- text_line_dataset(titanic_file)\n\n\nplot_batch_sizes <- function(ds) {    \n  batch_sizes <- ds %>% \n    coro::collect() %>% \n    sapply(function(x) as.numeric(x$shape[1]))\n  plot(seq_along(batch_sizes), batch_sizes)\n}\n\nApplying the dataset_repeat() transformation with no arguments will repeat the input indefinitely.\nThe dataset_repeat() transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a dataset_batch() applied after dataset_repeat() will yield batches that straddle epoch boundaries:\n\ntitanic_batches <- titanic_lines %>% \n  dataset_repeat(3) %>% \n  dataset_batch(128)\nplot_batch_sizes(titanic_batches)\n\n\n\n\nIf you need clear epoch separation, put dataset_batch() before the repeat:\n\ntitanic_batches <- titanic_lines %>% \n  dataset_batch(128) %>% \n  dataset_repeat(3)\n\nplot_batch_sizes(titanic_batches)\n\n\n\n\nIf you would like to perform a custom computation (e.g. to collect statistics) at the end of each epoch then it’s simplest to restart the dataset iteration on each epoch:\n\nepochs <- 3\ndataset <- titanic_lines %>% \n  dataset_batch(128)\n\nfor(epoch in seq_len(epochs)) {\n  coro::loop(for(batch in dataset) {\n    print(batch$shape)\n  })\n  cat(\"End of epoch \", epoch, \"\\n\")\n}\n\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  1 \nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  2 \nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  3 \n\n\n\n\nRandomly shuffling input data\nThe dataset_shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\nNote: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill. Consider using dataset_interleave() across files if this becomes a problem.\nAdd an index to the dataset so you can see the effect:\n\nlines <- text_line_dataset(titanic_file)\ncounter <- tf$data$experimental$Counter()\n\ndataset <- zip_datasets(counter, lines)\ndataset <- dataset %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(20)\ndataset\n\n<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n\n\nSince the buffer_size is 100, and the batch size is 20, the first batch contains no elements with an index over 120.\n\nline_batch <- coro::collect(dataset, 1)\nline_batch[[1]][[1]]\n\ntf.Tensor(\n[ 85  99  61  25  78 102  81  11  32  49   0   6  39  87  36  33  44  70\n  56  24], shape=(20), dtype=int64)\n\n\nAs with dataset_batch() the order relative to dataset_repeat() matters.\ndataset_shuffle() doesn’t signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:\n\ndataset <- zip_datasets(counter, lines)\nshuffled <- dataset %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(10) %>% \n  dataset_repeat(2)\n\ncat(\"Here are the item ID's near the epoch boundary:\\n\")\n\nHere are the item ID's near the epoch boundary:\n\nshuffled %>% \n  dataset_skip(60) %>% \n  dataset_take(5) %>% \n  dataset_collect() %>% \n  lapply(function(x) x[[1]])\n\n[[1]]\ntf.Tensor([617 498 594 517 576 611 497 603 580 595], shape=(10), dtype=int64)\n\n[[2]]\ntf.Tensor([499 555 556 453 441 599 346 604 531 622], shape=(10), dtype=int64)\n\n[[3]]\ntf.Tensor([621 424 312 536 614 554 501 533], shape=(8), dtype=int64)\n\n[[4]]\ntf.Tensor([ 8 71 81 39 73 63 30 97 56  0], shape=(10), dtype=int64)\n\n[[5]]\ntf.Tensor([ 49   1 102 104  19  18  89  47 114   6], shape=(10), dtype=int64)\n\n\n\nshuffle_repeat <- shuffled %>% \n  coro::collect() %>% \n  sapply(function(x) mean(as.numeric(x[[1]])))\nplot(shuffle_repeat)\n\n\n\n\nBut a repeat before a shuffle mixes the epoch boundaries together:\n\ndataset <- zip_datasets(counter, lines)\nshuffled <- dataset %>% \n  dataset_repeat(2) %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(10)\n\ncat(\"Here are the item ID's near the epoch boundary:\\n\")\n\nHere are the item ID's near the epoch boundary:\n\nshuffled %>% \n  dataset_skip(55) %>% \n  dataset_take(15) %>% \n  coro::collect() %>% \n  lapply(function(x) x[[1]]) %>% \n  str()\n\nList of 15\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([603, 615, 590, 135, 485, 621, 475, 624, 575, 584])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 14, 552, 497,  11, 614, 548,  37,  18, 535,  28])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([544, 474, 607, 576, 609,  24, 580, 578, 512,  44])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([538, 599,  40,  51,  47, 554,  50, 564, 509,  29])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 61,  59, 561, 522, 534,  33,  36, 581,  17, 533])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([  3, 625, 583, 525,  42,  73, 553, 300,  57, 311])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([559,  70,  46, 460,  75, 573,  22,  85, 600, 598])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 43,  71,  90,   7, 570, 602,  67,  35,  27,  10])>\n  [list output truncated]\n\n\n\nrepeat_shuffle <- shuffled %>% \n  coro::collect() %>% \n  sapply(function(x) mean(as.numeric(x[[1]])))\nplot(repeat_shuffle)"
  },
  {
    "objectID": "guides/data/data.html#preprocessing-data",
    "href": "guides/data/data.html#preprocessing-data",
    "title": "Build TensorFlow input pipelines",
    "section": "Preprocessing data",
    "text": "Preprocessing data\nThe dataset_map(f) transformation produces a new dataset by applying a given function f to each element of the input dataset. It is based on the map() function that is commonly applied to lists (and other structures) in functional programming languages. The function f takes the tf$Tensor objects that represent a single element in the input, and returns the tf$Tensor objects that will represent a single element in the new dataset. Its implementation uses standard TensorFlow operations to transform one element into another.\nThis section covers common examples of how to use dataset_map().\n\nDecoding image data and resizing it\n\nWhen training a neural network on real-world image data, it is often necessary to convert images of different sizes to a common size, so that they may be batched into a fixed size.\nRebuild the flower filenames dataset:\n\nlist_ds <- file_list_dataset(file.path(flowers_root, \"*\", \"*\"))\n\nWrite a function that manipulates the dataset elements.\n\n# Reads an image from a file, decodes it into a dense tensor, and resizes it\n# to a fixed shape.\n\nparse_image <- function(filename) {\n  parts <- tf$strings$split(filename, \"/\")\n  label <- parts[-2]\n\n  image <- tf$io$read_file(filename)\n  image <- tf$io$decode_jpeg(image)\n  image <- tf$image$convert_image_dtype(image, tf$float32)\n  image <- tf$image$resize(image, shape(128, 128))\n  \n  list(image, label)\n}\n\nTest that it works.\n\nparsed <- list_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  parse_image()\n\nshow <- function(parsed, maxcolor=1) {  \n  plot(as.raster(as.array(parsed[[1]]), max = maxcolor))\n  title(as.character(parsed[[2]]))\n}\n\nshow(parsed)\n\n\n\n\nMap it over the dataset.\n\nimages_ds <- list_ds %>% \n  dataset_map(parse_image)\n\nimages_ds %>% \n  dataset_take(2) %>% \n  coro::collect() %>% \n  lapply(show)\n\n\n\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParsing tf$Example protocol buffer messages\nMany input pipelines extract tf$train$Example protocol buffer messages from a TFRecord format. Each tf$train$Example record contains one or more “features”, and the input pipeline typically converts these features into tensors.\n\nfsns_test_file <- get_file(\n  \"fsns.tfrec\", \n  \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\"\n)\ndataset <- tfrecord_dataset(filenames = fsns_test_file)\ndataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n\nYou can work with tf$train$Example protos outside of a tf$data$Dataset to understand the data:\n\nraw_example <- dataset %>% reticulate::as_iterator() %>% reticulate::iter_next()\nparsed <- tf$train$Example$FromString(raw_example$numpy())\n\nfeature <- parsed$features$feature\nraw_img <- feature['image/encoded']$bytes_list$value[0]\nimg <- tf$image$decode_png(raw_img)\nimg %>% \n  as.array() %>% \n  as.raster(max = 255) %>% \n  plot()\n\n\n\n\n\nraw_example <- dataset %>% reticulate::as_iterator() %>% reticulate::iter_next()\n\n\ntf_parse <- function(eg) {\n  example <- tf$io$parse_example(\n      eg[tf$newaxis], \n      reticulate::dict(\n        'image/encoded' = tf$io$FixedLenFeature(shape = shape(), dtype = tf$string),\n        'image/text' = tf$io$FixedLenFeature(shape = shape(), dtype = tf$string)\n      )\n  )\n  out <- list(example[['image/encoded']][1], example[['image/text']][1])\n  out[[1]] <- tf$image$decode_png(out[[1]])\n  out\n}\n\n\nexample <- tf_parse(raw_example)\nprint(as.character(example[[2]]))\n\n[1] \"Rue Perreyon\"\n\nshow(example, maxcolor = 255)\n\n\n\n\n\ndecoded <- dataset %>% \n  dataset_map(tf_parse)\ndecoded\n\n<MapDataset element_spec=(TensorSpec(shape=(None, None, None), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n\n\n\ndecoded %>% \n  dataset_batch(10) %>% \n  coro::collect(1) %>% \n  str()\n\nList of 1\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(10, 150, 600, 3), dtype=uint8, numpy=…>\n  ..$ :<tf.Tensor: shape=(10), dtype=string, numpy=…>\n\n\n\n\nTime series windowing\nFor an end to end time series example see: Time series forecasting. Time series data is often organized with the time axis intact. Use a simple range_dataset() to demonstrate:\n\nrange_ds <- range_dataset(to = 100000)\n\nTypically, models based on this sort of data will want a contiguous time slice. The simplest approach would be to batch the data:\n\nUsing batch\n\nbatches <- range_ds %>% \n  dataset_batch(10, drop_remainder = TRUE)\n\nbatches %>% coro::collect(5) %>% str()\n\nList of 5\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])>\n\n\nOr to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:\n\ndense_1_step <- function(batch) {\n  # Shift features and labels one step relative to each other.\n  list(batch[NULL:-1], batch[2:NULL])\n}\n  \npredict_dense_1_step <- batches %>% dataset_map(dense_1_step)\n\npredict_dense_1_step %>% \n  coro::collect(3)\n\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([10 11 12 13 14 15 16 17 18 19], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([11 12 13 14 15 16 17 18 19], shape=(9), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([20 21 22 23 24 25 26 27 28 29], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([21 22 23 24 25 26 27 28 29], shape=(9), dtype=int64)\n\n\nTo predict a whole window instead of a fixed offset you can split the batches into two parts:\n\nbatches <- range_ds %>% \n  dataset_batch(15, drop_remainder = TRUE)\n\nlabel_next_5_steps <- function(batch) {\n list(\n   batch[NULL:-6],# Inputs: All except the last 5 steps\n   batch[-5:NULL] # Labels: The last 5 steps\n )   \n}\n\npredict_5_steps <- batches %>% dataset_map(label_next_5_steps)\n\npredict_5_steps %>% coro::collect(3)\n\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([10 11 12 13 14], shape=(5), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([15 16 17 18 19 20 21 22 23 24], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([25 26 27 28 29], shape=(5), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([30 31 32 33 34 35 36 37 38 39], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([40 41 42 43 44], shape=(5), dtype=int64)\n\n\nTo allow some overlap between the features of one batch and the labels of another, use zip_datasets():\n\nfeature_length <- 10\nlabel_length <- 3\n\nfeatures <- range_ds %>% \n  dataset_batch(feature_length, drop_remainder = TRUE)\nlabels <- range_ds %>% \n  dataset_batch(feature_length) %>% \n  dataset_skip(1) %>% \n  dataset_map(function(labels) labels[NULL:label_length])\n\npredicted_steps <- zip_datasets(features, labels)\ncoro::collect(predicted_steps, 5)\n\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([10 11 12], shape=(3), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([10 11 12 13 14 15 16 17 18 19], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([20 21 22], shape=(3), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([20 21 22 23 24 25 26 27 28 29], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([30 31 32], shape=(3), dtype=int64)\n\n\n[[4]]\n[[4]][[1]]\ntf.Tensor([30 31 32 33 34 35 36 37 38 39], shape=(10), dtype=int64)\n\n[[4]][[2]]\ntf.Tensor([40 41 42], shape=(3), dtype=int64)\n\n\n[[5]]\n[[5]][[1]]\ntf.Tensor([40 41 42 43 44 45 46 47 48 49], shape=(10), dtype=int64)\n\n[[5]][[2]]\ntf.Tensor([50 51 52], shape=(3), dtype=int64)\n\n\n\n\nUsing window\nWhile using dataset_batch works, there are situations where you may need finer control. The dataset_window() method gives you complete control, but requires some care: it returns a Dataset of Datasets. See Dataset structure for details.\n\nwindow_size <- 5\nwindows <- range_ds %>% \n  dataset_window(window_size, shift = 1)\ncoro::collect(windows, 5)\n\n[[1]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[2]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[3]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[4]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[5]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n\nThe dataset_flat_map method can take a dataset of datasets and flatten it into a single dataset:\n\nwindows %>% \n  dataset_flat_map(function(x) x) %>% \n  dataset_take(30) %>% \n  coro::collect()\n\n[[1]]\ntf.Tensor(0, shape=(), dtype=int64)\n\n[[2]]\ntf.Tensor(1, shape=(), dtype=int64)\n\n[[3]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[4]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[5]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[6]]\ntf.Tensor(1, shape=(), dtype=int64)\n\n[[7]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[8]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[9]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[10]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[11]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[12]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[13]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[14]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[15]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[16]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[17]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[18]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[19]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[20]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[21]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[22]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[23]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[24]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[25]]\ntf.Tensor(8, shape=(), dtype=int64)\n\n[[26]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[27]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[28]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[29]]\ntf.Tensor(8, shape=(), dtype=int64)\n\n[[30]]\ntf.Tensor(9, shape=(), dtype=int64)\n\n\nIn nearly all cases, you will want to .batch the dataset first:\n\nsub_to_batch <- function(sub) {\n  sub %>% \n    dataset_batch(window_size, drop_remainder = TRUE)\n}\n\nwindows %>% \n  dataset_flat_map(sub_to_batch) %>% \n  dataset_take(5) %>% \n  coro::collect()\n\n[[1]]\ntf.Tensor([0 1 2 3 4], shape=(5), dtype=int64)\n\n[[2]]\ntf.Tensor([1 2 3 4 5], shape=(5), dtype=int64)\n\n[[3]]\ntf.Tensor([2 3 4 5 6], shape=(5), dtype=int64)\n\n[[4]]\ntf.Tensor([3 4 5 6 7], shape=(5), dtype=int64)\n\n[[5]]\ntf.Tensor([4 5 6 7 8], shape=(5), dtype=int64)\n\n\nNow, you can see that the shift argument controls how much each window moves over. Putting this together you might write this function:\n\nmake_window_dataset <- function(ds, window_size = 5, shift = 1, stride = 1) {\n  windows <- ds %>% \n    dataset_window(window_size, shift = shift, stride = stride)\n\n  sub_to_batch <- function(sub) {\n    sub %>% \n      dataset_batch(window_size, drop_remainder = TRUE)\n  }\n  \n  windows %>% \n    dataset_flat_map(sub_to_batch)\n}\n\n\nds <- make_window_dataset(range_ds, window_size = 10, shift = 5, stride = 3)\ncoro::collect(ds, 10) %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 5,  8, 11, 14, 17, 20, 23, 26, 29, 32])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([10, 13, 16, 19, 22, 25, 28, 31, 34, 37])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([15, 18, 21, 24, 27, 30, 33, 36, 39, 42])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([20, 23, 26, 29, 32, 35, 38, 41, 44, 47])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([25, 28, 31, 34, 37, 40, 43, 46, 49, 52])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([30, 33, 36, 39, 42, 45, 48, 51, 54, 57])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([35, 38, 41, 44, 47, 50, 53, 56, 59, 62])>\n  [list output truncated]\n\n\nThen it’s easy to extract labels, as before:\n\ndense_labels_ds <- ds %>% \n  dataset_map(dense_1_step)\n\ncoro::collect(dense_labels_ds, 1)\n\n[[1]]\n[[1]][[1]]\ntf.Tensor([ 0  3  6  9 12 15 18 21 24 27], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([ 3  6  9 12 15 18 21 24 27], shape=(9), dtype=int64)\n\n\n\n\n\nResampling\nWhen working with a dataset that is very class-imbalanced, you may want to resample the dataset. tfdatasets provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\nNote: See Imbalanced Data for a full tutorial.\n\nzip_path <- get_file(\n  origin = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n  fname = 'creditcard.zip',\n  extract = TRUE\n)\n\ncsv_path <- gsub(\"zip\", \"csv\", zip_path)\n\n\ncreditcard_ds <- tf$data$experimental$make_csv_dataset(\n  csv_path, \n  batch_size = 1024L, \n  label_name = \"Class\",\n  # Set the column types: 30 floats and an int.\n  column_defaults = c(lapply(seq_len(30), function(x) tf$float32), tf$int64)\n)\n\nNow, check the distribution of classes, it is highly skewed:\n\ncount <- function(counts, batch) {\n  \n  class_1 <- batch[[2]] == 1\n  class_1 <- tf$cast(class_1, tf$int32)\n\n  class_0 <- batch[[2]] == 0\n  class_0 <- tf$cast(class_0, tf$int32)\n\n  counts[['class_0']] <- counts[['class_0']] + tf$reduce_sum(class_0)\n  counts[['class_1']] <- counts[['class_1']] + tf$reduce_sum(class_1)\n\n  counts\n}\n\n\ncounts <- creditcard_ds %>% \n  dataset_take(10) %>% \n  dataset_reduce(\n    initial_state = list('class_0' = 0L, 'class_1' = 0L),\n    reduce_func = count\n  )\n\ncounts\n\n$class_0\ntf.Tensor(10196, shape=(), dtype=int32)\n\n$class_1\ntf.Tensor(44, shape=(), dtype=int32)\n\n\nA common approach to training with an imbalanced dataset is to balance it. tfdatasets includes a few methods which enable this workflow:\n\nDatasets sampling\nOne approach to resampling a dataset is to use sample_from_datasets. This is more applicable when you have a separate data$Dataset for each class.\nHere, just use filter to generate them from the credit card fraud data:\n\nnegative_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_filter(function(features, label) label == 0) %>% \n  dataset_repeat()\n  \npositive_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_filter(function(features, label) label == 1) %>% \n  dataset_repeat()\n\n\npositive_ds %>% \n  coro::collect(1)\n\n[[1]]\n[[1]][[1]]\nDict (30 items)\n\n[[1]][[2]]\ntf.Tensor(1, shape=(), dtype=int64)\n\n\nTo use sample_from_datasets pass the datasets, and the weight for each:\n\nbalanced_ds <- list(negative_ds, positive_ds) %>% \n  sample_from_datasets(c(0.5, 0.5)) %>% \n  dataset_batch(10)\n\nNow the dataset produces examples of each class with 50/50 probability:\n\nbalanced_ds %>% \n  dataset_map(function(x, y) y) %>% \n  coro::collect(10) %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 0, 0, 0, 1, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 1, 1, 0, 1, 0, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 1, 0, 1, 1, 0, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 1, 1, 1, 0, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 1, 0, 0, 0, 1, 0, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 0, 1, 1, 1, 1, 0, 1, 0])>\n  [list output truncated]\n\n\n\n\nRejection resampling\nOne problem with the above sample_from_datasets approach is that it needs a separate TensorFlow Dataset per class. You could use dataset_filter to create those two datasets, but that results in all the data being loaded twice.\nThe dataset_rejection_resample() method can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped from the dataset to achieve balance.\ndataset_rejection_resample() takes a class_func argument. This class_func is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\nThe goal here is to balance the lable distribution, and the elements of creditcard_ds are already (features, label) pairs. So the class_func just needs to return those labels:\n\nclass_func <- function(features, label) { \n  label\n}\n\nThe resampling method deals with individual examples, so in this case you must unbatch the dataset before applying that method.\nThe method needs a target distribution, and optionally an initial distribution estimate as inputs.\n\nresample_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_rejection_resample(\n    class_func, \n    target_dist = c(0.5,0.5),\n    initial_dist = prop.table(sapply(counts, as.numeric))\n  ) %>% \n  dataset_batch(10)\n\nThe dataset_rejection_resample() method returns (class, example) pairs where the class is the output of the class_func. In this case, the example was already a (feature, label) pair, so use map to drop the extra copy of the labels:\n\nbalanced_ds <- resample_ds %>% \n  dataset_map(function(extra_label, features_and_label) features_and_label)\n\nNow the dataset produces examples of each class with 50/50 probability:\n\nbalanced_ds %>% \n  dataset_map(function(feat, label) label) %>% \n  coro::collect(10) %>% \n  str()\n\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 0, 1, 0, 1, 1, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 1, 0, 1, 1, 0, 0, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 0, 0, 1, 0, 1, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 0, 0, 1, 0, 1, 1, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 0, 0, 1, 0, 1, 1, 0, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 1, 1, 0, 1, 1, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0])>\n  [list output truncated]"
  },
  {
    "objectID": "guides/data/data.html#iterator-checkpointing",
    "href": "guides/data/data.html#iterator-checkpointing",
    "title": "Build TensorFlow input pipelines",
    "section": "Iterator Checkpointing",
    "text": "Iterator Checkpointing\nTensorflow supports taking checkpoints so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don’t want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as shuffle and prefetch require buffering elements within the iterator.\nTo include your iterator in a checkpoint, pass the iterator to the tf$train$Checkpoint constructor.\n\nrange_ds <- range_dataset(to = 20)\niterator <- reticulate::as_iterator(range_ds)\nckpt <- tf$train$Checkpoint(step = tf$Variable(0), iterator = iterator)\nmanager <- tf$train$CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep = 3)\n\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\n\nsave_path <- manager$save()\n\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n\nckpt$restore(manager$latest_checkpoint)\n\n<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x7fb91205afd0>\n\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n\n\nNote: It is not possible to checkpoint an iterator which relies on external state such as a tf$py_function. Attempting to do so will raise an exception complaining about the external state."
  },
  {
    "objectID": "guides/data/data.html#using-tfdatasets-with-keras",
    "href": "guides/data/data.html#using-tfdatasets-with-keras",
    "title": "Build TensorFlow input pipelines",
    "section": "Using tfdatasets with Keras",
    "text": "Using tfdatasets with Keras\nThe Keras API simplifies many aspects of creating and executing machine learning models. Its fit() and evaluate() and predict() APIs support datasets as inputs. Here is a quick dataset and model setup:\n\nc(train, test) %<-% dataset_fashion_mnist()\n\n\nfmnist_train_ds <- train %>% \n  tensor_slices_dataset() %>% \n  dataset_map(unname) %>% \n  dataset_shuffle(5000) %>% \n  dataset_batch(32)\n\nmodel <- keras_model_sequential() %>% \n  layer_rescaling(scale = 1/255) %>% \n  layer_flatten() %>% \n  layer_dense(10)\n\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE), \n  metrics = 'accuracy'\n)\n\nPassing a dataset of (feature, label) pairs is all that’s needed for fit() and evaluate():\n\nmodel %>% fit(fmnist_train_ds, epochs = 2)\n\nIf you pass an infinite dataset, for example by calling dataset_repeat(), you just need to also pass the steps_per_epoch argument:\n\nmodel %>% fit(\n  fmnist_train_ds %>% dataset_repeat(), \n  epochs = 2, \n  steps_per_epoch = 20\n)\n\nFor evaluation you can pass the number of evaluation steps:\n\nmodel %>% evaluate(fmnist_train_ds)\n\n     loss  accuracy \n0.4346840 0.8509333 \n\n\nFor long datasets, set the number of steps to evaluate:\n\nmodel %>% evaluate(\n  fmnist_train_ds %>% dataset_repeat(), \n  steps = 10\n)\n\n     loss  accuracy \n0.3874657 0.8562500 \n\n\nThe labels are not required in when calling Model$predict.\n\npredict_ds <- tensor_slices_dataset(train$x) %>% \n  dataset_batch(32)\nresult <- predict(model, predict_ds, steps = 10)\ndim(result)\n\n[1] 320  10\n\n\nBut the labels are ignored if you do pass a dataset containing them:\n\nresult <- predict(model, fmnist_train_ds, steps = 10)\ndim(result)\n\n[1] 320  10"
  },
  {
    "objectID": "guides/data/data.html#environment-details",
    "href": "guides/data/data.html#environment-details",
    "title": "Build TensorFlow input pipelines",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                                     \"Daniels-MBP\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html",
    "href": "guides/keras/customizing_what_happens_in_fit.html",
    "title": "Customizing what happens in fit()",
    "section": "",
    "text": "When you’re doing supervised learning, you can use fit() and everything works smoothly.\nWhen you need to write your own training loop from scratch, you can use the GradientTape and take control of every little detail.\nBut what if you need a custom training algorithm, but you still want to benefit from the convenient features of fit(), such as callbacks, built-in distribution support, or step fusing?\nA core principle of Keras is progressive disclosure of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn’t fall off a cliff if the high-level functionality doesn’t exactly match your use case. You should be able to gain more control over the small details while retaining a commensurate amount of high-level convenience.\nWhen you need to customize what fit() does, you should override the training step function of the Model class. This is the function that is called by fit() for every batch of data. You will then be able to call fit() as usual – and it will be running your own learning algorithm.\nNote that this pattern does not prevent you from building models with the Functional API. You can do this whether you’re building Sequential models, Functional API models, or subclassed models.\nLet’s see how that works."
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "href": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "title": "Customizing what happens in fit()",
    "section": "Setup",
    "text": "Setup\nRequires TensorFlow 2.2 or later.\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "title": "Customizing what happens in fit()",
    "section": "A first simple example",
    "text": "A first simple example\nLet’s start from a simple example:\n\nWe create a new model class by calling new_model_class().\nWe just override the method train_step(data).\nWe return a dictionary mapping metric names (including the loss) to their current value.\n\nThe input argument data is what gets passed to fit as training data:\n\nIf you pass arrays, by calling fit(x, y, ...), then data will be the tuple (x, y)\nIf you pass a tf$data$Dataset, by calling fit(dataset, ...), then data will be what gets yielded by dataset at each batch.\n\nIn the body of the train_step method, we implement a regular training update, similar to what you are already familiar with. Importantly, we compute the loss via self$compiled_loss, which wraps the loss(es) function(s) that were passed to compile().\nSimilarly, we call self$compiled_metrics$update_state(y, y_pred) to update the state of the metrics that were passed in compile(), and we query results from self$metrics at the end to retrieve their current value.\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\nLoaded Tensorflow version 2.9.1\n\n\nLet’s try this out:\n\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "href": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "title": "Customizing what happens in fit()",
    "section": "Going lower-level",
    "text": "Going lower-level\nNaturally, you could just skip passing a loss function in compile(), and instead do everything manually in train_step. Likewise for metrics.\nHere’s a lower-level example, that only uses compile() to configure the optimizer:\n\nWe start by creating Metric instances to track our loss and a MAE score.\nWe implement a custom train_step() that updates the state of these metrics (by calling update_state() on them), then query them (via result()) to return their current average value, to be displayed by the progress bar and to be pass to any callback.\nNote that we would need to call reset_states() on our metrics between each epoch! Otherwise calling result() would return an average since the start of training, whereas we usually work with per-epoch averages. Thankfully, the framework can do that for us: just list any metric you want to reset in the metrics property of the model. The model will call reset_states() on any object listed here at the beginning of each fit() epoch or at the beginning of a call to evaluate().\n\n\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "href": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "title": "Customizing what happens in fit()",
    "section": "Supporting sample_weight & class_weight",
    "text": "Supporting sample_weight & class_weight\nYou may have noticed that our first basic example didn’t make any mention of sample weighting. If you want to support the fit() arguments sample_weight and class_weight, you’d simply do the following:\n\nUnpack sample_weight from the data argument\nPass it to compiled_loss & compiled_metrics (of course, you could also just apply it manually if you don’t rely on compile() for losses & metrics)\nThat’s it. That’s the list.\n\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "href": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "title": "Customizing what happens in fit()",
    "section": "Providing your own evaluation step",
    "text": "Providing your own evaluation step\nWhat if you want to do the same for calls to model$evaluate()? Then you would override test_step in exactly the same way. Here’s what it looks like:\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n\n     loss       mae \n0.6314114 0.6621075"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "title": "Customizing what happens in fit()",
    "section": "Wrapping up: an end-to-end GAN example",
    "text": "Wrapping up: an end-to-end GAN example\nLet’s walk through an end-to-end example that leverages everything you just learned.\nLet’s consider:\n\nA generator network meant to generate 28x28x1 images.\nA discriminator network meant to classify 28x28x1 images into two classes (“fake” and “real”).\nOne optimizer for each.\nA loss function to train the discriminator.\n\n\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n\nHere’s a feature-complete GAN class, overriding compile() to use its own signature, and implementing the entire GAN algorithm in 17 lines in train_step:\n\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n\nLet’s test-drive it:\n\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n\nHappy training!"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#environment-details",
    "href": "guides/keras/customizing_what_happens_in_fit.html#environment-details",
    "title": "Customizing what happens in fit()",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/functional_api.html",
    "href": "guides/keras/functional_api.html",
    "title": "The Functional API",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/functional_api.html#introduction",
    "href": "guides/keras/functional_api.html#introduction",
    "title": "The Functional API",
    "section": "Introduction",
    "text": "Introduction\nThe Keras functional API is a way to create models that are more flexible than the sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\nThe main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\nConsider the following model:\n\n(input: 784-dimensional vectors)\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (10 units, softmax activation)]\n       ↧\n(output: logits of a probability distribution over 10 classes)\n\nThis is a basic graph with three layers. To build this model using the functional API, start by creating an input node:\n\ninputs <- layer_input(shape = c(784))\n\nLoaded Tensorflow version 2.9.1\n\n\nThe shape of the data is set as a 784-dimensional vector. The batch size is always omitted since only the shape of each sample is specified.\nIf, for example, you have an image input with a shape of (32, 32, 3), you would use:\n\n# Just for demonstration purposes.\nimg_inputs <- layer_input(shape = c(32, 32, 3))\n\nThe inputs that is returned contains information about the shape and dtype of the input data that you feed to your model. Here’s the shape:\n\ninputs$shape\n\nTensorShape([None, 784])\n\n\nHere’s the dtype:\n\ninputs$dtype\n\ntf.float32\n\n\nYou create a new node in the graph of layers by calling a layer on this inputs object:\n\ndense <- layer_dense(units = 64, activation = \"relu\")\nx <- dense(inputs)\n\nThe “layer call” action is like drawing an arrow from “inputs” to this layer you created. You’re “passing” the inputs to the dense layer, and you get x as the output.\nYou can also conveniently create the layer and compose it with inputs in one step, like this:\n\nx <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\") \n\nLet’s add a few more layers to the graph of layers:\n\noutputs <- x %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10)\n\nAt this point, you can create a Model by specifying its inputs and outputs in the graph of layers:\n\nmodel <- keras_model(inputs = inputs, outputs = outputs, \n                     name = \"mnist_model\")\n\nLet’s check out what the model summary looks like:\n\nmodel\n\nModel: \"mnist_model\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n input_1 (InputLayer)             [(None, 784)]                 0           \n dense_1 (Dense)                  (None, 64)                    50240       \n dense_3 (Dense)                  (None, 64)                    4160        \n dense_2 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 55,050\nTrainable params: 55,050\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nYou can also plot the model as a graph:\n\nplot(model)\n\n\n\n\nAnd, optionally, display the input and output shapes of each layer in the plotted graph:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nThis figure and the code are almost identical. In the code version, the connection arrows are replaced by %>% operator.\nA “graph of layers” is an intuitive mental image for a deep learning model, and the functional API is a way to create models that closely mirrors this."
  },
  {
    "objectID": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "href": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "title": "The Functional API",
    "section": "Training, evaluation, and inference",
    "text": "Training, evaluation, and inference\nTraining, evaluation, and inference work exactly in the same way for models built using the functional API as for Sequential models.\nThe Model class offers a built-in training loop (the fit() method) and a built-in evaluation loop (the evaluate() method). Note that you can easily customize these loops to implement training routines beyond supervised learning (e.g. GANs).\nHere, load the MNIST image data, reshape it into vectors, fit the model on the data (while monitoring performance on a validation split), then evaluate the model on the test data:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nx_train <- array_reshape(x_train, c(60000, 784)) / 255\nx_test <-  array_reshape(x_test, c(10000, 784)) / 255 \n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_rmsprop(),\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2)\n\ntest_scores <- model %>% evaluate(x_test, y_test, verbose = 2)\nprint(test_scores)\n\n     loss  accuracy \n0.1439689 0.9564000 \n\n\nFor further reading, see the training and evaluation guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#save-and-serialize",
    "href": "guides/keras/functional_api.html#save-and-serialize",
    "title": "The Functional API",
    "section": "Save and serialize",
    "text": "Save and serialize\nSaving the model and serialization work the same way for models built using the functional API as they do for Sequential models. The standard way to save a functional model is to call save_model_tf() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available.\nThis saved file includes the: - model architecture - model weight values (that were learned during training) - model training config, if any (as passed to compile) - optimizer and its state, if any (to restart training where you left off)\n\npath_to_my_model <- tempfile()\nsave_model_tf(model, path_to_my_model)\n\nrm(model)\n# Recreate the exact same model purely from the file:\nmodel <- load_model_tf(path_to_my_model)\n\nFor details, read the model serialization & saving guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "href": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "title": "The Functional API",
    "section": "Use the same graph of layers to define multiple models",
    "text": "Use the same graph of layers to define multiple models\nIn the functional API, models are created by specifying their inputs and outputs in a graph of layers. That means that a single graph of layers can be used to generate multiple models.\nIn the example below, you use the same stack of layers to instantiate two models: an encoder model that turns image inputs into 16-dimensional vectors, and an end-to-end autoencoder model for training.\n\nencoder_input <- layer_input(shape = c(28, 28, 1), \n                             name = \"img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, \n                       name = \"encoder\")\nencoder\n\nModel: \"encoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMaxP  (None, 16)                   0           \n ooling2D)                                                                  \n============================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n____________________________________________________________________________\n\ndecoder_output <- encoder_output %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\nautoencoder <- keras_model(encoder_input, decoder_output, \n                           name = \"autoencoder\")\nautoencoder\n\nModel: \"autoencoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMaxP  (None, 16)                   0           \n ooling2D)                                                                  \n reshape (Reshape)                (None, 4, 4, 1)               0           \n conv2d_transpose_3 (Conv2DTransp  (None, 6, 6, 16)             160         \n ose)                                                                       \n conv2d_transpose_2 (Conv2DTransp  (None, 8, 8, 32)             4640        \n ose)                                                                       \n up_sampling2d (UpSampling2D)     (None, 24, 24, 32)            0           \n conv2d_transpose_1 (Conv2DTransp  (None, 26, 26, 16)           4624        \n ose)                                                                       \n conv2d_transpose (Conv2DTranspos  (None, 28, 28, 1)            145         \n e)                                                                         \n============================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nHere, the decoding architecture is strictly symmetrical to the encoding architecture, so the output shape is the same as the input shape (28, 28, 1).\nThe reverse of a Conv2D layer is a Conv2DTranspose layer, and the reverse of a MaxPooling2D layer is an UpSampling2D layer."
  },
  {
    "objectID": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "href": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "title": "The Functional API",
    "section": "All models are callable, just like layers",
    "text": "All models are callable, just like layers\nYou can treat any model as if it were a layer by invoking it on an Input or on the output of another layer. By calling a model you aren’t just reusing the architecture of the model, you’re also reusing its weights.\nTo see this in action, here’s a different take on the autoencoder example that creates an encoder model, a decoder model, and chains them in two calls to obtain the autoencoder model:\n\nencoder_input <- layer_input(shape = c(28, 28, 1), name = \"original_img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, name = \"encoder\")\nencoder\n\nModel: \"encoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n original_img (InputLayer)        [(None, 28, 28, 1)]           0           \n conv2d_7 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_6 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d_1 (MaxPooling2D)   (None, 8, 8, 32)              0           \n conv2d_5 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d_4 (Conv2D)                (None, 4, 4, 16)              4624        \n global_max_pooling2d_1 (GlobalMa  (None, 16)                   0           \n xPooling2D)                                                                \n============================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n____________________________________________________________________________\n\ndecoder_input <- layer_input(shape = c(16), name = \"encoded_img\")\ndecoder_output <- decoder_input %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\ndecoder <- keras_model(decoder_input, decoder_output, \n                       name = \"decoder\")\ndecoder\n\nModel: \"decoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n encoded_img (InputLayer)         [(None, 16)]                  0           \n reshape_1 (Reshape)              (None, 4, 4, 1)               0           \n conv2d_transpose_7 (Conv2DTransp  (None, 6, 6, 16)             160         \n ose)                                                                       \n conv2d_transpose_6 (Conv2DTransp  (None, 8, 8, 32)             4640        \n ose)                                                                       \n up_sampling2d_1 (UpSampling2D)   (None, 24, 24, 32)            0           \n conv2d_transpose_5 (Conv2DTransp  (None, 26, 26, 16)           4624        \n ose)                                                                       \n conv2d_transpose_4 (Conv2DTransp  (None, 28, 28, 1)            145         \n ose)                                                                       \n============================================================================\nTotal params: 9,569\nTrainable params: 9,569\nNon-trainable params: 0\n____________________________________________________________________________\n\nautoencoder_input <- layer_input(shape = c(28, 28, 1), name = \"img\")\nencoded_img <- encoder(autoencoder_input)\ndecoded_img <- decoder(encoded_img)\nautoencoder <- keras_model(autoencoder_input, decoded_img, \n                           name = \"autoencoder\")\nautoencoder\n\nModel: \"autoencoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n encoder (Functional)             (None, 16)                    18672       \n decoder (Functional)             (None, 28, 28, 1)             9569        \n============================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nAs you can see, the model can be nested: a model can contain sub-models (since a model is just like a layer). A common use case for model nesting is ensembling. For example, here’s how to ensemble a set of models into a single model that averages their predictions:\n\nget_model <- function() {\n  inputs <- layer_input(shape = c(128))\n  outputs <- inputs %>% layer_dense(1)\n  keras_model(inputs, outputs)\n}\n\nmodel1 <- get_model()\nmodel2 <- get_model()\nmodel3 <- get_model()\n\ninputs <- layer_input(shape = c(128))\ny1 <- model1(inputs)\ny2 <- model2(inputs)\ny3 <- model3(inputs)\noutputs <- layer_average(list(y1, y2, y3))\nensemble_model <- keras_model(inputs = inputs, outputs = outputs)"
  },
  {
    "objectID": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "href": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "title": "The Functional API",
    "section": "Manipulate complex graph topologies",
    "text": "Manipulate complex graph topologies\n\nModels with multiple inputs and outputs\nThe functional API makes it easy to manipulate multiple inputs and outputs. This cannot be handled with the Sequential API.\nFor example, if you’re building a system for ranking customer issue tickets by priority and routing them to the correct department, then the model will have three inputs:\n\nthe title of the ticket (text input),\nthe text body of the ticket (text input), and\nany tags added by the user (categorical input)\n\nThis model will have two outputs:\n\nthe priority score between 0 and 1 (scalar sigmoid output), and\nthe department that should handle the ticket (softmax output over the set of departments).\n\nYou can build this model in a few lines with the functional API:\n\nnum_tags <- 12  # Number of unique issue tags\nnum_words <- 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments <- 4  # Number of departments for predictions\n\ntitle_input <- layer_input(shape = c(NA), name = \"title\")  # Variable-length sequence of ints\nbody_input <- layer_input(shape = c(NA), name = \"body\")  # Variable-length sequence of ints\ntags_input <- layer_input(shape = c(num_tags), name = \"tags\")  # Binary vectors of size `num_tags`\n\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features <- title_input %>% layer_embedding(num_words, 64)\n\n# Embed each word in the text into a 64-dimensional vector\nbody_features <- body_input %>% layer_embedding(num_words, 64)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features <- title_features %>% layer_lstm(128)\n\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features <- body_features %>% layer_lstm(32)\n\n# Merge all available features into a single large vector via concatenation\nx <- layer_concatenate(list(title_features, body_features, tags_input))\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred <- x %>% layer_dense(1, name = \"priority\")\n\n# Stick a department classifier on top of the features\ndepartment_pred <- x %>% layer_dense(num_departments, name = \"department\")\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel <- keras_model(\n  inputs <- list(title_input, body_input, tags_input),\n  outputs <- list(priority_pred, department_pred)\n)\n\nNow plot the model:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nWhen compiling this model, you can assign different losses to each output. You can even assign different weights to each loss – to modulate their contribution to the total training loss.\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_binary_crossentropy(from_logits = TRUE),\n    loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights <- c(1, 0.2)\n)\n\nSince the output layers have different names, you could also specify the losses and loss weights with the corresponding layer names:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    priority = loss_binary_crossentropy(from_logits = TRUE),\n    department = loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights = c(priority =  1.0, department = 0.2),\n)\n\nTrain the model by passing lists of NumPy arrays of inputs and targets:\n\n# some helpers to generate dummy input data\nrandom_uniform_array <- function(dim) \n  array(runif(prod(dim)), dim)\n\nrandom_vectorized_array <- function(num_words, dim)\n  array(sample(0:(num_words - 1), prod(dim), replace = TRUE), dim)\n\n# Dummy input data\ntitle_data <- random_vectorized_array(num_words, c(1280, 10))\nbody_data <- random_vectorized_array(num_words, c(1280, 100))\ntags_data <- random_vectorized_array(2, c(1280, num_tags))\n# storage.mode(tags_data) <- \"double\" # from integer\n\n# Dummy target data\npriority_targets <- random_uniform_array(c(1280, 1))\ndept_targets <- random_vectorized_array(2, c(1280, num_departments))\n\nmodel %>% fit(\n  list(title = title_data, body = body_data, tags = tags_data),\n  list(priority = priority_targets, department = dept_targets),\n  epochs = 2,\n  batch_size = 32\n)\n\nWhen calling fit with a tfdataset object, it should yield either a tuple of lists like tuple(list(title_data, body_data, tags_data), list(priority_targets, dept_targets)) or a tuple of named lists like tuple(list(title = title_data, body = body_data, tags = tags_data), list(priority= priority_targets, department= dept_targets)).\nFor more detailed explanation, refer to the training and evaluation guide.\n\n\nA toy ResNet model\nIn addition to models with multiple inputs and outputs, the functional API makes it easy to manipulate non-linear connectivity topologies – these are models with layers that are not connected sequentially, which the Sequential API cannot handle.\nA common use case for this is residual connections. Let’s build a toy ResNet model for CIFAR10 to demonstrate this:\n\ninputs <- layer_input(shape = c(32, 32, 3), name = \"img\")\nblock_1_output <- inputs %>% \n  layer_conv_2d(32, 3, activation = \"relu\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\") %>% \n  layer_max_pooling_2d(3)\n\nblock_2_output <- block_1_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_1_output)\n\nblock_3_output <- block_2_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_2_output) \n\noutputs <- block_3_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\") %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(10)\n\nmodel <- keras_model(inputs, outputs, name = \"toy_resnet\")\nmodel\n\nModel: \"toy_resnet\"\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n img (InputLayer)        [(None, 32, 32  0        []                        \n                         , 3)]                                              \n conv2d_9 (Conv2D)       (None, 30, 30,  896      ['img[0][0]']             \n                          32)                                               \n conv2d_8 (Conv2D)       (None, 28, 28,  18496    ['conv2d_9[0][0]']        \n                          64)                                               \n max_pooling2d_2 (MaxPoo  (None, 9, 9, 6  0       ['conv2d_8[0][0]']        \n ling2D)                 4)                                                 \n conv2d_11 (Conv2D)      (None, 9, 9, 6  36928    ['max_pooling2d_2[0][0]'] \n                         4)                                                 \n conv2d_10 (Conv2D)      (None, 9, 9, 6  36928    ['conv2d_11[0][0]']       \n                         4)                                                 \n add (Add)               (None, 9, 9, 6  0        ['conv2d_10[0][0]',       \n                         4)                        'max_pooling2d_2[0][0]'] \n conv2d_13 (Conv2D)      (None, 9, 9, 6  36928    ['add[0][0]']             \n                         4)                                                 \n conv2d_12 (Conv2D)      (None, 9, 9, 6  36928    ['conv2d_13[0][0]']       \n                         4)                                                 \n add_1 (Add)             (None, 9, 9, 6  0        ['conv2d_12[0][0]',       \n                         4)                        'add[0][0]']             \n conv2d_14 (Conv2D)      (None, 7, 7, 6  36928    ['add_1[0][0]']           \n                         4)                                                 \n global_average_pooling2  (None, 64)     0        ['conv2d_14[0][0]']       \n d (GlobalAveragePooling                                                    \n 2D)                                                                        \n dense_8 (Dense)         (None, 256)     16640    ['global_average_pooling2d\n                                                  [0][0]']                  \n dropout (Dropout)       (None, 256)     0        ['dense_8[0][0]']         \n dense_7 (Dense)         (None, 10)      2570     ['dropout[0][0]']         \n============================================================================\nTotal params: 223,242\nTrainable params: 223,242\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nPlot the model:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nNow train the model:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()  \n\nx_train <- x_train / 255\nx_test <- x_test / 255\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = loss_categorical_crossentropy(from_logits = TRUE),\n  metrics = \"acc\"\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# for this guide. Try to train on the entire dataset until convergence!\nmodel %>% fit(\n  x_train[1:1000, , , ],\n  y_train[1:1000, ],\n  batch_size = 64,\n  epochs = 1,\n  validation_split = 0.2\n)"
  },
  {
    "objectID": "guides/keras/functional_api.html#shared-layers",
    "href": "guides/keras/functional_api.html#shared-layers",
    "title": "The Functional API",
    "section": "Shared layers",
    "text": "Shared layers\nAnother good use for the functional API are models that use shared layers. Shared layers are layer instances that are reused multiple times in the same model – they learn features that correspond to multiple paths in the graph-of-layers.\nShared layers are often used to encode inputs from similar spaces (say, two different pieces of text that feature similar vocabulary). They enable sharing of information across these different inputs, and they make it possible to train such a model on less data. If a given word is seen in one of the inputs, that will benefit the processing of all inputs that pass through the shared layer.\nTo share a layer in the functional API, call the same layer instance multiple times. For instance, here’s an Embedding layer shared across two different text inputs:\n\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)\n\n# Variable-length sequence of integers\ntext_input_a <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Variable-length sequence of integers\ntext_input_b <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a <- shared_embedding(text_input_a)\nencoded_input_b <- shared_embedding(text_input_b)"
  },
  {
    "objectID": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "href": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "title": "The Functional API",
    "section": "Extract and reuse nodes in the graph of layers",
    "text": "Extract and reuse nodes in the graph of layers\nBecause the graph of layers you are manipulating is a static data structure, it can be accessed and inspected. And this is how you are able to plot functional models as images.\nThis also means that you can access the activations of intermediate layers (“nodes” in the graph) and reuse them elsewhere – which is very useful for something like feature extraction.\nLet’s look at an example. This is a VGG19 model with weights pretrained on ImageNet:\n\nvgg19 <- application_vgg19()\n\nAnd these are the intermediate activations of the model, obtained by querying the graph data structure:\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\n\nUse these features to create a new feature-extraction model that returns the values of the intermediate layer activations:\n\nfeat_extraction_model <-  keras_model(inputs = vgg19$input, \n                                      outputs = features_list)\n\nimg <- random_uniform_array(c(1, 224, 224, 3))\nextracted_features <- feat_extraction_model(img)\n\nThis comes in handy for tasks like neural style transfer, among other things."
  },
  {
    "objectID": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "href": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "title": "The Functional API",
    "section": "Extend the API using custom layers",
    "text": "Extend the API using custom layers\ntf$keras includes a wide range of built-in layers, for example:\n\nConvolutional layers: Conv1D, Conv2D, Conv3D, Conv2DTranspose\nPooling layers: MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D\nRNN layers: GRU, LSTM, ConvLSTM2D\nBatchNormalization, Dropout, Embedding, etc.\n\nBut if you don’t find what you need, it’s easy to extend the API by creating your own layers. All layers subclass the Layer class and implement:\n\ncall method, that specifies the computation done by the layer.\nbuild method, that creates the weights of the layer (this is just a style convention since you can create weights in __init__, as well).\n\nTo learn more about creating layers from scratch, read custom layers and models guide.\nThe following is a basic implementation of layer_dense():\n\nlibrary(tensorflow)\nlibrary(keras)\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units = as.integer(units)\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\n\nFor serialization support in your custom layer, define a get_config method that returns the constructor arguments of the layer instance:\n\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units <- as.integer(units)\n  },\n  \n  build = function(input_shape) {\n    self$w <-\n      self$add_weight(\n        shape = shape(tail(input_shape, 1), self$units),\n        initializer = \"random_normal\",\n        trainable = TRUE\n      )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  \n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  \n  get_config = function() {\n    list(units = self$units)\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\nconfig <- model %>% get_config()\n\nnew_model <- from_config(config, custom_objects = list(layer_custom_dense))\n\nOptionally, implement the class method from_config(class_constructor, config) which is used when recreating a layer instance given its config. The default implementation of from_config is approximately:\n\nfrom_config <- function(layer_constructor, config) \n  do.call(layer_constructor, config)"
  },
  {
    "objectID": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "href": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "title": "The Functional API",
    "section": "When to use the functional API",
    "text": "When to use the functional API\nShould you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier and safer, and has a number of features that subclassed models do not support.\nHowever, model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass Model directly.\nFor an in-depth look at the differences between the functional API and model subclassing, read What are Symbolic and Imperative APIs in TensorFlow 2.0?.\n\nFunctional API strengths:\nThe following properties are also true for Sequential models (which are also data structures), but are not true for subclassed models (which are R code, not data structures).\n\nLess verbose\nThere is no super$initialize(...), no call <- function(...) {   }, etc.\nCompare:\n\ninputs <- layer_input(shape = c(32))\noutputs <- inputs %>% \n  layer_dense(64, activation = 'relu') %>% \n  layer_dense(10)\nmlp <- keras_model(inputs, outputs)\n\nWith the subclassed version:\n\nMLP <- new_model_class(\n  classname = \"MLP\",\n  \n  initialize = function(...) {\n    super$initialize(...)\n    self$dense_1 <- layer_dense(units = 64, activation = 'relu')\n    self$dense_2 <- layer_dense(units = 10)\n  },\n  \n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2()\n  }\n)\n\n# Instantiate the model.\nmlp <- MLP()\n\n# Necessary to create the model's state.\n# The model doesn't have a state until it's called at least once.\ninvisible(mlp(tf$zeros(shape(1, 32))))\n\n\n\nModel validation while defining its connectivity graph\nIn the functional API, the input specification (shape and dtype) is created in advance (using layer_input). Every time you call a layer, the layer checks that the specification passed to it matches its assumptions, and it will raise a helpful error message if not.\nThis guarantees that any model you can build with the functional API will run. All debugging – other than convergence-related debugging – happens statically during the model construction and not at execution time. This is similar to type checking in a compiler.\n\n\nA functional model is plottable and inspectable\nYou can plot the model as a graph, and you can easily access intermediate nodes in this graph. For example, to extract and reuse the activations of intermediate layers (as seen in a previous example):\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\nfeat_extraction_model <- keras_model(inputs = vgg19$input,\n                                     outputs = features_list)\n\n\n\nA functional model can be serialized or cloned\nBecause a functional model is a data structure rather than a piece of code, it is safely serializable and can be saved as a single file that allows you to recreate the exact same model without having access to any of the original code. See the serialization & saving guide.\nTo serialize a subclassed model, it is necessary for the implementer to specify a get_config() and from_config() method at the model level.\n\n\n\nFunctional API weakness:\n\nIt does not support dynamic architectures\nThe functional API treats models as DAGs of layers. This is true for most deep learning architectures, but not all – for example, recursive networks or Tree RNNs do not follow this assumption and cannot be implemented in the functional API."
  },
  {
    "objectID": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "href": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "title": "The Functional API",
    "section": "Mix-and-match API styles",
    "text": "Mix-and-match API styles\nChoosing between the functional API or Model subclassing isn’t a binary decision that restricts you into one category of models. All models in the tf$keras API can interact with each other, whether they’re Sequential models, functional models, or subclassed models that are written from scratch.\nYou can always use a functional model or Sequential model as part of a subclassed model or layer:\n\nunits <- 32L\ntimesteps <- 10L\ninput_dim <- 5L\n\n# Define a Functional model\n\ninputs <- layer_input(c(NA, units))\noutputs <- inputs %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\n\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <-\n      layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <-\n      layer_dense(units = units, activation = \"tanh\")\n    # Our previously-defined Functional model\n    self$classifier <- model\n  },\n  \n  call = function(inputs) {\n    message(\"inputs shape: \", format(inputs$shape))\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis is 1-based\n    message(\"features shape: \", format(features$shape))\n    self$classifier(features)\n  }\n)\n\nlayer_custom_rnn(tf$zeros(shape(1, timesteps, input_dim)))\n\ninputs shape: (1, 10, 5)\n\n\nfeatures shape: (1, 10, 32)\n\n\nYou can use any subclassed layer or model in the functional API as long as it implements a call method that follows one of the following patterns:\n\ncall(inputs, ..., training = NULL, mask = NULL) – Where inputs is a tensor or a nested structure of tensors (e.g. a list of tensors), and where optional named arguments training and mask can be present.\nare non-tensor arguments (non-inputs).\ncall(self, inputs, training = NULL, **kwargs) – Where training is a boolean indicating whether the layer should behave in training mode and inference mode.\ncall(self, inputs, mask = NULL, **kwargs) – Where mask is a boolean mask tensor (useful for RNNs, for instance).\ncall(self, inputs, training = NULL, mask = NULL, **kwargs) – Of course, you can have both masking and training-specific behavior at the same time.\n\nAdditionally, if you implement the get_config method on your custom Layer or model, the functional models you create will still be serializable and cloneable.\nHere’s a quick example of a custom RNN, written from scratch, being used in a functional model:\n\nunits <- 32 \ntimesteps <- 10 \ninput_dim <- 5 \nbatch_size <- 16\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units  \n    self$projection_1 <- layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <- layer_dense(units = units, activation = \"tanh\")\n    self$classifier <- layer_dense(units = 1)\n  },\n  \n  call = function(inputs) {\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis arg is 1-based\n    self$classifier(features)\n  }\n)\n    \n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs <- layer_input(batch_shape = c(batch_size, timesteps, input_dim))\noutputs <- inputs %>% \n  layer_conv_1d(32, 3) %>% \n  layer_custom_rnn()\n\nmodel <- keras_model(inputs, outputs)\nmodel(tf$zeros(shape(1, 10, 5)))\n\ntf.Tensor(\n[[[0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]]], shape=(1, 8, 1), dtype=float32)"
  },
  {
    "objectID": "guides/keras/functional_api.html#environment-details",
    "href": "guides/keras/functional_api.html#environment-details",
    "title": "The Functional API",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "",
    "text": "library(magrittr)\n\nWarning: package 'magrittr' was built under R version 4.1.2\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n\nLoaded Tensorflow version 2.9.1\n\n\n[1] '2.9'"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Layer class: a combination of state (weights) and some computation",
    "text": "The Layer class: a combination of state (weights) and some computation\nOne of the central abstractions in Keras is the Layer class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).\nHere’s a densely-connected layer. It has a state: the variables w and b.\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \"float32\"\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \"float32\"),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nYou would use a layer by calling it on some tensor input(s), much like a regular function.\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[-0.00647149 -0.05578769  0.03210767  0.07069797]\n [-0.00647149 -0.05578769  0.03210767  0.07069797]], shape=(2, 4), dtype=float32)\n\n\nLinear behaves similarly to a layer present in the Python interface to keras (e.g., keras$layers$Dense).\nHowever, one additional step is needed to make it behave like the builtin layers present in the keras R package (e.g., layer_dense()).\nKeras layers in R are designed to compose nicely with the pipe operator (%>%), so that the layer instance is conveniently created on demand when an existing model or tensor is piped in. In order to make a custom layer similarly compose nicely with the pipe, you can call create_layer_wrapper() on the layer class constructor.\n\nlayer_linear <- create_layer_wrapper(Linear)\n\nNow layer_linear is a layer constructor that composes nicely with %>%, just like the built-in layers:\n\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n\ntf.Tensor(\n[[-0.03112034  0.06114008  0.09245743  0.0617287 ]\n [-0.03112034  0.06114008  0.09245743  0.0617287 ]], shape=(2, 4), dtype=float32)\n\nmodel\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n linear_1 (Linear)                (2, 4)                        12          \n============================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nBecause the pattern above is so common, there is a convenience function that combines the steps of subclassing keras$layers$Layer and calling create_layer_wrapper on the output: the Layer function. The layer_linear defined below is identical to the layer_linear defined above.\n\nlayer_linear <- Layer(\n  \"Linear\",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\nFor the remainder of this vignette we’ll be using the %py_class% constructor. However, in your own code feel free to use create_layer_wrapper and/or Layer if you prefer.\nNote that the weights w and b are automatically tracked by the layer upon being set as layer attributes:\n\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n\nYou also have access to a quicker shortcut for adding a weight to a layer: the add_weight() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[-0.04482888 -0.07073172  0.01847215  0.08916104]\n [-0.04482888 -0.07073172  0.01847215  0.08916104]], shape=(2, 4), dtype=float32)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers can have non-trainable weights",
    "text": "Layers can have non-trainable weights\nBesides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\nHere’s how to add and use a non-trainable weight:\n\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 2 2\n\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 4 4\n\n\nIt’s part of layer$weights, but it gets categorized as a non-trainable weight:\n\ncat(\"weights:\", length(my_sum$weights), \"\\n\")\n\nweights: 1 \n\ncat(\"non-trainable weights:\", length(my_sum$non_trainable_weights), \"\\n\")\n\nnon-trainable weights: 1 \n\n# It's not included in the trainable weights:\ncat(\"trainable_weights:\", my_sum$trainable_weights, \"\\n\")\n\ntrainable_weights:"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Best practice: deferring weight creation until the shape of the inputs is known",
    "text": "Best practice: deferring weight creation until the shape of the inputs is known\nOur Linear layer above took an input_dimargument that was used to compute the shape of the weights w and b in initialize():\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nIn many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\nIn the Keras API, we recommend creating layer weights in the build(self, inputs_shape) method of your layer. Like this:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nThe build() method of your layer will automatically run the first time your layer instance is called. You now have a layer that can handle an arbitrary number of input features:\n\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n\nImplementing build() separately as shown above nicely separates creating weights only once from using weights in every call. However, for some advanced custom layers, it can become impractical to separate the state creation and computation. Layer implementers are allowed to defer weight creation to the first call(), but need to take care that later calls use the same weights. In addition, since call() is likely to be executed for the first time inside a tf_function(), any variable creation that takes place in call() should be wrapped in a tf$init_scope()."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers are recursively composable",
    "text": "Layers are recursively composable\nIf you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer.\nWe recommend creating such sublayers in the initialize() method and leave it to the first call() to trigger building their weights.\n\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\ncat(\"weights:\", length(mlp$weights), \"\\n\")\n\nweights: 6 \n\ncat(\"trainable weights:\", length(mlp$trainable_weights), \"\\n\")\n\ntrainable weights: 6"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_loss() method",
    "text": "The add_loss() method\nWhen writing the call() method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling self$add_loss(value):\n\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n\nThese losses (including those created by any inner layer) can be retrieved via layer$losses. This property is reset at the start of every call() to the top-level layer, so that layer$losses always contains the loss values created during the last forward pass.\n\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n\nIn addition, the loss property also contains regularization losses created for the weights of any inner layer:\n\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n\n[[1]]\ntf.Tensor(0.0019065848, shape=(), dtype=float32)\n\n\nThese losses are meant to be taken into account when writing training loops, like this:\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n\nFor a detailed guide about writing training loops, see the guide to writing a training loop from scratch.\nThese losses also work seamlessly with fit() (they get automatically summed and added to the main loss, if any):\n\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \"adam\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_metric() method",
    "text": "The add_metric() method\nSimilarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training.\nConsider the following layer: a “logistic endpoint” layer. It takes as inputs predictions and targets, it computes a loss which it tracks via add_loss(), and it computes an accuracy scalar, which it tracks via add_metric().\n\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n\nMetrics tracked in this way are accessible via layer$metrics:\n\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\"layer$metrics: \")\n\nlayer$metrics: \n\nstr(layer$metrics)\n\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n\ncat(\"current accuracy value:\", as.numeric(layer$metrics[[1]]$result()), \"\\n\")\n\ncurrent accuracy value: 1 \n\n\nJust like for add_loss(), these metrics are tracked by fit():\n\ninputs <- layer_input(shape(3), name = \"inputs\")\ntargets <- layer_input(shape(10), name = \"targets\")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "You can optionally enable serialization on your layers",
    "text": "You can optionally enable serialization on your layers\nIf you need your custom layers to be serializable as part of a Functional model, you can optionally implement a get_config() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n\n$units\n[1] 64\n\nnew_layer <- Linear$from_config(config)\n\nNote that the initialize() method of the base Layer class takes some additional named arguments, in particular a name and a dtype. It’s good practice to pass these arguments to the parent class in initialize() and to include them in the layer config:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n\nList of 4\n $ name     : chr \"linear_9\"\n $ trainable: logi TRUE\n $ dtype    : chr \"float32\"\n $ units    : num 64\n\nnew_layer <- Linear$from_config(config)\n\nIf you need more flexibility when deserializing the layer from its config, you can also override the from_config() class method. This is the base implementation of from_config():\n\nfrom_config <- function(cls, config) do.call(cls, config)\n\nTo learn more about serialization and saving, see the complete guide to saving and serializing models."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged training argument in the call() method",
    "text": "Privileged training argument in the call() method\nSome layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call() method.\nBy exposing this argument in call(), you enable the built-in training and evaluation loops (e.g. fit()) to correctly use the layer in training and inference. Note, the default of NULL means that the training parameter will be inferred by keras from the training context (e.g., it will be TRUE if called from fit(), FALSE if called from predict())\n\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged mask argument in the call() method",
    "text": "Privileged mask argument in the call() method\nThe other privileged argument supported by call() is the mask argument.\nYou will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\nKeras will automatically pass the correct mask argument to call() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\nTo learn more about masking and how to write masking-enabled layers, please check out the guide “understanding padding and masking”."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Model class",
    "text": "The Model class\nIn general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model – the object you will train.\nFor instance, in a ResNet50 model, you would have several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.\nThe Model class has the same API as Layer, with the following differences:\n\nIt has support for built-in training, evaluation, and prediction methods (fit(), evaluate(), predict()).\nIt exposes the list of its inner layers, via the model$layers property.\nIt exposes saving and serialization APIs (save_model_tf(), save_model_weights_tf(), …)\n\nEffectively, the Layer class corresponds to what we refer to in the literature as a “layer” (as in “convolution layer” or “recurrent layer”) or as a “block” (as in “ResNet block” or “Inception block”).\nMeanwhile, the Model class corresponds to what is referred to in the literature as a “model” (as in “deep learning model”) or as a “network” (as in “deep neural network”).\nSo if you’re wondering, “should I use the Layer class or the Model class?”, ask yourself: will I need to call fit() on it? Will I need to call save() on it? If so, go with Model. If not (either because your class is just a block in a bigger system, or because you are writing training & saving code yourself), use Layer.\nFor instance, we could take our mini-resnet example above, and use it to build a Model that we could train with fit(), and that we could save with save_model_weights_tf():\n\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Putting it all together: an end-to-end example",
    "text": "Putting it all together: an end-to-end example\nHere’s what you’ve learned so far:\n\nA Layer encapsulates a state (created in initialize() or build()), and some computation (defined in call()).\nLayers can be recursively nested to create new, bigger computation blocks.\nLayers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\nThe outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n\nLet’s put all of these things together into an end-to-end example: we’re going to implement a Variational AutoEncoder (VAE). We’ll train it on MNIST digits.\nOur VAE will be a subclass of Model, built as a nested composition of layers that subclass Layer. It will feature a regularization loss (KL divergence).\n\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \"encoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \"Converts z, the encoded digit vector, back into a readable digit.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \"decoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \"Combines the encoder and decoder into an end-to-end model for training.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \"autoencoder\", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n\nLet’s write a simple training loop on MNIST:\n\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\"Start of epoch %d\\n\", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\"step %d: mean loss = %.4f\\n\", step, loss_metric$result()))\n      }\n    }\n  })\n}\n\nStart of epoch 1\nstep 100: mean loss = 0.1279\nstep 200: mean loss = 0.1003\nstep 300: mean loss = 0.0899\nstep 400: mean loss = 0.0848\nstep 500: mean loss = 0.0813\nstep 600: mean loss = 0.0791\nstep 700: mean loss = 0.0774\nstep 800: mean loss = 0.0762\nstep 900: mean loss = 0.0752\nStart of epoch 2\nstep 100: mean loss = 0.0742\nstep 200: mean loss = 0.0737\nstep 300: mean loss = 0.0732\nstep 400: mean loss = 0.0728\nstep 500: mean loss = 0.0724\nstep 600: mean loss = 0.0721\nstep 700: mean loss = 0.0718\nstep 800: mean loss = 0.0716\nstep 900: mean loss = 0.0713\n\n\nNote that since the VAE is subclassing Model, it features built-in training loops. So you could also have trained it like this:\n\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Beyond object-oriented development: the Functional API",
    "text": "Beyond object-oriented development: the Functional API\nIf you prefer a less object-oriented way of programming, you can also build models using the Functional API. Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\nFor instance, the Functional API example below reuses the same Sampling layer we defined in the example above:\n\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \"encoder_input\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \"z_mean\")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \"z_log_var\")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \"encoder\")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \"z_sampling\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \"sigmoid\")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \"decoder\")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \"vae\")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n\nFor more information, make sure to read the Functional API guide."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Defining custom layers and models in an R package",
    "text": "Defining custom layers and models in an R package\nUnfortunately you can’t use anything that creates references to Python objects, at the top-level of an R package.\nHere is why: when you build an R package, all the R files in the R/ directory get sourced in an R environment (the package namespace), and then that environment is saved as part of the package bundle. Loading the package means restoring the saved R environment. This means that the R code only gets sourced once, at build time. If you create references to external objects (e.g., Python objects) at package build time, they will be NULL pointers when the package is loaded, because the external objects they pointed to at build time no longer exist at load time.\nThe solution is to delay creating references to Python objects until run time. Fortunately, %py_class%, Layer(), and create_layer_wrapper(R6Class(...)) are all lazy about initializing the Python reference, so they are safe to define and export in an R package.\nIf you’re writing an R package that uses keras and reticulate, this article might be helpful to read over."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Summary",
    "text": "Summary\nIn this guide you learned about creating custom layers and models in keras.\n\nThe constructors available: new_layer_class(), %py_class%, create_layer_wrapper(), R6Class(), Layer().\nWhat methods to you might want to define to your model: initialize(), build(), call(), and get_config().\nWhat convenience methods are available when you subclass keras$layers$Layer: add_weight(), add_loss(), and add_metric()"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#environment-details",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#environment-details",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html",
    "href": "guides/keras/preprocessing_layers.html",
    "title": "Working with preprocessing layers",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "href": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "title": "Working with preprocessing layers",
    "section": "Keras preprocessing",
    "text": "Keras preprocessing\nThe Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "href": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "title": "Working with preprocessing layers",
    "section": "Available preprocessing layers",
    "text": "Available preprocessing layers\n\nText preprocessing\n\nlayer_text_vectorization(): turns raw strings into an encoded representation that can be read by a layer_embedding() or layer_dense() layer.\n\n\n\nNumerical features preprocessing\n\nlayer_normalization(): performs feature-wise normalization of input features.\nlayer_discretization(): turns continuous numerical features into integer categorical features.\n\n\n\nCategorical features preprocessing\n\nlayer_category_encoding(): turns integer categorical features into one-hot, multi-hot, or count-based, dense representations.\nlayer_hashing(): performs categorical feature hashing, also known as the “hashing trick”.\nlayer_string_lookup(): turns string categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\nlayer_integer_lookup(): turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\n\n\nImage preprocessing\nThese layers are for standardizing the inputs of an image model.\n\nlayer_resizing(): resizes a batch of images to a target size.\nlayer_rescaling(): rescales and offsets the values of a batch of images (e.g., going from inputs in the [0, 255] range to inputs in the [0, 1] range.\nlayer_center_crop(): returns a center crop of a batch of images.\n\n\n\nImage data augmentation\nThese layers apply random augmentation transforms to a batch of images. They are only active during training.\n\nlayer_random_crop()\nlayer_random_flip()\nlayer_random_flip()\nlayer_random_translation()\nlayer_random_rotation()\nlayer_random_zoom()\nlayer_random_height()\nlayer_random_width()\nlayer_random_contrast()"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "href": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "title": "Working with preprocessing layers",
    "section": "The adapt() function",
    "text": "The adapt() function\nSome preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n\nlayer_text_vectorization(): holds a mapping between string tokens and integer indices\nlayer_string_lookup() and layer_integer_lookup(): hold a mapping between input values and integer indices.\nlayer_normalization(): holds the mean and standard deviation of the features.\nlayer_discretization(): holds information about value bucket boundaries.\n\nCrucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by “adapting” them on data.\nYou set the state of a preprocessing layer by exposing it to training data, via adapt():\n\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n\nLoaded Tensorflow version 2.9.1\n\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\"Features mean: %.2f\", mean(normalized_data))\n\n[1] \"Features mean: -0.00\"\n\nsprintf(\"Features std: %.2f\", sd(normalized_data))\n\n[1] \"Features std: 1.06\"\n\n\nadapt() takes either an array or a tf_dataset. In the case of layer_string_lookup() and layer_text_vectorization(), you can also pass a character vector:\n\ndata <- c(\n  \"Congratulations!\",\n  \"Today is your day.\",\n  \"You're off to Great Places!\",\n  \"You're off and away!\",\n  \"You have brains in your head.\",\n  \"You have feet in your shoes.\",\n  \"You can steer yourself\",\n  \"any direction you choose.\",\n  \"You're on your own. And you know what you know.\",\n  \"And YOU are the one who'll decide where to go.\"\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n\n\nIn addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the adapt() call, they can be set without relying on the layer’s internal computation. For instance, if external vocabulary files for the layer_text_vectorization(), layer_string_lookup(), or layer_integer_lookup() layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer’s constructor arguments.\nHere’s an example where we instantiate a layer_string_lookup() layer with precomputed vocabulary:\n\nvocab <- c(\"a\", \"b\", \"c\", \"d\")\ndata <- as_tensor(rbind(c(\"a\", \"c\", \"d\"),\n                        c(\"d\", \"z\", \"b\")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing data before the model or inside the model",
    "text": "Preprocessing data before the model or inside the model\nThere are two ways you could be using preprocessing layers:\nOption 1: Make them part of the model, like this:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n\nWith this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you’re training on GPU, this is the best option for the layer_normalization() layer, and for all image preprocessing and data augmentation layers.\nOption 2: apply it to your tf_dataset, so as to obtain a dataset that yields batches of preprocessed data, like this:\n\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\nWith this option, your preprocessing will happen on CPU, asynchronously, and will be buffered before going into the model. In addition, if you call tfdatasets::dataset_prefetch() on your dataset, the preprocessing will happen efficiently in parallel with training:\n\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n\nThis is the best option for layer_text_vectorization(), and all structured data preprocessing layers. It can also be a good option if you’re training on CPU and you use image preprocessing layers."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "href": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "title": "Working with preprocessing layers",
    "section": "Benefits of doing preprocessing inside the model at inference time",
    "text": "Benefits of doing preprocessing inside the model at inference time\nEven if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew.\nWhen all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to [-1, +1] or to [0, 1], etc. This is especially powerful if you’re exporting your model to another runtime, such as TensorFlow.js: you won’t have to reimplement your preprocessing pipeline in JavaScript.\nIf you initially put your preprocessing layers in your tf_dataset pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing during multi-worker training",
    "text": "Preprocessing during multi-worker training\nPreprocessing layers are compatible with the tf.distribute API for running training across multiple machines.\nIn general, preprocessing layers should be placed inside a strategy$scope() and called either inside or before the model as discussed above.\n\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n\nFor more details, refer to the preprocessing section of the distributed input guide."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#quick-recipes",
    "href": "guides/keras/preprocessing_layers.html#quick-recipes",
    "title": "Working with preprocessing layers",
    "section": "Quick recipes",
    "text": "Quick recipes\n\nImage data augmentation\nNote that image data augmentation layers are only active during training (similar to the layer_dropout() layer).\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n\nYou can see a similar setup in action in the example image classification from scratch.\n\n\nNormalizing numerical features\n\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"adam\",\n          loss = \"sparse_categorical_crossentropy\")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n\n\n\nEncoding string categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(c(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\"a\", \"b\", \"c\", \"d\", \"e\", \"\")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n\n\nNote that, here, index 0 is reserved for out-of-vocabulary values (values that were not seen during adapt()).\nYou can see the layer_string_lookup() in action in the Structured data classification from scratch example.\n\n\nEncoding integer categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \"int32\")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \"int32\")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n\n\nNote that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_token and oov_token constructor arguments of layer_integer_lookup().\nYou can see the layer_integer_lookup() in action in the example structured data classification from scratch.\n\n\nApplying the hashing trick to an integer categorical feature\nIf you have a categorical feature that can take many different values (on the order of 10e3 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the “hashing trick”: hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing.\n\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \"int64\")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n\nTensorShape([10000, 64])\n\n\n\n\nEncoding text as a sequence of token indices\nThis is how you should preprocess text to be passed to an Embedding layer.\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\",\n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 2 19 14 1 9 2 1\n\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer = \"rmsprop\", loss = \"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output:\", as.array(test_output), \"\\n\")\n\nModel output: 0.1007235 \n\n\nYou can see the layer_text_vectorization() layer in action, combined with an Embedding mode, in the example text classification from scratch.\nNote that when training such a model, for best performance, you should always use the layer_text_vectorization() layer as part of the input pipeline.\n\n\nEncoding text as a dense matrix of ngrams with multi-hot encoding\nThis is how you can preprocess text to be passed to a Dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"multi_hot\" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output); cat(\"\\n\")\n\nModel output: \n\n\ntf.Tensor([[0.46172485]], shape=(1, 1), dtype=float32)\n\n\n\n\nEncoding text as a dense matrix of ngrams with TF-IDF weighting\nThis is an alternative way of preprocessing text before passing it to a layer_dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"tf-idf\" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\"CPU\"), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\"Training model...\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output)\n\nModel output: \n\n\ntf.Tensor([[-0.00118748]], shape=(1, 1), dtype=float32)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#important-gotchas",
    "href": "guides/keras/preprocessing_layers.html#important-gotchas",
    "title": "Working with preprocessing layers",
    "section": "Important gotchas",
    "text": "Important gotchas\n\nWorking with lookup layers with very large vocabularies\nYou may find yourself working with a very large vocabulary in a layer_text_vectorization(), a layer_string_lookup() layer, or an layer_integer_lookup() layer. Typically, a vocabulary larger than 500MB would be considered “very large”.\nIn such case, for best performance, you should avoid using adapt(). Instead, pre-compute your vocabulary in advance (you could use Apache Beam or TF Transform for this) and store it in a file. Then load the vocabulary into the layer at construction time by passing the filepath as the vocabulary argument."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#environment-details",
    "href": "guides/keras/preprocessing_layers.html#environment-details",
    "title": "Working with preprocessing layers",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/python_subclasses.html",
    "href": "guides/keras/python_subclasses.html",
    "title": "Python Subclasses",
    "section": "",
    "text": "When using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses R6::R6Class as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R."
  },
  {
    "objectID": "guides/keras/python_subclasses.html#examples",
    "href": "guides/keras/python_subclasses.html#examples",
    "title": "Python Subclasses",
    "section": "Examples",
    "text": "Examples\n\nA custom constraint (R6)\nFor demonstration purposes, let’s say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\nNonNegative <- R6::R6Class(\"NonNegative\",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \"__call__\" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n\nLoaded Tensorflow version 2.9.1\n\n\nThe r_to_py method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\nNew class instances are generated by calling the class directly: NonNegative() (not NonNegative$new())\nAll methods (functions) are (potentially) modified to ensure their first argument is self.\nAll methods have in scope __class__, super and the class name (NonNegative).\nFor convenience, some method names are treated as aliases:\n\ninitialize is treated as an alias for __init__()\nfinalize is treated as an alias for __del__()\n\nsuper can be accessed in 3 ways:\n\nR6 style, which supports only single inheritance (the most common type)\n\nsuper$initialize()\n\nPython 2 style, which requires explicitly providing the class generator and instance\n\nsuper(NonNegative, self)$`__init__`()\n\nPython 3 style\n\nsuper()$`__init__`()\nWhen subclassing Keras base classes, it is generally your responsibility to call super$initialize() if you are masking a superclass initializer by providing your own initialize method.\nPassing convert=FALSE to r_to_py() will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with convert=TRUE, namely, modifying some Python objects, like dictionaries or lists, in-place.\nActive bindings (methods supplied to R6Class(active=...)) are converted to Python @property-decorated methods.\nR6 classes with private methods or attributes are not supported.\nThe argument supplied to inherit can be:\n\nmissing or NULL\na Python class generator\nan R6 class generator, as long as it can be converted to a Python class generator as well\na list of Python/R6 classes (for multiple inheritance)\nA list of superclasses, with optional additional keywords (e.g., metaclass=, only for advanced Python use cases)\n\n\n\n\nA custom constraint (%py_class%)\nAs an alternative to r_to_py(R6Class(...)), we also provide %py_class%, a more concise alternative syntax for achieving the same outcome. %py_class% is heavily inspired by the Python class statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using %py_class%:\n\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \"__call__\" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n\nNotice, this is very similar to the equivalent Python code:\n\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n\nSome (potentially surprising) notes about %py_class%:\n\nJust like the Python class statement, it assigns the constructed class in the current scope! (There is no need to write NonNegative <- ...).\nThe left hand side can be:\n\nA bare symbol, ClassName\nA pseudo-call, with superclasses and keywords as arguments: ClassName(Superclass1, Superclass2, metaclass=my_metaclass)\n\nThe right hand side is evaluated in a new environment to form the namespace for the class methods.\n%py_class% objects can be safely defined at the top level of an R package. (see details about delay_load below)\nTwo keywords are treated specially: convert and delay_load.\nIf you want to call r_to_py with convert=FALSE, pass it as a keyword:\n\n\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n\n\nYou can delay creating the python type object until this first time a class instance is created by passing delay_load=TRUE. The default value is FALSE for most contexts, but TRUE if you are in an R package. (The actual test performed is identical(topenv(), globalenv())). If a %py_class% type object is delayed, it will display \"<<R6type>.ClassName> (delayed)\" when printed.\nAn additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the __doc__ attribute of the class or method. The doc string will then be visible to both python and R tools e.g. reticulate::py_help(). See ?py_class for an example.\n\nIn all other regards, %py_class% is equivalent to r_to_py(R6Class()) (indeed, under the hood, they do the same thing).\n\n\nA custom layer (R6)\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \"CustomLayer\",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n\n\n\nA custom layer (%py_class%)\nor using %py_class%:\n\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}"
  },
  {
    "objectID": "guides/keras/python_subclasses.html#environment-details",
    "href": "guides/keras/python_subclasses.html#environment-details",
    "title": "Python Subclasses",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/sequential_model.html",
    "href": "guides/keras/sequential_model.html",
    "title": "The Sequential model",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "href": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "title": "The Sequential model",
    "section": "When to use a Sequential model",
    "text": "When to use a Sequential model\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\nSchematically, the following Sequential model:\n\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>% \n  layer_dense(2, activation = \"relu\", name = \"layer1\") %>% \n  layer_dense(3, activation = \"relu\", name = \"layer2\") %>% \n  layer_dense(4, name = \"layer3\")\n\nLoaded Tensorflow version 2.9.1\n\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n\nis equivalent to this function:\n\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \"relu\", name = \"layer1\")\nlayer2 <- layer_dense(units = 3, activation = \"relu\", name = \"layer2\")\nlayer3 <- layer_dense(units = 4, name = \"layer3\")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n\nA Sequential model is not appropriate when:\n\nYour model has multiple inputs or multiple outputs\nAny of your layers has multiple inputs or multiple outputs\nYou need to do layer sharing\nYou want non-linear topology (e.g. a residual connection, a multi-branch model)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "href": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "title": "The Sequential model",
    "section": "Creating a Sequential model",
    "text": "Creating a Sequential model\nYou can create a Sequential model by piping a model through a series layers.\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(4)\n\nIts layers are accessible via the layers attribute:\n\nmodel$layers\n\n[[1]]\n<keras.layers.core.dense.Dense object at 0x7fd46b1bdb50>\n\n[[2]]\n<keras.layers.core.dense.Dense object at 0x7fd46b1bdbe0>\n\n[[3]]\n<keras.layers.core.dense.Dense object at 0x7fd46b1145b0>\n\n\nYou can also create a Sequential model incrementally:\n\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \"relu\")\nmodel %>% layer_dense(3, activation = \"relu\")\nmodel %>% layer_dense(4)\n\nNote that there’s also a corresponding pop() method to remove layers: a Sequential model behaves very much like a stack of layers.\n\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n\n[1] 2\n\n\nAlso note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nmodel <- keras_model_sequential(name = \"my_sequential\")\nmodel %>% layer_dense(2, activation = \"relu\", name = \"layer1\")\nmodel %>% layer_dense(3, activation = \"relu\", name = \"layer2\")\nmodel %>% layer_dense(4, name = \"layer3\")"
  },
  {
    "objectID": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "href": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "title": "The Sequential model",
    "section": "Specifying the input shape in advance",
    "text": "Specifying the input shape in advance\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n\nlist()\n\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n\n[[1]]\n<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.8162029 ,  0.59313834,  0.5534251 ],\n       [ 0.30918205, -0.82670474,  0.730705  ],\n       [ 0.6677761 ,  0.39252794,  0.11466074],\n       [ 0.4278376 ,  0.51385915, -0.21443146]], dtype=float32)>\n\n[[2]]\n<tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn’t “built”: it has no weights (and calling model$weights results in an error stating just this). The weights are created when the model first sees some input data:\n\nmodel <- keras_model_sequential() %>% \n        layer_dense(2, activation = \"relu\") %>% \n        layer_dense(3, activation = \"relu\") %>% \n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n\nError in py_get_attr_impl(x, name, silent) : \n  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n\n# The model summary is also not available:\nsummary(model)\n\nModel: <no summary available, model was not built>\n\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\"Number of weights after calling the model:\", length(model$weights), \"\\n\")  # 6\n\nNumber of weights after calling the model: 6 \n\n\nOnce a model is “built”, you can call its summary() method to display its contents (the summary() method is also called by the default print() method:\n\nsummary(model)\n\nModel: \"sequential_3\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_9 (Dense)                  (1, 2)                        10          \n dense_8 (Dense)                  (1, 3)                        9           \n dense_7 (Dense)                  (1, 4)                        16          \n============================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an input_shape argument to your model, so that it knows its input shape from the start:\n\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \"relu\")\n\nmodel\n\nModel: \"sequential_4\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_10 (Dense)                 (None, 2)                     10          \n============================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\nIn general, it’s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is."
  },
  {
    "objectID": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "href": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "title": "The Sequential model",
    "section": "A common debugging workflow: %>% + summary()",
    "text": "A common debugging workflow: %>% + summary()\nWhen building a new Sequential architecture, it’s useful to incrementally stack layers and print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n  \nmodel %>% \n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) \n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n\nModel: \"sequential_5\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n============================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n____________________________________________________________________________\n\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(2) \n\n# And now?\nmodel\n\nModel: \"sequential_5\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n conv2d_5 (Conv2D)                (None, 38, 38, 32)            9248        \n conv2d_4 (Conv2D)                (None, 36, 36, 32)            9248        \n max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 32)            0           \n conv2d_3 (Conv2D)                (None, 10, 10, 32)            9248        \n conv2d_2 (Conv2D)                (None, 8, 8, 32)              9248        \n max_pooling2d_1 (MaxPooling2D)   (None, 4, 4, 32)              0           \n============================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n____________________________________________________________________________\n\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n\nVery practical, right?"
  },
  {
    "objectID": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "href": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "title": "The Sequential model",
    "section": "What to do once you have a model",
    "text": "What to do once you have a model\nOnce your model architecture is ready, you will want to:\n\nTrain your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\nSave your model to disk and restore it. See our guide to serialization & saving.\nSpeed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training."
  },
  {
    "objectID": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Feature extraction with a Sequential model",
    "text": "Feature extraction with a Sequential model\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n\nHere’s a similar example that only extract features from one layer:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\", name = \"my_intermediate_layer\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \"my_intermediate_layer\")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Transfer learning with a Sequential model",
    "text": "Transfer learning with a Sequential model\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren’t familiar with it, make sure to read our guide to transfer learning.\nHere are two common transfer learning blueprint involving Sequential models.\nFirst, let’s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model$layers and set layer$trainable = FALSE on each layer, except the last one. Like this:\n\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:"
  },
  {
    "objectID": "guides/keras/sequential_model.html#environment-details",
    "href": "guides/keras/sequential_model.html#environment-details",
    "title": "The Sequential model",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html",
    "href": "guides/keras/serialization_and_saving.html",
    "title": "Serialization and saving",
    "section": "",
    "text": "A Keras model consists of multiple components:\n\nThe architecture, or configuration, which specifies what layers the model contain, and how they’re connected.\nA set of weights values (the “state of the model”).\nAn optimizer (defined by compiling the model).\nA set of losses and metrics (defined by compiling the model or calling add_loss() or add_metric()).\n\nThe Keras API makes it possible to save all of these pieces to disk at once, or to only selectively save some of them:\n\nSaving everything into a single archive in the TensorFlow SavedModel format (or in the older Keras H5 format). This is the standard practice.\nSaving the architecture / configuration only, typically as a JSON file.\nSaving the weights values only. This is generally used when training the model.\n\nLet’s take a look at each of these options. When would you use one or the other, and how do they work?"
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#how-to-save-and-load-a-model",
    "href": "guides/keras/serialization_and_saving.html#how-to-save-and-load-a-model",
    "title": "Serialization and saving",
    "section": "How to save and load a model",
    "text": "How to save and load a model\nIf you only have 10 seconds to read this guide, here’s what you need to know.\nSaving a Keras model:\n\nmodel <- ...  # Get model (Sequential, Functional Model, or Model subclass)\nsave_model_tf(\"path/to/location\")\n\nLoading the model back:\n\nlibrary(keras)\nmodel <- load_model_tf(\"path/to/location\")\n\nNow, let’s look at the details."
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#setup",
    "href": "guides/keras/serialization_and_saving.html#setup",
    "title": "Serialization and saving",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#whole-model-saving-loading",
    "href": "guides/keras/serialization_and_saving.html#whole-model-saving-loading",
    "title": "Serialization and saving",
    "section": "Whole-model saving & loading",
    "text": "Whole-model saving & loading\nYou can save an entire model to a single artifact. It will include:\n\nThe model’s architecture/config\nThe model’s weight values (which were learned during training)\nThe model’s compilation information (if compile() was called)\nThe optimizer and its state, if any (this enables you to restart training where you left)\n\n\nAPIs\n\nmodel$save() or save_model_tf()\nload_model_tf()\n\nThere are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format. The recommended format is SavedModel. It is the default when you use model$save().\nYou can switch to the H5 format by:\n\nPassing save_format = 'h5' to save_model_hdf5().\nPassing a filename that ends in .h5 or .keras to $save().\n\n\n\nSavedModel format\nSavedModel is the more comprehensive save format that saves the model architecture, weights, and the traced Tensorflow subgraphs of the call functions. This enables Keras to restore both built-in layers as well as custom objects.\nExample:\n\nget_model <- function() {\n  # Create a simple model.\n  inputs <- layer_input(shape = shape(32))\n  outputs <- layer_dense(inputs, 1)\n  model <- keras_model(inputs, outputs)\n  model %>% compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n  model\n}\nmodel <- get_model()\n\nLoaded Tensorflow version 2.9.1\n\n# Train the model.\ntest_input <- array(runif(128*32), dim = c(128, 32))\ntest_target <- array(runif(128), dim = c(128, 1))\nmodel %>% fit(test_input, test_target)\n# Calling `save('my_model')` creates a SavedModel folder `my_model`.\nsave_model_tf(model, \"my_model\")\n# It can be used to reconstruct the model identically.\nreconstructed_model <- load_model_tf(\"my_model\")\n# Let's check:\nall.equal(\n  predict(model, test_input),\n  predict(reconstructed_model, test_input)\n)\n\n[1] TRUE\n\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\nreconstructed_model %>% fit(test_input, test_target)\n\n\nWhat the SavedModel contains\nCalling save_model_tf(model, 'my_model') creates a folder named my_model, containing the following:\n\nls my_model\n\nassets\nkeras_metadata.pb\nsaved_model.pb\nvariables\n\n\nThe model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in saved_model.pb. The weights are saved in the variables/ directory.\nFor detailed information on the SavedModel format, see the SavedModel guide (The SavedModel format on disk).\n\n\nHow SavedModel handles custom objects\nWhen saving the model and its layers, the SavedModel format stores the class name, call function, losses, and weights (and the config, if implemented). The call function defines the computation graph of the model/layer.\nIn the absence of the model/layer config, the call function is used to create a model that exists like the original model which can be trained, evaluated, and used for inference.\nNevertheless, it is always a good practice to define the get_config and from_config methods when writing a custom model or layer class. This allows you to easily update the computation later if needed. See the section about Custom objects for more information.\n\n\n\n\n\n\nNote\n\n\n\nThe default from_config definition in R is something similar just calls the initialize method with the config lsit using do.call. That way you don’t need to implement a from_config method unless get_config() dictionary names don’t match the initialize arguments.\n\n\nExample:\n\ncustom_model <- new_model_class(\n  \"custom_model\",\n  initialize = function(hidden_units) {\n    super()$`__init__`()\n    self$hidden_units <- hidden_units\n    self$dense_layers <- lapply(hidden_units, function(x) layer_dense(units = x))\n  },\n  call = function(inputs) {\n    x <- inputs\n    for (layer in self$dense_layers) {\n      x <- layer(x)\n    }\n    x\n  },\n  get_config = function() {\n    list(hidden_units = self$hidden_units)\n  }\n)\nmodel <- custom_model(c(16, 16, 10))\n# Build the model by calling it\ninput_arr <- tf$random$uniform(shape(1, 5))\noutputs <- model(input_arr)\nsave_model_tf(model, \"my_model\")\n# Option 1: Load with the custom_object argument.\nloaded_1 <- load_model_tf(\n    \"my_model\", custom_objects = list(\"custom_model\" = custom_model)\n)\n# Option 2: Load without the CustomModel class.\n# Delete the custom-defined model class to ensure that the loader does not have\n# access to it.\nrm(custom_model); gc();\n\n          used (Mb) gc trigger  (Mb) limit (Mb) max used  (Mb)\nNcells 1861943 99.5    3665865 195.8         NA  2534025 135.4\nVcells 3287266 25.1    8388608  64.0     102400  5238666  40.0\n\nloaded_2 <- load_model_tf(\"my_model\")\nall.equal(predict(loaded_1, input_arr), as.array(outputs))\n\n[1] TRUE\n\nall.equal(predict(loaded_2, input_arr), as.array(outputs))\n\n[1] TRUE\n\n\nThe first loaded model is loaded using the config and custom_model class. The second model is loaded by dynamically creating the model class that acts like the original model.\n\n\nConfiguring the SavedModel\nNew in TensoFlow 2.4\nThe argument save_traces has been added to model$save, which allows you to toggle SavedModel function tracing. Functions are saved to allow the Keras to re-load custom objects without the original class definitons, so when save_traces = FALSE, all custom objects must have defined get_config/from_config methods. When loading, the custom objects must be passed to the custom_objects argument. save_traces = FALSE reduces the disk space used by the SavedModel and saving time.\n\n\n\nKeras H5 format\nKeras also supports saving a single HDF5 file containing the model’s architecture, weights values, and compile() information. It is a light-weight alternative to SavedModel.\nExample:\n\nmodel <- get_model()\n# Train the model.\ntest_input <- array(runif(128*32), dim = c(128, 32))\ntest_target <- array(runif(128), dim = c(128, 1))\nmodel %>% fit(test_input, test_target)\n# Calling `save_model_hdf5('my_model.h5')` creates a h5 file `my_model.h5`.\nsave_model_hdf5(model, \"my_h5_model.h5\")\n# It can be used to reconstruct the model identically.\nreconstructed_model <- load_model_hdf5(\"my_h5_model.h5\")\n# Let's check:\nall.equal(\n  predict(model, test_input), \n  predict(reconstructed_model, test_input)\n)\n\n[1] TRUE\n\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\nreconstructed_model %>% fit(test_input, test_target)\n\n\n\nFormat Limitations\nKeras SavedModel format limitations:\nThe tracing done by SavedModel to produce the graphs of the layer call functions allows SavedModel be more portable than H5, but it comes with drawbacks.\n\nCan be slower and bulkier than H5.\nCannot serialize the ops generated from the mask argument (i$e. if a layer is called with layer(..., mask = mask_value), the mask argument is not saved to SavedModel).\nDoes not save the overridden train_step() in subclassed models.\n\nCustom objects that use masks or have a custom training loop can still be saved and loaded from SavedModel, except they must override get_config()/from_config(), and the classes must be passed to the custom_objects argument when loading.\nH5 limitations:\n\nExternal losses & metrics added via model$add_loss() & model$add_metric() are not saved (unlike SavedModel). If you have such losses & metrics on your model and you want to resume training, you need to add these losses back yourself after loading the model. Note that this does not apply to losses/metrics created inside layers via self$add_loss() & self$add_metric(). As long as the layer gets loaded, these losses & metrics are kept, since they are part of the call method of the layer.\nThe computation graph of custom objects such as custom layers is not included in the saved file. At loading time, Keras will need access to the Python classes/functions of these objects in order to reconstruct the model. See Custom objects.\nDoes not support preprocessing layers."
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#saving-the-architecture",
    "href": "guides/keras/serialization_and_saving.html#saving-the-architecture",
    "title": "Serialization and saving",
    "section": "Saving the architecture",
    "text": "Saving the architecture\nThe model’s configuration (or architecture) specifies what layers the model contains, and how these layers are connected*. If you have the configuration of a model, then the model can be created with a freshly initialized state for the weights and no compilation information.\n*Note this only applies to models defined using the functional or Sequential apis not subclassed models.\n\nConfiguration of a Sequential model or Functional API model\nThese types of models are explicit graphs of layers: their configuration is always available in a structured form.\n\nAPIs\n\nget_config() and from_config()\nmodel_to_json() and model_from_json()\n\n\n\nget_config() and from_config()\nCalling config = model$get_config() will return a Python dict containing the configuration of the model. The same model can then be reconstructed via Sequential$from_config(config) (for a Sequential model) or Model$from_config(config) (for a Functional API model).\nThe same workflow also works for any serializable layer.\nLayer example:\n\nlayer <- layer_dense(units = 3, activation = \"relu\")\nlayer_config <- get_config(layer)\nnew_layer <- from_config(config = layer_config)\n\nSequential model example:\n\nmodel <- keras_model_sequential(list(\n  layer_input(shape = 32), \n  layer_dense(units = 1)\n))\nconfig <- get_config(model)\nnew_model <- from_config(config)\n\nFunctional model example:\n\ninputs <- layer_input(shape = 32)\noutputs <- layer_dense(inputs, 1)\nmodel <- keras_model(inputs, outputs)\nconfig <- get_config(model)\nnew_model <- from_config(config)\n\n\n\nmodel_to_json() and model_from_json()\nThis is similar to get_config / from_config, except it turns the model into a JSON string, which can then be loaded without the original model class. It is also specific to models, it isn’t meant for layers.\nExample:\n\nmodel <- keras_model_sequential(list(\n  layer_input(shape = 32), \n  layer_dense(units = 1)\n))\njson_config <- model_to_json(model)\nnew_model <- model_from_json(json_config)\n\n\n\n\nCustom objects\nModels and layers\nThe architecture of subclassed models and layers are defined in the methods initialize and call. They are considered R bytecode, which cannot be serialized into a JSON-compatible config – you could try serializing the bytecode (e.g. via saveRDS), but it’s completely unsafe and means your model cannot be loaded on a different system.\nIn order to save/load a model with custom-defined layers, or a subclassed model, you should overwrite the get_config and optionally from_config methods. Additionally, you should use register the custom object so that Keras is aware of it.\nCustom functions\nCustom-defined functions (e.g. activation loss or initialization) do not need a get_config method. The function name is sufficient for loading as long as it is registered as a custom object.\nLoading the TensorFlow graph only\nIt’s possible to load the TensorFlow graph generated by the Keras. If you do so, you won’t need to provide any custom_objects. You can do so like this:\n\nsave_model_tf(model, \"my_model\")\ntensorflow_graph <- tf$saved_model$load(\"my_model\")\nx <- as_tensor(array(runif(4*32), dim = c(4, 32)), \"float32\")\npredicted <- tensorflow_graph(x)$numpy()\n\nNote that this method has several drawbacks: * For traceability reasons, you should always have access to the custom objects that were used. You wouldn’t want to put in production a model that you cannot re-create. * The object returned by tf$saved_model$load isn’t a Keras model. So it’s not as easy to use. For example, you won’t have access to predict() or fit()\nEven if its use is discouraged, it can help you if you’re in a tight spot, for example, if you lost the code of your custom objects or have issues loading the model with load_model_tf().\nYou can find out more in the page about tf$saved_model$load\n\nDefining the config methods\nSpecifications:\n\nget_config should return a JSON-serializable dictionary in order to be compatible with the Keras architecture - and model-saving APIs.\nfrom_config(config) (classmethod) should return a new layer or model object that is created from the config. The default implementation returns do.call(cls, config).\n\nExample:\n\ncustom_layer <- new_layer_class(\n  \"custom_layer\",\n  initialize = function(a) {\n    self$var <- tf$Variable(a, name = \"var_a\")\n  },\n  call = function(inputs, training = FALSE) {\n    if(training) {\n      inputs*self$var\n    } else {\n      inputs\n    }\n  },\n  get_config = function() {\n    list(\"a\" = as.array(self$var))\n  }\n)\n\nlayer <- custom_layer(a = 5)\nlayer$var$assign(2)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0>\n\nserialized_layer <- keras$layers$serialize(layer)\nnew_layer <- keras$layers$deserialize(\n    serialized_layer, custom_objects = list(\"custom_layer\" = custom_layer)\n)\n\n\n\nRegistering the custom object\nKeras keeps a note of which class generated the config. From the example above, tf$keras$layers$serialize generates a serialized form of the custom layer:\n\n\nlist(class_name = \"custom_layer\", config = list(a = 2))\n\n\nKeras keeps a master list of all built-in layer, model, optimizer, and metric classes, which is used to find the correct class to call from_config. If the class can’t be found, then an error is raised (Value Error: Unknown layer). There are a few ways to register custom classes to this list: 1. Setting custom_objects argument in the loading function. (see the example in section above “Defining the config methods”) 2. tf$keras$utils$custom_object_scope or tf$keras$utils$CustomObjectScope 3. tf$keras$utils$register_keras_serializable\n\n\nCustom layer and function example\n\ncustom_layer <- new_layer_class(\n  \"custom_layer\",\n  initialize = function(units = 32, ...) {\n    super()$`__init__`(...)\n    self$units <- units\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  get_config = function() {\n    config <- super()$get_config()\n    config$units <- self$units\n    config\n  }\n)\n\ncustom_activation <- function(x) {\n  tf$nn$tanh(x)^2\n}\n\n# Make a model with the custom_layer and custom_activation\ninputs <- layer_input(shape = shape(32))\nx <- custom_layer(inputs, 32)\noutputs <- layer_activation(x, custom_activation)\nmodel <- keras_model(inputs, outputs)\n\n# Retrieve the config\nconfig <- get_config(model)\n\n# At loading time, register the custom objects with a `custom_object_scope`:\ncustom_objects <- list(\n  \"custom_layer\" = custom_layer,\n  \"python_function\" = custom_activation\n)\n\nwith(tf$keras$utils$custom_object_scope(custom_objects), {\n  new_model <- keras$Model$from_config(config)\n})\n\n\n\n\nIn-memory model cloning\nYou can also do in-memory cloning of a model via tf$keras$models$clone_model(). This is equivalent to getting the config then recreating the model from its config (so it does not preserve compilation information or layer weights values).\nExample:\n\nwith(tf$keras$utils$custom_object_scope(custom_objects), {\n  new_model <- clone_model(model)\n})"
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#saving-loading-only-the-models-weights-values",
    "href": "guides/keras/serialization_and_saving.html#saving-loading-only-the-models-weights-values",
    "title": "Serialization and saving",
    "section": "Saving & loading only the model’s weights values",
    "text": "Saving & loading only the model’s weights values\nYou can choose to only save & load a model’s weights. This can be useful if: - You only need the model for inference: in this case you won’t need to restart training, so you don’t need the compilation information or optimizer state. - You are doing transfer learning: in this case you will be training a new model reusing the state of a prior model, so you don’t need the compilation information of the prior model.\n\nAPIs for in-memory weight transfer\nWeights can be copied between different objects by using get_weights and set_weights:\n\nget_weights(): Returns a list of arrays.\nset_weights(): Sets the model weights to the values in the weights argument.\n\nExamples below.\nTransfering weights from one layer to another, in memory\n\ncreate_layer <- function() {\n  layer <- layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\n  layer$build(shape(NULL, 784))\n  layer\n}\n    \nlayer_1 <- create_layer()\nlayer_2 <- create_layer()\n\n# Copy weights from layer 1 to layer 2\nset_weights(layer_2, get_weights(layer_1))\n\nTransfering weights from one model to another model with a compatible architecture, in memory"
  },
  {
    "objectID": "guides/keras/serialization_and_saving.html#environment-details",
    "href": "guides/keras/serialization_and_saving.html#environment-details",
    "title": "Serialization and saving",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html",
    "href": "guides/keras/training_with_built_in_methods.html",
    "title": "Training & evaluation with the built-in methods",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#introduction",
    "href": "guides/keras/training_with_built_in_methods.html#introduction",
    "title": "Training & evaluation with the built-in methods",
    "section": "Introduction",
    "text": "Introduction\nThis guide covers training, evaluation, and prediction (inference) models when using built-in APIs for training & validation.\nIf you are interested in leveraging fit() while specifying your own training step function, see the Customizing what happens in fit() guide.\nIf you are interested in writing your own training & evaluation loops from scratch, see the guide “writing a training loop from scratch”.\nIn general, whether you are using built-in loops or writing your own, model training & evaluation works strictly in the same way across every kind of Keras model – Sequential models, models built with the Functional API, and models written from scratch via model subclassing.\nThis guide doesn’t cover distributed training, which is covered in our guide to multi-GPU & distributed training."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#api-overview-a-first-end-to-end-example",
    "href": "guides/keras/training_with_built_in_methods.html#api-overview-a-first-end-to-end-example",
    "title": "Training & evaluation with the built-in methods",
    "section": "API overview: a first end-to-end example",
    "text": "API overview: a first end-to-end example\nWhen passing data to the built-in training loops of a model, you should either use NumPy arrays (if your data is small and fits in memory) or tf$data Dataset objects. In the next few paragraphs, we’ll use the MNIST dataset as NumPy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\nLet’s consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\n\nLoaded Tensorflow version 2.9.1\n\nx <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\noutputs <- x %>% \n  layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nHere’s what the typical end-to-end workflow looks like, consisting of:\n\nTraining\nValidation on a holdout set generated from the original training data\nEvaluation on the test data\n\nWe’ll use MNIST data for this example.\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\n# Reserve 10,000 samples for validation\nx_val <- tail(x_train, 10000)\ny_val <- tail(y_train, 10000)\nx_train <- head(x_train, 50000)\ny_train <- head(y_train, 50000)\n\nWe specify the training configuration (optimizer, loss, metrics):\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(),  # Optimizer\n    # Loss function to minimize\n    loss = loss_sparse_categorical_crossentropy(),\n    # List of metrics to monitor\n    metrics = list(metric_sparse_categorical_accuracy()),\n)\n\nWe call fit(), which will train the model by slicing the data into “batches” of size batch_size, and repeatedly iterating over the entire dataset for a given number of epochs.\n\nhistory <- model %>% fit(\n    x_train,\n    y_train,\n    batch_size = 64,\n    epochs = 2,\n    # We pass some validation for\n    # monitoring validation loss and metrics\n    # at the end of each epoch\n    validation_data = list(x_val, y_val),\n)\n\nThe returned history object holds a record of the loss values and metric values during training:\n\nhistory\n\n\nFinal epoch (plot to see history):\n                           loss: 0.155\n    sparse_categorical_accuracy: 0.9544\n                       val_loss: 0.1476\nval_sparse_categorical_accuracy: 0.9576 \n\n\nWe evaluate the model on the test data via evaluate():\n\n# Evaluate the model on the test data using `evaluate`\nresults <- model %>% evaluate(x_test, y_test, batch_size = 128)\ncat(\"test loss, test acc:\", results)\n\ntest loss, test acc: 0.1497131 0.9563\n\n# Generate predictions (probabilities -- the output of the last layer)\n\n# on new data using `predict`\n\npredictions <- predict(model, x_test[1:3,])\ndim(predictions)\n\n[1]  3 10\n\n\nNow, let’s review each piece of this workflow in detail."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#the-compile-method-specifying-a-loss-metrics-and-an-optimizer",
    "href": "guides/keras/training_with_built_in_methods.html#the-compile-method-specifying-a-loss-metrics-and-an-optimizer",
    "title": "Training & evaluation with the built-in methods",
    "section": "The compile() method: specifying a loss, metrics, and an optimizer",
    "text": "The compile() method: specifying a loss, metrics, and an optimizer\nTo train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\nYou pass these to the model as arguments to the compile() method:\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_categorical_crossentropy(),\n    metrics = list(metric_sparse_categorical_accuracy())\n)\n\nThe metrics argument should be a list – your model can have any number of metrics.\nIf your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate the contribution of each output to the total loss of the model. You will find more details about this in the Passing data to multi-input, multi-output models section.\nNote that if you’re satisfied with the default settings, in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:\n\nmodel %>% compile(\n    optimizer = \"rmsprop\",\n    loss = \"sparse_categorical_crossentropy\",\n    metrics = list(\"sparse_categorical_accuracy\")\n)\n\nFor later reuse, let’s put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n\nget_uncompiled_model <- function() {\n  inputs <- layer_input(shape = shape(784), name = \"digits\")\n  x <- inputs %>% \n    layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>% \n    layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\n  outputs <- x %>% \n    layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\n  model <- keras_model(inputs = inputs, outputs = outputs)\n  model\n}\n\nget_compiled_model <- function() {\n  model <- get_uncompiled_model()\n  model %>% compile(\n    optimizer = \"rmsprop\",\n    loss = \"sparse_categorical_crossentropy\",\n    metrics = list(\"sparse_categorical_accuracy\"),\n  )\n  model\n}\n\n\nMany built-in optimizers, losses, and metrics are available\nIn general, you won’t have to create your own losses, metrics, or optimizers from scratch, because what you need is likely to be already part of the Keras API:\nOptimizers:\n\noptimizer_sgd() (with or without momentum)\noptimizer_rmsprop()\noptimizer_adam()\netc.\n\nLosses:\n\nloss_mean_squared_error()\nloss_kl_divergence()\nloss_cosine_similarity()\netc.\n\nMetrics:\n\nmetric_auc()\nmetric_precision()\nmetric_recall()\netc.\n\n\n\nCustom losses\nIf you need to create a custom loss, Keras provides two ways to do so.\nThe first method involves creating a function that accepts inputs y_true and y_pred. The following example shows a loss function that computes the mean squared error between the real data and the predictions:\n\ncustom_mean_squared_error <- function(y_true, y_pred) {\n  tf$math$reduce_mean(tf$square(y_true - y_pred))\n}\n\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(optimizer = optimizer_adam(), loss = custom_mean_squared_error)\n\n# We need to one-hot encode the labels to use MSE\n\ny_train_one_hot <- tf$one_hot(y_train, depth = 10L)\nmodel %>% fit(x_train, y_train_one_hot, batch_size = 64, epochs = 1)\n\nIf you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf$keras$losses$Loss class and implement the following two methods:\n\ninitialize(): accept parameters to pass during the call of your loss function\ncall(y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model’s loss\n\nLet’s say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won’t know if it works until we try!).\nHere’s how you would do it:\n\ncustom_mse <- new_loss_class(\n  classname = \"custom_mse\",\n  initialize = function(regularization_factor = 0.1, name = \"custom_mse\") {\n    super()$`__init__`(name = name)\n    self$regularization_factor <- regularization_factor\n  },\n  call = function(y_true, y_pred) {\n    mse <- tf$math$reduce_mean(tf$square(y_true - y_pred))\n    reg <- tf$math$reduce_mean(tf$square(0.5 - y_pred))\n    mse + reg * self$regularization_factor\n  }\n)\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(optimizer = optimizer_adam(), loss = custom_mse())\n\ny_train_one_hot <- tf$one_hot(y_train, depth = 10L)\nmodel %>% fit(x_train, y_train_one_hot, batch_size = 64, epochs = 1)\n\n\n\nCustom metrics\nIf you need a metric that isn’t part of the API, you can easily create custom metrics by subclassing the tf$keras$metrics$Metric class. You will need to implement 4 methods:\n\ninitialize(), in which you will create state variables for your metric.\nupdate_state(y_true, y_pred, sample_weight = NULL), which uses the targets y_true and the model predictions y_pred to update the state variables.\nresult(), which uses the state variables to compute the final results.\nreset_state(), which reinitializes the state of the metric.\n\nState update and results computation are kept separate (in update_state() and result(), respectively) because in some cases, the results computation might be very expensive and would only be done periodically.\nHere’s a simple example showing how to implement a CategoricalTRUEPositives metric that counts how many samples were correctly classified as belonging to a given class:\n\ncategorical_true_positives <- new_metric_class(\n  classname = \"categorical_true_positives\",\n  initialize = function(name = \"categorical_true_positives\", ...) {\n    super()$`__init__`(name, ...)\n    self$true_positives <- self$add_weight(name = \"ctp\", initializer = \"zeros\")\n  },\n  update_state = function(y_true, y_pred, sample_weight = NULL) {\n    y_pred <- tf$reshape(tf$argmax(y_pred, axis = 1L), shape = c(-1L,1L))\n    values <- tf$cast(y_true, \"int32\") == tf$cast(y_pred, \"int32\")\n    values <- tf$cast(values, \"float32\")\n    if (!is.null(sample_weight)) {\n      sample_weight <- tf$cast(sample_weight, \"float32\")\n      values <- tf$multiply(values, sample_weight)\n    }\n\n    self$true_positives$assign_add(tf$reduce_sum(values))\n  },\n  result = function() {\n    self$true_positives\n  },\n  reset_state = function() {\n    self$true_positives$assign(0.0)\n  }\n)\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(),\n    metrics = list(categorical_true_positives()),\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 3)\n\n\n\nHandling losses and metrics that don’t fit the standard signature\nThe overwhelming majority of losses and metrics can be computed from y_true and y_pred, where y_pred is an output of your model – but not all of them. For instance, a regularization loss may only require the activation of a layer (there are no targets in this case), and this activation may not be a model output.\nIn such cases, you can call self$add_loss(loss_value) from inside the call method of a custom layer. Losses added in this way get added to the “main” loss during training (the one passed to compile()). Here’s a simple example that adds activity regularization (note that activity regularization is built-in in all Keras layers – this layer is just for the sake of providing a concrete example):\n\nlayer_activity_regularization <- new_layer_class(\n  classname = \"activity_regularization\",\n  call = function(inputs) {\n    self$add_loss(tf$reduce_sum(inputs) * 0.1)\n    inputs # Pass-through layer.\n  }\n)\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\n# Insert activity regularization as a layer\nx <- layer_activity_regularization(x)\nx <- layer_dense(x, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x, 10, name = \"predictions\")\n\nmodel <- keras_model(inputs = inputs, outputs = outputs)\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\n\n# The displayed loss will be much higher than before\n# due to the regularization component.\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nYou can do the same for logging metric values, using add_metric():\n\nlayer_metric_logging <- new_layer_class(\n  \"metric_logging\",\n  call = function(inputs) {\n    self$add_metric(\n      keras$backend$std(inputs), \n      name = \"std_of_activation\", \n      aggregation = \"mean\"\n    )\n    inputs\n  }\n)\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\n\n# Insert std logging as a layer.\nx <- layer_metric_logging(x)\nx <- layer_dense(x, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x, 10, name = \"predictions\")\n\nmodel <- keras_model(inputs = inputs, outputs = outputs)\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nIn the Functional API, you can also call model$add_loss(loss_tensor), or model$add_metric(metric_tensor, name, aggregation).\nHere’s a simple example:\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx1 <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\nx2 <- layer_dense(x1, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x2, 10, name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nmodel$add_loss(tf$reduce_sum(x1) * 0.1)\nmodel$add_metric(\n  keras$backend$std(x1), \n  name = \"std_of_activation\", \n  aggregation = \"mean\"\n)\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nNote that when you pass losses via add_loss(), it becomes possible to call compile() without a loss function, since the model already has a loss to minimize.\nConsider the following LogisticEndpoint layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via add_loss(). It also tracks classification accuracy via add_metric().\n\nlayer_logistic_endpoint <- new_layer_class(\n  \"logistic_endpoint\",\n  initialize = function(name = NULL) {\n    super()$`__init__`(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  },\n  call = function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n    \n    # Log accuracy as a metric and add it\n    # to the layer using `self$add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n    \n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n)\n\nYou can use it in a model with two inputs (input data & targets), compiled without a loss argument, like this:\n\ninputs <- layer_input(shape = shape(3), name = \"inputs\")\ntargets <- layer_input(shape = shape(10), name = \"targets\")\nlogits <- layer_dense(inputs, 10)\npredictions <- layer_logistic_endpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")  # No loss argument!\n\ndata <- list(\n    \"inputs\" = array(runif(3*3), dim = c(3,3)),\n    \"targets\" = array(runif(3*10), dim = c(3, 10))\n)\nmodel %>% fit(data, epochs = 1)\n\nFor more information about training multi-input models, see the section Passing data to multi-input, multi-output models.\n\n\nAutomatically setting apart a validation holdout set\nIn the first end-to-end example you saw, we used the validation_data argument to pass a listy of arrays (x_val, y_val) to the model for evaluating a validation loss and validation metrics at the end of each epoch.\nHere’s another option: the argument validation_split allows you to automatically reserve part of your training data for validation. The argument value represents the fraction of the data to be reserved for validation, so it should be set to a number higher than 0 and lower than 1. For instance, validation_split = 0.2 means “use 20% of the data for validation”, and validation_split = 0.6 means “use 60% of the data for validation”.\nThe way the validation is computed is by taking the last x% samples of the arrays received by the fit() call, before any shuffling.\nNote that you can only use validation_split when training with array data.\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, batch_size = 64, validation_split = 0.2, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#training-evaluation-from-tensorflow-datasets",
    "href": "guides/keras/training_with_built_in_methods.html#training-evaluation-from-tensorflow-datasets",
    "title": "Training & evaluation with the built-in methods",
    "section": "Training & evaluation from TensorFlow Datasets",
    "text": "Training & evaluation from TensorFlow Datasets\nIn the past few paragraphs, you’ve seen how to handle losses, metrics, and optimizers, and you’ve seen how to use the validation_data and validation_split arguments in fit(), when your data is passed as R arrays.\nLet’s now take a look at the case where your data comes in the form of a TensorFlow dataset object.\n\n\n\n\n\n\nNote\n\n\n\nThe tfdatasets package in R is an interface for the tf.data module in Python.\n\n\nThe tf.data API is a set of utilities in TensorFlow 2.0 for loading and preprocessing data in a way that’s fast and scalable.\nFor a complete guide about creating Datasets, see the tf.data documentation.\nYou can pass a Dataset instance directly to the methods fit(), evaluate(), and predict():\n\nlibrary(tfdatasets)\nmodel <- get_compiled_model()\n\n# First, let's create a training Dataset instance.\n# For the sake of our example, we'll use the same MNIST data as before.\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train))\n# Shuffle and slice the dataset.\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Now we get a test dataset.\ntest_dataset <- list(x_test, y_test) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\n# Since the dataset already takes care of batching,\n# we don't pass a `batch_size` argument.\nmodel %>% fit(train_dataset, epochs = 3)\n\n# You can also evaluate or predict on a dataset.\nresult <- model %>% evaluate(test_dataset)\nprint(result)\n\n                       loss sparse_categorical_accuracy \n                  0.1161979                   0.9644000 \n\n\nNote that the Dataset is reset at the end of each epoch, so it can be reused of the next epoch.\nIf you want to run training only on a specific number of batches from this Dataset, you can pass the steps_per_epoch argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.\nIf you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Only use the 100 batches per epoch (that's 64 * 100 samples)\nmodel %>% fit(train_dataset, epochs = 3, steps_per_epoch = 100)\n\n\nUsing a validation dataset\nYou can pass a Dataset instance as the validation_data argument in fit():\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Prepare the validation dataset\nval_dataset <- list(x_val, y_val) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\nmodel %>% fit(train_dataset, epochs = 1, validation_data = val_dataset)\n\nAt the end of each epoch, the model will iterate over the validation dataset and compute the validation loss and validation metrics.\nIf you want to run validation only on a specific number of batches from this dataset, you can pass the validation_steps argument, which specifies how many validation steps the model should run with the validation dataset before interrupting validation and moving on to the next epoch:\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Prepare the validation dataset\nval_dataset <- list(x_val, y_val) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\nmodel %>% fit(\n    train_dataset,\n    epochs = 1,\n    # Only run validation using the first 10 batches of the dataset\n    # using the `validation_steps` argument\n    validation_data = val_dataset,\n    validation_steps = 10,\n)\n\nNote that the validation dataset will be reset after each use (so that you will always be evaluating on the same samples from epoch to epoch).\nThe argument validation_split (generating a holdout set from the training data) is not supported when training from Dataset objects, since this feature requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-sample-weighting-and-class-weighting",
    "href": "guides/keras/training_with_built_in_methods.html#using-sample-weighting-and-class-weighting",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using sample weighting and class weighting",
    "text": "Using sample weighting and class weighting\nWith the default settings the weight of a sample is decided by its frequency in the dataset. There are two methods to weight the data, independent of sample frequency:\n\nClass weights\nSample weights\n\n\nClass weights\nThis is set by passing a dictionary to the class_weight argument to Model %>% fit(). This dictionary maps class indices to the weight that should be used for samples belonging to this class.\nThis can be used to balance classes without resampling, or to train a model that gives more importance to a particular class.\nFor instance, if class “0” is half as represented as class “1” in your data, you could use Model %>% fit(..., class_weight = list(0= 1., 1= 0.5)).\nHere’s an example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit “5” in the MNIST dataset).\n\nclass_weight <- list(\n    \"0\" = 1.0,\n    \"1\" = 1.0,\n    \"2\" = 1.0,\n    \"3\" = 1.0,\n    \"4\" = 1.0,\n    # Set weight \"2\" for class \"5\",\n    # making this class 2x more important\n    \"5\" = 2.0,\n    \"6\" = 1.0,\n    \"7\" = 1.0,\n    \"8\" = 1.0,\n    \"9\" = 1.0\n)\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, class_weight = class_weight, batch_size = 64, epochs = 1)\n\n\n\nSample weights\nFor fine grained control, or if you are not building a classifier, you can use “sample weights”.\n\nWhen training from R data: Pass the sample_weight argument to Model %>% fit().\nWhen training from tfdatasets or any other sort of iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples.\n\nA “sample weights” array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\nWhen the weights used are ones and zeros, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss).\n\nsample_weight <- rep(1, length(y_train))\nsample_weight[y_train == 5] <- 2.0\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, sample_weight = sample_weight, batch_size = 64, epochs = 1)\n\nHere’s a matching Dataset example:\n\nsample_weight <- rep(1, length(y_train))\nsample_weight[y_train == 5] <- 2.0\n\n# Create a Dataset that includes sample weights\n# (3rd element in the return tuple).\ntrain_dataset <- list(x_train, y_train, sample_weight) %>% \n  tensor_slices_dataset()\n\n# Shuffle and slice the dataset.\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\nmodel <- get_compiled_model()\nmodel %>% fit(train_dataset, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#passing-data-to-multi-input-multi-output-models",
    "href": "guides/keras/training_with_built_in_methods.html#passing-data-to-multi-input-multi-output-models",
    "title": "Training & evaluation with the built-in methods",
    "section": "Passing data to multi-input, multi-output models",
    "text": "Passing data to multi-input, multi-output models\nIn the previous examples, we were considering a model with a single input (a tensor of shape (764)) and a single output (a prediction tensor of shape (10)). But what about models that have multiple inputs or outputs?\nConsider the following model, which has an image input of shape (32, 32, 3) (that’s (height, width, channels)) and a time series input of shape (NULL, 10) (that’s (timesteps, features)). Our model will have two outputs computed from the combination of these inputs: a “score” (of shape (1)) and a probability distribution over five classes (of shape (5)).\n\nimage_input <- layer_input(shape = shape(32, 32, 3), name = \"img_input\")\ntimeseries_input <- layer_input(shape = shape(NULL, 10), name = \"ts_input\")\n\nx1 <- layer_conv_2d(image_input, 3, 3)\nx1 <- layer_global_max_pooling_2d(x1)\n\nx2 <- layer_conv_1d(timeseries_input, 3, 3)\nx2 <- layer_global_max_pooling_1d(x2)\n\nx <- layer_concatenate(list(x1, x2))\n\nscore_output <- layer_dense(x, 1, name = \"score_output\")\nclass_output <- layer_dense(x, 5, name = \"class_output\")\n\nmodel <- keras_model(\n    inputs = list(image_input, timeseries_input), \n    outputs = list(score_output, class_output)\n)\n\nLet’s plot this model, so you can clearly see what we’re doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes).\n\nkeras$utils$plot_model(\n  model, \"img/multi_input_and_output_model.png\", \n  show_shapes = TRUE\n)\n\n<IPython.core.display.Image object>\n\n\n\nAt compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  )\n)\n\nIf we only passed a single loss function to the model, the same loss function would be applied to every output (which is not appropriate here).\nLikewise for metrics:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    ),\n    list(\n      metric_categorical_accuracy()\n    )\n  )\n)\n\nSince we gave names to our output layers, we could also specify per-output losses and metrics via a dict:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    score_output = loss_mean_squared_error(),\n    class_output = loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    class_output = list(\n      metric_categorical_accuracy()\n    ),\n    score_output = list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    )\n  )\n)\n\nWe recommend the use of explicit names and dicts if you have more than 2 outputs.\nIt’s possible to give different weights to different output-specific losses (for instance, one might wish to privilege the “score” loss in our example, by giving to 2x the importance of the class loss), using the loss_weights argument:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    score_output = loss_mean_squared_error(),\n    class_output = loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    class_output = list(\n      metric_categorical_accuracy()\n    ),\n    score_output = list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    )\n  ),\n  loss_weights = list(score_output = 2.0, class_output = 1.0)\n)\n\nYou could also choose not to compute a loss for certain outputs, if these outputs are meant for prediction but not for training:\n\n# List loss version\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    NULL,\n    loss_categorical_crossentropy()\n  )\n)\n\n# Or dict loss version\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    class_output = loss_categorical_crossentropy()\n  )\n)\n\nPassing data to a multi-input or multi-output model in fit() works in a similar way as specifying a loss function in compile: you can pass lists of R arrays (with 1:1 mapping to the outputs that received a loss function) or named list mapping output names to R arrays.\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  )\n)\n# Generate dummy NumPy data\n\nimg_data <- array(runif(100*32*32*3), dim = c(100, 32, 32, 3))\nts_data <- array(runif(100*20*10), dim = c(100, 20, 10))\nscore_targets <- array(runif(100), dim = c(100, 1))\nclass_targets <- array(runif(100*5), dim = c(100, 5))\n\n# Fit on lists\nmodel %>% fit(\n  list(img_data, ts_data), \n  list(score_targets, class_targets), \n  batch_size = 32, \n  epochs = 1\n)\n\n# Alternatively, fit on named lists\nmodel %>% fit(\n    list(\"img_input\" = img_data, \"ts_input\" = ts_data),\n    list(\"score_output\" = score_targets, \"class_output\" = class_targets),\n    batch_size = 32,\n    epochs = 1\n)\n\nHere’s the Dataset use case: similarly as what we did for R arrays, the Dataset should return a tuple of dicts.\n\ntrain_dataset <- list(\n  list(\"img_input\" = img_data, \"ts_input\" = ts_data),\n  list(\"score_output\" = score_targets, \"class_output\" = class_targets)\n) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\nmodel %>% fit(train_dataset, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-callbacks",
    "href": "guides/keras/training_with_built_in_methods.html#using-callbacks",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using callbacks",
    "text": "Using callbacks\nCallbacks in Keras are objects that are called at different points during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They can be used to implement certain behaviors, such as:\n\nDoing validation at different points during training (beyond the built-in per-epoch validation)\nCheckpointing the model at regular intervals or when it exceeds a certain accuracy threshold\nChanging the learning rate of the model when training seems to be plateauing\nDoing fine-tuning of the top layers when training seems to be plateauing\nSending email or instant message notifications when training ends or where a certain performance threshold is exceeded\nEtc.\n\nCallbacks can be passed as a list to your call to fit():\n\nmodel <- get_compiled_model()\n\ncallbacks <- list(\n  callback_early_stopping(\n    # Stop training when `val_loss` is no longer improving\n    monitor = \"val_loss\",\n    # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n    min_delta = 1e-2,\n    # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n    patience = 2,\n    verbose = 1,\n  )\n)\n\nmodel %>% fit(\n    x_train,\n    y_train,\n    epochs = 20,\n    batch_size = 64,\n    callbacks = callbacks,\n    validation_split = 0.2,\n)\n\n\nMany built-in callbacks are available\nThere are many built-in callbacks already available in Keras, such as:\n\ncallback_model_checkpoint(): Periodically save the model.\ncallback_early_stopping(): Stop training when training is no longer improving the validation metrics.\ncallback_tensorboard(): periodically write model logs that can be visualized in TensorBoard (more details in the section “Visualization”).\ncallback_csv_logger(): streams loss and metrics data to a CSV file.\netc.\n\nSee the callbacks documentation for the complete list.\n\n\nWriting your own callback\nYou can create a custom callback by extending the base class keras$callbacks$Callback. A callback has access to its associated model through the class property self$model.\nMake sure to read the complete guide to writing custom callbacks.\nHere’s a simple example saving a list of per-batch loss values during training:\n\ncallback_loss_history <- new_callback_class(\n  \"loss_history\",\n  on_train_begin = function(logs) {\n    self$per_batch_losses <- list()\n  },\n  on_batch_end = function(batch, logs) {\n    self$per_batch_losses <- c(\n      self$per_batch_losses,\n      logs$get(\"loss\")\n    )\n  }\n)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#checkpointing-models",
    "href": "guides/keras/training_with_built_in_methods.html#checkpointing-models",
    "title": "Training & evaluation with the built-in methods",
    "section": "Checkpointing models",
    "text": "Checkpointing models\nWhen you’re training model on relatively large datasets, it’s crucial to save checkpoints of your model at frequent intervals.\nThe easiest way to achieve this is with the callback_model_checkpoint() callback:\n\nmodel <- get_compiled_model()\n\ncallbacks <- list(\n  callback_model_checkpoint(\n    # Path where to save the model\n    # The two parameters below mean that we will overwrite\n    # the current checkpoint if and only if\n    # the `val_loss` score has improved.\n    # The saved model name will include the current epoch.\n    filepath = \"mymodel_{epoch}\",\n    save_best_only = TRUE,  # Only save a model if `val_loss` has improved.\n    monitor = \"val_loss\",\n    verbose = 1,\n  )\n)\n\nmodel %>% fit(\n    x_train, \n    y_train, \n    epochs = 2, \n    batch_size = 64, \n    callbacks = callbacks, \n    validation_split = 0.2\n)\n\nThe callback_model_checkpoint() callback can be used to implement fault-tolerance: the ability to restart training from the last saved state of the model in case training gets randomly interrupted. Here’s a basic example:\n\n# Prepare a directory to store all the checkpoints.\ncheckpoint_dir <- \"./ckpt\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n\nmake_or_restore_model <- function() {\n  # Either restore the latest model, or create a fresh one\n  # if there is no checkpoint available.\n  checkpoints <- list.files(checkpoint_dir, full.names = TRUE)\n  details <- file.info(checkpoints)\n  if (length(checkpoints) > 0) {\n    latest_checkpoint <- checkpoints[which.max(as.POSIXct(details$mtime))]\n    cat(\"Restoring from\", latest_checkpoint)\n    return(load_model_tf(latest_checkpoint))\n  }\n  \n  cat(\"Creating a new model\")\n  get_compiled_model()\n}\n\nmodel <- make_or_restore_model()\n\nCreating a new model\n\ncallbacks <- list(\n    # This callback saves a SavedModel every 100 batches.\n    # We include the training loss in the saved model name.\n    callback_model_checkpoint(\n        filepath = paste0(checkpoint_dir, \"/ckpt-loss={loss:.2f}\"), \n        save_freq = 100\n    )\n)\nmodel %>% fit(x_train, y_train, epochs = 1, callbacks = callbacks)\n\nYou call also write your own callback for saving and restoring models.\nFor a complete guide on serialization and saving, see the guide to saving and serializing Models."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-learning-rate-schedules",
    "href": "guides/keras/training_with_built_in_methods.html#using-learning-rate-schedules",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using learning rate schedules",
    "text": "Using learning rate schedules\nA common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as “learning rate decay”.\nThe learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss).\n\nPassing a schedule to an optimizer\nYou can easily use a static learning rate decay schedule by passing a schedule object as the learning_rate argument in your optimizer:\n\ninitial_learning_rate <- 0.1\nlr_schedule <- learning_rate_schedule_exponential_decay(\n  initial_learning_rate = initial_learning_rate,\n  decay_steps = 100000, \n  decay_rate = 0.96, \n  staircase = TRUE\n)\n\noptimizer <- keras$optimizers$RMSprop(learning_rate = lr_schedule)\n\nSeveral built-in schedules are available: learning_rate_schedule_cosine_decay, learning_rate_schedule_cosine_decay_restarts, learning_rate_schedule_exponential_decay, learning_rate_schedule_inverse_time_decay, learning_rate_schedule_piecewise_constant_decay, learning_rate_schedule_polynomial_decay\n\n\nUsing callbacks to implement a dynamic learning rate schedule\nA dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects, since the optimizer does not have access to validation metrics.\nHowever, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the callback_reduce_lr_on_plateau() callback."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#visualizing-loss-and-metrics-during-training",
    "href": "guides/keras/training_with_built_in_methods.html#visualizing-loss-and-metrics-during-training",
    "title": "Training & evaluation with the built-in methods",
    "section": "Visualizing loss and metrics during training",
    "text": "Visualizing loss and metrics during training\nThe best way to keep an eye on your model during training is to use TensorBoard – a browser-based application that you can run locally that provides you with:\n\nLive plots of the loss and metrics for training and evaluation\n(optionally) Visualizations of the histograms of your layer activations\n(optionally) 3D visualizations of the embedding spaces learned by your Embedding layers\n\nIf you have installed TensorFlow with pip, you should be able to launch TensorBoard from R with:\n\ntensorflow::tensorboard(log_dir = \"/full_path_to_your_logs\")\n\n\nUsing the TensorBoard callback\nThe easiest way to use TensorBoard with a Keras model and the fit() method is the TensorBoard callback.\nIn the simplest case, just specify where you want the callback to write logs, and you’re good to go:\n\ncallback_tensorboard(\n    log_dir = \"/full_path_to_your_logs\",\n    histogram_freq = 0,  # How often to log histogram visualizations\n    embeddings_freq = 0,  # How often to log embedding visualizations\n    update_freq = \"epoch\" # How often to write logs (default: once per epoch)\n)  \n\n<keras.callbacks.TensorBoard object at 0x7fea244b90a0>\n\n\nFor more information, see the documentation for the TensorBoard callback."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#environment-details",
    "href": "guides/keras/training_with_built_in_methods.html#environment-details",
    "title": "Training & evaluation with the built-in methods",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/transfer_learning.html",
    "href": "guides/keras/transfer_learning.html",
    "title": "Transfer learning and fine-tuning",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#introduction",
    "href": "guides/keras/transfer_learning.html#introduction",
    "title": "Transfer learning and fine-tuning",
    "section": "Introduction",
    "text": "Introduction\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\nTake layers from a previously trained model.\nFreeze them, so as to avoid destroying any of the information they contain during future training rounds.\nAdd some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\nTrain the new layers on your dataset.\n\nA last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\nFirst, we will go over the Keras trainable API in detail, which underlies most transfer learning and fine-tuning workflows.\nThen, we’ll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle “cats vs dogs” classification dataset.\nThis is adapted from Deep Learning with R and the 2016 blog post “building powerful image classification models using very little data”."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Freezing layers: understanding the trainable attribute",
    "text": "Freezing layers: understanding the trainable attribute\nLayers and models have three weight attributes:\n\nweights is the list of all weights variables of the layer.\ntrainable_weights is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\nnon_trainable_weights is the list of those that aren’t meant to be trained. Typically they are updated by the model during the forward pass.\n\nExample: the Dense layer has 2 trainable weights (kernel and bias)\n\nlayer <- layer_dense(units = 3)\n\nLoaded Tensorflow version 2.9.1\n\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 0\n\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is layer_batch_normalization(). It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the guide to writing new layers from scratch.\nExample: The layer instance returned by layer_batch_normalization() has 2 trainable weights and 2 non-trainable weights\n\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 4\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nLayers and models also feature a boolean attribute trainable. Its value can be changed. Setting layer$trainable to FALSE moves all the layer’s weights from trainable to non-trainable. This is called “freezing” the layer: the state of a frozen layer won’t be updated during training (either when training with fit() or when training with any custom loop that relies on trainable_weights to apply gradient updates).\nExample: setting trainable to False\n\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 0\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \"relu\")\nlayer2 <- layer_dense(units = 3, activation = \"sigmoid\")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n\nDo not confuse the layer$trainable attribute with the training argument in a layer instance’s call signature layer(training =) (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the Keras FAQ."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Recursive setting of the trainable attribute",
    "text": "Recursive setting of the trainable attribute\nIf you set trainable = FALSE on a model or on any layer that has sublayers, all child layers become non-trainable as well.\nExample:\n\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \"sigmoid\")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "href": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "title": "Transfer learning and fine-tuning",
    "section": "The typical transfer-learning workflow",
    "text": "The typical transfer-learning workflow\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\nInstantiate a base model and load pre-trained weights into it.\nFreeze all layers in the base model by setting trainable = FALSE.\nCreate a new model on top of the output of one (or several) layers from the base model.\nTrain your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\nInstantiate a base model and load pre-trained weights into it.\nRun your new dataset through it and record the output of one (or several) layers from the base model. This is called feature extraction.\nUse that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it’s a lot faster and cheaper.\nAn issue with that second workflow, though, is that it doesn’t allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\nHere’s what the first workflow looks like in Keras:\nFirst, instantiate a base model with pre-trained weights.\n\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\nThen, freeze the base model.\n\nbase_model$trainable <- FALSE\n\nCreate a new model on top.\n\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n\nTrain the model on new data.\n\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#fine-tuning",
    "href": "guides/keras/transfer_learning.html#fine-tuning",
    "title": "Transfer learning and fine-tuning",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting – keep that in mind.\nIt is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\nIt’s also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\nThis is how to implement fine-tuning of the whole base model:\n\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n\nImportant note about compile() and trainable\nCalling compile() on a model is meant to “freeze” the behavior of that model. This implies that the trainable attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until compile is called again. Hence, if you change any trainable value, make sure to call compile() again on your model for your changes to be taken into account.\nImportant notes about layer_batch_normalization()\nMany image models contain BatchNormalization layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\nBatchNormalization contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\nWhen you set bn_layer$trainable = FALSE, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as weight trainability and inference/training modes are two orthogonal concepts. But the two are tied in the case of the BatchNormalization layer.\nWhen you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = FALSE when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou’ll see this pattern in action in the end-to-end example at the end of this guide."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "href": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "title": "Transfer learning and fine-tuning",
    "section": "Transfer learning and fine-tuning with a custom training loop",
    "text": "Transfer learning and fine-tuning with a custom training loop\nIf instead of fit(), you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list model$trainable_weights when applying gradient updates:\n\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n\nLikewise for fine-tuning."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "href": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "title": "Transfer learning and fine-tuning",
    "section": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset",
    "text": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\nTo solidify these concepts, let’s walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats vs. dogs” classification dataset.\n\nGetting the data\nFirst, let’s fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you’ll probably want to use the utility image_dataset_from_directory() to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n# reticulate::py_install(\"tensorflow_datasets\", pip = TRUE)\ntfds <- reticulate::import(\"tensorflow_datasets\")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\"Number of training samples: %d\", length(train_ds))\n\nNumber of training samples: 9305\n\nprintf(\"Number of validation samples: %d\", length(validation_ds) )\n\nNumber of validation samples: 2326\n\nprintf(\"Number of test samples: %d\", length(test_ds))\n\nNumber of test samples: 2326\n\n\nThese are the first 9 images in the training dataset – as you can see, they’re all different sizes.\n\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\"label: %s   size: %s\",\n                  label, paste(dim(image), collapse = \" x \")))\n  })\n\n\n\n\nWe can also see that label 1 is “dog” and label 0 is “cat”.\n\n\nStandardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn’t a great fit for feeding a neural network. We need to do 2 things:\n\nStandardize to a fixed image size. We pick 150x150.\nNormalize pixel values between -1 and 1. We’ll do this using a layer_normalization() as part of the model itself.\n\nIn general, it’s a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you’ll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\nHere, we’ll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we’ll do the input value scaling as part of the model, when we create it.\nLet’s resize images to 150x150:\n\nlibrary(magrittr, include.only = \"%<>%\")\n\nWarning: package 'magrittr' was built under R version 4.1.2\n\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n\nBesides, let’s batch the data and use caching and prefetching to optimize loading speed.\n\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n\n\n\nUsing random data augmentation\nWhen you don’t have a large image dataset, it’s a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(.1)\n\nLet’s visualize what the first image of the first batch looks like after various random transformations:\n\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\")\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#build-a-model",
    "href": "guides/keras/transfer_learning.html#build-a-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Build a model",
    "text": "Build a model\nNow let’s build a model that follows the blueprint we’ve explained earlier.\nNote that:\n\nWe add layer_rescaling() to scale input values (initially in the [0, 255] range) to the [-1, 1] range.\nWe add a layer_dropout() before the classification layer, for regularization.\nWe make sure to pass training = FALSE when calling the base model, so that it runs in inference mode, so that batchnorm statistics don’t get updated even after we unfreeze the base model for fine-tuning.\n\n\nbase_model = application_xception(\n  weights = \"imagenet\", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs <- layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  N          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n____________________________________________________________________________"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#train-the-top-layer",
    "href": "guides/keras/transfer_learning.html#train-the-top-layer",
    "title": "Transfer learning and fine-tuning",
    "section": "Train the top layer",
    "text": "Train the top layer\n\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 2\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "href": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Do a round of fine-tuning of the entire model",
    "text": "Do a round of fine-tuning of the entire model\nFinally, let’s unfreeze the base model and train the entire model end-to-end with a low learning rate.\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed training = FALSE when calling it when we built the model. This means that the batch normalization layers inside won’t update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  Y          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n____________________________________________________________________________\n\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 1\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#environment-details",
    "href": "guides/keras/transfer_learning.html#environment-details",
    "title": "Transfer learning and fine-tuning",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html",
    "href": "guides/keras/understanding_masking_and_padding.html",
    "title": "Understanding masking & padding",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#introduction",
    "href": "guides/keras/understanding_masking_and_padding.html#introduction",
    "title": "Understanding masking & padding",
    "section": "Introduction",
    "text": "Introduction\nMasking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data.\nPadding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences.\nLet’s take a close look."
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#padding-sequence-data",
    "href": "guides/keras/understanding_masking_and_padding.html#padding-sequence-data",
    "title": "Understanding masking & padding",
    "section": "Padding sequence data",
    "text": "Padding sequence data\nWhen processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):\n\nlist(\n  c(\"Hello\", \"world\", \"!\"),\n  c(\"How\", \"are\", \"you\", \"doing\", \"today\"),\n  c(\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\")\n)\n\n[[1]]\n[1] \"Hello\" \"world\" \"!\"    \n\n[[2]]\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"today\"\n\n[[3]]\n[1] \"The\"      \"weather\"  \"will\"     \"be\"       \"nice\"     \"tomorrow\"\n\n\nAfter vocabulary lookup, the data might be vectorized as integers, e.g.:\n\nlist(\n  c(71, 1331, 4231),\n  c(73, 8, 3215, 55, 927),\n  c(83, 91, 1, 645, 1253, 927)\n)\n\n[[1]]\n[1]   71 1331 4231\n\n[[2]]\n[1]   73    8 3215   55  927\n\n[[3]]\n[1]   83   91    1  645 1253  927\n\n\nThe data is a nested list where individual samples have length 3, 5, and 6, respectively. Since the input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size) in this case), samples that are shorter than the longest item need to be padded with some placeholder value (alternatively, one might also truncate long samples before padding short samples).\nKeras provides a utility function to truncate and pad lists to a common length: pad_sequences().\n\nraw_inputs <- list(\n  c(711, 632, 71),\n  c(73, 8, 3215, 55, 927),\n  c(83, 91, 1, 645, 1253, 927)\n)\n\n# By default, this will pad using 0s; it is configurable via the\n# \"value\" parameter.\n# Note that you could use \"pre\" padding (at the beginning) or\n# \"post\" padding (at the end).\n# We recommend using \"post\" padding when working with RNN layers\n# (in order to be able to use the\n# CuDNN implementation of the layers).\n\npadded_inputs <- pad_sequences(raw_inputs, padding = \"post\")\n\nLoaded Tensorflow version 2.9.1\n\nprint(padded_inputs)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  711  632   71    0    0    0\n[2,]   73    8 3215   55  927    0\n[3,]   83   91    1  645 1253  927"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#masking",
    "href": "guides/keras/understanding_masking_and_padding.html#masking",
    "title": "Understanding masking & padding",
    "section": "Masking",
    "text": "Masking\nNow that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is masking.\nThere are three ways to introduce input masks in Keras models:\n\nAdd a layer_masking() layer.\nConfigure a layer_embedding() layer with mask_zero = TRUE.\nPass a mask argument manually when calling layers that support this argument (e.g. RNN layers)."
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#mask-generating-layers-embedding-and-masking",
    "href": "guides/keras/understanding_masking_and_padding.html#mask-generating-layers-embedding-and-masking",
    "title": "Understanding masking & padding",
    "section": "Mask-generating layers: embedding and masking",
    "text": "Mask-generating layers: embedding and masking\nUnder the hood, these layers will create a mask tensor (2D tensor with shape (batch, sequence_length)), and attach it to the tensor output returned by the layer_masking() or layer_embedding() layer.\n\nembedding <- layer_embedding(input_dim = 5000, output_dim = 16, mask_zero = TRUE)\nmasked_output <- embedding(padded_inputs)\n\nprint(masked_output$`_keras_mask`)\n\ntf.Tensor(\n[[ True  True  True False False False]\n [ True  True  True  True  True False]\n [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n\nmasking_layer <- layer_masking()\n# Simulate the embedding lookup by expanding the 2D input to 3D,\n# with embedding dimension of 10.\nunmasked_embedding <- tf$cast(\n    tf$tile(tf$expand_dims(padded_inputs, axis = -1L), c(1L, 1L, 10L)), tf$float32\n)\nmasked_embedding <- masking_layer(unmasked_embedding)\nprint(masked_embedding$`_keras_mask`)\n\ntf.Tensor(\n[[ True  True  True False False False]\n [ True  True  True  True  True False]\n [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n\n\nAs you can see from the printed result, the mask is a 2D boolean tensor with shape (batch_size, sequence_length), where each individual FALSE entry indicates that the corresponding timestep should be ignored during processing."
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#mask-propagation-in-the-functional-api-and-sequential-api",
    "href": "guides/keras/understanding_masking_and_padding.html#mask-propagation-in-the-functional-api-and-sequential-api",
    "title": "Understanding masking & padding",
    "section": "Mask propagation in the Functional API and Sequential API",
    "text": "Mask propagation in the Functional API and Sequential API\nWhen using the Functional API or the Sequential API, a mask generated by an layer_embedding() or layer_masking() will be propagated through the network for any layer that is capable of using them (for example, RNN layers). Keras will automatically fetch the mask corresponding to an input and pass it to any layer that knows how to use it.\nFor instance, in the following Sequential model, the LSTM layer will automatically receive a mask, which means it will ignore padded values:\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(input_dim = 5000, output_dim = 16, mask_zero = TRUE) %>% \n  layer_lstm(32)\n\nThis is also the case for the following Functional API model:\n\ninputs <- layer_input(shape = shape(NULL), dtype = \"int32\")\noutputs <- inputs %>% \n  layer_embedding(input_dim = 5000, output_dim = 16, mask_zero = TRUE) %>% \n  layer_lstm(units = 32)\nmodel <- keras_model(inputs, outputs)"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#passing-mask-tensors-directly-to-layers",
    "href": "guides/keras/understanding_masking_and_padding.html#passing-mask-tensors-directly-to-layers",
    "title": "Understanding masking & padding",
    "section": "Passing mask tensors directly to layers",
    "text": "Passing mask tensors directly to layers\nLayers that can handle masks (such as the LSTM layer) have a mask argument in their call method.\nMeanwhile, layers that produce a mask (e.g. Embedding) expose a compute_mask(input, previous_mask) method which you can call.\nThus, you can pass the output of the compute_mask() method of a mask-producing layer to the call method of a mask-consuming layer, like this:\n\nmy_layer <- new_layer_class(\n  \"my_layer\",\n  initialize = function(...) {\n    super()$`__init__`(...)\n    self$embedding <- layer_embedding(\n      input_dim = 5000, \n      output_dim = 16, \n      mask_zero = TRUE\n    )\n    self$lstm <- layer_lstm(units = 32)\n  },\n  call = function(inputs) {\n    x <- self$embedding(inputs)\n    # Note that you could also prepare a `mask` tensor manually.\n    # It only needs to be a boolean tensor\n    # with the right shape, i$e. (batch_size, timesteps).\n    mask <- self$embedding$compute_mask(inputs)\n    output <- self$lstm(x, mask = mask)  # The layer will ignore the masked values\n    output\n  }\n)\n\n\nlayer <- my_layer()\nx <- array(as.integer(runif(32*10)*100), dim = c(32, 10))\nlayer(x)\n\ntf.Tensor(\n[[-0.00205488  0.00106485  0.0013517  ... -0.00200369 -0.00046926\n   0.00044221]\n [-0.01361378 -0.0092069  -0.0084025  ...  0.00248097 -0.00110209\n  -0.00467769]\n [-0.00342713  0.00777482 -0.00901797 ... -0.00059411  0.00423267\n   0.000895  ]\n ...\n [ 0.00578892  0.00677103  0.00049386 ... -0.00435947  0.00212847\n   0.00391663]\n [ 0.00251169 -0.00628054 -0.00592228 ... -0.00285684 -0.00215456\n   0.00113647]\n [ 0.00398869 -0.00239744  0.00408143 ... -0.0066454   0.00028073\n  -0.00198999]], shape=(32, 32), dtype=float32)"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#supporting-masking-in-your-custom-layers",
    "href": "guides/keras/understanding_masking_and_padding.html#supporting-masking-in-your-custom-layers",
    "title": "Understanding masking & padding",
    "section": "Supporting masking in your custom layers",
    "text": "Supporting masking in your custom layers\nSometimes, you may need to write layers that generate a mask (like Embedding), or layers that need to modify the current mask.\nFor instance, any layer that produces a tensor with a different time dimension than its input, such as a Concatenate layer that concatenates on the time dimension, will need to modify the current mask so that downstream layers will be able to properly take masked timesteps into account.\nTo do this, your layer should implement the layer$compute_mask() method, which produces a new mask given the input and the current mask.\nHere is an example of a temporal_split layer that needs to modify the current mask.\n\nlayer_temporal_split <- new_layer_class(\n  \"temporal_split\",\n  call = function(inputs) {\n    # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n    # subtensors along the time axis (axis 1).\n    tf$split(inputs, 2L, axis = 1L)\n  },\n  compute_mask = function(inputs, mask = NULL) {\n    # Also split the mask into 2 if it presents.\n    if (is.null(mask)) return(NULL)\n    tf$split(mask, 2L, axis = 1L)\n  }\n)\n\nc(first_half, second_half) %<-% layer_temporal_split()(masked_embedding)\nprint(first_half$`_keras_mask`)\n\ntf.Tensor(\n[[ True  True  True]\n [ True  True  True]\n [ True  True  True]], shape=(3, 3), dtype=bool)\n\nprint(second_half$`_keras_mask`)\n\ntf.Tensor(\n[[False False False]\n [ True  True False]\n [ True  True  True]], shape=(3, 3), dtype=bool)\n\n\nHere is another example of a custom_embedding layer that is capable of generating a mask from input values:\n\nlayer_custom_embedding <- new_layer_class(\n  \"custom_embedding\",\n  initialize = function(input_dim, output_dim, mask_zero = FALSE, ...) {\n    super()$ `__init__`(...)\n    self$input_dim <- input_dim\n    self$output_dim <- output_dim\n    self$mask_zero <- mask_zero  \n  },\n  build = function(input_shape) {\n    self$embeddings <- self$add_weight(\n      shape = shape(self$input_dim, self$output_dim),\n      initializer = \"random_normal\",\n      dtype = \"float32\"\n    )\n  },\n  call = function(inputs) {\n    tf$nn$embedding_lookup(self$embeddings, inputs)\n  },\n  compute_mask = function(inputs, mask = NULL) {\n    if (!self$mask_zero) return(NULL)\n    tf$not_equal(inputs, 0L)\n  }\n)\n\nlayer <- layer_custom_embedding(\n  input_dim = 10, \n  output_dim = 32, \n  mask_zero = TRUE\n)\n\nx <- array(as.integer(runif(3*10)*9), dim = c(3, 10))\ny <- layer(x)\nmask <- layer$compute_mask(x)\n\nprint(mask)\n\ntf.Tensor(\n[[ True  True False  True False False  True  True  True  True]\n [ True  True  True False  True False  True  True  True  True]\n [ True  True  True  True  True  True False  True  True  True]], shape=(3, 10), dtype=bool)\n\n\nNote: For more details about format limitations related to masking, see the serialization guide."
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#opting-in-to-mask-propagation-on-compatible-layers",
    "href": "guides/keras/understanding_masking_and_padding.html#opting-in-to-mask-propagation-on-compatible-layers",
    "title": "Understanding masking & padding",
    "section": "Opting-in to mask propagation on compatible layers",
    "text": "Opting-in to mask propagation on compatible layers\nMost layers don’t modify the time dimension, so don’t need to modify the current mask. However, they may still want to be able to propagate the current mask, unchanged, to the next layer. This is an opt-in behavior. By default, a custom layer will destroy the current mask (since the framework has no way to tell whether propagating the mask is safe to do).\nIf you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set self$supports_masking = TRUE in the layer constructor. In this case, the default behavior of compute_mask() is to just pass the current mask through.\nHere’s an example of a layer that is whitelisted for mask propagation:\n\nlayer_my_activation <- new_layer_class(\n  \"my_activation\",\n  initialize = function(...) {\n    super()$`__init__`(...)\n    self$supports_masking <- TRUE\n  },\n  call = function(inputs) {\n    tf$nn$relu(inputs)\n  }\n)\n\nYou can now use this custom layer in-between a mask-generating layer (like Embedding) and a mask-consuming layer (like LSTM), and it will pass the mask along so that it reaches the mask-consuming layer.\n\ninputs <- layer_input(shape = shape(NULL), dtype = \"int32\")\nx <- inputs %>% \n  layer_embedding(input_dim = 5000, output_dim = 16, mask_zero = TRUE) %>% \n  layer_my_activation() # Will pass the mask along\nprint(x$`_keras_mask`)\n\nKerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name='Placeholder_1:0')\n\noutputs <- layer_lstm(x, 32)# Will receive the mask\nmodel <- keras_model(inputs, outputs)"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#writing-layers-that-need-mask-information",
    "href": "guides/keras/understanding_masking_and_padding.html#writing-layers-that-need-mask-information",
    "title": "Understanding masking & padding",
    "section": "Writing layers that need mask information",
    "text": "Writing layers that need mask information\nSome layers are mask consumers: they accept a mask argument in call and use it to determine whether to skip certain time steps.\nTo write such a layer, you can simply add a mask = NULL argument in your call signature. The mask associated with the inputs will be passed to your layer whenever it is available.\nHere’s a simple example below: a layer that computes a softmax over the time dimension (axis 1) of an input sequence, while discarding masked timesteps.\n\nlayer_temporal_softmax <- new_layer_class(\n  \"temporal_softmax\",\n  call = function(inputs, mask = NULL) {\n   broadcast_float_mask <- tf$expand_dims(tf$cast(mask, \"float32\"), -1L)\n   inputs_exp <- tf$exp(inputs) * broadcast_float_mask\n   inputs_sum <- tf$reduce_sum(\n     inputs_exp * broadcast_float_mask, \n     axis = -1L, \n     keepdims = TRUE\n   )\n   inputs_exp / inputs_sum \n  }\n)\n\ninputs <- layer_input(shape = shape(NULL), dtype = \"int32\")\noutputs <- inputs %>% \n  layer_embedding(input_dim = 10, output_dim = 32, mask_zero = TRUE) %>% \n  layer_dense(1) %>% \n  layer_temporal_softmax()\n\nmodel <- keras_model(inputs, outputs)\ny <- model(\n  array(sample.int(9, 32*100, replace = TRUE), dim = c(32, 100)),\n  array(runif(32*100), dim = c(32, 100, 1))\n)"
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#summary",
    "href": "guides/keras/understanding_masking_and_padding.html#summary",
    "title": "Understanding masking & padding",
    "section": "Summary",
    "text": "Summary\nThat is all you need to know about padding & masking in Keras. To recap:\n\n“Masking” is how layers are able to know when to skip / ignore certain timesteps in sequence inputs.\nSome layers are mask-generators: Embedding can generate a mask from input values (if mask_zero = TRUE), and so can the Masking layer.\nSome layers are mask-consumers: they expose a mask argument in their __call__ method. This is the case for RNN layers.\nIn the Functional API and Sequential API, mask information is propagated automatically.\nWhen using layers in a standalone way, you can pass the mask arguments to layers manually.\nYou can easily write layers that modify the current mask, that generate a new mask, or that consume the mask associated with the inputs."
  },
  {
    "objectID": "guides/keras/understanding_masking_and_padding.html#environment-details",
    "href": "guides/keras/understanding_masking_and_padding.html#environment-details",
    "title": "Understanding masking & padding",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/working_with_rnns.html",
    "href": "guides/keras/working_with_rnns.html",
    "title": "Working with RNNs",
    "section": "",
    "text": "Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\nThe Keras RNN API is designed with a focus on:\n\nEase of use: the built-in layer_rnn(), layer_lstm(), layer_gru() layers enable you to quickly build recurrent models without having to make difficult configuration choices.\nEase of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic layer_rnn layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#setup",
    "href": "guides/keras/working_with_rnns.html#setup",
    "title": "Working with RNNs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "href": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "title": "Working with RNNs",
    "section": "Built-in RNN layers: a simple example",
    "text": "Built-in RNN layers: a simple example\nThere are three built-in RNN layers in Keras:\n\nlayer_simple_rnn(), a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\nlayer_gru(), first proposed in Cho et al., 2014.\nlayer_lstm(), first proposed in Hochreiter & Schmidhuber, 1997.\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a layer_lstm().\n\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n\nLoaded Tensorflow version 2.9.1\n\nmodel\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 64)              64000       \n lstm (LSTM)                      (None, 128)                   98816       \n dense (Dense)                    (None, 10)                    1290        \n============================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nBuilt-in RNNs support a number of useful features:\n\nRecurrent dropout, via the dropout and recurrent_dropout arguments\nAbility to process an input sequence in reverse, via the go_backwards argument\nLoop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the unroll argument\n…and more.\n\nFor more information, see the RNN API documentation."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#outputs-and-states",
    "href": "guides/keras/working_with_rnns.html#outputs-and-states",
    "title": "Working with RNNs",
    "section": "Outputs and states",
    "text": "Outputs and states\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is (batch_size, units) where units corresponds to the units argument passed to the layer’s constructor.\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences = TRUE. The shape of this output is (batch_size, timesteps, units).\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_1\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding_1 (Embedding)          (None, None, 64)              64000       \n gru (GRU)                        (None, None, 256)             247296      \n simple_rnn (SimpleRNN)           (None, 128)                   49280       \n dense_1 (Dense)                  (None, 10)                    1290        \n============================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\nTo configure a RNN layer to return its internal state, set return_state = TRUE when creating the layer. Note that LSTM has 2 state tensors, but GRU only has one.\nTo configure the initial state of the layer, call the layer instance with the additional named argument initial_state. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\"encoder\")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \"decoder\")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n\nModel: \"model\"\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, None)]  0        []                        \n input_2 (InputLayer)    [(None, None)]  0        []                        \n embedding_2 (Embedding)  (None, None, 6  64000   ['input_1[0][0]']         \n                         4)                                                 \n embedding_3 (Embedding)  (None, None, 6  128000  ['input_2[0][0]']         \n                         4)                                                 \n encoder (LSTM)          [(None, 64),    33024    ['embedding_2[0][0]']     \n                          (None, 64),                                       \n                          (None, 64)]                                       \n decoder (LSTM)          (None, 64)      33024    ['embedding_3[0][0]',     \n                                                   'encoder[0][1]',         \n                                                   'encoder[0][2]']         \n dense_2 (Dense)         (None, 10)      650      ['decoder[0][0]']         \n============================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n____________________________________________________________________________"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "href": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "title": "Working with RNNs",
    "section": "RNN layers and RNN cells",
    "text": "RNN layers and RNN cells\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\nThe cell is the inside of the for loop of a RNN layer. Wrapping a cell inside a layer_rnn() layer gives you a layer capable of processing a sequence, e.g. layer_rnn(layer_lstm_cell(10)).\nMathematically, layer_rnn(layer_lstm_cell(10)) produces the same result as layer_lstm(10). In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in layer_gru() and layer_lstm() layers enable the use of CuDNN and you may see better performance.\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\nlayer_simple_rnn_cell() corresponds to the layer_simple_rnn() layer.\nlayer_gru_cell corresponds to the layer_gru layer.\nlayer_lstm_cell corresponds to the layer_lstm layer.\n\nThe cell abstraction, together with the generic layer_rnn() class, makes it very easy to implement custom RNN architectures for your research."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "href": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "title": "Working with RNNs",
    "section": "Cross-batch statefulness",
    "text": "Cross-batch statefulness\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of cross-batch statefulness.\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer’s state. That way, the layer can retain information about the entirety of the sequence, even though it’s only seeing one sub-sequence at a time.\nYou can do this by setting stateful = TRUE in the constructor.\nIf you have a sequence s = c(t0, t1, ... t1546, t1547), you would split it into e.g.\n\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n\nThen you would process it via:\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n\nWhen you want to clear the state, you can use layer$reset_states().\n\nNote: In this setup, sample i in a given batch is assumed to be the continuation of sample i in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], the next batch should contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200].\n\nHere is a complete example:\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n\n\nRNN State Reuse\nThe recorded states of the RNN layer are not included in the layer$weights(). If you would like to reuse the state from a RNN layer, you can retrieve the states value by layer$states and use it as the initial state of a new layer instance via the Keras functional API like new_layer(inputs, initial_state = layer$states), or model subclassing.\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "href": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "title": "Working with RNNs",
    "section": "Bidirectional RNNs",
    "text": "Bidirectional RNNs\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\nKeras provides an easy API for you to build such bidirectional RNNs: the bidirectional() wrapper.\n\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_2\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n bidirectional_1 (Bidirectional)  (None, 5, 128)                38400       \n bidirectional (Bidirectional)    (None, 64)                    41216       \n dense_3 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nUnder the hood, bidirectional() will copy the RNN layer passed in, and flip the go_backwards field of the newly copied layer, so that it will process the inputs in reverse order.\nThe output of the bidirectional RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the merge_mode parameter in the bidirectional wrapper constructor. For more details about bidirectional, please check the API docs."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "href": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "title": "Working with RNNs",
    "section": "Performance optimization and CuDNN kernels",
    "text": "Performance optimization and CuDNN kernels\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior layer_cudnn_gru/layer_cudnn_lstm layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\nSince the CuDNN kernel is built with certain assumptions, this means the layer will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers. E.g.:\n\nChanging the activation function from \"tanh\" to something else.\nChanging the recurrent_activation function from \"sigmoid\" to something else.\nUsing recurrent_dropout > 0.\nSetting unroll to TRUE, which forces LSTM/GRU to decompose the inner tf$while_loop into an unrolled for loop.\nSetting use_bias to FALSE.\nUsing masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the LSTM and GRU layers.\n\nUsing CuDNN kernels when available\nLet’s build a simple LSTM model to demonstrate the performance difference.\nWe’ll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we’ll predict the digit’s label.\n\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n\nLet’s load the MNIST dataset:\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n\nLet’s create a model instance and train it.\nWe choose sparse_categorical_crossentropy() as the loss function for the model. The output of the model has shape of (batch_size, 10). The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"sgd\",\n    metrics = \"accuracy\"\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nNow, let’s compare to a model that does not use the CuDNN kernel:\n\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\"sgd\",\n    metrics=\"accuracy\",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The tf$device() annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\nYou simply don’t have to worry about the hardware you’re running on anymore. Isn’t that pretty cool?\n\nwith(tf$device(\"CPU:0\"), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \"Predicted result is: %s, target result is: %s\\n\", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n\nPredicted result is: 3, target result is: 5"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "href": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "title": "Working with RNNs",
    "section": "RNNs with list/dict inputs, or nested inputs",
    "text": "RNNs with list/dict inputs, or nested inputs\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\nDefine a custom cell that supports nested input/output\nSee Making new Layers & Models via subclassing for details on writing your own layers.\n\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \"uniform\",\n      name = \"kernel_1\"\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \"uniform\",\n      name = \"kernel_2_3\"\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\"bij,ijkl->bkl\", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\"unit_1\" = self$unit_1,\n         \"unit_2\" = self$unit_2,\n         \"unit_3\" = self$unit_3)\n  }\n}\n\n\n\nBuild a RNN model with nested input/output\nLet’s build a Keras model that uses a layer_rnn layer and the custom cell we just defined.\n\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"accuracy\")\n\n\n\nTrain the model with randomly generated data\nSince there isn’t a good candidate dataset for this model, we use random data for demonstration.\n\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n\nWith keras::layer_rnn(), you are only expected to define the math logic for an individual step within the sequence, and the layer_rnn() will handle the sequence iteration for you. It’s an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\nFor more details, please visit the API docs."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#environment-details",
    "href": "guides/keras/working_with_rnns.html#environment-details",
    "title": "Working with RNNs",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html",
    "title": "Writing a training loop from scratch",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)"
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#introduction",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#introduction",
    "title": "Writing a training loop from scratch",
    "section": "Introduction",
    "text": "Introduction\nKeras provides default training and evaluation loops, fit() and evaluate(). Their usage is covered in the guide Training & evaluation with the built-in methods.\nIf you want to customize the learning algorithm of your model while still leveraging the convenience of fit() (for instance, to train a GAN using fit()), you can subclass the Model class and implement your own train_step() method, which is called repeatedly during fit(). This is covered in the guide Customizing what happens in fit().\nNow, if you want very low-level control over training & evaluation, you should write your own training & evaluation loops from scratch. This is what this guide is about."
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#using-the-gradienttape-a-first-end-to-end-example",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#using-the-gradienttape-a-first-end-to-end-example",
    "title": "Writing a training loop from scratch",
    "section": "Using the GradientTape: a first end-to-end example",
    "text": "Using the GradientTape: a first end-to-end example\nCalling a model inside a GradientTape scope enables you to retrieve the gradients of the trainable weights of the layer with respect to a loss value. Using an optimizer instance, you can use these gradients to update these variables (which you can retrieve using model$trainable_weights).\nLet’s consider a simple MNIST model:\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\n\nLoaded Tensorflow version 2.9.1\n\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10, name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nLet’s train it using mini-batch gradient with a custom training loop. First, we’re going to need an optimizer, a loss function, and a dataset:\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\n\n# Instantiate a loss function.\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Prepare the training dataset.\nbatch_size <- 64\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\nx_train <- x_train %>% array_reshape(dim = c(60000, 784))/255\nx_test <- x_test %>% array_reshape(dim = c(10000, 784))/255\n\n# Reserve 10,000 samples for validation.\nx_val <- x_train[-(1:50000),]\ny_val <- y_train[-(1:50000)]\nx_train <- x_train[(1:50000),]\ny_train <- y_train[(1:50000)]\n\n# Prepare the training dataset.\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(buffer_size = 1024) %>% \n  dataset_batch(batch_size)\n\n# Prepare the validation dataset.\nval_dataset <- list(x_val, y_val) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(batch_size)\n\nHere’s our training loop:\n\nWe open a for loop that iterates over epochs\nFor each epoch, we open a for loop that iterates over the dataset, in batches\nFor each batch, we open a GradientTape() scope\nInside this scope, we call the model (forward pass) and compute the loss\nOutside the scope, we retrieve the gradients of the weights of the model with regard to the loss\nFinally, we use the optimizer to update the weights of the model based on the gradients\n\n\n\n\n\n\n\nNote\n\n\n\nIn the example train_dataset is a TensorFlow Dataset, thus it can’t be iterated in normal R for loops. That’s why we wrap the second loop into a autograph call. autograph will compile the expression into efficient TensorFlow code to quickly evaluate the loop.\n\n\n\nepochs <- 2\nfor(epoch in seq_len(epochs)) {\n  cat(\"Start of epoch \", epoch, \"\\n\")\n  # Iterate over the batches of the dataset.\n  tfautograph::autograph(for (batch in train_dataset) {\n    # Open a GradientTape to record the operations run\n    # during the forward pass, which enables auto-differentiation.\n    with(tf$GradientTape() %as% tape, {\n      # Run the forward pass of the layer.\n      # The operations that the layer applies\n      # to its inputs are going to be recorded\n      # on the GradientTape.\n      logits <- model(batch[[1]], training = TRUE)  # Logits for this minibatch\n      \n      # Compute the loss value for this minibatch.\n      loss_value <- loss_fn(batch[[2]], logits)\n    })\n    \n    # Use the gradient tape to automatically retrieve\n    # the gradients of the trainable variables with respect to the loss.\n    grads <- tape$gradient(loss_value, model$trainable_weights)\n    \n    # Run one step of gradient descent by updating\n    # the value of the variables to minimize the loss.\n    optimizer$apply_gradients(zip_lists(grads, model$trainable_weights))\n    \n  })\n}\n\nStart of epoch  1 \nStart of epoch  2"
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#low-level-handling-of-metrics",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#low-level-handling-of-metrics",
    "title": "Writing a training loop from scratch",
    "section": "Low-level handling of metrics",
    "text": "Low-level handling of metrics\nLet’s add metrics monitoring to this basic loop.\nYou can readily reuse the built-in metrics (or custom ones you wrote) in such training loops written from scratch. Here’s the flow:\n\nInstantiate the metric at the start of the loop\nCall metric$update_state() after each batch\nCall metric$result() when you need to display the current value of the metric\nCall metric$reset_states() when you need to clear the state of the metric (typically at the end of an epoch)\n\nLet’s use this knowledge to compute sparse_categorical_accuracy on validation data at the end of each epoch:\n\n# Get model\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10, name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\n\n# Instantiate a loss function.\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Prepare the metrics.\ntrain_acc_metric <- metric_sparse_categorical_accuracy()\nval_acc_metric <- metric_sparse_categorical_accuracy()\n\nHere’s our training & evaluation loop:\n\nepochs <- 2\nfor(epoch in seq_len(epochs)) {\n  cat(\"Start of epoch \", epoch, \"\\n\")\n  tfautograph::autograph(for (batch in train_dataset) {\n    with(tf$GradientTape() %as% tape, {\n      logits <- model(batch[[1]], training = TRUE)\n      loss_value <- loss_fn(batch[[2]], logits)\n    })\n    grads <- tape$gradient(loss_value, model$trainable_weights)\n    optimizer$apply_gradients(zip_lists(grads, model$trainable_weights))\n\n    # Update training metric.\n    train_acc_metric$update_state(batch[[2]], logits)\n  })\n  \n  train_acc <- as.numeric(train_acc_metric$result())\n  cat(\"Training acc over epoch: \", train_acc, \"\\n\")\n  train_acc_metric$reset_states()\n\n  # Run a validation loop at the end of each epoch.\n  tfautograph::autograph(for(batch in val_dataset) {\n    val_logits <- model(batch[[1]], training = FALSE)\n    # Update val metrics\n    val_acc_metric$update_state(batch[[2]], val_logits)\n  })\n  \n  val_acc <- as.numeric(val_acc_metric$result())\n  cat(\"Validation acc over epoch: \", val_acc, \"\\n\")\n  val_acc_metric$reset_states()\n}\n\nStart of epoch  1 \nTraining acc over epoch:  0.27868 \nValidation acc over epoch:  0.4074 \nStart of epoch  2 \nTraining acc over epoch:  0.52598 \nValidation acc over epoch:  0.6312 \n\n\nIt’s common to extract out the expressin inside the second loop into a new function called train_step. For example:\n\ntrain_step <- function(batch) {\n  with(tf$GradientTape() %as% tape, {\n      logits <- model(batch[[1]], training = TRUE)\n      loss_value <- loss_fn(batch[[2]], logits)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(zip_lists(grads, model$trainable_weights))\n  \n  # Update training metric.\n  train_acc_metric$update_state(batch[[2]], logits)\n}"
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#low-level-handling-of-losses-tracked-by-the-model",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#low-level-handling-of-losses-tracked-by-the-model",
    "title": "Writing a training loop from scratch",
    "section": "Low-level handling of losses tracked by the model",
    "text": "Low-level handling of losses tracked by the model\nLayers & models recursively track any losses created during the forward pass by layers that call self$add_loss(value). The resulting list of scalar loss values are available via the property model$losses at the end of the forward pass.\nIf you want to be using these loss components, you should sum them and add them to the main loss in your training step.\nConsider this layer, that creates an activity regularization loss:\n\nlayer_activity_regularization <- new_layer_class(\n  \"activity_regularization\",\n  call = function(inputs) {\n    self$add_loss(1e-2 * tf$reduce_sum(inputs))\n    inputs\n  }\n)\n\nLet’s build a really simple model that uses it:\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\") %>% \n  # Insert activity regularization as a layer\n  layer_activity_regularization() %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10, name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nHere’s what our training step should look like now:\n\ntrain_step <- function(batch) {\n  with(tf$GradientTape() %as% tape, {    \n    logits <- model(batch[[1]], training = TRUE)\n    loss_value <- loss_fn(batch[[2]], logits)\n    # Add any extra losses created during the forward pass.\n    loss_value <- loss_value + do.call(sum, model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(zip_lists(grads, model$trainable_weights))\n  train_acc_metric$update_state(batch[[2]], logits)\n  loss_value\n}"
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#summary",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#summary",
    "title": "Writing a training loop from scratch",
    "section": "Summary",
    "text": "Summary\nNow you know everything there is to know about using built-in training loops and writing your own from scratch.\nTo conclude, here’s a simple end-to-end example that ties together everything you’ve learned in this guide: a DCGAN trained on MNIST digits."
  },
  {
    "objectID": "guides/keras/writing_a_training_loop_from_scratch.html#end-to-end-example-a-gan-training-loop-from-scratch",
    "href": "guides/keras/writing_a_training_loop_from_scratch.html#end-to-end-example-a-gan-training-loop-from-scratch",
    "title": "Writing a training loop from scratch",
    "section": "End-to-end example: a GAN training loop from scratch",
    "text": "End-to-end example: a GAN training loop from scratch\nYou may be familiar with Generative Adversarial Networks (GANs). GANs can generate new images that look almost real, by learning the latent distribution of a training dataset of images (the “latent space” of the images).\nA GAN is made of two parts: a “generator” model that maps points in the latent space to points in image space, a “discriminator” model, a classifier that can tell the difference between real images (from the training dataset) and fake images (the output of the generator network).\nA GAN training loop looks like this:\n\nTrain the discriminator.\n\n\nSample a batch of random points in the latent space.\nTurn the points into fake images via the “generator” model.\nGet a batch of real images and combine them with the generated images.\nTrain the “discriminator” model to classify generated vs. real images.\n\n\nTrain the generator.\n\n\nSample random points in the latent space.\nTurn the points into fake images via the “generator” network.\nGet a batch of real images and combine them with the generated images.\nTrain the “generator” model to “fool” the discriminator and classify the fake images as real.\n\nFor a much more detailed overview of how GANs works, see Deep Learning with Python.\nLet’s implement this training loop. First, create the discriminator meant to classify fake vs real digits:\n\ndiscriminator <- keras_model_sequential(\n  name = \"discriminator\", \n  input_shape = shape(28, 28, 1)\n) %>% \n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_global_max_pooling_2d() %>% \n  layer_dense(1)\nsummary(discriminator)\n\nModel: \"discriminator\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 14, 14, 64)            640         \n leaky_re_lu_1 (LeakyReLU)        (None, 14, 14, 64)            0           \n conv2d (Conv2D)                  (None, 7, 7, 128)             73856       \n leaky_re_lu (LeakyReLU)          (None, 7, 7, 128)             0           \n global_max_pooling2d (GlobalMaxP  (None, 128)                  0           \n ooling2D)                                                                  \n dense_6 (Dense)                  (None, 1)                     129         \n============================================================================\nTotal params: 74,625\nTrainable params: 74,625\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nThen let’s create a generator network, that turns latent vectors into outputs of shape (28, 28, 1) (representing MNIST digits):\n\nlatent_dim <- 128\ngenerator <- keras_model_sequential(\n  input_shape = shape(latent_dim), \n  name = \"generator\"\n) %>% \n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_reshape(c(7, 7, 128)) %>% \n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>% \n  layer_activation_leaky_relu(alpha = 0.2) %>% \n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n\nHere’s the key bit: the training loop. As you can see it is quite straightforward. The training step function only takes 17 lines.\n\n# Instantiate one optimizer for the discriminator and another for the generator.\nd_optimizer <- optimizer_adam(learning_rate = 0.0003)\ng_optimizer <- optimizer_adam(learning_rate = 0.0004)\n\n# Instantiate a loss function.\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n\ntrain_step <- function(real_images) {\n  # Sample random points in the latent space\n  random_latent_vectors <- tf$random$normal(shape = shape(batch_size, latent_dim))\n  # Decode them to fake images\n  generated_images <- generator(random_latent_vectors)\n  # Combine them with real images\n  combined_images <- tf$concat(list(generated_images, real_images), axis = 0L)\n  \n  # Assemble labels discriminating real from fake images\\\n  labels <- tf$concat(list(\n    tf$ones(shape(batch_size, 1)), \n    tf$zeros(shape(real_images$shape[[1]], 1))), \n    axis = 0L\n  )\n  # Add random noise to the labels - important trick!\n  labels <- labels + 0.05 * tf$random$uniform(labels$shape)\n  \n  # Train the discriminator\n  with(tf$GradientTape() %as% tape, {    \n    predictions <- discriminator(combined_images)\n    d_loss <- loss_fn(labels, predictions)  \n  })\n  \n  grads <- tape$gradient(d_loss, discriminator$trainable_weights)\n  d_optimizer$apply_gradients(zip_lists(grads, discriminator$trainable_weights))\n  \n  # Sample random points in the latent space\n  random_latent_vectors <- tf$random$normal(shape = shape(batch_size, latent_dim))\n  # Assemble labels that say \"all real images\"\n  misleading_labels <- tf$zeros(shape(batch_size, 1))\n  \n  # Train the generator (note that we should *not* update the weights\n  # of the discriminator)!\n  with(tf$GradientTape() %as% tape, {    \n    predictions <- discriminator(generator(random_latent_vectors))\n    g_loss <- loss_fn(misleading_labels, predictions)\n  })\n  \n  grads <- tape$gradient(g_loss, generator$trainable_weights)\n  g_optimizer$apply_gradients(zip_lists(grads, generator$trainable_weights))\n  list(d_loss, g_loss, generated_images)\n}\n\nLet’s train our GAN, by repeatedly calling train_step on batches of images. Since our discriminator and generator are convnets, you’re going to want to run this code on a GPU.\n\n# Prepare the dataset. We use both the training & test MNIST digits.\nbatch_size <- 64\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\nx_train[] <- x_train/255\nx_test[] <- x_test/255\n\ndataset <- tensor_slices_dataset(x_train) %>% \n  dataset_concatenate(tensor_slices_dataset(x_test)) %>% \n  dataset_map(function(x) {\n    tf$cast(tf$expand_dims(x, -1L), tf$float32)\n  }) %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(batch_size)\n\nepochs <- 1  # In practice you need at least 20 epochs to generate nice digits.\nsave_dir <- \"./\"\nfor (epoch in seq_len(epochs)) {\n  cat(\"\\nStart epoch \", epoch, \"\\n\")\n  tfautograph::autograph(for (real_images in dataset) {\n    c(d_loss, g_loss, generated_images) %<-% train_step(real_images)\n  })\n}\n\n\nStart epoch  1 \n\ngenerated_images[1,,,] %>% \n  image_array_save(path = \"generated_img.png\")\n\n\nThat’s it! You’ll get nice-looking fake MNIST digits after just ~30s of training on the Colab GPU."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html",
    "href": "guides/keras/writing_your_own_callbacks.html",
    "title": "Writing your own callbacks",
    "section": "",
    "text": "A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include callback_tensorboard() to visualize training progress and results with TensorBoard, or callback_model_checkpoint() to periodically save your model during training.\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#setup",
    "href": "guides/keras/writing_your_own_callbacks.html#setup",
    "title": "Writing your own callbacks",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "href": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "title": "Writing your own callbacks",
    "section": "Keras callbacks overview",
    "text": "Keras callbacks overview\nAll callbacks subclass the keras$callbacks$Callback class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\nYou can pass a list of callbacks (as a named argument callbacks) to the following keras model methods:\n\nfit()\nevaluate()\npredict()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "href": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "title": "Writing your own callbacks",
    "section": "An overview of callback methods",
    "text": "An overview of callback methods\n\nGlobal methods\n\non_(train|test|predict)_begin(self, logs=None)\nCalled at the beginning of fit/evaluate/predict.\n\n\non_(train|test|predict)_end(self, logs=None)\nCalled at the end of fit/evaluate/predict.\n\n\n\nBatch-level methods for training/testing/predicting\n\non_(train|test|predict)_batch_begin(self, batch, logs=None)\nCalled right before processing a batch during training/testing/predicting.\n\n\non_(train|test|predict)_batch_end(self, batch, logs=None)\nCalled at the end of training/testing/predicting a batch. Within this method, logs is a dict containing the metrics results.\n\n\n\nEpoch-level methods (training only)\n\non_epoch_begin(self, epoch, logs=None)\nCalled at the beginning of an epoch during training.\n\n\non_epoch_end(self, epoch, logs=None)\nCalled at the end of an epoch during training."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "href": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "title": "Writing your own callbacks",
    "section": "A basic example",
    "text": "A basic example\nLet’s take a look at a concrete example. To get started, let’s import tensorflow and define a simple Sequential Keras model:\n\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n\nNow, define a simple custom callback that logs:\n\nWhen fit/evaluate/predict starts & ends\nWhen each epoch starts & ends\nWhen each training batch starts & ends\nWhen each evaluation (test) batch starts & ends\nWhen each inference (prediction) batch starts & ends\n\n\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  str(logs); cat(\"\\n\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n\nLet’s try it out:\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n\n\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n\n\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n\n\nUsage of logs dict\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "href": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "title": "Writing your own callbacks",
    "section": "Usage of self$model attribute",
    "text": "Usage of self$model attribute\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: self$model.\nHere are of few of the things you can do with self$model in a callback:\n\nSet self$model$stop_training <- TRUE to immediately interrupt training.\nMutate hyperparameters of the optimizer (available as self$model$optimizer), such as self$model$optimizer$learning_rate.\nSave the model at period intervals.\nRecord the output of predict(model) on a few test samples at the end of each epoch, to use as a sanity check during training.\nExtract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\netc.\n\nLet’s see this in action in a couple of examples."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "href": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "title": "Writing your own callbacks",
    "section": "Examples of Keras callback applications",
    "text": "Examples of Keras callback applications\n\nEarly stopping at minimum loss\nThis first example shows the creation of a Callback that stops training when the minimum of loss has been reached, by setting the attribute self$model$stop_training (boolean). Optionally, you can provide an argument patience to specify how many epochs we should wait before stopping after having reached a local minimum.\nkeras$callbacks$EarlyStopping provides a more complete and general implementation.\n\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n\n\n\nLearning rate scheduling\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\nSee keras$callbacks$LearningRateScheduler for a more general implementations (in RStudio, press F1 while the cursor is over LearningRateScheduler and a browser will open to this page).\n\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n\n\n\nBuilt-in Keras callbacks\nBe sure to check out the existing Keras callbacks by reading the API docs. Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#environment-details",
    "href": "guides/keras/writing_your_own_callbacks.html#environment-details",
    "title": "Writing your own callbacks",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html",
    "href": "guides/tensorflow/autodiff.html",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "href": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Automatic Differentiation and Gradients",
    "text": "Automatic Differentiation and Gradients\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#setup",
    "href": "guides/tensorflow/autodiff.html#setup",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#computing-gradients",
    "href": "guides/tensorflow/autodiff.html#computing-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Computing gradients",
    "text": "Computing gradients\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradient-tapes",
    "href": "guides/tensorflow/autodiff.html#gradient-tapes",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradient tapes",
    "text": "Gradient tapes\nTensorFlow provides the tf$GradientTape() API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf$Variables. TensorFlow “records” relevant operations executed inside the context of a tf$GradientTape() onto a “tape”. TensorFlow then uses that tape to compute the gradients of a “recorded” computation using reverse mode differentiation.\nHere is a simple example:\n\nx <- tf$Variable(3)\n\nLoaded Tensorflow version 2.9.1\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n\nOnce you’ve recorded some operations, use GradientTape$gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model’s variables):\n\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\nThe above example uses scalars, but tf$GradientTape works as easily on any tensor:\n\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \"float32\", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf$nest).\n\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n\nThe gradient with respect to each source has the shape of the source:\n\nw$shape\n\nTensorShape([3, 2])\n\ndl_dw$shape\n\nTensorShape([3, 2])\n\n\nHere is the gradient calculation again, this time passing a named list of variables:\n\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b\n\ntf.Tensor([-3.2715926 -2.462634 ], shape=(2), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "href": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients with respect to a model",
    "text": "Gradients with respect to a model\nIt’s common to collect tf$Variables into a tf$Module or one of its subclasses (tf$keras$layers$Layer, tf$keras$Model) for checkpointing and exporting.\nIn most cases, you will want to calculate gradients with respect to a model’s trainable variables. Since all subclasses of tf$Module aggregate their variables in the Module$trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \"float32\", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n\n\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}\n\ndense/kernel:0, shape: (3, 2)\ndense/bias:0, shape: (2)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "href": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Controlling what the tape watches",
    "text": "Controlling what the tape watches\nThe default behavior is to record all operations after accessing a trainable tf$Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\nThe tape holds references to intermediate outputs, so you don’t want to record unnecessary operations.\nThe most common use case involves calculating the gradient of a loss with respect to all a model’s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf$Tensor is not “watched” by default, and the tf$Variable is not trainable:\n\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n\nList of 4\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nYou can list the variables being watched by the tape using the GradientTape$watched_variables method:\n\ntape$watched_variables()\n\n[[1]]\n<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n\n\ntf$GradientTape provides hooks that give the user control over what is or is not watched.\nTo record gradients with respect to a tf$Tensor, you need to call GradientTape$watch(x):\n\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n\n[1] 6\n\n\nConversely, to disable the default behavior of watching all tf$Variables, set watch_accessed_variables = FALSE when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n\nSince GradientTape$watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\n\ndy/dx0: \n\ncat('dy/dx1: ', as.array(grad$x1))\n\ndy/dx1:  0.9999546"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#intermediate-results",
    "href": "guides/tensorflow/autodiff.html#intermediate-results",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Intermediate results",
    "text": "Intermediate results\nYou can also request gradients of the output with respect to intermediate values computed inside the tf$GradientTape context.\n\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n\n[1] 18\n\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape$gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent = TRUE. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\n\n[1]   4 108\n\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n\n[1] 2 6\n\n\n\nrm(tape)   # Drop the reference to the tape"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#notes-on-performance",
    "href": "guides/tensorflow/autodiff.html#notes-on-performance",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Notes on performance",
    "text": "Notes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\nFor efficiency, some ops (like ReLU) don’t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent = TRUE on your tape, nothing is discarded and your peak memory usage will be higher."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "href": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients of non-scalar targets",
    "text": "Gradients of non-scalar targets\nA gradient is fundamentally an operation on a scalar.\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\n\n[1] 4\n\nas.array(tape$gradient(y1, x))\n\n[1] -0.25\n\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\nThe sum of the gradients of each target.\n\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n\n[1] 3.75\n\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n\n[1] 7\n\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\nIf you need a separate gradient for each item, refer to Jacobians.\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n\n\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \"royalblue\", lwd = 2)\nlines(x, dy_dx, col = \"coral\", lwd=2)\nlegend(\"topleft\", inset = .05,\n       expression(y, dy/dx),\n       col = c(\"royalblue\", \"coral\"), lwd = 2)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#control-flow",
    "href": "guides/tensorflow/autodiff.html#control-flow",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Control flow",
    "text": "Control flow\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\ndv1\n\nNULL\n\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1 ^ 2. The gradient with respect to x is always NULL.\n\n(dx <- tape$gradient(result, x))\n\nNULL"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "href": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Getting a gradient of NULL",
    "text": "Getting a gradient of NULL\nWhen a target is not connected to a source you will get a gradient of NULL.\n\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\nIn the section on “controlling what the tape watches” you saw that the tape will automatically watch a tf$Variable but not a tf$Tensor.\nOne common error is to inadvertently replace a tf$Variable with a tf$Tensor, instead of using Variable$assign to update the tf$Variable. Here is an example:\n\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \": \")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n\nResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\nEagerTensor : NULL\n\n\n\n\n2. Did calculations outside of TensorFlow\nThe tape can’t record the gradient path if the calculation exits TensorFlow. For example:\n\nnp <- reticulate::import(\"numpy\", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n\nNULL\n\n\n\n\n3. Took gradients through an integer or string\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\nNobody expects strings to be differentiable, but it’s easy to accidentally create an int constant or variable if you don’t specify the dtype.\n\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nTensorFlow doesn’t automatically cast between types, so, in practice, you’ll often get a type error instead of a missing gradient.\n\n\n4. Took gradients through a stateful object\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\nA tf$Tensor is immutable. You can’t change a tensor once it’s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf$matmul only depends on its inputs.\nA tf$Variable has internal state—its value. When you use the variable, the state is read. It’s normal to calculate a gradient with respect to a variable, but the variable’s state blocks gradient calculations from going farther back. For example:\n\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n\nNULL\n\n\nSimilarly, tf$data$Dataset iterators and tf$queues are stateful, and will stop all gradients on tensors that pass through them."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "href": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "No gradient registered",
    "text": "No gradient registered\nSome tf$Operations are registered as being non-differentiable* and will return NULL. Others have no gradient registered**.\nThe tf$raw_ops page shows which low-level ops have gradients registered.\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning NULL. This way you know something has gone wrong.\nFor example, the tf$image$adjust_contrast function wraps raw_ops$AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  LookupError: gradient registry has no entry for: AdjustContrastv2\n\n\nIf you need to differentiate through this op, you’ll either need to implement the gradient and register it (using tf$RegisterGradient) or re-implement the function using other ops."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "href": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Zeros instead of NULL",
    "text": "Zeros instead of NULL\nIn some cases it would be convenient to get 0 instead of NULL for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)\n\ntf.Tensor([0. 0.], shape=(2), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#environment-details",
    "href": "guides/tensorflow/autodiff.html#environment-details",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/tensorflow/basics.html",
    "href": "guides/tensorflow/basics.html",
    "title": "Tensorflow Basics",
    "section": "",
    "text": "This guide provides a quick overview of TensorFlow basics. Each section of this doc is an overview of a larger topic—you can find links to full guides at the end of each section.\nTensorFlow is an end-to-end platform for machine learning. It supports the following:\n\nMultidimensional-array based numeric computation (similar to Numpy\nGPU and distributed processing\nAutomatic differentiation\nModel construction, training, and export\nAnd more"
  },
  {
    "objectID": "guides/tensorflow/basics.html#tensors",
    "href": "guides/tensorflow/basics.html#tensors",
    "title": "Tensorflow Basics",
    "section": "Tensors",
    "text": "Tensors\nTensorFlow operates on multidimensional arrays or tensors represented as tensorflow.tensor objects. Here is a two-dimensional tensor:\n\nlibrary(tensorflow)\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n\nLoaded Tensorflow version 2.9.1\n\nx\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)\n\nx$shape\n\nTensorShape([2, 3])\n\nx$dtype\n\ntf.float32\n\n\nThe most important attributes of a tensor are its shape and dtype:\n\ntensor$shape: tells you the size of the tensor along each of its axes.\ntensor$dtype: tells you the type of all the elements in the tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as well as many operations specialized for machine learning.\nFor example:\n\nx + x\n\ntf.Tensor(\n[[ 2.  4.  6.]\n [ 8. 10. 12.]], shape=(2, 3), dtype=float32)\n\n\n\n5 * x\n\ntf.Tensor(\n[[ 5. 10. 15.]\n [20. 25. 30.]], shape=(2, 3), dtype=float32)\n\n\n\ntf$matmul(x, t(x)) \n\ntf.Tensor(\n[[14. 32.]\n [32. 77.]], shape=(2, 2), dtype=float32)\n\n\n\ntf$concat(list(x, x, x), axis = 0L)\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]], shape=(6, 3), dtype=float32)\n\n\n\ntf$nn$softmax(x, axis = -1L)\n\ntf.Tensor(\n[[0.09003057 0.24472848 0.66524094]\n [0.09003057 0.24472848 0.66524094]], shape=(2, 3), dtype=float32)\n\n\n\nsum(x) # same as tf$reduce_sum(x)\n\ntf.Tensor(21.0, shape=(), dtype=float32)\n\n\nRunning large calculations on CPU can be slow. When properly configured, TensorFlow can use accelerator hardware like GPUs to execute operations very quickly.\n\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n\nTensorFlow **IS NOT** using the GPU\n\n\nRefer to the Tensor guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#variables",
    "href": "guides/tensorflow/basics.html#variables",
    "title": "Tensorflow Basics",
    "section": "Variables",
    "text": "Variables\nNormal tensor objects are immutable. To store model weights (or other mutable state) in TensorFlow use a tf$Variable.\n\nvar <- tf$Variable(c(0, 0, 0))\nvar\n\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n\n\n\nvar$assign(c(1, 2, 3))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n\n\n\nvar$assign_add(c(1, 1, 1))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>\n\n\nRefer to the Variables guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#automatic-differentiation",
    "href": "guides/tensorflow/basics.html#automatic-differentiation",
    "title": "Tensorflow Basics",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nGradient descent and related algorithms are a cornerstone of modern machine learning.\nTo enable this, TensorFlow implements automatic differentiation (autodiff), which uses calculus to compute gradients. Typically you’ll use this to calculate the gradient of a model’s error or loss with respect to its weights.\n\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n\n\nf(x)\n\ntf.Tensor(-2.0, shape=(), dtype=float32)\n\n\nAt x = 1.0, y = f(x) = (1^2 + 2*1 - 5) = -2.\nThe derivative of y is y' = f'(x) = (2*x + 2) = 4. TensorFlow can calculate this automatically:\n\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\nThis simplified example only takes the derivative with respect to a single scalar (x), but TensorFlow can compute the gradient with respect to any number of non-scalar tensors simultaneously.\nRefer to the Autodiff guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "href": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "title": "Tensorflow Basics",
    "section": "Graphs and tf_function",
    "text": "Graphs and tf_function\nWhile you can use TensorFlow interactively like any R library, TensorFlow also provides tools for:\n\nPerformance optimization: to speed up training and inference.\nExport: so you can save your model when it’s done training.\n\nThese require that you use tf_function() to separate your pure-TensorFlow code from R.\n\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n\nThe first time you run the tf_function, although it executes in R, it captures a complete, optimized graph representing the TensorFlow computations done within the function.\n\nx <- as_tensor(1:3)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(6, shape=(), dtype=int32)\n\n\nOn subsequent calls TensorFlow only executes the optimized graph, skipping any non-TensorFlow steps. Below, note that my_func doesn’t print \"Tracing.\" since message is an R function, not a TensorFlow function.\n\nx <- as_tensor(10:8)\nmy_func(x)\n\ntf.Tensor(27, shape=(), dtype=int32)\n\n\nA graph may not be reusable for inputs with a different signature (shape and dtype), so a new graph is generated instead:\n\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(27.3, shape=(), dtype=float32)\n\n\nThese captured graphs provide two benefits:\n\nIn many cases they provide a significant speedup in execution (though not this trivial example).\nYou can export these graphs, using tf$saved_model, to run on other systems like a server or a mobile device, no Python installation required.\n\nRefer to Intro to graphs for more details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#modules-layers-and-models",
    "href": "guides/tensorflow/basics.html#modules-layers-and-models",
    "title": "Tensorflow Basics",
    "section": "Modules, layers, and models",
    "text": "Modules, layers, and models\ntf$Module is a class for managing your tf$Variable objects, and the tf_function objects that operate on them. The tf$Module class is necessary to support two significant features:\n\nYou can save and restore the values of your variables using tf$train$Checkpoint. This is useful during training as it is quick to save and restore a model’s state.\nYou can import and export the tf$Variable values and the tf$function graphs using tf$saved_model. This allows you to run your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple tf$Module object:\n\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n\n\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nSave the Module:\n\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n\nThe resulting SavedModel is independent of the code that created it. You can load a SavedModel from R, Python, other language bindings, or TensorFlow Serving. You can also convert it to run with TensorFlow Lite or TensorFlow JS.\n\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nThe tf$keras$layers$Layer and tf$keras$Model classes build on tf$Module providing additional functionality and convenience methods for building, training, and saving models. Some of these are demonstrated in the next section.\nRefer to Intro to modules for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#training-loops",
    "href": "guides/tensorflow/basics.html#training-loops",
    "title": "Tensorflow Basics",
    "section": "Training loops",
    "text": "Training loops\nNow put this all together to build a basic model and train it from scratch.\nFirst, create some example data. This generates a cloud of points that loosely follows a quadratic curve:\n\nx <- as_tensor(seq(-2, 2, length.out = 201), \"float32\")\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n\n\n\n\nCreate a model:\n\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n\n\nmodel <- Model(64)\n\n\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n\n\n\n\nWrite a basic training loop:\n\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n\nMean squared error: 1.168\n\n\nMean squared error: 1.143\n\n\nMean squared error: 1.126\n\n\nMean squared error: 1.112\n\n\nMean squared error: 1.100\n\n\nMean squared error: 1.090\n\n\nMean squared error: 1.081\n\n\nMean squared error: 1.074\n\n\nMean squared error: 1.068\n\n\nMean squared error: 1.063\n\n\n\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n\n\n\n\nThat’s working, but remember that implementations of common training utilities are available in the tf$keras module. So consider using those before writing your own. To start with, the compile and fit methods for Keras Models implement a training loop for you:\n\nnew_model <- Model(64)\n\n\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n\n\n\n\n\nplot(history, metrics = 'loss', method = \"base\") \n\n\n\n# see ?plot.keras_training_history for more options.\n\nRefer to Basic training loops and the Keras guide for more details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#environment-details",
    "href": "guides/tensorflow/basics.html#environment-details",
    "title": "Tensorflow Basics",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html",
    "href": "guides/tensorflow/intro_to_graphs.html",
    "title": "Introduction to graphs",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#overview",
    "href": "guides/tensorflow/intro_to_graphs.html#overview",
    "title": "Introduction to graphs",
    "section": "Overview",
    "text": "Overview\nThis guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the collection of Keras guides.\nIn this guide, you’ll learn how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.\nNote: For those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.\nThis is a big-picture overview that covers how tf_function() allows you to switch from eager execution to graph execution. For a more complete specification of tf_function(), go to the tf_function() guide.\n\nWhat are graphs?\nIn the previous three guides, you ran TensorFlow eagerly. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\nWhile eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. Graph execution means that tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf$Graph or simply a “graph.”\nGraphs are data structures that contain a set of tf$Operation objects, which represent units of computation; and tf$Tensor objects, which represent the units of data that flow between operations. They are defined in a tf$Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original R code.\nThis is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard.\n\n\n\nA simple TensorFlow g\n\n\n\n\nThe benefits of graphs\nWith a graph, you have a great deal of flexibility. You can use your TensorFlow graph in environments that don’t have an R interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for saved models when it exports them from R.\nGraphs are also easily optimized, allowing the compiler to do transformations like:\n\nStatically infer the value of tensors by folding constant nodes in your computation (“constant folding”).\nSeparate sub-parts of a computation that are independent and split them between threads or devices.\nSimplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system, Grappler, to perform this and other speedups.\nIn short, graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices.\nHowever, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#setup",
    "href": "guides/tensorflow/intro_to_graphs.html#setup",
    "title": "Introduction to graphs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \"%>%\")\n\nWarning: package 'magrittr' was built under R version 4.1.2"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "href": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "title": "Introduction to graphs",
    "section": "Taking advantage of graphs",
    "text": "Taking advantage of graphs\nYou create and run a graph in TensorFlow by using tf_function(), either as a direct call or as a decorator. tf_function() takes a regular function as input and returns a Function. A Function is a callable that builds TensorFlow graphs from the R function. You use a Function in the same way as its R equivalent.\n\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\nLoaded Tensorflow version 2.9.1\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \"float64\", shape = c(1, 2))\ny1 <- as_tensor(2:3, \"float64\", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n\nOn the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf$Graphs behind one API. That is how Function is able to give you the benefits of graph execution, like speed and deployability.\ntf_function applies to a function and all other functions it calls:\n\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \"float64\", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \"float64\", shape = c(1, 2))) #%>% as.array()\n\ntf.Tensor([[12.]], shape=(1, 1), dtype=float64)\n\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you need to define a Placeholder or tf$Session().\n\nConverting Python functions to graphs\nAny function you write with TensorFlow will contain a mixture of built-in TF operations and R control-flow logic, such as if-then clauses, loops, break, return, next, and more. While TensorFlow operations are easily captured by a tf$Graph, R-specific logic needs to undergo an extra step in order to become part of the graph. tf_function() uses a library called {tfautograph} to evaluate the R code in a special way so that it generates a graph.\n\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \"First branch, with graph: \", format(tf_simple_relu(as_tensor(1))), \"\\n\",\n  \"Second branch, with graph: \", format(tf_simple_relu(as_tensor(-1))), \"\\n\",\n  sep = \"\"\n)\n\nFirst branch, with graph: tf.Tensor(1.0, shape=(), dtype=float64)\nSecond branch, with graph: tf.Tensor(0.0, shape=(), dtype=float64)\n\n\nThough it is unlikely that you will need to view graphs directly, you can inspect the outputs to check the exact results. These are not easy to read, so no need to look too carefully!\n\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n\nMost of the time, tf_function() will work without special considerations. However, there are some caveats, and the tf_function guide can help here, as well as the tfautograph Getting Started vignette\n\n\nPolymorphism: one Function, many graphs\nA tf$Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()) (i.e, the same memory address).\nEach time you invoke a Function with a set of arguments that can’t be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf$Graph specialized to those new arguments. The type specification of a tf$Graph’s inputs is known as its input signature or just a signature. For more information regarding when a new tf$Graph is generated and how that can be controlled, see the rules of retracing.\nThe Function stores the tf$Graph corresponding to that signature in a ConcreteFunction. A ConcreteFunction is a wrapper around a tf$Graph.\n\nmy_relu <- tf_function(function(x) {\n  message(\"Tracing my_relu(x) with: \", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\n\nTracing my_relu(x) with: Tensor(\"x:0\", shape=(), dtype=float64)\n\n\ntf.Tensor(5.5, shape=(), dtype=float64)\n\nmy_relu(c(1, -1))\n\nTracing my_relu(x) with: 1-1\n\n\ntf.Tensor([1. 0.], shape=(2), dtype=float64)\n\nmy_relu(as_tensor(c(3, -3)))\n\nTracing my_relu(x) with: Tensor(\"x:0\", shape=(2,), dtype=float64)\n\n\ntf.Tensor([3. 0.], shape=(2), dtype=float64)\n\n\nIf the Function has already been called with that signature, Function does not create a new tf$Graph.\n\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\n\ntf.Tensor(0.0, shape=(), dtype=float64)\n\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n\ntf.Tensor([0. 1.], shape=(2), dtype=float64)\n\n\nBecause it’s backed by multiple graphs, a Function is polymorphic. That enables it to support more input types than a single tf$Graph could represent, as well as to optimize each tf$Graph for better performance.\n\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())\n\nfn(x)\n  Args:\n    x: float64 Tensor, shape=()\n  Returns:\n    float64 Tensor, shape=()\n\nfn(x=[1.0, -1.0])\n  Returns:\n    float64 Tensor, shape=(2,)\n\nfn(x)\n  Args:\n    x: float64 Tensor, shape=(2,)\n  Returns:\n    float64 Tensor, shape=(2,)"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "href": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "title": "Introduction to graphs",
    "section": "Using tf_function()",
    "text": "Using tf_function()\nSo far, you’ve learned how to convert a Python function into a graph simply by using tf_function() as function wrapper. But in practice, getting tf_function to work correctly can be tricky! In the following sections, you’ll learn how you can make your code work as expected with tf_function().\n\nGraph execution vs. eager execution\nThe code in a Function can be executed both eagerly and as a graph. By default, Function executes its code as a graph:\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n\n\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\ntf.Tensor([3 7 9 5 0], shape=(5), dtype=int32)\n\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\ntf.Tensor([8 3 4 7 4], shape=(5), dtype=int32)\n\n\n\nget_MSE(y_true, y_pred)\n\ntf.Tensor(17, shape=(), dtype=int32)\n\n\nTo verify that your Function’s graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with tf$config$run_functions_eagerly(TRUE). This is a switch that turns off Function’s ability to create and run graphs, instead executing the code normally.\n\ntf$config$run_functions_eagerly(TRUE)\n\n\nget_MSE(y_true, y_pred)\n\ntf.Tensor(17, shape=(), dtype=int32)\n\n\n\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n\nHowever, Function can behave differently under graph and eager execution. The R print() function is one example of how these two modes differ. Let’s check out what happens when you insert a print statement to your function and call it repeatedly.\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\"Calculating MSE!\")\n  mean((y_true - y_pred) ^ 2)\n  })\n\nObserve what is printed:\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\nIs the output surprising? get_MSE only printed once even though it was called three times.\nTo explain, the print statement is executed when Function runs the original code in order to create the graph in a process known as “tracing”. Tracing captures the TensorFlow operations into a graph, and print() is not captured in the graph. That graph is then executed for all three calls without ever running the R code again.\nAs a sanity check, let’s turn off graph execution to compare:\n\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n\n\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\n\n\ntf$config$run_functions_eagerly(FALSE)\n\nprint is an R side effect, and there are other differences that you should be aware of when converting a function into a Function. Learn more in the Limitations section of the Better performance with tf_function guide.\n\n\n\n\n\n\nNote\n\n\n\nNote: If you would like to print values in both eager and graph execution, use tf$print() instead.\n\n\n\n\nNon-strict execution\nGraph execution only executes the operations necessary to produce the observable effects, which includes:\n\nThe return value of the function\nDocumented well-known side-effects such as:\n\nInput/output operations, like tf$print()\nDebugging operations, such as the assert functions in tf$debugging() (also, stopifnot())\nMutations of tf$Variable()\n\n\nThis behavior is usually known as “Non-strict execution”, and differs from eager execution, which steps through all of the program operations, needed or not.\nIn particular, runtime error checking does not count as an observable effect. If an operation is skipped because it is unnecessary, it cannot raise any runtime errors.\nIn the following example, the “unnecessary” operation tf$gather() is skipped during graph execution, so the runtime error InvalidArgumentError is not raised as it would be in eager execution. Do not rely on an error being raised while executing a graph.\n\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0] = 2 is not in [0, 1) [Op:GatherV2]\n\n# All operations are run during eager execution so an error is raised.\n\n\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n\ntf.Tensor([0.], shape=(1), dtype=float64)\n\n\n\n\ntf_function() best practices\nIt may take some time to get used to the behavior of Function. To get started quickly, first-time users should play around with wrapping toy functions with tf_function() to get experience with going from eager to graph execution.\nDesigning for tf_function may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n\nToggle between eager and graph execution early and often with tf$config$run_functions_eagerly() to pinpoint if/when the two modes diverge.\nCreate tf$Variables outside the Python function and modify them on the inside. The same goes for objects that use tf$Variable, like keras$layers, keras$Models and tf$optimizers.\nAvoid writing functions that depend on outer Python variables, excluding tf$Variables and Keras objects.\nPrefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful!\nInclude as much computation as possible under a tf_function to maximize the performance gain. For example, wrap a whole training step or the entire training loop."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "href": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "title": "Introduction to graphs",
    "section": "Seeing the speed-up",
    "text": "Seeing the speed-up\ntf_function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:\n\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n\n\nplot(bench::mark(\n  \"Eager execution\" = power(x, 100),\n  \"Graph execution\" = power_as_graph(x, 100)))\n\nLoading required namespace: tidyr\n\n\n\n\n\ntf_function is commonly used to speed up training loops, and you can learn more about it in Writing a training loop from scratch with Keras.\nNote: You can also try tf_function(jit_compile = TRUE) for a more significant performance boost, especially if your code is heavy on TF control flow and uses many small tensors.\n\nPerformance and trade-offs\nGraphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. This investment is usually quickly paid back with the performance boost of subsequent executions, but it’s important to be aware that the first few steps of any large model training can be slower due to tracing.\nNo matter how large your model, you want to avoid tracing frequently. The tf_function() guide discusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it’s a good idea to check if you are retracing accidentally."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "href": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "title": "Introduction to graphs",
    "section": "When is a Function tracing?",
    "text": "When is a Function tracing?\nTo figure out when your Function is tracing, add a print or message() statement to its code. As a rule of thumb, Function will execute the message statement every time it traces.\n\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\"Tracing!\") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\nTracing!\n\n\ntf.Tensor(6.0, shape=(), dtype=float64)\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n\ntf.Tensor(11.0, shape=(), dtype=float64)\n\n\n\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\n\nTracing!\n\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\na_function_with_r_side_effect(3)\n\nTracing!\n\n\ntf.Tensor(11.0, shape=(), dtype=float32)\n\n\nNew (non-tensor) R arguments always trigger the creation of a new graph, hence the extra tracing."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "href": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "title": "Introduction to graphs",
    "section": "Next steps",
    "text": "Next steps\nYou can learn more about tf_function() on the API reference page and by following the Better performance with tf_function guide."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#environment-details",
    "href": "guides/tensorflow/intro_to_graphs.html#environment-details",
    "title": "Introduction to graphs",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/tensorflow/tensor.html",
    "href": "guides/tensorflow/tensor.html",
    "title": "Introduction to Tensors",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nlibrary(tensorflow)\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes with names(tf$dtypes).\nIf you’re familiar with R array or NumPy, tensors are (kind of) like R or NumPy arrays.\nAll tensors are immutable: you can never update the contents of a tensor, only create a new one."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#basics",
    "href": "guides/tensorflow/tensor.html#basics",
    "title": "Introduction to Tensors",
    "section": "Basics",
    "text": "Basics\nLet’s create some basic tensors.\nHere is a “scalar” or “rank-0” tensor . A scalar contains a single value, and no “axes”.\n\n# This will be an float64 tensor by default; see \"dtypes\" below.\nrank_0_tensor <- as_tensor(4)\n\nLoaded Tensorflow version 2.9.1\n\nprint(rank_0_tensor)\n\ntf.Tensor(4.0, shape=(), dtype=float64)\n\n\nA “vector” or “rank-1” tensor is like a list of values. A vector has one axis:\n\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3), dtype=float64)\n\n\nA “matrix” or “rank-2” tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]\n [5. 6.]], shape=(3, 2), dtype=float16)\n\n\n\n\n\n\n\n\n\n\nA scalar, shape: []\nA vector, shape: [3]\nA matrix, shape: [3, 2]\n\n\n\n\n\n\n\n\n\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of\n# axes (sometimes called \"dimensions\")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nThere are many ways you might visualize a tensor with more than two axes.\n\n\n\n\n\n\nA 3-axis tensor, shape: [3, 2, 5]\n\n\n\n\n\n\n\n!  \n\n\n\nYou can convert a tensor to an R array using as.array():\n\nas.array(rank_2_tensor)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nTensors often contain floats and ints, but have many other types, including:\n\ncomplex numbers\nstrings\n\nThe base tf$Tensor class requires tensors to be “rectangular”—that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\n\ntf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n\na * b # element-wise multiplication, same as tf$multiply(a, b)\n\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n\ntf$matmul(a, b) # matrix multiplication\n\ntf.Tensor(\n[[3 3]\n [7 7]], shape=(2, 2), dtype=int32)\n\n\nTensors are used in all kinds of operations (ops).\n\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n\ntf.Tensor(10.0, shape=(), dtype=float64)\n\n# Find the index of the largest value\ntf$math$argmax(x) \n\ntf.Tensor([1 0], shape=(2), dtype=int64)\n\ntf$nn$softmax(x) # Compute the softmax\n\ntf.Tensor(\n[[2.68941421e-01 7.31058579e-01]\n [9.99876605e-01 1.23394576e-04]], shape=(2, 2), dtype=float64)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#about-shapes",
    "href": "guides/tensorflow/tensor.html#about-shapes",
    "title": "Introduction to Tensors",
    "section": "About shapes",
    "text": "About shapes\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\nAxis or Dimension: A particular dimension of a tensor.\nSize: The total number of items in the tensor, the product of the shape vector’s elements.\n\nNote: Although you may see reference to a “tensor of two dimensions”, a rank-2 tensor does not usually describe a 2D space.\nTensors and tf$TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n\n\n\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\n\n\n\n\n\n\n\n\nmessage(\"Type of every element: \", rank_4_tensor$dtype)\n\nType of every element: <dtype: 'float32'>\n\nmessage(\"Number of axes: \", length(dim(rank_4_tensor)))\n\nNumber of axes: 4\n\nmessage(\"Shape of tensor: \", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\n\nShape of tensor: 3245\n\nmessage(\"Elements along axis 0 of tensor: \", dim(rank_4_tensor)[1])\n\nElements along axis 0 of tensor: 3\n\nmessage(\"Elements along the last axis of tensor: \", dim(rank_4_tensor) |> tail(1)) \n\nElements along the last axis of tensor: 5\n\nmessage(\"Total number of elements (3*2*4*5): \", length(rank_4_tensor)) # can also call tf$size()\n\nTotal number of elements (3*2*4*5): 120\n\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\n\n\n\n\n\nTypical axis order"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#indexing",
    "href": "guides/tensorflow/tensor.html#indexing",
    "title": "Introduction to Tensors",
    "section": "Indexing",
    "text": "Indexing\n\nSingle-axis indexing\nSee ?`[.tensorflow.tensor` for details\n\n\nMulti-axis indexing\nHigher rank tensors are indexed by passing multiple indices.\nThe exact same rules as in the single-axis case apply to each axis independently.\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#manipulating-shapes",
    "href": "guides/tensorflow/tensor.html#manipulating-shapes",
    "title": "Introduction to Tensors",
    "section": "Manipulating Shapes",
    "text": "Manipulating Shapes\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n\nTensorShape([1, 3])\n\n\n\n# You can convert this object into an R vector too\nas.integer(x$shape)\n\n[1] 1 3\n\n\nYou can reshape a tensor into a new shape. The tf$reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n\n\nx$shape\n\nTensorShape([1, 3])\n\nreshaped$shape\n\nTensorShape([1, 3])\n\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style “row-major” memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says \"Whatever fits\".\ntf$reshape(rank_3_tensor, c(-1L))\n\ntf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29], shape=(30), dtype=int32)\n\n\nA typical and reasonable use of tf$reshape is to combine or split adjacent axes (or add/remove 1s).\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\n\ntf.Tensor(\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [20 21 22 23 24]\n [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\n\n\n\n\n\n\n\nSome good reshapes.\n\n\n\n\n  \n\n\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png https://www.tensorflow.org/guide/ https://www.tensorflow.org/guide/images/tensor/reshape-good2.png\nReshaping will “work” for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\nSwapping axes in tf$reshape does not work; you need tf$transpose for that.\n\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]\n  [10 11 12 13 14]]\n\n [[15 16 17 18 19]\n  [20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32)\n\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]], shape=(5, 6), dtype=int32)\n\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\n\n\n\n\n\n\n\nSome bad reshapes.\n\n\n\n\n  \n\n\n\nYou may run across not-fully-specified shapes. Either the shape contains a NULL (an axis-length is unknown) or the whole shape is NULL (the rank of the tensor is unknown).\nExcept for tf$RaggedTensor, such shapes will only occur in the context of TensorFlow’s symbolic, graph-building APIs:\n\ntf_function\nThe keras functional API."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#more-on-dtypes",
    "href": "guides/tensorflow/tensor.html#more-on-dtypes",
    "title": "Introduction to Tensors",
    "section": "More on DTypes",
    "text": "More on DTypes\nTo inspect a tf$Tensor’s data type use the Tensor$dtype property.\nWhen creating a tf$Tensor from a Python object you may optionally specify the datatype.\nIf you don’t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts R integers to tf$int32 and R floating point numbers to tf$float64.\nYou can cast from type to type.\n\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor\n\ntf.Tensor([2 3 4], shape=(3), dtype=uint8)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#broadcasting",
    "href": "guides/tensorflow/tensor.html#broadcasting",
    "title": "Introduction to Tensors",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are recycled automatically to fit larger tensors when running combined operations on them.\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * y\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * z\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n\ntf.Tensor(\n[[1.]\n [2.]\n [3.]], shape=(3, 1), dtype=float64)\n\n(y <- tf$range(1, 5,  dtype = \"float64\"))\n\ntf.Tensor([1. 2. 3. 4.], shape=(4), dtype=float64)\n\nx * y\n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\n\n\n\n\n\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\n\n\n\n\\\n\n\n\nHere is the same operation without broadcasting:\n\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\nYou see what broadcasting looks like using tf$broadcast_to.\n\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n\ntf.Tensor(\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]], shape=(3, 3), dtype=float64)\n\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\nIt can get even more complicated. This section of Jake VanderPlas’s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy)."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "href": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "title": "Introduction to Tensors",
    "section": "tf$convert_to_tensor",
    "text": "tf$convert_to_tensor\nMost ops, like tf$matmul and tf$reshape take arguments of class tf$Tensor. However, you’ll notice in the above case, objects shaped like tensors are also accepted.\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy’s ndarray, TensorShape, Python lists, and tf$Variable will all convert automatically.\nSee tf$register_tensor_conversion_function for more details, and if you have your own type you’d like to automatically convert to a tensor."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#ragged-tensors",
    "href": "guides/tensorflow/tensor.html#ragged-tensors",
    "title": "Introduction to Tensors",
    "section": "Ragged Tensors",
    "text": "Ragged Tensors\nA tensor with variable numbers of elements along some axis is called “ragged”. Use tf$ragged$RaggedTensor for ragged data.\nFor example, This cannot be represented as a regular tensor:\n\n\n\n\n\n\nA tf$RaggedTensor, shape: [4, NULL]\n\n\n\n\n\n\n\n\n\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n\n\ntry(tensor <- as_tensor(ragged_list))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Can't convert non-rectangular Python sequence to Tensor.\n\n\nInstead create a tf$RaggedTensor using tf$ragged$constant:\n\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n\n<tf.RaggedTensor [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0], [6.0, 7.0, 8.0], [9.0]]>\n\n\nThe shape of a tf$RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor$shape)\n\nTensorShape([4, None])"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#string-tensors",
    "href": "guides/tensorflow/tensor.html#string-tensors",
    "title": "Introduction to Tensors",
    "section": "String tensors",
    "text": "String tensors\ntf$string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\nThe length of the string is not one of the axes of the tensor. See tf$strings for functions to manipulate them.\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\"Gray wolf\"))\n\ntf.Tensor(b'Gray wolf', shape=(), dtype=string)\n\n\nAnd a vector of strings:\n\n\n\n\n\n\nA vector of strings, shape: [3,]\n\n\n\n\n\n\n\n\n\ntensor_of_strings <- as_tensor(c(\"Gray wolf\",\n                                 \"Quick brown fox\",\n                                 \"Lazy dog\"))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n\ntf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3), dtype=string)\n\n\nIn the above printout the b prefix indicates that tf$string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\nIf you pass unicode characters they are utf-8 encoded.\n\nas_tensor(\"🥳👍\")\n\ntf.Tensor(b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d', shape=(), dtype=string)\n\n\nSome basic functions with strings can be found in tf$strings, including tf$strings$split.\n\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\" \")\n\ntf.Tensor([b'Gray' b'wolf'], shape=(2), dtype=string)\n\n\n\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n\n<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n\n\n\n\n\n\n\n\nThree strings split, shape: [3, NULL]\n\n\n\n\n\n\n\n\nAnd tf$string$to_number:\n\ntext <- as_tensor(\"1 10 100\")\ntf$strings$to_number(tf$strings$split(text, \" \"))\n\ntf.Tensor([  1.  10. 100.], shape=(3), dtype=float32)\n\n\nAlthough you can’t use tf$cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings <- tf$strings$bytes_split(as_tensor(\"Duck\"))\nbyte_ints <- tf$io$decode_raw(as_tensor(\"Duck\"), tf$uint8)\ncat(\"Byte strings: \"); print(byte_strings)\n\nByte strings: \n\n\ntf.Tensor([b'D' b'u' b'c' b'k'], shape=(4), dtype=string)\n\ncat(\"Bytes: \"); print(byte_ints)\n\nBytes: \n\n\ntf.Tensor([ 68 117  99 107], shape=(4), dtype=uint8)\n\n\n\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\"アヒル 🦆\")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \"UTF-8\")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \"UTF-8\")\n\ncat(\"Unicode bytes: \"); unicode_bytes\n\nUnicode bytes: \n\n\ntf.Tensor(b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86', shape=(), dtype=string)\n\ncat(\"Unicode chars: \"); unicode_char_bytes\n\nUnicode chars: \n\n\ntf.Tensor([b'\\xe3\\x82\\xa2' b'\\xe3\\x83\\x92' b'\\xe3\\x83\\xab' b' ' b'\\xf0\\x9f\\xa6\\x86'], shape=(5), dtype=string)\n\ncat(\"Unicode values: \"); unicode_values\n\nUnicode values: \n\n\ntf.Tensor([ 12450  12498  12523     32 129414], shape=(5), dtype=int32)\n\n\nThe tf$string dtype is used for all raw bytes data in TensorFlow. The tf$io module contains functions for converting data to and from bytes, including decoding images and parsing csv."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#sparse-tensors",
    "href": "guides/tensorflow/tensor.html#sparse-tensors",
    "title": "Introduction to Tensors",
    "section": "Sparse tensors",
    "text": "Sparse tensors\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf$sparse$SparseTensor and related operations to store sparse data efficiently.\n\n\n\n\n\n\nA tf$SparseTensor, shape: [3, 4]\n\n\n\n\n\n\n\n\n\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n\nSparseTensor(indices=tf.Tensor(\n[[0 0]\n [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2), dtype=int64))\n\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)\n\ntf.Tensor(\n[[1. 0. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#environment-details",
    "href": "guides/tensorflow/tensor.html#environment-details",
    "title": "Introduction to Tensors",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html",
    "href": "guides/tensorflow/tensor_slicing.html",
    "title": "Tensor Slicing",
    "section": "",
    "text": "# Copyright 2020 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#setup",
    "href": "guides/tensorflow/tensor_slicing.html#setup",
    "title": "Tensor Slicing",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "href": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "title": "Tensor Slicing",
    "section": "Extract tensor slices",
    "text": "Extract tensor slices\nPerform slicing using the [ operator:\n\nt1 <- as_tensor(c(1, 2, 3, 4, 5, 6, 7))\n\nLoaded Tensorflow version 2.9.1\n\nt1[1:3]\n\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike base R’s [ operator, TensorFlow’s [ uses negative indexes for selecting starting from the end.\nNULL can be used instead of the last dimension or first, depending if it appears before or after the :.\n\n\n\nt1[-3:NULL]\n\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n\n\ntf.Tensor([5. 6. 7.], shape=(3), dtype=float64)\n\n\n\nFor 2-dimensional tensors,you can use something like:\n\nt2 <- as_tensor(rbind(c(0, 1, 2, 3, 4),\n                      c(5, 6, 7, 8, 9),\n                      c(10, 11, 12, 13, 14),\n                      c(15, 16, 17, 18, 19)))\n\nt2[NULL:-1, 2:3]\n\ntf.Tensor(\n[[ 1.  2.]\n [ 6.  7.]\n [11. 12.]\n [16. 17.]], shape=(4, 2), dtype=float64)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntf$slice can be used instead of the [ operator. However, not that when using functions directly from the tf module, dimensions and indexes will start from 0, unlike in R.\nYou also need to make sure that indexes are passed to TensorFlow with the integer type, for example using the L suffix notation.\n\n\nYou can use tf$slice on higher dimensional tensors as well.\n\nt3 <- as_tensor(array(seq(from=1, to = 31, by = 2), dim = c(2,2,4)))\ntf$slice(\n  t3,\n  begin = list(1L, 1L, 0L),\n  size = list(1L, 1L, 2L)\n)\n\ntf.Tensor([[[ 7. 15.]]], shape=(1, 1, 2), dtype=float64)\n\n\nYou can also use tf$strided_slice to extract slices of tensors by ‘striding’ over the tensor dimensions.\nUse tf$gather to extract specific indices from a single axis of a tensor.\n\ntf$gather(t1, indices = c(0L, 3L, 6L))\n\ntf.Tensor([1. 4. 7.], shape=(3), dtype=float64)\n\n\n\ntf$gather does not require indices to be evenly spaced.\n\nalphabet <- as_tensor(strsplit(\"abcdefghijklmnopqrstuvwxyz\", \"\")[[1]])\ntf$gather(alphabet, indices = c(2L, 0L, 19L, 18L))\n\ntf.Tensor([b'c' b'a' b't' b's'], shape=(4), dtype=string)\n\n\n\nTo extract slices from multiple axes of a tensor, use tf$gather_nd. This is useful when you want to gather the elements of a matrix as opposed to just its rows or columns.\n\nt4 <- as_tensor(rbind(c(0, 5),\n                      c(1, 6),\n                      c(2, 7),\n                      c(3, 8),\n                      c(4, 9)))\n\ntf$gather_nd(t4, indices = list(list(2L), list(3L), list(0L)))\n\ntf.Tensor(\n[[2. 7.]\n [3. 8.]\n [0. 5.]], shape=(3, 2), dtype=float64)\n\n\n\n\nt5 <- array(1:18, dim = c(2,3,3))\ntf$gather_nd(t5, indices = list(c(0L, 0L, 0L), c(1L, 2L, 1L)))\n\ntf.Tensor([ 1 12], shape=(2), dtype=int32)\n\n\n\n# Return a list of two matrices\ntf$gather_nd(\n  t5,\n  indices = list(\n    list(c(0L, 0L), c(0L, 2L)), \n    list(c(1L, 0L), c(1L, 2L)))\n)\n\ntf.Tensor(\n[[[ 1  7 13]\n  [ 5 11 17]]\n\n [[ 2  8 14]\n  [ 6 12 18]]], shape=(2, 2, 3), dtype=int32)\n\n\n\n# Return one matrix\ntf$gather_nd(\n  t5,\n  indices = list(c(0L, 0L), c(0L, 2L), c(1L, 0L), c(1L, 2L))\n)\n\ntf.Tensor(\n[[ 1  7 13]\n [ 5 11 17]\n [ 2  8 14]\n [ 6 12 18]], shape=(4, 3), dtype=int32)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "href": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "title": "Tensor Slicing",
    "section": "Insert data into tensors",
    "text": "Insert data into tensors\nUse tf$scatter_nd to insert data at specific slices/indices of a tensor. Note that the tensor into which you insert values is zero-initialized.\n\nt6 <- as_tensor(list(10L))\nindices <- as_tensor(list(list(1L), list(3L), list(5L), list(7L), list(9L)))\ndata <- as_tensor(c(2, 4, 6, 8, 10))\n\ntf$scatter_nd(\n  indices = indices,\n  updates = data,\n  shape = t6\n)\n\ntf.Tensor([ 0.  2.  0.  4.  0.  6.  0.  8.  0. 10.], shape=(10), dtype=float64)\n\n\nMethods like tf$scatter_nd which require zero-initialized tensors are similar to sparse tensor initializers. You can use tf$gather_nd and tf$scatter_nd to mimic the behavior of sparse tensor ops.\nConsider an example where you construct a sparse tensor using these two methods in conjunction.\n\n# Gather values from one tensor by specifying indices\nnew_indices <- as_tensor(rbind(c(0L, 2L), c(2L, 1L), c(3L, 3L)))\nt7 <- tf$gather_nd(t2, indices = new_indices)\n\n\n\n# Add these values into a new tensor\nt8 <- tf$scatter_nd(\n  indices = new_indices, \n  updates = t7, \n  shape = as_tensor(c(4L, 5L))\n)\nt8\n\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float64)\n\n\nThis is similar to:\n\nt9 <- tf$SparseTensor(\n  indices = list(c(0L, 2L), c(2L, 1L), c(3L, 3L)),\n  values = c(2, 11, 18),\n  dense_shape = c(4L, 5L)\n)\nt9\n\nSparseTensor(indices=tf.Tensor(\n[[0 2]\n [2 1]\n [3 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([ 2. 11. 18.], shape=(3), dtype=float32), dense_shape=tf.Tensor([4 5], shape=(2), dtype=int64))\n\n\n\n# Convert the sparse tensor into a dense tensor\nt10 <- tf$sparse$to_dense(t9)\nt10\n\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float32)\n\n\nTo insert data into a tensor with pre-existing values, use tf$tensor_scatter_nd_add.\n\nt11 <- as_tensor(rbind(c(2, 7, 0),\n                       c(9, 0, 1),\n                       c(0, 3, 8)))\n\n# Convert the tensor into a magic square by inserting numbers at appropriate indices\nt12 <- tf$tensor_scatter_nd_add(\n  t11,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt12\n\ntf.Tensor(\n[[2. 7. 6.]\n [9. 5. 1.]\n [4. 3. 8.]], shape=(3, 3), dtype=float64)\n\n\nSimilarly, use tf$tensor_scatter_nd_sub to subtract values from a tensor with pre-existing values.\n\n# Convert the tensor into an identity matrix\nt13 <- tf$tensor_scatter_nd_sub(\n  t11,\n  indices = list(c(0L, 0L), c(0L, 1L), c(1L, 0L), c(1L, 1L), c(1L, 2L), c(2L, 1L), c(2L, 2L)),\n  updates = c(1, 7, 9, -1, 1, 3, 7)\n)\n\nprint(t13)\n\ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float64)\n\n\nUse tf$tensor_scatter_nd_min to copy element-wise minimum values from one tensor to another.\n\nt14 <- as_tensor(rbind(c(-2, -7, 0),\n                       c(-9, 0, 1),\n                       c(0, -3, -8)))\n\nt15 <- tf$tensor_scatter_nd_min(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(-6, -5, -4)\n)\nt15\n\ntf.Tensor(\n[[-2. -7. -6.]\n [-9. -5.  1.]\n [-4. -3. -8.]], shape=(3, 3), dtype=float64)\n\n\nSimilarly, use tf$tensor_scatter_nd_max to copy element-wise maximum values from one tensor to another.\n\nt16 <- tf$tensor_scatter_nd_max(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt16\n\ntf.Tensor(\n[[-2. -7.  6.]\n [-9.  5.  1.]\n [ 4. -3. -8.]], shape=(3, 3), dtype=float64)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "href": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "title": "Tensor Slicing",
    "section": "Further reading and resources",
    "text": "Further reading and resources\nIn this guide, you learned how to use the tensor slicing ops available with TensorFlow to exert finer control over the elements in your tensors.\n\nCheck out the slicing ops available with TensorFlow NumPy such as tf$experimental$numpy$take_along_axis and tf$experimental$numpy$take.\nAlso check out the Tensor guide and the Variable guide."
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#environment-details",
    "href": "guides/tensorflow/tensor_slicing.html#environment-details",
    "title": "Tensor Slicing",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "guides/tensorflow/variable.html",
    "href": "guides/tensorflow/variable.html",
    "title": "Introduction to Variables",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nA TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of tf$Variable in TensorFlow.\nVariables are created and tracked via the tf$Variable class. A tf$Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like tf$keras use tf$Variable to store model parameters."
  },
  {
    "objectID": "guides/tensorflow/variable.html#setup",
    "href": "guides/tensorflow/variable.html#setup",
    "title": "Introduction to Variables",
    "section": "Setup",
    "text": "Setup\nThis notebook discusses variable placement. If you want to see on what device your variables are placed, uncomment this line.\n\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)"
  },
  {
    "objectID": "guides/tensorflow/variable.html#create-a-variable",
    "href": "guides/tensorflow/variable.html#create-a-variable",
    "title": "Introduction to Variables",
    "section": "Create a variable",
    "text": "Create a variable\nTo create a variable, provide an initial value. The tf$Variable will have the same dtype as the initialization value.\n\nmy_tensor <- as_tensor(1:4, \"float32\", shape = c(2, 2))\n\nLoaded Tensorflow version 2.9.1\n\n(my_variable <- tf$Variable(my_tensor))\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=bool, numpy=array([False, False, False,  True])>\n\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n\n\nA variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf$Tensor. Like tensors, they have a dtype and a shape, and can be exported to regular R arrays.\n\ncat(\"Shape: \"); my_variable$shape\n\nShape: \n\n\nTensorShape([2, 2])\n\ncat(\"DType: \"); my_variable$dtype\n\nDType: \n\n\ntf.float32\n\ncat(\"As R array: \"); str(as.array(my_variable))\n\nAs R array: \n\n\n num [1:2, 1:2] 1 3 2 4\n\n\nMost tensor operations work on variables as expected, although variables cannot be reshaped.\n\nmessage(\"A variable: \")\n\nA variable: \n\nmy_variable\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\nmessage(\"Viewed as a tensor: \")\n\nViewed as a tensor: \n\nas_tensor(my_variable)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\nmessage(\"Index of highest value: \")\n\nIndex of highest value: \n\ntf$math$argmax(my_variable)\n\ntf.Tensor([1 1], shape=(2), dtype=int64)\n\n# This creates a new tensor; it does not reshape the variable.\nmessage(\"Copying and reshaping: \") \n\nCopying and reshaping: \n\ntf$reshape(my_variable, c(1L, 4L))\n\ntf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n\n\nAs noted above, variables are backed by tensors. You can reassign the tensor using tf$Variable$assign. Calling assign does not (usually) allocate a new tensor; instead, the existing tensor’s memory is reused.\n\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n\n\nIf you use a variable like a tensor in operations, you will usually operate on the backing tensor.\nCreating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory.\n\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([5., 6.], dtype=float32)>\n\n# a and b are different\n\nas.array(a)\n\n[1] 5 6\n\nas.array(b)\n\n[1] 2 3\n\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\n\n[1] 7 9\n\nas.array(a$assign_sub(c(7,9))) # c(0, 0)\n\n[1] 0 0"
  },
  {
    "objectID": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "href": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "title": "Introduction to Variables",
    "section": "Lifecycles, naming, and watching",
    "text": "Lifecycles, naming, and watching\nIn TensorFlow, tf$Variable instance have the same lifecycle as other R objects. When there are no references to a variable it is automatically deallocated (garbage-collected).\nVariables can also be named which can help you track and debug them. You can give two variables the same name.\n\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \"Mark\")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \"Mark\")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n\ntf.Tensor(\n[[False False]\n [False False]], shape=(2, 2), dtype=bool)\n\n\nVariable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.\nAlthough variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter.\n\n(step_counter <- tf$Variable(1L, trainable = FALSE))\n\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>"
  },
  {
    "objectID": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "href": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "title": "Introduction to Variables",
    "section": "Placing variables and tensors",
    "text": "Placing variables and tensors\nFor better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype. This means most variables are placed on a GPU if one is available.\nHowever, you can override this. In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available. By turning on device placement logging (see above), you can see where the variable is placed.\nNote: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation.\nIf you run this notebook on different backends with and without a GPU you will see different logging. Note that logging device placement must be turned on at the start of the session.\n\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \"float32\")\n  c <- tf$matmul(a, b)\n})\n\nc\n\ntf.Tensor(\n[[22. 49.]\n [28. 64.]], shape=(2, 2), dtype=float32)\n\n\nIt’s possible to set the location of a variable or tensor on one device and do the computation on another device. This will introduce delay, as data needs to be copied between the devices.\nYou might do this, however, if you had multiple GPU workers but only want one copy of the variables.\n\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \"float32\")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n\ntf.Tensor(\n[[ 1.  6. 15.]\n [ 2.  8. 18.]], shape=(2, 3), dtype=float32)\n\n\nNote: Because tf$config$set_soft_device_placement() is turned on by default, even if you run this code on a device without a GPU, it will still run. The multiplication step will happen on the CPU.\nFor more on distributed training, refer to the guide."
  },
  {
    "objectID": "guides/tensorflow/variable.html#next-steps",
    "href": "guides/tensorflow/variable.html#next-steps",
    "title": "Introduction to Variables",
    "section": "Next steps",
    "text": "Next steps\nTo understand how variables are typically used, see our guide on automatic differentiation."
  },
  {
    "objectID": "guides/tensorflow/variable.html#environment-details",
    "href": "guides/tensorflow/variable.html#environment-details",
    "title": "Introduction to Variables",
    "section": "Environment Details",
    "text": "Environment Details\n\n\n\n\n\n\nTensorflow Version\n\n\n\n\n\n\ntensorflow::tf_version()\n\n[1] '2.9'\n\n\n\n\n\n\n\n\n\n\n\nR Environment Information\n\n\n\n\n\n\nSys.info()\n\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\""
  },
  {
    "objectID": "install/custom.html",
    "href": "install/custom.html",
    "title": "Custom Installation",
    "section": "",
    "text": "The install_tensorflow() function is provided as a convenient way to get started, but is not required. If you have an existing installation of TensorFlow or just prefer your own custom installation that’s fine too.\nThe full instructions for installing TensorFlow on various platforms are here: https://www.tensorflow.org/install/. After installing, please refer to the sections below on locating TensorFlow and meeting additional dependencies to ensure that the tensorflow for R package functions correctly with your installation."
  },
  {
    "objectID": "install/custom.html#supported-platforms",
    "href": "install/custom.html#supported-platforms",
    "title": "Custom Installation",
    "section": "Supported Platforms",
    "text": "Supported Platforms\nNote that binary installations of TensorFlow are provided for Windows, OS X, and Ubuntu 16.04 or higher. It’s possible that binary installations will work on other Linux variants but Ubuntu is the only platform tested and supported.\nIn particular, if you are running on RedHat or CentOS you will need to install from source then follow the instructions in the [Custom Installation] section to ensure that your installation of TensorFlow can be used with the tensorflow R package."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html",
    "href": "install/gpu/cloud_desktop_gpu/index.html",
    "title": "Cloud Desktop GPUs",
    "section": "",
    "text": "Cloud desktops with various GPU configurations are available from Paperspace. With Paperspace, you can access a full Linux desktop running Ubuntu 16.04 all from within a web browser. An SSH interface is also available, as is a browser based RStudio Server interface (via SSH tunnel).\nPaperspace offers an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0) pre-installed, along with the GPU version of TensorFlow v1.4 and the R keras, tfestimators, and tensorflow packages. Follow the instructions below to get started with using RStudio on Paperspace."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#getting-started",
    "href": "install/gpu/cloud_desktop_gpu/index.html#getting-started",
    "title": "Cloud Desktop GPUs",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, sign up for a Paperspace account here: https://www.paperspace.com/account/signup (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\n\nAfter you’ve signed up and verified your account email, you will be taken to a Create Machine page. Here you’ll select various options including your compute region and machine template. You should select the RStudio template:\n\nBe sure to select one of the GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\nAfter your machine is provisioned (this can take a few minutes) you are ready to access it via a web browser. Hover over the machine in the Paperspace Console and click the “Launch” link:\n\nAfter the machine is launched you’ll see your Linux desktop within the browser you launched it from. You may need to use the Scaling Settings to adjust the desktop to a comfortable resolution:\n\nYou should also change your default password using the passwd utility (your default password should have been sent to you in an email titled “Your new Paperspace Linux machine is ready”):\n\nYou now have a Linux desktop equipped ready to use with TensorFlow for R! Go ahead and run RStudio from the application bar:\n\nNVIDIA GPU libraries (CUDA 9 and cuDNN 7) are pre-installed, along with the GPU version of TensorFlow v1.7. The R keras, tfestimators, and tensorflow packages are also pre-installed, as are all of the packages from the [tidyverse[(https://www.tidyverse.org/)] (dplyr, ggplot2, etc.).\nAn important note about the pre-installed dependencies: Since the NVIDIA CUDA libraries, TensorFlow, and Keras are all pre-installed on the Paperspace instances, you should not use the install_tensorflow() or install_keras() functions, but rather rely on the existing, pre-configured versions of these libraries. Installing or updating other versions of these libraries will likely not work at all!"
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "href": "install/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "title": "Cloud Desktop GPUs",
    "section": "Automatic Shutdown",
    "text": "Automatic Shutdown\nYou can set Paperspace machines to automatically shutdown when they have not been used for a set period of time (this is especially important since machine time is billed by the hour). You can access this setting from the Paperspace console for your machine:\n\nHere the auto-shutdown time is set to 1 day, however you can also choose shorter or longer intervals."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "href": "install/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "title": "Cloud Desktop GPUs",
    "section": "Terminal Access",
    "text": "Terminal Access\n\nWeb Terminal\nYou can use the Open Terminal command on the Paperspace console for your machine to open a web based terminal to your machine:\n\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\n\nSSH Login\nYou can also login to your Paperspace instance using a standard SSH client. This requires that you first Assign a public IP address to your machine (note that public IP addresses cost an additional $3/month).\nOnce you have your public IP address, you can SSH into your machine as follows:\n$ ssh paperspace@<public IP>\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "href": "install/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "title": "Cloud Desktop GPUs",
    "section": "RStudio Server",
    "text": "RStudio Server\nYou may prefer using the RStudio Server browser-based interface to the virtual Linux desktop provided by Paperspace (especially when on slower internet connections). This section describes how to access your Paperspace machine using an SSH tunnel.\nTo start with, follow the instructions for SSH Login immediately above and ensure that you can login to your machine remotely via SSH.\nOnce you’ve verified this, you should also be able to setup an SSH tunnel to RStudio Server as follows:\n$ ssh -N -L 8787:127.0.0.1:8787 paperspace@<public-ip>\nYou can access RStudio Server by navigating to port 8787 on your local machine and logging in using the paperspace account and either the default password emailed to you when you created the machine or to the new password which you subsequently created.\nhttp://localhost:8787"
  },
  {
    "objectID": "install/gpu/cloud_server_gpu/index.html",
    "href": "install/gpu/cloud_server_gpu/index.html",
    "title": "Cloud Server GPUs",
    "section": "",
    "text": "Cloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally."
  },
  {
    "objectID": "install/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "href": "install/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "title": "Cloud Server GPUs",
    "section": "Amazon EC2",
    "text": "Amazon EC2\nRStudio has AWS Marketplace offerings that are designed to provide stable, secure, and high performance execution environments for deep learning applications running on Amazon EC2. The tensorflow, tfestimators, and keras R packages (along with their pre-requisites, including the GPU version of TensorFlow) are installed as part of the image.\n\nLaunching the Server\nThere are AMIs on the Amazon Cloud Marketplace for both the open-source and Professional versions of RStudio Server. You can find them here:\n\nOpen Source: https://aws.amazon.com/marketplace/pp/B0785SXYB2\nProfessional: https://aws.amazon.com/marketplace/pp/B07B8G3FZP\n\nYou should launch these AMIs on the p2.xlarge instance type. This type includes a single GPU whereas other GPU-based images include up to 16 GPUs (however they are commensurately much more expensive). Note that you may need to select a different region than your default to be able to launch p2.xlarge instances (for example, selecting “US East (Ohio)” rather than “US East (N Virginia)”).\n\n\n\nAccessing the Server\nAfter you’ve launched the server you can access an instance of RStudio Server running on port 8787. For example:\nhttp://ec2-18-217-204-43.us-east-2.compute.amazonaws.com:8787\nNote that the above server address needs to be substituted for the public IP of the server you launched, which you can find in the EC2 Dashboard.\nThe first time you access the server you will be presented with a login screen:\n\nLogin with user id “rstudio-user” and password the instance ID of your AWS server (for example “i-0a8ea329c18892dfa”, your specific ID is available via the EC2 dashboard).\nThen, use the RStudio Terminal to change the default password using the passwd utility:\n\nYour EC2 deep learning instance is now ready to use (the tensorflow, tfestimators, and keras R packages along with their pre-requisites, including the GPU version of TensorFlow, are installed as part of the image).\nSee the sections below for discussion of various ways in which you can make your EC2 instance more secure.\n\n\nLimiting Inbound Traffic\nThe EC2 instance is by default configured to allow access to SSH and HTTP traffic from all IP addresses on the internet, whereas it would be more desirable to restrict this to IP addresses that you know you will access the server from (this can however be challenging if you plan on accessing the server from a variety of public networks).\nYou can see these settings in the Security Group of your EC2 instance:\n\nEdit the Source for the SSH and HTTP protocols to limit access to specific blocks of IP addresses.\n\n\nUsing HTTPS\nBy default the EC2 instance which you launched is accessed over HTTP, a non-encrypted channel. This means that data transmitted to the instance (including your username and password) can potentially be compromised during transmission.\nThere are many ways to add HTTPS support to a server including AWS Elastic Load Balancing, CloudFlare SSL, and setting up reverse proxy from an Nginx or Apache web server configured with SSL support.\nThe details of adding HTTPS support to your server are beyond the scope of this article (see the links above to learn more). An alternative to this is to prohibit external HTTP connections entirely and access the server over an SSH Tunnel, this option is covered in the next section.\n\n\nSSH Tunnel\nUsing an SSH Tunnel to access your EC2 instance provides a number of benefits, including:\n\nUse of the SSH authentication protocol to identify and authorize remote users\nEncrypting traffic that would otherwise be sent in the clear\n\nNote that SSH tunnel access as described below works only for Linux and OS X clients.\n\nSecurity Group\nTo use an SSH Tunnel with your EC2 instance, first configure the Security Group of your instance to only accept SSH traffic (removing any HTTP entry that existed previously):\n\nNote that you may also want to restrict the Source of SSH traffic to the specific block of IP addresses you plan to access the server from.\n\n\nServer Configuration\nNext, connect to your instance over SSH (click the Connect button in the EC2 console for instructions specific to your server):\nssh -i \"my-security-key.pem\" ubuntu@my-ec2-server-address\nNote that if you copied and pasted the command from the EC2 console you may see this error message:\nPlease login as the user \"ubuntu\" rather than the user \"root\".\nIn that case be sure that you use ubuntu@my-ec2-server-address rather than root@my-ec2-server-address.\nExecute the following commands to configure RStudio Server to only accept local connections:\n# Configure RStudio to only allow local connections \nsudo /bin/bash -c \"echo 'www-address=127.0.0.1' >> /etc/rstudio/rserver.conf\"\n\n# Restart RStudio with new settings\nsudo rstudio-server restart\n\n\nConnecting to the Server\nYou should now be able to connect to the server via SSH tunnel as follows:\nssh -N -L 8787:localhost:8787 -i my-security-key.pem ubuntu@my-ec2-server-address\n(where my-security-key.pem and my-ec2-server-address are specific to your server configuration).\nOnce the SSH connection is established, RStudio Server will be available at http://localhost:8787/"
  },
  {
    "objectID": "install/gpu/index.html",
    "href": "install/gpu/index.html",
    "title": "Overview",
    "section": "",
    "text": "If your local workstation doesn’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple, low-cost way for you to get started without having to buy any additional hardware. See the documentation below for details on using both local and cloud GPUs.\n\n\n\n\n\n\n\nLocal GPU\nFor systems that have a recent, high-end NVIDIA® GPU, TensorFlow is available in a GPU version that takes advantage of the CUDA and cuDNN libraries to accelerate training performance. Note that the GPU version of TensorFlow is currently only supported on Windows and Linux (there is no GPU version available for Mac OS X since NVIDIA GPUs are not commonly available on that platform).\n\n\nCloudML\nGoogle CloudML is a managed service that provides on-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA. CloudML also provides hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\n\n\nCloud Server\nCloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally.\n\n\nCloud Desktop\nVirtual cloud desktops with GPUs are available from Paperspace. This provides an Ubuntu 16.04 desktop environment that you can access entirely within a web browser (note that this requires a reasonbly fast internet connection to be usable)."
  },
  {
    "objectID": "install/gpu/local_gpu/index.html",
    "href": "install/gpu/local_gpu/index.html",
    "title": "Local GPU",
    "section": "",
    "text": "TensorFlow can be configured to run on either CPUs or GPUs. The CPU version is much easier to install and configure so is the best starting place especially when you are first learning how to use TensorFlow. Here’s the guidance on CPU vs. GPU versions from the TensorFlow website:\n\nTensorFlow with CPU support only. If your system does not have a NVIDIA® GPU, you must install this version. Note that this version of TensorFlow is typically much easier to install (typically, in 5 or 10 minutes), so even if you have an NVIDIA GPU, we recommend installing this version first.\nTensorFlow with GPU support. TensorFlow programs typically run significantly faster on a GPU than on a CPU. Therefore, if your system has a NVIDIA® GPU meeting the prerequisites shown below and you need to run performance-critical applications, you should ultimately install this version.\n\nSo if you are just getting started with TensorFlow you may want to stick with the CPU version to start out, then install the GPU version once your training becomes more computationally demanding.\nThe prerequisites for the GPU version of TensorFlow on each platform are covered below. Once you’ve met the prerequisites installing the GPU version in a single-user / desktop environment is as simple as:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf you are using Keras you can install both Keras and the GPU version of TensorFlow with:\n\nlibrary(keras)\ninstall_keras(tensorflow = \"gpu\")\n\nNote that on all platforms you must be running an NVIDIA® GPU with CUDA® Compute Capability 3.5 or higher in order to run the GPU version of TensorFlow. See the list of CUDA-enabled GPU cards."
  },
  {
    "objectID": "install/gpu/local_gpu/index.html#prerequisties",
    "href": "install/gpu/local_gpu/index.html#prerequisties",
    "title": "Local GPU",
    "section": "Prerequisties",
    "text": "Prerequisties\n\nWindows\nThis article describes how to detect whether your graphics card uses an NVIDIA® GPU:\nhttp://nvidia.custhelp.com/app/answers/detail/a_id/2040/~/identifying-the-graphics-card-model-and-device-id-in-a-pc\nOnce you’ve confirmed that you have an NVIDIA® GPU, the following article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/gpu#hardware_requirements\nNote that the documentation on installation of the last component (cuDNN v7.4.1) is a bit sparse. Once you join the NVIDIA® developer program and download the zip file containing cuDNN you need to extract the zip file and add the location where you extracted it to your system PATH.\n\n\nUbuntu\nThis article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support\nThe specifics of installing required software differ by Linux version so please review the NVIDIA® documentation carefully to ensure you install everything correctly.\nThe following section provides as example of the installation commands you might use on Ubuntu 16.04.\n\nUbuntu 16.04 Example\nFirst, install the NVIDIA drivers:\n# Add NVIDIA package repositories\n# Add HTTPS support for apt-key\nsudo apt-get install gnupg-curl\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\nsudo apt-get update\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt-get update\n\n# Install NVIDIA driver\n# Issue with driver install requires creating /usr/lib/nvidia\nsudo mkdir /usr/lib/nvidia\nsudo apt-get install --no-install-recommends nvidia-410\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\nNext install CUDA Toolkit v10.0 and cuDNN v7.4.1 with:\n# Install development and runtime libraries (~4GB)\nsudo apt-get install --no-install-recommends \\\n    cuda-10-0 \\\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\nNote that it’s important to download CUDA 10.0 (rather than CUDA 10.1, which may be the choice initially presented) as v10.0 is what TensorFlow is built against.\nYou can see more for the installation here.\n\n\nEnvironment Variables\nOn Linux, part of the setup for CUDA libraries is adding the path to the CUDA binaries to your PATH and LD_LIBRARY_PATH as well as setting the CUDA_HOME environment variable. You will set these variables in distinct ways depending on whether you are installing TensorFlow on a single-user workstation or on a multi-user server. If you are running RStudio Server there is some additional setup required which is also covered below.\nIn all cases these are the environment variables that need to be set/modified in order for TensorFlow to find the required CUDA libraries. For example (paths will change depending on your specific installation of CUDA):\nexport CUDA_HOME=/usr/local/cuda\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${CUDA_HOME}/lib64 \nPATH=${CUDA_HOME}/bin:${PATH} \nexport PATH\n\n\nSingle-User Installation\nIn a single-user environment (e.g. a desktop system) you should define the environment variables within your ~/.profile file. It’s necessary to use ~/.profile rather than ~/.bashrc, because ~/.profile is read by desktop applications (e.g. RStudio) as well as terminal sessions whereas ~/.bashrc applies only to terminal sessions.\nNote that you need to restart your system after editing the ~/.profile file for the changes to take effect. Note also that the ~/.profile file will not be read by bash if you have either a ~/.bash_profile or ~/.bash_login file.\nTo summarize the recommendations above:\n\nDefine CUDA related environment variables in ~/.profile rather than ~/.bashrc;\nEnsure that you don’t have either a ~/.bash_profile or ~/.bash_login file (as these will prevent bash from seeing the variables you’ve added into ~/.profile);\nRestart your system after editing ~/.profile so that the changes take effect.\n\n\n\nMulti-User Installation\nIn a multi-user installation (e.g. a server) you should define the environment variables within the system-wide bash startup file (/etc/profile) so all users have access to them.\nIf you are running RStudio Server you need to also provide these variable definitions in an R / RStudio specific fashion (as RStudio Server doesn’t execute system profile scripts for R sessions).\nTo modify the LD_LIBRARY_PATH you use the rsession-ld-library-path in the /etc/rstudio/rserver.conf configuration file\n/etc/rstudio/rserver.conf\nrsession-ld-library-path=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nYou should set the CUDA_HOME and PATH variables in the /usr/lib/R/etc/Rprofile.site configuration file:\n/usr/lib/R/etc/Rprofile.site\nSys.setenv(CUDA_HOME=\"/usr/local/cuda\")\nSys.setenv(PATH=paste(Sys.getenv(\"PATH\"), \"/usr/local/cuda/bin\", sep = \":\"))\nIn a server environment you might also find it more convenient to install TensorFlow into a system-wide location where all users of the server can share access to it. Details on doing this are covered in the multi-user installation section below.\n\n\n\nMac OS X\nAs of version 1.2 of TensorFlow, GPU support is no longer available on Mac OS X. If you want to use a GPU on Mac OS X you will need to install TensorFlow v1.1 as follows:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"1.1-gpu\")\n\nHowever, before you install you should ensure that you have an NVIDIA® GPU and that you have the required CUDA libraries on your system.\nWhile some older Macs include NVIDIA® GPU’s, most Macs (especially newer ones) do not, so you should check the type of graphics card you have in your Mac before proceeding.\nHere is a list of Mac systems which include built in NVIDIA GPU’s:\nhttps://support.apple.com/en-us/HT204349\nYou can check which graphics card your Mac has via the System Report button found within the About This Mac dialog:\n\nThe MacBook Pro system displayed above does not have an NVIDIA® GPU installed (rather it has an Intel Iris Pro).\nIf you do have an NVIDIA® GPU, the following article describes how to install the base CUDA libraries:\nhttp://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\nYou also need to intall the cuDNN library 5.1 library for OS X from here:\nhttps://developer.nvidia.com/cudnn\nAfter installing these components, you need to ensure that both CUDA and cuDNN are available to your R session via the DYLD_LIBRARY_PATH. This typically involves setting environment variables in your .bash_profile as described in the NVIDIA documentation for CUDA and cuDNN.\nNote that environment variables set in .bash_profile will not be available by default to OS X desktop applications like R GUI and RStudio. To use CUDA within those environments you should start the application from a system terminal as follows:\nopen -a R         # R GUI\nopen -a RStudio   # RStudio"
  },
  {
    "objectID": "install/gpu/local_gpu/index.html#installation",
    "href": "install/gpu/local_gpu/index.html#installation",
    "title": "Local GPU",
    "section": "Installation",
    "text": "Installation\n\nSingle User\nIn a single-user desktop environment you can install TensorFlow with GPU support via:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf this version doesn’t load successfully you should review the prerequisites above and ensure that you’ve provided definitions of CUDA environment variables as recommended above.\nSee the main installation article for details on other available options (e.g. virtualenv vs. conda installation, installing development versions, etc.).\n\n\nMultiple Users\nIn a multi-user server environment you may want to install a system-wide version of TensorFlow with GPU support so all users can share the same configuration. To do this, start by following the directions for native pip installation of the GPU version of TensorFlow here:\nhttps://www.tensorflow.org/install/install_linux#InstallingNativePip\nThere are some components of TensorFlow (e.g. the Keras library) which have dependencies on additional Python packages.\nYou can install Keras and it’s optional dependencies with the following command (ensuring you have the correct privilege to write to system library locations as required via sudo, etc.):\npip install keras h5py pyyaml requests Pillow scipy\nIf you have any trouble with locating the system-wide version of TensorFlow from within R please see the section on locating TensorFlow."
  },
  {
    "objectID": "install/index.html",
    "href": "install/index.html",
    "title": "Quick start",
    "section": "",
    "text": "Prior to using the tensorflow R package you need to install a version of TensorFlow on your system. Below we describe how to install TensorFlow as well the various options available for customizing your installation.\nNote that this article principally covers the use of the R install_tensorflow() function, which provides an easy to use wrapper for the various steps required to install TensorFlow.\nYou can also choose to install TensorFlow manually (as described at https://www.tensorflow.org/install/). In that case the Custom Installation section covers how to arrange for the tensorflow R package to use the version you installed.\nTensorFlow is tested and supported on the following 64-bit systems:"
  },
  {
    "objectID": "install/index.html#installation",
    "href": "install/index.html#installation",
    "title": "Quick start",
    "section": "Installation",
    "text": "Installation\nFirst, install the tensorflow R package from GitHub as follows:\n\ninstall.packages(\"tensorflow\")\n\nNext, configure R (reticulate) with a Python installation it can use, like this:\n\nlibrary(reticulate)\npath_to_python <- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n\nThen, use the install_tensorflow() function to install TensorFlow.\n\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-reticulate\")\n\nYou can confirm that the installation succeeded with:\n\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow\")\n\nLoaded Tensorflow version 2.9.1\n\n\ntf.Tensor(b'Hello Tensorflow', shape=(), dtype=string)\n\n\nThis will provide you with a default installation of TensorFlow suitable for use with the tensorflow R package. Read on if you want to learn about additional installation options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed."
  },
  {
    "objectID": "install/index.html#installation-methods",
    "href": "install/index.html#installation-methods",
    "title": "Quick start",
    "section": "Installation methods",
    "text": "Installation methods\nTensorFlow is distributed as a Python package and so needs to be installed within a Python environment on your system. By default, the install_tensorflow() function attempts to install TensorFlow within an isolated Python environment (“r-reticulate”).\nThese are the available methods and their behavior:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nauto\nAutomatically choose an appropriate default for the current platform.\n\n\nvirtualenv\nInstall into a Python virtual environment at ~/.virtualenvs/r-reticulate\n\n\nconda\nInstall into an Anaconda Python environment named r-reticulate\n\n\nsystem\nInstall into the system Python environment\n\n\n\ninstall_tensorflow is a wrapper around reticulate::py_install. Please refer to ‘Installing Python Packages’ for more information."
  },
  {
    "objectID": "install/index.html#alternate-versions",
    "href": "install/index.html#alternate-versions",
    "title": "Quick start",
    "section": "Alternate Versions",
    "text": "Alternate Versions\nBy default, install_tensorflow() install the latest release version of TensorFlow. You can override this behavior by specifying the version parameter. For example:\n\ninstall_tensorflow(version = \"2.7\")\n\nNote that you can provide a full major.minor.patch version specification, or just a major.minor specification, in which case the latest patch is automatically selected.\nYou can install the nightly build of TensorFlow (CPU or GPU version) with:\n\ninstall_tensorflow(version = \"nightly\")      # cpu+gpu version\ninstall_tensorflow(version = \"nightly-cpu\")  # cpu version\n\nYou can install any other build of TensorFlow by specifying a URL to a TensorFlow binary. For example:\n\ninstall_tensorflow(version = \"https://files.pythonhosted.org/packages/c2/c1/a035e377cf5a5b90eff27f096448fa5c5a90cbcf13b7eb0673df888f2c2d/tf_nightly-1.12.0.dev20180918-cp36-cp36m-manylinux1_x86_64.whl\")"
  },
  {
    "objectID": "tutorials/keras/classification.html",
    "href": "tutorials/keras/classification.html",
    "title": "Basic Image Classification",
    "section": "",
    "text": "In this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts. It’s fine if you don’t understand all the details, this is a fast-paced overview of a complete Keras program with the details explained as we go."
  },
  {
    "objectID": "tutorials/keras/classification.html#import-the-fashion-mnist-dataset",
    "href": "tutorials/keras/classification.html#import-the-fashion-mnist-dataset",
    "title": "Basic Image Classification",
    "section": "Import the Fashion MNIST dataset",
    "text": "Import the Fashion MNIST dataset\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n\nFigure 1. Fashion-MNIST samples (by Zalando, MIT License).\n\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we’ll use here.\nThis guide uses Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashion_mnist <- dataset_fashion_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nc(train_images, train_labels) %<-% fashion_mnist$train\nc(test_images, test_labels) %<-% fashion_mnist$test\n\nAt this point we have four arrays: The train_images and train_labels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the test_images, and test_labels arrays.\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n\n\nDigit\nClass\n\n\n\n\n0\nT-shirt/top\n\n\n1\nTrouser\n\n\n2\nPullover\n\n\n3\nDress\n\n\n4\nCoat\n\n\n5\nSandal\n\n\n6\nShirt\n\n\n7\nSneaker\n\n\n8\nBag\n\n\n9\nAnkle boot\n\n\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we’ll store them in a vector to use later when plotting the images.\n\nclass_names = c('T-shirt/top',\n                'Trouser',\n                'Pullover',\n                'Dress',\n                'Coat', \n                'Sandal',\n                'Shirt',\n                'Sneaker',\n                'Bag',\n                'Ankle boot')"
  },
  {
    "objectID": "tutorials/keras/classification.html#explore-the-data",
    "href": "tutorials/keras/classification.html#explore-the-data",
    "title": "Basic Image Classification",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\n\n\n[1] 60000    28    28\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\n\n\n[1] 60000\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\n\n\n[1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\n\n\n[1] 10000    28    28\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000\n\n\n[1] 10000"
  },
  {
    "objectID": "tutorials/keras/classification.html#preprocess-the-data",
    "href": "tutorials/keras/classification.html#preprocess-the-data",
    "title": "Basic Image Classification",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\nlibrary(ggplot2)\n\nimage_1 <- as.data.frame(train_images[1, , ])\ncolnames(image_1) <- seq_len(ncol(image_1))\nimage_1$y <- seq_len(nrow(image_1))\nimage_1 <- gather(image_1, \"x\", \"value\", -y)\nimage_1$x <- as.integer(image_1$x)\n\nggplot(image_1, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\n  scale_y_reverse() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())   +\n  theme(aspect.ratio = 1) +\n  xlab(\"\") +\n  ylab(\"\")\n\n\n\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255.\nIt’s important that the training set and the testing set are preprocessed in the same way:\n\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n\nDisplay the first 25 images from the training set and display the class name below each image. Verify that the data is in the correct format and we’re ready to build and train the network.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- train_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste(class_names[train_labels[i] + 1]))\n}"
  },
  {
    "objectID": "tutorials/keras/classification.html#build-the-model",
    "href": "tutorials/keras/classification.html#build-the-model",
    "title": "Basic Image Classification",
    "section": "Build the model",
    "text": "Build the model\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense, have parameters that are learned during training.\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_flatten(input_shape = c(28, 28)) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe first layer in this network, layer_flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two dense layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n\nCompile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function — This measures how accurate the model is during training. We want to minimize this function to “steer” the model in the right direction.\nOptimizer — This is how the model is updated based on the data it sees and its loss function.\nMetrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n\nmodel %>% compile(\n  optimizer = 'adam', \n  loss = 'sparse_categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\n\n\nTrain the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model — in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set — in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n\nTo start training, call the fit method — the model is “fit” to the training data:\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n\n\nEvaluate accuracy\nNext, compare how the model performs on the test dataset:\n\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\n\ncat('Test loss:', score[\"loss\"], \"\\n\")\n\nTest loss: 0.356607 \n\ncat('Test accuracy:', score[\"acc\"], \"\\n\")\n\nTest accuracy: NA \n\n\nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n\n\nMake predictions\nWith the model trained, we can use it to make predictions about some images.\n\npredictions <- model %>% predict(test_images)\n\nHere, the model has predicted the label for each image in the testing set. Let’s take a look at the first prediction:\n\npredictions[1, ]\n\n [1] 0.00000610363713 0.00000013651153 0.00000006553638 0.00000079706132\n [5] 0.00000156569411 0.01494382042438 0.00000785118391 0.03553177043796\n [9] 0.00002116547330 0.94948673248291\n\n\nA prediction is an array of 10 numbers. These describe the “confidence” of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\n[1] 10\n\n\nAs the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot. And we can check the test label to see this is correct:\n\ntest_labels[1]\n\n[1] 9\n\n\nLet’s plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- test_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  # subtract 1 as labels go from 0 to 9\n  predicted_label <- which.max(predictions[i, ]) - 1\n  true_label <- test_labels[i]\n  if (predicted_label == true_label) {\n    color <- '#008800' \n  } else {\n    color <- '#bb0000'\n  }\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste0(class_names[predicted_label + 1], \" (\",\n                      class_names[true_label + 1], \")\"),\n        col.main = color)\n}\n\n\n\n\nFinally, use the trained model to make a prediction about a single image.\n\n# Grab an image from the test dataset\n# take care to keep the batch dimension, as this is expected by the model\nimg <- test_images[1, , , drop = FALSE]\ndim(img)\n\n[1]  1 28 28\n\n\nNow predict the image:\n\npredictions <- model %>% predict(img)\npredictions\n\n               [,1]            [,2]             [,3]           [,4]\n[1,] 0.000006103648 0.0000001365119 0.00000006553637 0.000000797062\n               [,5]       [,6]           [,7]       [,8]          [,9]\n[1,] 0.000001565694 0.01494383 0.000007851183 0.03553182 0.00002116551\n         [,10]\n[1,] 0.9494866\n\n\npredict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\n# subtract 1 as labels are 0-based\nprediction <- predictions[1, ] - 1\nwhich.max(prediction)\n\n[1] 10\n\n\nAnd, as before, the model predicts a label of 9."
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html",
    "href": "tutorials/keras/overfit_and_underfit.html",
    "title": "Overfit and underfit",
    "section": "",
    "text": "As always, the code in this example will use the Keras API, which you can learn more about in the TensorFlow Keras guide.\nIn both of the previous examples — classifying text and predicting fuel efficiency — the accuracy of models on the validation data would peak after training for a number of epochs and then stagnate or start decreasing.\nIn other words, your model would overfit to the training data. Learning how to deal with overfitting is important. Although it’s often possible to achieve high accuracy on the training set, what you really want is to develop models that generalize well to a testing set (or data they haven’t seen before).\nThe opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the train data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\nIf you train for too long though, the model will start to overfit and learn patterns from the training data that don’t generalize to the test data. You need to strike a balance. Understanding how to train for an appropriate number of epochs as you’ll explore below is a useful skill.\nTo prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases.\nA model trained on more complete data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\nIn this notebook, you’ll explore several common regularization techniques, and use them to improve on a classification model."
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html#setup",
    "href": "tutorials/keras/overfit_and_underfit.html#setup",
    "title": "Overfit and underfit",
    "section": "Setup",
    "text": "Setup\nBefore getting started, import the necessary packages:\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(ggplot2)\n\n\nlogdir <- tempfile()"
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html#the-higgs-dataset",
    "href": "tutorials/keras/overfit_and_underfit.html#the-higgs-dataset",
    "title": "Overfit and underfit",
    "section": "The Higgs dataset",
    "text": "The Higgs dataset\nThe goal of this tutorial is not to do particle physics, so don’t dwell on the details of the dataset. It contains 11,000,000 examples, each with 28 features, and a binary class label.\n\ngz <- get_file(\n  'HIGGS.csv.gz', \n  'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz',\n  cache_dir = \"higgs\"\n)\n\nLoaded Tensorflow version 2.9.1\n\n\n\nFEATURES <- 28\n\nThe tf$data$experimental$CsvDataset class can be used to read csv records directly from a gzip file with no intermediate decompression step.\n\nds <- tf$data$experimental$CsvDataset(\n  gz,\n  lapply(seq_len(FEATURES + 1), function(x) tf$float32), \n  compression_type = \"GZIP\"\n)\n\nThat csv reader class returns a list of scalars for each record. The following function repacks that list of scalars into a (feature_vector, label) pair.\n\npack_row <- function(...) {\n  row <- list(...)\n  label <- row[[1]]\n  features <- tf$stack(row[2:length(row)], 1L)\n  list(features, label)\n}\n\nTensorFlow is most efficient when operating on large batches of data.\nSo, instead of repacking each row individually make a new tf$data$Dataset that takes batches of 10,000 examples, applies the pack_row function to each batch, and then splits the batches back up into individual records:\n\npacked_ds <- ds %>% \n  dataset_batch(10000) %>% \n  dataset_map(pack_row)\npacked_ds <- packed_ds$unbatch()\n\nInspect some of the records from this new packed_ds.\nThe features are not perfectly normalized, but this is sufficient for this tutorial.\n\nbatch <- packed_ds %>% \n  dataset_batch(1000) %>% \n  dataset_take(1) %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nstr(batch)\n\nList of 2\n $ :<tf.Tensor: shape=(1000, 28), dtype=float32, numpy=…>\n $ :<tf.Tensor: shape=(1000), dtype=float32, numpy=…>\n\n\nTo keep this tutorial relatively short, use just the first 1,000 samples for validation, and the next 10,000 for training:\n\nN_VALIDATION <- 1e3\nN_TRAIN <- 1e4\nBUFFER_SIZE <- 1e4\nBATCH_SIZE <- 500\nSTEPS_PER_EPOCH <- N_TRAIN %/% BATCH_SIZE\n\nThe dataset_skip() and dataset_take() methods make this easy.\nAt the same time, use the dataset_cache() method to ensure that the loader doesn’t need to re-read the data from the file on each epoch:\n\nvalidate_ds <- packed_ds %>% \n  dataset_take(N_VALIDATION) %>% \n  dataset_cache()\ntrain_ds <- packed_ds %>% \n  dataset_skip(N_VALIDATION) %>% \n  dataset_take(N_TRAIN) %>% \n  dataset_cache()\n\n\ntrain_ds\n\n<CacheDataset element_spec=(TensorSpec(shape=(28,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>\n\n\nThese datasets return individual examples. Use the dataset_batch() method to create batches of an appropriate size for training. Before batching, also remember to use dataset_shuffle() and dataset_repeat() on the training set.\n\nvalidate_ds <- validate_ds %>% dataset_batch(BATCH_SIZE)\ntrain_ds <- train_ds %>% \n  dataset_shuffle(BUFFER_SIZE) %>% \n  dataset_repeat() %>% \n  dataset_batch(BATCH_SIZE)"
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html#demonstrate-overfitting",
    "href": "tutorials/keras/overfit_and_underfit.html#demonstrate-overfitting",
    "title": "Overfit and underfit",
    "section": "Demonstrate overfitting",
    "text": "Demonstrate overfitting\nThe simplest way to prevent overfitting is to start with a small model. A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model’s “capacity”.\nIntuitively, a model with more parameters will have more “memorization capacity” and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\nAlways keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\nOn the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between “too much capacity” and “not enough capacity”.\nUnfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or the right size for each layer). You will have to experiment using a series of different architectures.\nTo find an appropriate model size, it’s best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\nStart with a simple model using only densely-connected layers (tf$keras$layers$Dense) as a baseline, then create larger models, and compare them.\n\nTraining procedure\nMany models train better if you gradually reduce the learning rate during training. Use keras::learning_rate_schedule* to reduce the learning rate over time:\n\nlr_schedule <- learning_rate_schedule_inverse_time_decay(\n  0.001,\n  decay_steps = STEPS_PER_EPOCH*1000,\n  decay_rate = 1,\n  staircase = FALSE\n)\n\nget_optimizer <- function() {\n  optimizer_adam(lr_schedule)\n}\n\nThe code above sets a learning_rate_schedule_inverse_time_decay() to hyperbolically decrease the learning rate to 1/2 of the base rate at 1,000 epochs, 1/3 at 2,000 epochs, and so on.\n\nstep <- seq(1, 100000)\nlr <- lr_schedule(step)\ndata.frame(step = as.numeric(step), lr = as.numeric(lr)) %>% \n  ggplot(aes(x = step, y = lr)) +\n  geom_line()\n\n\n\n\nEach model in this tutorial will use the same training configuration. So set these up in a reusable way, starting with the list of callbacks.\nThe training for this tutorial runs for many short epochs.\nWe include callback_early_stopping() to avoid long and unnecessary training times. Note that this callback is set to monitor the val_binary_crossentropy, not the val_loss. This difference will be important later.\nUse callback_tensorboard() to generate TensorBoard logs for the training.\n\nget_callbacks <- function(name) {\n  list(\n    callback_early_stopping(monitor = 'val_binary_crossentropy', patience = 200),\n    callback_tensorboard(file.path(logdir, name))\n  )\n}\n\nSimilarly each model will use the same compile() and fit() settings:\n\ncompile_and_fit <- function(model, name, optimizer = NULL, max_epochs = 10000) {\n  if (is.null(optimizer)) {\n    optimizer <- get_optimizer()\n  }\n  \n  model %>% compile(\n    optimizer = optimizer,\n    loss = loss_binary_crossentropy(from_logits = TRUE),\n    metrics = list(\n      loss_binary_crossentropy(from_logits = TRUE, name = \"binary_crossentropy\"),\n      \"accuracy\"\n    )\n  )\n  \n  summary(model)\n\n  history <- model %>% fit(\n    train_ds,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = max_epochs,\n    validation_data = validate_ds,\n    callbacks = get_callbacks(name),\n    verbose = 0\n  )\n  \n  history\n}\n\n\n\nTiny model\nStart by training a model:\n\ntiny_model <- keras_model_sequential() %>% \n  layer_dense(16, activation = \"elu\", input_shape = shape(FEATURES)) %>% \n  layer_dense(1)\n\n\nsize_histories <- list()\n\n\nsize_histories[['Tiny']] <- compile_and_fit(tiny_model, 'sizes/Tiny')\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_1 (Dense)                  (None, 16)                    464         \n dense (Dense)                    (None, 1)                     17          \n============================================================================\nTotal params: 481\nTrainable params: 481\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nNow check how the model did:\n\nsize_histories[[\"Tiny\"]] %>% \n  plot() +\n  coord_cartesian(xlim = c(0, 5000))\n\n\n\n\n\n\nSmall model\nTo check if you can beat the performance of the small model, progressively train some larger models. Try two hidden layers with 16 units each:\n\nsmall_model <- keras_model_sequential() %>% \n  # `input_shape` is only required here so that `.summary` works.\n  layer_dense(16, activation = 'elu', input_shape = shape(FEATURES)) %>% \n  layer_dense(16, activation = 'elu') %>% \n  layer_dense(1)\n\n\nsize_histories[['Small']] <- compile_and_fit(small_model, 'sizes/Small')\n\nModel: \"sequential_1\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_4 (Dense)                  (None, 16)                    464         \n dense_3 (Dense)                  (None, 16)                    272         \n dense_2 (Dense)                  (None, 1)                     17          \n============================================================================\nTotal params: 753\nTrainable params: 753\nNon-trainable params: 0\n____________________________________________________________________________\n\n\n\n\nMedium model\nNow try three hidden layers with 64 units each:\n\nmedium_model <- keras_model_sequential() %>% \n  layer_dense(64, activation = 'elu', input_shape = shape(FEATURES)) %>% \n  layer_dense(64, activation = 'elu') %>% \n  layer_dense(64, activation = 'elu') %>% \n  layer_dense(1)\n\nAnd train the model using the same data:\n\nsize_histories[['Medium']] <- compile_and_fit(medium_model, \"sizes/Medium\")\n\nModel: \"sequential_2\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_8 (Dense)                  (None, 64)                    1856        \n dense_7 (Dense)                  (None, 64)                    4160        \n dense_6 (Dense)                  (None, 64)                    4160        \n dense_5 (Dense)                  (None, 1)                     65          \n============================================================================\nTotal params: 10,241\nTrainable params: 10,241\nNon-trainable params: 0\n____________________________________________________________________________\n\n\n\n\nLarge model\nAs an exercise, you can create an even larger model and check how quickly it begins overfitting. Next, add to this benchmark a network that has much more capacity, far more than the problem would warrant:\n\nlarge_model <- keras_model_sequential() %>% \n  layer_dense(512, activation = 'elu', input_shape = shape(FEATURES)) %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dense(1)\n\nAnd, again, train the model using the same data:\n\nsize_histories[['large']] <- compile_and_fit(large_model, \"sizes/large\")\n\nModel: \"sequential_3\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_13 (Dense)                 (None, 512)                   14848       \n dense_12 (Dense)                 (None, 512)                   262656      \n dense_11 (Dense)                 (None, 512)                   262656      \n dense_10 (Dense)                 (None, 512)                   262656      \n dense_9 (Dense)                  (None, 1)                     513         \n============================================================================\nTotal params: 803,329\nTrainable params: 803,329\nNon-trainable params: 0\n____________________________________________________________________________\n\n\n\n\nPlot the training and validation losses\nThe solid lines show the training loss, and the dashed lines show the validation loss (remember: a lower validation loss indicates a better model).\nWhile building a larger model gives it more power, if this power is not constrained somehow it can easily overfit to the training set.\nIn this example, typically, only the \"Tiny\" model manages to avoid overfitting altogether, and each of the larger models overfit the data more quickly. This becomes so severe for the \"large\" model that you need to switch the plot to a log-scale to really figure out what’s happening.\nThis is apparent if you plot and compare the validation metrics to the training metrics.\n\nIt’s normal for there to be a small difference.\nIf both metrics are moving in the same direction, everything is fine.\nIf the validation metric begins to stagnate while the training metric continues to improve, you are probably close to overfitting.\nIf the validation metric is going in the wrong direction, the model is clearly overfitting.\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nplot_histories <- function(histories) {\n  histories %>% \n    purrr::map_dfr(as.data.frame, .id = \"model\") %>% \n    filter(metric == \"binary_crossentropy\") %>% \n    ggplot(aes(x = epoch, y = value, color = model, linetype = data)) +\n    geom_smooth(se = FALSE) +\n    coord_cartesian(ylim = c(0.5, 0.7)) +\n    scale_x_log10()  \n}\nplot_histories(size_histories)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nNote: All the above training runs used the callback_ealry_stopping() to end the training once it was clear the model was not making progress.\n\n\nView in TensorBoard\nThese models all wrote TensorBoard logs during training.\n\ntensorboard(log_dir = logdir)\n\nYou can view the results of a previous run of this notebook on TensorBoard.dev.\nTensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\nIt’s also included in an <iframe> for convenience:\n\n\nIf you want to share TensorBoard results you can upload the logs to TensorBoard.dev by copying the following into a terminal session.\ntensorboard dev upload --logdir logdir/sizes\nCaution: This command does not terminate. It’s designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the “interrupt execution” option in your terminal tool."
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html#strategies-to-prevent-overfitting",
    "href": "tutorials/keras/overfit_and_underfit.html#strategies-to-prevent-overfitting",
    "title": "Overfit and underfit",
    "section": "Strategies to prevent overfitting",
    "text": "Strategies to prevent overfitting\nBefore getting into the content of this section copy the training logs from the \"Tiny\" model above, to use as a baseline for comparison.\n\nregularizer_histories <- list()\nregularizer_histories[['Tiny']] <- size_histories[['Tiny']]\n\n\nAdd weight regularization\nYou may be familiar with Occam’s Razor principle: given two explanations for something, the explanation most likely to be correct is the “simplest” one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\nA “simple model” in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as demonstrated in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more “regular”. This is called “weight regularization”, and it is done by adding to the loss function of the network a cost associated with(having large weights. This cost comes in two flavors, { })\n\nL1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i$e. to what is called the “L1 norm” of the weights).\nL2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i$e. to what is called the squared “L2 norm” of the weights). L2 regularization is also called weight decay in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n\nL1 regularization pushes weights towards exactly zero, encouraging a sparse model. L2 regularization will penalize the weights parameters without making them sparse since the penalty goes to zero for small weights—one reason why L2 is more common.\nIn tf$keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Add L2 weight regularization:\n\nl2_model <- keras_model_sequential() %>% \n  layer_dense(512, activation = 'elu',\n              kernel_regularizer = regularizer_l2(0.001),\n              input_shape <- shape(FEATURES)) %>% \n  layer_dense(512, activation = 'elu',\n              kernel_regularizer = regularizer_l2(0.001)) %>% \n  layer_dense(512, activation = 'elu',\n              kernel_regularizer = regularizer_l2(0.001)) %>% \n  layer_dense(512, activation = 'elu',\n              kernel_regularizer = regularizer_l2(0.001)) %>% \n  layer_dense(1)\n\nregularizer_histories[['l2']] <- compile_and_fit(l2_model, \"regularizers/l2\")\n\nModel: <no summary available, model was not built>\n\n\nl2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2 to the total loss of the network.\nThat is why we’re monitoring the binary_crossentropy directly. Because it doesn’t have this regularization component mixed in.\nSo, that same \"Large\" model with an L2 regularization penalty performs much better:\n\nplot_histories(regularizer_histories)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nAs demonstrated in the diagram above, the \"L2\" regularized model is now much more competitive with the \"Tiny\" model. This \"L2\" model is also much more resistant to overfitting than the \"Large\" model it was based on despite having the same number of parameters.\n\nMore info\nThere are two important things to note about this sort of regularization:\n\nIf you are writing your own training loop, then you need to be sure to ask the model for its regularization losses.\n\n\nresult <- l2_model(features)\nregularization_loss <- tf$add_n(l2_model$losses)\n\n\nThis implementation works by adding the weight penalties to the model’s loss, and then applying a standard optimization procedure after that.\n\nThere is a second approach that instead only runs the optimizer on the raw loss, and then while applying the calculated step the optimizer also applies some weight decay. This “decoupled weight decay” is used in optimizers like optimizer_ftrl() and optimizer_adamw().\n\n\n\nAdd dropout\nDropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto.\nThe intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own.\nDropout, applied to a layer, consists of randomly “dropping out” (i$e. set to zero) a number of output features of the layer during training. For example, a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1].\nThe “dropout rate” is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer’s output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\nIn Keras, you can introduce dropout in a network via the tf$keras$layers$Dropout layer, which gets applied to the output of layer right before.\nAdd two dropout layers to your network to check how well they do at reducing overfitting:\n\ndropout_model <- keras_model_sequential() %>% \n  layer_dense(512, activation = 'elu', input_shape = shape(FEATURES)) %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(1)\n\nregularizer_histories[['dropout']] <- compile_and_fit(dropout_model, \"regularizers/dropout\")\n\nModel: \"sequential_5\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_23 (Dense)                 (None, 512)                   14848       \n dropout_3 (Dropout)              (None, 512)                   0           \n dense_22 (Dense)                 (None, 512)                   262656      \n dropout_2 (Dropout)              (None, 512)                   0           \n dense_21 (Dense)                 (None, 512)                   262656      \n dropout_1 (Dropout)              (None, 512)                   0           \n dense_20 (Dense)                 (None, 512)                   262656      \n dropout (Dropout)                (None, 512)                   0           \n dense_19 (Dense)                 (None, 1)                     513         \n============================================================================\nTotal params: 803,329\nTrainable params: 803,329\nNon-trainable params: 0\n____________________________________________________________________________\n\n\n\nplot_histories(regularizer_histories)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nIt’s clear from this plot that both of these regularization approaches improve the behavior of the \"Large\" model. But this still doesn’t beat even the \"Tiny\" baseline.\nNext try them both, together, and see if that does better.\n\n\nCombined L2 + dropout\n\ncombined_model <- keras_model_sequential() %>% \n  layer_dense(512, kernel_regularizer = regularizer_l2(0.0001),\n              activation = 'elu', input_shape = shape(FEATURES)) %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, kernel_regularizer = regularizer_l2(0.0001),\n              activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, kernel_regularizer = regularizer_l2(0.0001),\n              activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(512, kernel_regularizer = regularizer_l2(0.0001),\n              activation = 'elu') %>% \n  layer_dropout(0.5) %>% \n  layer_dense(1)\n\nregularizer_histories[['combined']] <- compile_and_fit(combined_model, \"regularizers/combined\")\n\nModel: \"sequential_6\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_28 (Dense)                 (None, 512)                   14848       \n dropout_7 (Dropout)              (None, 512)                   0           \n dense_27 (Dense)                 (None, 512)                   262656      \n dropout_6 (Dropout)              (None, 512)                   0           \n dense_26 (Dense)                 (None, 512)                   262656      \n dropout_5 (Dropout)              (None, 512)                   0           \n dense_25 (Dense)                 (None, 512)                   262656      \n dropout_4 (Dropout)              (None, 512)                   0           \n dense_24 (Dense)                 (None, 1)                     513         \n============================================================================\nTotal params: 803,329\nTrainable params: 803,329\nNon-trainable params: 0\n____________________________________________________________________________\n\n\n\nplot_histories(regularizer_histories)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThis model with the \"Combined\" regularization is obviously the best one so far.\n\n\nView in TensorBoard\nThese models also recorded TensorBoard logs. Use the following to visualize the tensorboard:\n\ntensorboard(file.path(logdir, \"regularizers\"))\n\nYou can view the results of a previous run of this notebook on TensorBoard.dev."
  },
  {
    "objectID": "tutorials/keras/overfit_and_underfit.html#conclusions",
    "href": "tutorials/keras/overfit_and_underfit.html#conclusions",
    "title": "Overfit and underfit",
    "section": "Conclusions",
    "text": "Conclusions\nTo recap, here are the most common ways to prevent overfitting in neural networks:\n\nGet more training data.\nReduce the capacity of the network.\nAdd weight regularization.\nAdd dropout.\n\nTwo important approaches not covered in this guide are:\n\nData augmentation\nBatch normalization (layer_batch_normalization())\n\nRemember that each method can help on its own, but often combining them can be even more effective."
  },
  {
    "objectID": "tutorials/keras/regression.html",
    "href": "tutorials/keras/regression.html",
    "title": "Regression",
    "section": "",
    "text": "In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\nThis tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\nThis example uses the Keras API. (Visit the Keras tutorials and guides to learn more.)"
  },
  {
    "objectID": "tutorials/keras/regression.html#the-auto-mpg-dataset",
    "href": "tutorials/keras/regression.html#the-auto-mpg-dataset",
    "title": "Regression",
    "section": "The Auto MPG dataset",
    "text": "The Auto MPG dataset\nThe dataset is available from the UCI Machine Learning Repository.\n\nGet the data\nFirst download and import the dataset using pandas:\n\nurl <- \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\ncol_names <- c(\"mpg\",\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model_year\", \"origin\",\"car_name\")\n\nraw_dataset <- read.table(\n  url, \n  header = T, \n  col.names = col_names, \n  na.strings = \"?\"\n)\n\n\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n\n    mpg cylinders displacement horsepower weight acceleration model_year\n392  27         4          151         90   2950         17.3         82\n393  27         4          140         86   2790         15.6         82\n394  44         4           97         52   2130         24.6         82\n395  32         4          135         84   2295         11.6         82\n396  28         4          120         79   2625         18.6         82\n397  31         4          119         82   2720         19.4         82\n    origin\n392      1\n393      1\n394      2\n395      1\n396      1\n397      1\n\n\n\n\nClean the data\nThe dataset contains a few unknown values:\n\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n\nList of 8\n $ mpg         : int 0\n $ cylinders   : int 0\n $ displacement: int 0\n $ horsepower  : int 6\n $ weight      : int 0\n $ acceleration: int 0\n $ model_year  : int 0\n $ origin      : int 0\n\n\nDrop those rows to keep this initial tutorial simple:\n\ndataset <- na.omit(dataset)\n\nThe \"origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with the recipes package.\nNote: You can set up the keras_model() to do this kind of transformation for you but that’s beyond the scope of this tutorial. Check out the Classify structured data using Keras preprocessing layers or Load CSV data tutorials for examples.\n\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>% \n  step_num2factor(origin, levels = c(\"USA\", \"Europe\", \"Japan\")) %>% \n  step_dummy(origin, one_hot = TRUE) %>% \n  prep() %>% \n  bake(new_data = NULL)\n\n\nglimpse(dataset)\n\nRows: 391\nColumns: 10\n$ cylinders     <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…\n$ displacement  <dbl> 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower    <dbl> 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight        <dbl> 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…\n$ acceleration  <dbl> 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…\n$ model_year    <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ mpg           <dbl> 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …\n$ origin_USA    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…\n$ origin_Europe <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ origin_Japan  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n\n\n\n\nSplit the data into training and test sets\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n\n\n\nInspect the data\nReview the joint distribution of a few pairs of columns from the training set.\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.\n\ntrain_dataset %>% \n  select(mpg, cylinders, displacement, weight) %>% \n  GGally::ggpairs()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\nLet’s also check the overall statistics. Note how each feature covers a very different range:\n\nskimr::skim(train_dataset)\n\n\nData summary\n\n\nName\ntrain_dataset\n\n\nNumber of rows\n312\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncylinders\n0\n1\n5.50\n1.72\n3\n4.00\n4.00\n8.0\n8.0\n▇▁▃▁▅\n\n\ndisplacement\n0\n1\n196.99\n105.81\n68\n105.00\n151.00\n302.0\n455.0\n▇▂▂▃▁\n\n\nhorsepower\n0\n1\n104.93\n38.85\n46\n76.75\n92.00\n126.0\n230.0\n▆▇▃▂▁\n\n\nweight\n0\n1\n2995.04\n844.26\n1613\n2232.50\n2849.00\n3622.5\n4997.0\n▇▇▆▅▂\n\n\nacceleration\n0\n1\n15.57\n2.83\n8\n13.88\n15.50\n17.2\n24.8\n▁▆▇▂▁\n\n\nmodel_year\n0\n1\n76.06\n3.71\n70\n73.00\n76.00\n79.0\n82.0\n▇▆▇▆▇\n\n\nmpg\n0\n1\n23.48\n7.91\n9\n17.00\n22.15\n29.0\n46.6\n▆▇▆▃▁\n\n\norigin_USA\n0\n1\n0.62\n0.48\n0\n0.00\n1.00\n1.0\n1.0\n▅▁▁▁▇\n\n\norigin_Europe\n0\n1\n0.17\n0.38\n0\n0.00\n0.00\n0.0\n1.0\n▇▁▁▁▂\n\n\norigin_Japan\n0\n1\n0.21\n0.40\n0\n0.00\n0.00\n0.0\n1.0\n▇▁▁▁▂\n\n\n\n\n\n\n\nSplit features from labels\nSeparate the target value—the “label”—from the features. This label is the value that you will train the model to predict.\n\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)"
  },
  {
    "objectID": "tutorials/keras/regression.html#normalization",
    "href": "tutorials/keras/regression.html#normalization",
    "title": "Regression",
    "section": "Normalization",
    "text": "Normalization\nIn the table of statistics it’s easy to see how different the ranges of each feature are:\n\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>% \n  select(where(~is.numeric(.x))) %>% \n  pivot_longer(\n    cols = everything(), names_to = \"variable\", values_to = \"values\") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(values), sd = sd(values))\n\n# A tibble: 10 × 3\n   variable          mean      sd\n   <chr>            <dbl>   <dbl>\n 1 acceleration    15.6     2.83 \n 2 cylinders        5.5     1.72 \n 3 displacement   197.    106.   \n 4 horsepower     105.     38.9  \n 5 model_year      76.1     3.71 \n 6 mpg             23.5     7.91 \n 7 origin_Europe    0.170   0.376\n 8 origin_Japan     0.205   0.404\n 9 origin_USA       0.625   0.485\n10 weight        2995.    844.   \n\n\nIt is good practice to normalize features that use different scales and ranges.\nOne reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\nAlthough a model might converge without feature normalization, normalization makes training much more stable.\nNote: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial.\n\nThe Normalization layer\nThe layer_normalization() is a clean and simple way to add feature normalization into your model.\nThe first step is to create the layer:\n\nnormalizer <- layer_normalization(axis = -1L)\n\nLoaded Tensorflow version 2.9.1\n\n\nThen, fit the state of the preprocessing layer to the data by calling adapt():\n\nnormalizer %>% adapt(as.matrix(train_features))\n\nCalculate the mean and variance, and store them in the layer:\n\nprint(normalizer$mean)\n\ntf.Tensor(\n[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01\n  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\n\nFirst example: 8 318 150 4077 14 72 1 0 0\n\ncat('Normalized:', as.matrix(normalizer(first)))\n\nNormalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745966 -0.452364 -0.5080005"
  },
  {
    "objectID": "tutorials/keras/regression.html#linear-regression",
    "href": "tutorials/keras/regression.html#linear-regression",
    "title": "Regression",
    "section": "Linear regression",
    "text": "Linear regression\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\nLinear regression with one variable\nBegin with a single-variable linear regression to predict 'mpg' from 'horsepower'.\nTraining a model with Keras typically starts by defining the model architecture. Use a Sequential model, which represents a sequence of steps.\nThere are two steps in your single-variable linear regression model:\n\nNormalize the 'horsepower' input features using the normalization preprocessing layer.\nApply a linear transformation (\\(y = mx+b\\)) to produce 1 output using a linear layer (dense).\n\nThe number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.\nFirst, create a matrix made of the 'horsepower' features. Then, instantiate the layer_normalization and fit its state to the horsepower data:\n\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n\nBuild the Keras Sequential model:\n\nhorsepower_model <- keras_model_sequential() %>% \n  horsepower_normalizer() %>% \n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense (Dense)               (None, 1)                 2         Y          \n============================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n____________________________________________________________________________\n\n\nThis model will predict 'mpg' from 'horsepower'.\nRun the untrained model on the first 10 ‘horsepower’ values. The output won’t be good, but notice that it has the expected shape of (10, 1):\n\npredict(horsepower_model, horsepower[1:10,])\n\n              [,1]\n [1,]  0.918656170\n [2,]  0.918656170\n [3,] -0.182085037\n [4,] -0.222853214\n [5,] -0.610151052\n [6,] -0.161700934\n [7,]  0.001371827\n [8,]  0.103292309\n [9,]  0.103292309\n[10,] -0.447078258\n\n\nOnce the model is built, configure the training procedure using the Keras compile() method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the optimizer_adam).\n\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n\nUse Keras fit() to execute the training for 100 epochs:\n\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n\nVisualize the model’s training progress using the stats stored in the history object:\n\nplot(history)\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results <- list()\ntest_results[[\"horsepower_model\"]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels), \n  verbose = 0\n)\n\nSince this is a single variable regression, it’s easy to view the model’s predictions as a function of the input:\n\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n\n\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \"data\")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \"prediction\"))\n\n\n\n\n\n\nLinear regression with multiple inputs\nYou can use an almost identical setup to make predictions based on multiple inputs. This model still does the same \\(y = mx+b\\) except that \\(m\\) is a matrix and \\(b\\) is a vector.\nCreate a two-step Keras Sequential model again with the first layer being normalizer (layer_normalization(axis = -1)) you defined earlier and adapted to the whole dataset:\n\nlinear_model <- keras_model_sequential() %>% \n  normalizer() %>% \n  layer_dense(units = 1)\n\nWhen you call predict() on a batch of inputs, it produces units = 1 outputs for each example:\n\npredict(linear_model, as.matrix(train_features[1:10, ]))\n\n            [,1]\n [1,] -2.9093571\n [2,] -3.0792081\n [3,]  1.2285657\n [4,]  1.3383932\n [5,]  2.0067687\n [6,]  0.9256226\n [7,] -0.9580150\n [8,] -0.7418302\n [9,] -0.5034288\n[10,]  0.2313302\n\n\nWhen you call the model, its weight matrices will be built—check that the kernel weights (the \\(m\\) in \\(y = mx+b\\)) have a shape of (9, 1):\n\nlinear_model$layers[[2]]$kernel\n\n<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=\narray([[ 0.09463781],\n       [-0.04923761],\n       [-0.73521256],\n       [-0.61125565],\n       [ 0.66817605],\n       [ 0.3159597 ],\n       [-0.6834484 ],\n       [ 0.23534286],\n       [-0.00442344]], dtype=float32)>\n\n\nConfigure the model with Keras compile() and train with fit() for 100 epochs:\n\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n\n\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input:\n\nplot(history)\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results[['linear_model']] <- linear_model %>% \n  evaluate(\n    as.matrix(test_features), \n    as.matrix(test_labels), \n    verbose = 0\n  )"
  },
  {
    "objectID": "tutorials/keras/regression.html#regression-with-a-deep-neural-network-dnn",
    "href": "tutorials/keras/regression.html#regression-with-a-deep-neural-network-dnn",
    "title": "Regression",
    "section": "Regression with a deep neural network (DNN)",
    "text": "Regression with a deep neural network (DNN)\nIn the previous section, you implemented two linear models for single and multiple inputs.\nHere, you will implement single-input and multiple-input DNN models.\nThe code is basically the same except the model is expanded to include some “hidden” non-linear layers. The name “hidden” here just means not directly connected to the inputs or outputs.\nThese models will contain a few more layers than the linear model:\n\nThe normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).\nTwo hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.\nA linear Dense single-output layer.\n\nBoth models will use the same training procedure so the compile method is included in the build_and_compile_model function below.\n\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>% \n    norm() %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n  \n  model\n}\n\n\nRegression using a DNN and a single input\nCreate a DNN model with only 'Horsepower' as input and horsepower_normalizer (defined earlier) as the normalization layer:\n\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n\nThis model has quite a few more trainable parameters than the linear models:\n\nsummary(dnn_horsepower_model)\n\nModel: \"sequential_2\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense_4 (Dense)             (None, 64)                128       Y          \n dense_3 (Dense)             (None, 64)                4160      Y          \n dense_2 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,356\nTrainable params: 4,353\nNon-trainable params: 3\n____________________________________________________________________________\n\n\nTrain the model with Keras Model$fit:\n\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n\nThis model does slightly better than the linear single-input horsepower_model:\n\nplot(history)\n\n\n\n\nIf you plot the predictions as a function of 'horsepower', you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n\n\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \"data\")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \"prediction\"))\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower), \n  as.matrix(test_labels),\n  verbose = 0\n)\n\n\n\nRegression using a DNN and multiple inputs\nRepeat the previous process using all the inputs. The model’s performance slightly improves on the validation dataset.\n\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n\nModel: \"sequential_3\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization (Normalizatio  (None, 9)                19        Y          \n n)                                                                         \n dense_7 (Dense)             (None, 64)                640       Y          \n dense_6 (Dense)             (None, 64)                4160      Y          \n dense_5 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,884\nTrainable params: 4,865\nNon-trainable params: 19\n____________________________________________________________________________\n\n\n\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n\n\nplot(history)\n\n\n\n\nCollect the results on the test set:\n\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)"
  },
  {
    "objectID": "tutorials/keras/regression.html#performance",
    "href": "tutorials/keras/regression.html#performance",
    "title": "Regression",
    "section": "Performance",
    "text": "Performance\nSince all models have been trained, you can review their test set performance:\n\nsapply(test_results, function(x) x)\n\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n\n\nThese results match the validation error observed during training.\n\nMake predictions\nYou can now make predictions with the dnn_model on the test set using Keras predict() and review the loss:\n\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\")\n\n\n\n\nIt appears that the model predicts reasonably well.\nNow, check the error distribution:\n\nqplot(test_predictions - test_labels$mpg, geom = \"density\")\n\n\n\nerror <- test_predictions - test_labels\n\nIf you’re happy with the model, save it for later use with Model$save:\n\nsave_model_tf(dnn_model, 'dnn_model')\n\nIf you reload the model, it gives identical output:\n\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)\n\n\nsapply(test_results, function(x) x)\n\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n            reloaded.loss \n                 1.801497"
  },
  {
    "objectID": "tutorials/keras/regression.html#conclusion",
    "href": "tutorials/keras/regression.html#conclusion",
    "title": "Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n\nMean squared error (MSE) (loss_mean_squared_error()) and mean absolute error (MAE) (loss_mean_absolute_error()) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\nSimilarly, evaluation metrics used for regression differ from classification.\nWhen numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\nOverfitting is a common problem for DNN models, though it wasn’t a problem for this tutorial. Visit the Overfit and underfit tutorial for more help with this."
  },
  {
    "objectID": "tutorials/keras/save_and_load.html",
    "href": "tutorials/keras/save_and_load.html",
    "title": "Save and load",
    "section": "",
    "text": "Model progress can be saved during and after training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\nSharing this data helps others understand how the model works and try it themselves with new data.\nCaution: TensorFlow models are code and it is important to be careful with untrusted code. See Using TensorFlow Securely for details."
  },
  {
    "objectID": "tutorials/keras/save_and_load.html#setup",
    "href": "tutorials/keras/save_and_load.html#setup",
    "title": "Save and load",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)\n\n\nGet an example dataset\nTo demonstrate how to save and load weights, you’ll use the MNIST dataset. To speed up these runs, use the first 1000 examples:\n\nc(c(train_images, train_labels), c(test_images, test_labels)) %<-% dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\ntrain_labels <- train_labels[1:1000]\ntest_labels <- test_labels[1:1000]\n\ntrain_images <- train_images[1:1000,,] %>% array_reshape(dim = c(1000, 784))/255\ntest_images <- test_images[1:1000,,] %>% array_reshape(dim = c(1000, 784))/255\n\n\n\nDefine a model\nStart by building a simple sequential model:\n\n# Define a simple sequential model\ncreate_model <- function() {\n  model <- keras_model_sequential() %>% \n    layer_dense(512, activation = 'relu', input_shape = shape(784)) %>% \n    layer_dropout(0.2) %>% \n    layer_dense(10)\n  \n  model %>% compile(\n    optimizer = 'adam',\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    metrics = list(metric_sparse_categorical_accuracy())\n  )\n  \n  model\n}\n\n# Create a basic model instance\nmodel <- create_model()\n\n# Display the model's architecture\nsummary(model)\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_1 (Dense)                  (None, 512)                   401920      \n dropout (Dropout)                (None, 512)                   0           \n dense (Dense)                    (None, 10)                    5130        \n============================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n____________________________________________________________________________"
  },
  {
    "objectID": "tutorials/keras/save_and_load.html#save-checkpoints-during-training",
    "href": "tutorials/keras/save_and_load.html#save-checkpoints-during-training",
    "title": "Save and load",
    "section": "Save checkpoints during training",
    "text": "Save checkpoints during training\nYou can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The callback_model_checkpoint() callback allows you to continually save the model both during and at the end of training.\n\nCheckpoint callback usage\nCreate a callback_model_checkpoint() callback that saves weights only during training:\n\ncheckpoint_path <- \"training_1/cp.ckpt\"\ncheckpoint_dir <- fs::path_dir(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback <- callback_model_checkpoint(\n  filepath = checkpoint_path,\n  save_weights_only = TRUE,\n  verbose = 1\n)\n\n# Train the model with the new callback\nmodel %>% fit(\n  train_images, \n  train_labels,  \n  epochs = 10,\n  validation_data = list(test_images, test_labels),\n  callbacks = list(cp_callback) # Pass callback to training\n)  \n\n# This may generate warnings related to saving the state of the optimizer.\n# These warnings (and similar warnings throughout this notebook)\n# are in place to discourage outdated usage, and can be ignored.\n\nThis creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch:\n\nfs::dir_tree(checkpoint_dir)\n\ntraining_1\n├── checkpoint\n├── cp.ckpt.data-00000-of-00001\n└── cp.ckpt.index\n\n\nAs long as two models share the same architecture you can share weights between them. So, when restoring a model from weights-only, create a model with the same architecture as the original model and then set its weights.\nNow rebuild a fresh, untrained model and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):\n\n# Create a basic model instance\nmodel <- create_model()\n\n# Evaluate the model\nuntrained_results <- model %>% evaluate(test_images, test_labels, verbose = 2)\ncat(\"Untrained model results: \\n\")\n\nUntrained model results: \n\nuntrained_results\n\n                       loss sparse_categorical_accuracy \n                   2.314683                    0.122000 \n\n\nThen load the weights from the checkpoint and re-evaluate:\n\n# Loads the weights\nload_model_weights_tf(model, checkpoint_path)\n\n# Re-evaluate the model\nrestored_model <- model %>% evaluate(test_images, test_labels, verbose = 2)\ncat(\"Restored model results: \\n\")\n\nRestored model results: \n\nrestored_model\n\n                       loss sparse_categorical_accuracy \n                  0.4228736                   0.8610000 \n\n\n\n\nCheckpoint callback options\nThe callback provides several options to provide unique names for checkpoints and adjust the checkpointing frequency. Train a new model, and save uniquely named checkpoints once every five epochs:\n\n# Include the epoch in the file name \ncheckpoint_path <- \"training_2/cp-list{epoch:04d}.ckpt\"\ncheckpoint_dir <- fs::path_dir(checkpoint_path)\n\nbatch_size <- 32\n\n# Create a callback that saves the model's weights every 5 epochs\ncp_callback <- callback_model_checkpoint(\n  filepath = checkpoint_path,\n  verbose = 1,\n  save_weights_only = TRUE,\n  save_freq = 5*batch_size\n)\n\n# Create a new model instance\nmodel <- create_model()\n\n# Train the model with the new callback\nmodel %>% fit(\n  train_images, \n  train_labels,\n  epochs = 50, \n  batch_size = batch_size, \n  callbacks = list(cp_callback),\n  validation_data = list(test_images, test_labels),\n  verbose = 0\n)\n\nNow, look at the resulting checkpoints and choose the latest one:\n\nfs::dir_tree(checkpoint_dir)\n\ntraining_2\n├── checkpoint\n├── cp-list0005.ckpt.data-00000-of-00001\n├── cp-list0005.ckpt.index\n├── cp-list0010.ckpt.data-00000-of-00001\n├── cp-list0010.ckpt.index\n├── cp-list0015.ckpt.data-00000-of-00001\n├── cp-list0015.ckpt.index\n├── cp-list0020.ckpt.data-00000-of-00001\n├── cp-list0020.ckpt.index\n├── cp-list0025.ckpt.data-00000-of-00001\n├── cp-list0025.ckpt.index\n├── cp-list0030.ckpt.data-00000-of-00001\n├── cp-list0030.ckpt.index\n├── cp-list0035.ckpt.data-00000-of-00001\n├── cp-list0035.ckpt.index\n├── cp-list0040.ckpt.data-00000-of-00001\n├── cp-list0040.ckpt.index\n├── cp-list0045.ckpt.data-00000-of-00001\n├── cp-list0045.ckpt.index\n├── cp-list0050.ckpt.data-00000-of-00001\n└── cp-list0050.ckpt.index\n\n\n\nlatest <- tf$train$latest_checkpoint(checkpoint_dir)\nlatest\n\n[1] \"training_2/cp-list0050.ckpt\"\n\n\nNote: the default TensorFlow format only saves the 5 most recent checkpoints. To test, reset the model and load the latest checkpoint:\n\n# Create a new model instance\nmodel <- create_model()\n\n# Load the previously saved weights\nload_model_weights_tf(model, latest)\n\n# Re-evaluate the model\nmodel %>% evaluate(test_images, test_labels, verbose = 0)\n\n                       loss sparse_categorical_accuracy \n                  0.4657353                   0.8810000"
  },
  {
    "objectID": "tutorials/keras/save_and_load.html#what-are-these-files",
    "href": "tutorials/keras/save_and_load.html#what-are-these-files",
    "title": "Save and load",
    "section": "What are these files?",
    "text": "What are these files?\nThe above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain: * One or more shards that contain your model’s weights. * An index file that indicates which weights are stored in which shard.\nIf you are training a model on a single machine, you’ll have one shard with(the suffix, { }) .data-00000-of-00001"
  },
  {
    "objectID": "tutorials/keras/save_and_load.html#manually-save-weights",
    "href": "tutorials/keras/save_and_load.html#manually-save-weights",
    "title": "Save and load",
    "section": "Manually save weights",
    "text": "Manually save weights\nTo save weights manually, use save_model_weights_tf(). By default, Keras —and the save_model_weights_tf() method in particular—uses the TensorFlow Checkpoint format with a .ckpt extension. To save in the HDF5 format with a .h5 extension, refer to the Save and load models guide.\n\n# Save the weights\nsave_model_weights_tf(model, './checkpoints/my_checkpoint')\n\n# Create a new model instance\nmodel <- create_model()\n\n# Restore the weights\nload_model_weights_tf(model, './checkpoints/my_checkpoint')\n\n# Evaluate the model\nrestored_model <- model %>% evaluate(test_images, test_labels, verbose = 2)\ncat(\"Restored model results: \\n\")\n\nRestored model results: \n\nrestored_model\n\n                       loss sparse_categorical_accuracy \n                  0.4657353                   0.8810000"
  },
  {
    "objectID": "tutorials/keras/save_and_load.html#save-the-entire-model",
    "href": "tutorials/keras/save_and_load.html#save-the-entire-model",
    "title": "Save and load",
    "section": "Save the entire model",
    "text": "Save the entire model\nCall save_model_tf() to save a model’s architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original Python code*. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\nAn entire model can be saved in two different file formats (SavedModel and HDF5). The TensorFlow SavedModel format is the default file format in TF2$x. However, models can be saved in HDF5 format. More details on saving entire models in the two file formats is described below.\nSaving a fully-functional model is very useful—you can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n*Custom objects (e.g. subclassed models or layers) require special attention when saving and loading. See the Saving custom objects section below\n\nSavedModel format\nThe SavedModel format is another way to serialize models. Models saved in this format can be restored using load_model_tf() and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to serve/inspect the SavedModel. The section below illustrates the steps to save and restore the model.\n\n# Create and train a new model instance.\nmodel <- create_model()\nmodel %>% fit(train_images, train_labels, epochs = 5)\n\n# Save the entire model as a SavedModel.\nsave_model_tf(model, \"saved_model/my_model\")\n\nThe SavedModel format is a directory containing a protobuf binary and a TensorFlow checkpoint. Inspect the saved model directory:\n\n# my_model directory\n# Contains an assets folder, saved_model.pb, and variables folder.\nfs::dir_tree(\"saved_model/\")\n\nsaved_model/\n└── my_model\n    ├── assets\n    ├── keras_metadata.pb\n    ├── saved_model.pb\n    └── variables\n        ├── variables.data-00000-of-00001\n        └── variables.index\n\n\nReload a fresh Keras model from the saved model:\n\nnew_model <- load_model_tf('saved_model/my_model')\n\n# Check its architecture\nsummary(new_model)\n\nModel: \"sequential_5\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_11 (Dense)                 (None, 512)                   401920      \n dropout_5 (Dropout)              (None, 512)                   0           \n dense_10 (Dense)                 (None, 10)                    5130        \n============================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nThe restored model is compiled with the same arguments as the original model. Try running evaluate and predict with the loaded model.\n\n# Evaluate the restored model\nrestored_model <- new_model %>% evaluate(test_images, test_labels, verbose = 2)\nrestored_model\n\n                       loss sparse_categorical_accuracy \n                  0.4132914                   0.8640000 \n\ndim(predict(new_model, test_images))\n\n[1] 1000   10\n\n\n\n\nHDF5 format\nKeras provides a basic save format using the HDF5 standard.\n\n# Create and train a new model instance.\nmodel <- create_model()\nmodel %>% fit(train_images, train_labels, epochs = 5)\n\n# Save the entire model to a HDF5 file.\n# The '.h5' extension indicates that the model should be saved to HDF5.\nsave_model_hdf5(model, 'my_model.h5') \n\nNow, recreate the model from that file:\n\n# Recreate the exact same model, including its weights and the optimizer\nnew_model <- load_model_hdf5('my_model.h5')\n\n# Show the model architecture\nsummary(new_model)\n\nModel: \"sequential_6\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_13 (Dense)                 (None, 512)                   401920      \n dropout_6 (Dropout)              (None, 512)                   0           \n dense_12 (Dense)                 (None, 10)                    5130        \n============================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nCheck its accuracy:\n\nnew_model %>% evaluate(test_images, test_labels, verbose = 0)\n\n                       loss sparse_categorical_accuracy \n                  0.4181948                   0.8630000 \n\n\nKeras saves models by inspecting their architectures. This technique saves everything:\n\nThe weight values\nThe model’s architecture\nThe model’s training configuration (what you pass to the compile() method)\nThe optimizer and its state, if any (this enables you to restart training where you left off)\n\nKeras is not able to save the v1$x optimizers (from tf.compat$v1$train) since they aren’t compatible with checkpoints. For v1$x optimizers, you need to re-compile the model after loading—losing the state of the optimizer.\n\n\nSaving custom objects\nIf you are using the SavedModel format, you can skip this section. The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the original code.\nTo save custom objects to HDF5, you must do the following:\n\nDefine a get_config method in your object, and optionally a from_config classmethod.\n\n\nget_config(self) returns a JSON-serializable dictionary of parameters needed to recreate the object.\nfrom_config(cls, config) uses the returned config from get_config to create a new object. By default, this function will use the config as initialization kwargs (return do.call(cls, config)).\n\n\nPass the object to the custom_objects argument when loading the model. The argument must be a dictionary mapping the string class name to the Python class. Eg. load_model_tf(path, custom_objects = list('CustomLayer'= CustomLayer))\n\nSee the Writing layers and models from scratch tutorial for examples of custom objects and get_config."
  },
  {
    "objectID": "tutorials/keras/text_classification.html",
    "href": "tutorials/keras/text_classification.html",
    "title": "Basic text classification",
    "section": "",
    "text": "This tutorial demonstrates text classification starting from plain text files stored on disk. You’ll train a binary classifier to perform sentiment analysis on an IMDB dataset. At the end of the notebook, there is an exercise for you to try, in which you’ll train a multi-class classifier to predict the tag for a programming question on Stack Overflow."
  },
  {
    "objectID": "tutorials/keras/text_classification.html#sentiment-analysis",
    "href": "tutorials/keras/text_classification.html#sentiment-analysis",
    "title": "Basic text classification",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nThis notebook trains a sentiment analysis model to classify movie reviews as positive or negative, based on the text of the review. This is an example of binary—or two-class—classification, an important and widely applicable kind of machine learning problem.\nYou’ll use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n\nDownload and explore the IMDB dataset\nLet’s download and extract the dataset, then explore the directory structure.\n\nurl <- \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n\ndataset <- get_file(\n  \"aclImdb_v1\", \n  url,\n  untar = TRUE, \n  cache_dir = '.',\n  cache_subdir = ''\n)\n\nLoaded Tensorflow version 2.9.1\n\ndataset_dir <- file.path(\"aclImdb\")\n\n\nlist.files(dataset_dir)\n\n[1] \"imdb.vocab\" \"imdbEr.txt\" \"README\"     \"test\"       \"train\"     \n\n\n\ntrain_dir <- file.path(dataset_dir, 'train')\nlist.files(train_dir)\n\n[1] \"labeledBow.feat\" \"neg\"             \"pos\"             \"unsup\"          \n[5] \"unsupBow.feat\"   \"urls_neg.txt\"    \"urls_pos.txt\"    \"urls_unsup.txt\" \n\n\nThe aclImdb/train/pos and aclImdb/train/neg directories contain many text files, each of which is a single movie review. Let’s take a look at one of them.\n\nsample_file <- file.path(train_dir, 'pos/1181_9.txt')\nreadr::read_file(sample_file)\n\n[1] \"Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\"\n\n\n\n\nLoad the dataset\nNext, you will load the data off disk and prepare it into a format suitable for training. To do so, you will use the helpful text_dataset_from_directory utility, which expects a directory structure as follows.\nmain_directory/\n...class_a/\n......a_text_1.txt\n......a_text_2.txt\n...class_b/\n......b_text_1.txt\n......b_text_2.txt\nTo prepare a dataset for binary classification, you will need two folders on disk, corresponding to class_a and class_b. These will be the positive and negative movie reviews, which can be found in aclImdb/train/pos and aclImdb/train/neg. As the IMDB dataset contains additional folders, you will remove them before using this utility.\n\nremove_dir <- file.path(train_dir, 'unsup')\nunlink(remove_dir, recursive = TRUE)\n\nNext, you will use the text_dataset_from_directory utility to create a labeled TensorFlow Dataset. tfdatasets is a powerful collection of tools for working with data.\nWhen running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test.\nThe IMDB dataset has already been divided into train and test, but it lacks a validation set. Let’s create a validation set using an 80:20 split of the training data by using the validation_split argument below.\n\nbatch_size <- 32\nseed <- 42\n\nraw_train_ds <- text_dataset_from_directory(\n  'aclImdb/train', \n  batch_size = batch_size, \n  validation_split = 0.2, \n  subset = 'training', \n  seed = seed\n)\n\nAs you can see above, there are 25,000 examples in the training folder, of which you will use 80% (or 20,000) for training. As you will see in a moment, you can train a model by passing a dataset directly to fit(). If you’re new to tfdatasets, you can also iterate over the dataset and print out a few examples as follows.\n\nbatch <- raw_train_ds %>% \n  reticulate::as_iterator() %>% \n  coro::collect(n = 1)\n\nbatch[[1]][[1]][1]\n\ntf.Tensor(b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)', shape=(), dtype=string)\n\nbatch[[1]][[2]][1]\n\ntf.Tensor(0, shape=(), dtype=int32)\n\n\nNotice the reviews contain raw text (with punctuation and occasional HTML tags like <br/>). You will show how to handle these in the following section.\nThe labels are 0 or 1. To see which of these correspond to positive and negative movie reviews, you can check the class_names property on the dataset.\n\ncat(\"Label 0 corresponds to\", raw_train_ds$class_names[1])\n\nLabel 0 corresponds to neg\n\ncat(\"Label 1 corresponds to\", raw_train_ds$class_names[2])\n\nLabel 1 corresponds to pos\n\n\nNext, you will create a validation and test dataset. You will use the remaining 5,000 reviews from the training set for validation.\nNote: When using the validation_split and subset arguments, make sure to either specify a random seed, or to pass shuffle = FALSE, so that the validation and training splits have no overlap.\n\nraw_val_ds <- text_dataset_from_directory(\n  'aclImdb/train', \n  batch_size = batch_size, \n  validation_split = 0.2, \n  subset = 'validation', \n  seed = seed\n)\n\n\nraw_test_ds <- text_dataset_from_directory(\n  'aclImdb/test', \n  batch_size = batch_size\n)\n\n\n\nPrepare the dataset for training\nNext, you will standardize, tokenize, and vectorize the data using the helpful layer_text_vectorization() layer.\nStandardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset. Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words, by splitting on whitespace). Vectorization refers to converting tokens into numbers so they can be fed into a neural network. All of these tasks can be accomplished with this layer.\nAs you saw above, the reviews contain various HTML tags like <br />. These tags will not be removed by the default standardizer in the TextVectorization layer (which converts text to lowercase and strips punctuation by default, but doesn’t strip HTML). You will write a custom standardization function to remove the HTML.\nNote: To prevent training-testing skew (also known as training-serving skew), it is important to preprocess the data identically at train and test time. To facilitate this, the text_vectorization layer can be included directly inside your model, as shown later in this tutorial.\n\n# creating a regex with all punctuation characters for replacing.\nre <- reticulate::import(\"re\")\n\npunctuation <- c(\"!\", \"\\\\\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \n\"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \n\"\\\\\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\")\n\npunctuation_group <- punctuation %>% \n  sapply(re$escape) %>% \n  paste0(collapse = \"\") %>% \n  sprintf(\"[%s]\", .)\n\ncustom_standardization <- function(input_data) {\n  lowercase <- tf$strings$lower(input_data)\n  stripped_html <- tf$strings$regex_replace(lowercase, '<br />', ' ')\n  tf$strings$regex_replace(\n    stripped_html,\n    punctuation_group,\n    \"\"\n  )\n}\n\nNext, you will create a TextVectorization layer. You will use this layer to standardize, tokenize, and vectorize our data. You set the output_mode to int to create unique integer indices for each token.\nNote that you’re using the default split function, and the custom standardization function you defined above. You’ll also define some constants for the model, like an explicit maximum sequence_length, which will cause the layer to pad or truncate sequences to exactly sequence_length values.\n\nmax_features <- 10000\nsequence_length <- 250\n\nvectorize_layer <- layer_text_vectorization(\n  standardize = custom_standardization,\n  max_tokens = max_features,\n  output_mode = \"int\",\n  output_sequence_length = sequence_length\n)\n\nNext, you will call adapt() to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.\nNote: It’s important to only use your training data when calling adapt (using the test set would leak information).\n\n# Make a text-only dataset (without labels), then call adapt\ntrain_text <- raw_train_ds %>% \n  dataset_map(function(text, label) text)\nvectorize_layer %>% adapt(train_text)\n\nLet’s create a function to see the result of using this layer to preprocess some data.\n\nvectorize_text <- function(text, label) {\n  text <- tf$expand_dims(text, -1L)\n  list(vectorize_layer(text), label)\n}\n\n\n# retrieve a batch (of 32 reviews and labels) from the dataset\nbatch <- reticulate::as_iterator(raw_train_ds) %>% \n  reticulate::iter_next()\nfirst_review <- as.array(batch[[1]][1])\nfirst_label <- as.array(batch[[2]][1])\ncat(\"Review:\\n\", first_review)\n\nReview:\n Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.\n\ncat(\"Label: \", raw_train_ds$class_names[first_label+1])\n\nLabel:  neg\n\ncat(\"Vectorized review: \\n\")\n\nVectorized review: \n\nprint(vectorize_text(first_review, first_label))\n\n[[1]]\ntf.Tensor(\n[[  86   17  260    2  222    1  571   31  229   11 2418    1   51   22\n    25  404  251   12  306  282    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n\n[[2]]\n[1] 0\n\n\nAs you can see above, each token has been replaced by an integer. You can lookup the token (string) that each integer corresponds to by calling .get_vocabulary() on the layer.\n\ncat(\"9257 ---> \",get_vocabulary(vectorize_layer)[9257 + 1]) # because R is onde indexed\n\n9257 --->  recipe\n\ncat(\" 15 ---> \",get_vocabulary(vectorize_layer)[15 + 1])\n\n 15 --->  for\n\ncat(\"Vocabulary size: \" , length(get_vocabulary(vectorize_layer)))\n\nVocabulary size:  10000\n\n\nYou are nearly ready to train your model. As a final preprocessing step, you will apply the text_vectorization layer you created earlier to the train, validation, and test dataset.\n\ntrain_ds <- raw_train_ds %>% dataset_map(vectorize_text)\nval_ds <- raw_val_ds %>% dataset_map(vectorize_text)\ntest_ds <- raw_test_ds %>% dataset_map(vectorize_text)\n\n\n\nConfigure the dataset for performance\nThese are two important methods you should use when loading data to make sure that I/O does not become blocking.\ndataset_cache() keeps data in memory after it’s loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\ndataset_prefetch() overlaps data preprocessing and model execution while training.\nYou can learn more about both methods, as well as how to cache data to disk in the data performance guide.\n\nAUTOTUNE <- tf$data$AUTOTUNE\n\ntrain_ds <- train_ds %>% \n  dataset_cache() %>% \n  dataset_prefetch(buffer_size = AUTOTUNE)\nval_ds <- val_ds %>% \n  dataset_cache() %>% \n  dataset_prefetch(buffer_size = AUTOTUNE)\ntest_ds <- test_ds %>% \n  dataset_cache() %>% \n  dataset_prefetch(buffer_size = AUTOTUNE)\n\n\n\nCreate the model\nIt’s time to create your neural network:\n\nembedding_dim <- 16\n\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features + 1, embedding_dim) %>% \n  layer_dropout(0.2) %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dropout(0.2) %>% \n  layer_dense(1)\n\nsummary(model)\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 16)              160016      \n dropout_1 (Dropout)              (None, None, 16)              0           \n global_average_pooling1d (Global  (None, 16)                   0           \n AveragePooling1D)                                                          \n dropout (Dropout)                (None, 16)                    0           \n dense (Dense)                    (None, 1)                     17          \n============================================================================\nTotal params: 160,033\nTrainable params: 160,033\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nThe layers are stacked sequentially to build the classifier:\n\nThe first layer is an embedding layer. This layer takes the integer-encoded reviews and looks up an embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding). To learn more about embeddings, check out the Word embeddings tutorial.\nNext, a global_average_pooling_1d layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\nThis fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\nThe last layer is densely connected with a single output node.\n\n\n\nLoss function and optimizer\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), you’ll use losses$BinaryCrossentropy loss function.\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  optimizer = 'adam',\n  metrics = metric_binary_accuracy(threshold = 0)\n)\n\n\n\nTrain the model\nYou will train the model by passing the dataset object to the fit method.\n\nepochs <- 10\nhistory <- model %>% \n  fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = epochs\n  )\n\n\n\nEvaluate the model\nLet’s see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nmodel %>% evaluate(test_ds)\n\n           loss binary_accuracy \n      0.3103359       0.8733600 \n\n\nThis fairly naive approach achieves an accuracy of about 86%.\n\n\nCreate a plot of accuracy and loss over time\nmodel %>% fit() returns a History object that contains everything that happened during training. You can use as.data.frame(history) to obtain a data.frame with metrics per epoch or plot(history) as below:\n\nplot(history)\n\n\n\n\nNotice the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\nThis isn’t the case for the validation loss and accuracy—they seem to peak before the training accuracy. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations specific to the training data that do not generalize to test data.\nFor this particular case, you could prevent overfitting by simply stopping the training when the validation accuracy is no longer increasing. One way to do so is to use the callback_early_stopping() callback."
  },
  {
    "objectID": "tutorials/keras/text_classification.html#export-the-model",
    "href": "tutorials/keras/text_classification.html#export-the-model",
    "title": "Basic text classification",
    "section": "Export the model",
    "text": "Export the model\nIn the code above, you applied the text_vectorization layer to the dataset before feeding text to the model. If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the text_vectorization layer inside your model. To do so, you can create a new model using the weights you just trained.\n\nexport_model <- keras_model_sequential() %>% \n  vectorize_layer() %>% \n  model() %>% \n  layer_activation(activation = \"sigmoid\")\n  \nexport_model %>% compile(\n  loss = loss_binary_crossentropy(from_logits = FALSE), \n  optimizer = \"adam\", \n  metrics = 'accuracy'\n)\n\n# Test it with `raw_test_ds`, which yields raw strings\nexport_model %>% evaluate(raw_test_ds)\n\n     loss  accuracy \n0.3103358 0.8733600 \n\n\n\nInference on new data\nTo get predictions for new examples, you can simply call predict().\n\nexamples <- c(\n  \"The movie was great!\",\n  \"The movie was okay.\",\n  \"The movie was terrible...\"\n)\n\npredict(export_model, examples)\n\n          [,1]\n[1,] 0.6125446\n[2,] 0.4341286\n[3,] 0.3514057\n\n\nIncluding the text preprocessing logic inside your model enables you to export a model for production that simplifies deployment, and reduces the potential for train/test skew.\nThere is a performance difference to keep in mind when choosing where to apply your TextVectorization layer. Using it outside of your model enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So, if you’re training your model on the GPU, you probably want to go with this option to get the best performance while developing your model, then switch to including the TextVectorization layer inside your model when you’re ready to prepare for deployment.\nVisit this tutorial to learn more about saving models."
  },
  {
    "objectID": "tutorials/keras/text_classification.html#exercise-multi-class-classification-on-stack-overflow-questions",
    "href": "tutorials/keras/text_classification.html#exercise-multi-class-classification-on-stack-overflow-questions",
    "title": "Basic text classification",
    "section": "Exercise: multi-class classification on Stack Overflow questions",
    "text": "Exercise: multi-class classification on Stack Overflow questions\nThis tutorial showed how to train a binary classifier from scratch on the IMDB dataset. As an exercise, you can modify this notebook to train a multi-class classifier to predict the tag of a programming question on Stack Overflow.\nA dataset has been prepared for you to use containing the body of several thousand programming questions (for example, “How can I sort a dictionary by value in Python?”) posted to Stack Overflow. Each of these is labeled with exactly one tag (either Python, CSharp, JavaScript, or Java). Your task is to take a question as input, and predict the appropriate tag, in this case, Python.\nThe dataset you will work with contains several thousand questions extracted from the much larger public Stack Overflow dataset on BigQuery, which contains more than 17 million posts.\nAfter downloading the dataset, you will find it has a similar directory structure to the IMDB dataset you worked with previously.\ntrain/\n...python/\n......0$txt\n......1$txt\n...javascript/\n......0$txt\n......1$txt\n...csharp/\n......0$txt\n......1$txt\n...java/\n......0$txt\n......1$txt\nNote: To increase the difficulty of the classification problem, occurrences of the words Python, CSharp, JavaScript, or Java in the programming questions have been replaced with the word blank (as many questions contain the language they’re about).\nTo complete this exercise, you should modify this notebook to work with(the Stack Overflow dataset by making the following modifications, { })\n\nAt the top of your notebook, update the code that downloads the IMDB dataset with code to download the Stack Overflow dataset that has already been prepared. As the Stack Overflow dataset has a similar directory structure, you will not need to make many modifications.\nModify the last layer of your model to Dense(4), as there are now four output classes.\nWhen compiling the model, change the loss to loss_sparse_categorical_crossentropy(). This is the correct loss function to use for a multi-class classification problem, when the labels for each class are integers (in this case, they can be 0, 1, 2, or 3). In addition, change the metrics to metrics = 'accuracy', since this is a multi-class classification problem (metric_binary_accuracy() is only used for binary classifiers).\nWhen plotting accuracy over time, change binary_accuracy and val_binary_accuracy to accuracy and val_accuracy, respectively.\nOnce these changes are complete, you will be able to train a multi-class classifier."
  },
  {
    "objectID": "tutorials/keras/text_classification.html#learning-more",
    "href": "tutorials/keras/text_classification.html#learning-more",
    "title": "Basic text classification",
    "section": "Learning more",
    "text": "Learning more\nThis tutorial introduced text classification from scratch. To learn more about the text classification workflow in general, check out the Text classification guide from Google Developers."
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html",
    "href": "tutorials/keras/text_classification_with_hub.html",
    "title": "Text Classification with Hub",
    "section": "",
    "text": "This notebook classifies movie reviews as positive or negative using the text of the review. This is an example of binary—or two-class—classification, an important and widely applicable kind of machine learning problem.\nThe tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\nIt uses the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\nThis notebook uses keras, a high-level API to build and train models in TensorFlow, and TensorFlow hub, a library for loading trained models from TFHub in a single line of code. For a more advanced text classification tutorial using Keras, see the MLCC Text Classification Guide."
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#download-the-imdb-dataset",
    "href": "tutorials/keras/text_classification_with_hub.html#download-the-imdb-dataset",
    "title": "Text Classification with Hub",
    "section": "Download the IMDB dataset",
    "text": "Download the IMDB dataset\nThe IMDB dataset is available on imdb reviews or on TensorFlow datasets. The following code downloads the IMDB dataset to your machine:\n\nif (dir.exists(\"aclImdb/\"))\n  unlink(\"aclImdb/\", recursive = TRUE)\nurl <- \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\ndataset <- get_file(\n  \"aclImdb_v1\", \n  url,\n  untar = TRUE, \n  cache_dir = '.',\n  cache_subdir = ''\n)\n\nLoaded Tensorflow version 2.9.1\n\nunlink(\"aclImdb/train/unsup/\", recursive = TRUE)\n\nWe can then create a TensorFlow dataset from the directory structure using the text_dataset_from_directory() function:\n\nbatch_size <- 512\nseed <- 42\n\ntrain_data <- text_dataset_from_directory(\n  'aclImdb/train', \n  batch_size = batch_size, \n  validation_split = 0.2, \n  subset = 'training', \n  seed = seed\n)\nvalidation_data <- text_dataset_from_directory(\n  'aclImdb/train', \n  batch_size = batch_size, \n  validation_split = 0.2, \n  subset = 'validation', \n  seed = seed\n)\ntest_data <- text_dataset_from_directory(\n  'aclImdb/test', \n  batch_size = batch_size\n)"
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#explore-the-data",
    "href": "tutorials/keras/text_classification_with_hub.html#explore-the-data",
    "title": "Text Classification with Hub",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\nLet’s print first 10 examples.\n\nbatch <- train_data %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\n\nbatch[[1]][1]\n\ntf.Tensor(b'Upon seeing this film once again it appeared infinitely superior to me this time than the previous times I have viewed it. The acting is stunningly wonderful. The characters are very clearly drawn. Brad Pitt is simply superb as the errant son who rebels. The other actors and actresses are equally fine in every respect. Robert Redford creates a wonderful period piece from the days of speakeasies of the 1920s. The scenery is incredibly beautiful of the mountains and streams of western Montana. All in all, this is one of the finest films made in the 1990s.<br /><br />You must see this movie!<br /><br />', shape=(), dtype=string)\n\n\nLet’s also print the first 10 labels.\n\nbatch[[2]][1:10]\n\ntf.Tensor([1 0 1 0 0 0 1 1 0 0], shape=(10), dtype=int32)"
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#build-the-model",
    "href": "tutorials/keras/text_classification_with_hub.html#build-the-model",
    "title": "Text Classification with Hub",
    "section": "Build the model",
    "text": "Build the model\nThe neural network is created by stacking layers—this requires three main architectural decisions:\n\nHow to represent the text?\nHow many layers to use in the model?\nHow many hidden units to use for each layer?\n\nIn this example, the input data consists of sentences. The labels to predict are either 0 or 1.\nOne way to represent the text is to convert sentences into embeddings vectors. Use a pre-trained text embedding as the first layer, which will have three advantages:\n\nYou don’t have to worry about text preprocessing,\nBenefit from transfer learning,\nthe embedding has a fixed size, so it’s simpler to process.\n\nFor this example you use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2.\nThere are many other pre-trained text embeddings from TFHub that can be used in this tutorial:\n\ngoogle/nnlm-en-dim128/2 - trained with the same NNLM architecture on the same data as google/nnlm-en-dim50/2, but with a larger embedding dimension. Larger dimensional embeddings can improve on your task but it may take longer to train your model.\ngoogle/nnlm-en-dim128-with-normalization/2 - the same as google/nnlm-en-dim128/2, but with additional text normalization such as removing punctuation. This can help if the text in your task contains additional characters or punctuation.\ngoogle/universal-sentence-encoder/4 - a much larger model yielding 512 dimensional embeddings trained with a deep averaging network (DAN) encoder.\n\nAnd many more! Find more text embedding models on TFHub.\nLet’s first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension).\n\nembedding <- \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer <- tfhub::layer_hub(handle = embedding, trainable = TRUE)\nhub_layer(batch[[1]][1:2])\n\ntf.Tensor(\n[[ 0.2563908   0.3877134   0.11458009  0.46377143 -0.27114576 -0.23548658\n  -0.05462555  0.05912564 -0.5467191   0.31118706 -0.16002828 -0.0705342\n  -0.2470538   0.09001822 -0.04209791 -0.33874798 -0.24183154 -0.32309988\n   0.10837323 -0.63822275  0.07474954 -0.47535443  0.40693292  0.31290898\n  -0.15077832  0.16694833 -0.6367394   0.18927394  0.4457423  -0.24568915\n  -0.46415132  0.2513454   0.14228597 -0.44085875 -0.2652811   0.0990484\n   0.18815233 -0.05307329  0.26779363 -0.6057923  -0.27559572  0.05044953\n  -0.48596263  0.21479745 -0.1746156  -0.6422215  -0.3165063  -0.33656728\n  -0.09484117 -0.07192937]\n [ 1.2899797   0.32863247 -0.00310844  0.82322246 -0.4098285  -0.5109544\n   0.08370163 -0.13269287 -1.170054    0.5531664  -0.05031176  0.14853315\n  -0.15995876  0.26997143 -0.34048218 -0.4932976  -0.25939593  0.03390278\n   0.25013083 -1.4177161   0.19143653 -0.23920083  1.2250862   0.41607666\n  -0.6656541   0.4240742  -1.280362    0.4722922   0.5342671  -0.84027433\n  -0.7578844   0.4437543   0.57404625 -0.51910627 -0.673646    0.62554073\n   0.5437521   0.22559977  0.17738008 -1.0557282   0.03807905  0.44274873\n  -0.45797473  0.17220233 -0.20477422 -0.30913746 -0.79076797 -0.72301215\n   0.00783109 -0.0088165 ]], shape=(2, 50), dtype=float32)\n\n\nLet’s now build the full model:\n\nmodel <- keras_model_sequential() %>% \n  hub_layer() %>% \n  layer_dense(16, activation = 'relu') %>% \n  layer_dense(1)\n\nsummary(model)\n\nModel: <no summary available, model was not built>\n\n\nThe layers are stacked sequentially to build the classifier:\n\nThe first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that you are using (google/nnlm-en-dim50/2) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension). For this NNLM model, the embedding_dimension is 50.\nThis fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\nThe last layer is densely connected with a single output node.\n\nLet’s compile the model.\n\nLoss function and optimizer\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), you’ll use the binary_crossentropy loss function.\nThis isn’t the only choice for a loss function, you could, for instance, choose mean_squared_error. But, generally, binary_crossentropy is better for dealing with probabilities—it measures the “distance” between probability distributions, or in our case, between the ground-truth distribution and the predictions.\nLater, when you are exploring regression problems (say, to predict the price of a house), you’ll see how to use another loss function called mean squared error.\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = 'accuracy'\n)"
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#train-the-model",
    "href": "tutorials/keras/text_classification_with_hub.html#train-the-model",
    "title": "Text Classification with Hub",
    "section": "Train the model",
    "text": "Train the model\nTrain the model for 10 epochs in mini-batches of 512 samples. This is 10 iterations over all samples in the x_train and y_train tensors. While training, monitor the model’s loss and accuracy on the 10,000 samples from the validation set:\n\nhistory <- model %>% fit(\n  train_data,\n  epochs = 10,\n  validation_data = validation_data,\n  verbose <- 1\n)"
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#evaluate-the-model",
    "href": "tutorials/keras/text_classification_with_hub.html#evaluate-the-model",
    "title": "Text Classification with Hub",
    "section": "Evaluate the model",
    "text": "Evaluate the model\nAnd let’s see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nresults <- model %>% evaluate(test_data, verbose = 2)\nresults\n\n     loss  accuracy \n0.3699927 0.8572000 \n\n\nThis fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
  },
  {
    "objectID": "tutorials/keras/text_classification_with_hub.html#further-reading",
    "href": "tutorials/keras/text_classification_with_hub.html#further-reading",
    "title": "Text Classification with Hub",
    "section": "Further reading",
    "text": "Further reading\n\nFor a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, see the Text classification with preprocessed text tutorial.\nTry out more text-related tutorials using trained models from TFHub."
  },
  {
    "objectID": "tutorials/quickstart/advanced.html",
    "href": "tutorials/quickstart/advanced.html",
    "title": "Advanced",
    "section": "",
    "text": "TensorFlow 2 quickstart for experts\nImport TensorFlow into your program. If you haven’t installed TensorFlow yet, go to the installation guide.\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\nLoad and prepare the MNIST dataset.\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nx_train[] <- x_train / 255\nx_test[] <-  x_test / 255 \n\nUse TensorFlow datasets to batch and shuffle the dataset:\n\nexpand_and_cast <- function(x, y) {\n  list(\n    tf$cast(tf$expand_dims(x, axis = 2L), tf$float32),\n    y\n  )\n}\n\n\ntrain_ds <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(10000) %>% \n  dataset_batch(32)\n\ntest_ds <- tensor_slices_dataset(list(x_test, y_test)) %>% \n  dataset_batch(32)\n\nBuild the a model using the Keras model subclassing API:\n\nmy_model <- new_model_class(\n  \"my_model\",\n  initialize = function() {\n    super()$`__init__`()\n    self$conv1 <- layer_conv_2d(filters = 32, kernel_size = 3, activation = 'relu')\n    self$flatten <- layer_flatten()\n    self$d1 <- layer_dense(units = 128, activation = 'relu')\n    self$d2 <- layer_dense(units = 10)\n  },\n  call = function(inputs) {\n    inputs %>% \n      tf$expand_dims(3L) %>% \n      self$conv1() %>% \n      self$flatten() %>% \n      self$d1() %>% \n      self$d2()\n  }\n)\n\n# Create an instance of the model\nmodel <- my_model()\n\nChoose an optimizer and loss function for training:\n\nloss_object <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\nSelect metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n\ntrain_loss <- metric_mean(name = \"train_loss\")\ntrain_accuracy <- metric_sparse_categorical_accuracy(name = \"train_accuracy\")\n\ntest_loss <- metric_mean(name = \"test_loss\")\ntest_accuracy <- metric_sparse_categorical_accuracy(name = \"test_accuracy\")\n\nUse tf$GradientTape to train the model:\n\ntrain_step <- function(images, labels) {\n  with(tf$GradientTape() %as% tape, {\n    # training = TRUE is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions <- model(images, training = TRUE)\n    loss <- loss_object(labels, predictions)\n  })\n  gradients <- tape$gradient(loss, model$trainable_variables)\n  optimizer$apply_gradients(zip_lists(gradients, model$trainable_variables))\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n  1\n}\n\ntrain <- tfautograph::autograph(function(train_ds) {\n  for (batch in train_ds) {\n    train_step(batch[[1]], batch[[2]])\n  }\n})\n\nTest the model:\n\ntest_step <- function(images, labels) {\n  # training = FALSE is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions <- model(images, training = FALSE)\n  t_loss <- loss_object(labels, predictions)\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n}\ntest <- tfautograph::autograph(function(test_ds) {\n  for (batch in test_ds) {\n    test_step(batch[[1]], batch[[2]])\n  }\n})\n\n\nEPOCHS <- 1\nfor (epoch in seq_len(EPOCHS)) {\n  # Reset the metrics at the start of the next epoch\n  train_loss$reset_states()\n  train_accuracy$reset_states()\n  test_loss$reset_states()\n  test_accuracy$reset_states()\n  train(train_ds)\n  test(test_ds)\n  cat(sprintf('Epoch %d', epoch), \"\\n\")\n  cat(sprintf('Loss: %f', as.numeric(train_loss$result())), \"\\n\")\n  cat(sprintf('Accuracy: %f', train_accuracy$result() * 100), \"\\n\")\n  cat(sprintf('Test Loss: %f', test_loss$result()), \"\\n\")\n  cat(sprintf('Test Accuracy: %f', test_accuracy$result() * 100), \"\\n\")\n}\n\nEpoch 1 \nLoss: 0.140223 \nAccuracy: 95.784996 \nTest Loss: 0.069381 \nTest Accuracy: 97.879997 \n\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the TensorFlow tutorials."
  },
  {
    "objectID": "tutorials/quickstart/beginner.html",
    "href": "tutorials/quickstart/beginner.html",
    "title": "Beginner",
    "section": "",
    "text": "##### Copyright 2019 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "tutorials/quickstart/beginner.html#set-up-tensorflow",
    "href": "tutorials/quickstart/beginner.html#set-up-tensorflow",
    "title": "Beginner",
    "section": "Set up TensorFlow",
    "text": "Set up TensorFlow\nImport TensorFlow into your program to get started:\n\nlibrary(tensorflow)\nlibrary(keras)\n\nSee the installation guide to learn how to correctly install TensorFlow for R."
  },
  {
    "objectID": "tutorials/quickstart/beginner.html#load-a-dataset",
    "href": "tutorials/quickstart/beginner.html#load-a-dataset",
    "title": "Beginner",
    "section": "Load a dataset",
    "text": "Load a dataset\nLoad and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\nx_train[] <- x_train / 255\nx_test[] <-  x_test / 255"
  },
  {
    "objectID": "tutorials/quickstart/beginner.html#build-a-machine-learning-model",
    "href": "tutorials/quickstart/beginner.html#build-a-machine-learning-model",
    "title": "Beginner",
    "section": "Build a machine learning model",
    "text": "Build a machine learning model\nBuild a sequential model by stacking layers.\n\nmodel <- keras_model_sequential(input_shape = c(28, 28)) %>% \n  layer_flatten() %>% \n  layer_dense(128, activation = \"relu\") %>% \n  layer_dropout(0.2) %>% \n  layer_dense(10)\n\nFor each example, the model returns a vector of logits or log-odds scores, one for each class.\n\npredictions <- predict(model, x_train[1:2,,])\npredictions\n\nThe tf$nn$softmax function converts these logits to probabilities for each class:\n\ntf$nn$softmax(predictions)\n\nNote: It is possible to bake the tf$nn$softmax function into the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it’s impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.\nDefine a loss function for training using loss_sparse_categorical_crossentropy(), which takes a vector of logits and a TRUE index and returns a scalar loss for each example.\n\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\nThis loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class. This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf$math$log(1/10) ~= 2.3.\n\nloss_fn(y_train[1:2], predictions)\n\nBefore you start training, configure and compile the model using Keras compile(). Set the optimizer class to adam, set the loss to the loss_fn function you defined earlier, and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy.\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)"
  },
  {
    "objectID": "tutorials/quickstart/beginner.html#train-and-evaluate-your-model",
    "href": "tutorials/quickstart/beginner.html#train-and-evaluate-your-model",
    "title": "Beginner",
    "section": "Train and evaluate your model",
    "text": "Train and evaluate your model\nUse the fit() method to adjust your model parameters and minimize the loss:\n\nmodel %>% fit(x_train, y_train, epochs = 5)\n\nThe evaluate() method checks the models performance, usually on a Validation-set or Test-set.\n\nmodel %>% evaluate(x_test,  y_test, verbose = 2)\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the TensorFlow tutorials.\nIf you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n\nprobability_model <- keras_model_sequential() %>% \n  model() %>% \n  layer_activation_softmax()\n\n\nprobability_model(x_test[1:5,,])"
  },
  {
    "objectID": "tutorials/quickstart/beginner.html#conclusion",
    "href": "tutorials/quickstart/beginner.html#conclusion",
    "title": "Beginner",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the Keras API.\nFor more examples of using Keras, check out the tutorials. To learn more about building models with Keras, read the guides. If you want learn more about loading and preparing data, see the tutorials on image data loading or CSV data loading."
  }
]