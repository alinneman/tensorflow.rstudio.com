[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "Francois Chollet, Tomasz Kalinowski\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScott Zhu, Francois Chollet, Tomasz Kalinowski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of how to leverage preprocessing layers to create end-to-end models.\n\n\n\n\n\n\nFrancois Chollet, Mark Omernick, Tomasz Kalinowski\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRick Chao, Francois Chollet, Tomasz Kalinowski\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tensorflow/guide/basics.html#tensors",
    "href": "tensorflow/guide/basics.html#tensors",
    "title": "Tensorflow Basics",
    "section": "Tensors",
    "text": "Tensors\nTensorFlow operates on multidimensional arrays or tensors represented as tensorflow.tensor objects. Here is a two-dimensional tensor:\n\nlibrary(tensorflow)\n\nError in reticulate::use_virtualenv(\"r-tensorflow-site\", required = TRUE) : \n  Directory ~/.virtualenvs/r-tensorflow-site is not a Python virtualenv\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n\nLoaded Tensorflow version 2.9.0\n\nx\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)\n\nx$shape\n\nTensorShape([2, 3])\n\nx$dtype\n\ntf.float32\n\n\nThe most important attributes of a tensor are its shape and dtype:\n\ntensor$shape: tells you the size of the tensor along each of its axes.\ntensor$dtype: tells you the type of all the elements in the tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as well as many operations specialized for machine learning.\nFor example:\n\nx + x\n\ntf.Tensor(\n[[ 2.  4.  6.]\n [ 8. 10. 12.]], shape=(2, 3), dtype=float32)\n\n\n\n5 * x\n\ntf.Tensor(\n[[ 5. 10. 15.]\n [20. 25. 30.]], shape=(2, 3), dtype=float32)\n\n\n\ntf$matmul(x, t(x)) \n\ntf.Tensor(\n[[14. 32.]\n [32. 77.]], shape=(2, 2), dtype=float32)\n\n\n\ntf$concat(list(x, x, x), axis = 0L)\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]], shape=(6, 3), dtype=float32)\n\n\n\ntf$nn$softmax(x, axis = -1L)\n\ntf.Tensor(\n[[0.09003057 0.24472848 0.6652409 ]\n [0.09003057 0.24472848 0.6652409 ]], shape=(2, 3), dtype=float32)\n\n\n\nsum(x) # same as tf$reduce_sum(x)\n\ntf.Tensor(21.0, shape=(), dtype=float32)\n\n\nRunning large calculations on CPU can be slow. When properly configured, TensorFlow can use accelerator hardware like GPUs to execute operations very quickly.\n\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n\nTensorFlow **IS** using the GPU\n\n\nRefer to the Tensor guide for details."
  },
  {
    "objectID": "tensorflow/guide/basics.html#variables",
    "href": "tensorflow/guide/basics.html#variables",
    "title": "Tensorflow Basics",
    "section": "Variables",
    "text": "Variables\nNormal tensor objects are immutable. To store model weights (or other mutable state) in TensorFlow use a tf$Variable.\n\nvar <- tf$Variable(c(0, 0, 0))\nvar\n\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n\n\n\nvar$assign(c(1, 2, 3))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n\n\n\nvar$assign_add(c(1, 1, 1))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>\n\n\nRefer to the Variables guide for details."
  },
  {
    "objectID": "tensorflow/guide/basics.html#automatic-differentiation",
    "href": "tensorflow/guide/basics.html#automatic-differentiation",
    "title": "Tensorflow Basics",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nGradient descent and related algorithms are a cornerstone of modern machine learning.\nTo enable this, TensorFlow implements automatic differentiation (autodiff), which uses calculus to compute gradients. Typically you’ll use this to calculate the gradient of a model’s error or loss with respect to its weights.\n\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n\n\nf(x)\n\ntf.Tensor(-2.0, shape=(), dtype=float32)\n\n\nAt x = 1.0, y = f(x) = (1^2 + 2*1 - 5) = -2.\nThe derivative of y is y' = f'(x) = (2*x + 2) = 4. TensorFlow can calculate this automatically:\n\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\nThis simplified example only takes the derivative with respect to a single scalar (x), but TensorFlow can compute the gradient with respect to any number of non-scalar tensors simultaneously.\nRefer to the Autodiff guide for details."
  },
  {
    "objectID": "tensorflow/guide/basics.html#graphs-and-tf_function",
    "href": "tensorflow/guide/basics.html#graphs-and-tf_function",
    "title": "Tensorflow Basics",
    "section": "Graphs and tf_function",
    "text": "Graphs and tf_function\nWhile you can use TensorFlow interactively like any R library, TensorFlow also provides tools for:\n\nPerformance optimization: to speed up training and inference.\nExport: so you can save your model when it’s done training.\n\nThese require that you use tf_function() to separate your pure-TensorFlow code from R.\n\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n\nThe first time you run the tf_function, although it executes in R, it captures a complete, optimized graph representing the TensorFlow computations done within the function.\n\nx <- as_tensor(1:3)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(6, shape=(), dtype=int32)\n\n\nOn subsequent calls TensorFlow only executes the optimized graph, skipping any non-TensorFlow steps. Below, note that my_func doesn’t print \"Tracing.\" since message is an R function, not a TensorFlow function.\n\nx <- as_tensor(10:8)\nmy_func(x)\n\ntf.Tensor(27, shape=(), dtype=int32)\n\n\nA graph may not be reusable for inputs with a different signature (shape and dtype), so a new graph is generated instead:\n\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(27.3, shape=(), dtype=float32)\n\n\nThese captured graphs provide two benefits:\n\nIn many cases they provide a significant speedup in execution (though not this trivial example).\nYou can export these graphs, using tf$saved_model, to run on other systems like a server or a mobile device, no Python installation required.\n\nRefer to Intro to graphs for more details."
  },
  {
    "objectID": "tensorflow/guide/basics.html#modules-layers-and-models",
    "href": "tensorflow/guide/basics.html#modules-layers-and-models",
    "title": "Tensorflow Basics",
    "section": "Modules, layers, and models",
    "text": "Modules, layers, and models\ntf$Module is a class for managing your tf$Variable objects, and the tf_function objects that operate on them. The tf$Module class is necessary to support two significant features:\n\nYou can save and restore the values of your variables using tf$train$Checkpoint. This is useful during training as it is quick to save and restore a model’s state.\nYou can import and export the tf$Variable values and the tf$function graphs using tf$saved_model. This allows you to run your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple tf$Module object:\n\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n\n\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3)))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nSave the Module:\n\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n\nThe resulting SavedModel is independent of the code that created it. You can load a SavedModel from R, Python, other language bindings, or TensorFlow Serving. You can also convert it to run with TensorFlow Lite or TensorFlow JS.\n\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3)))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nThe tf$keras$layers$Layer and tf$keras$Model classes build on tf$Module providing additional functionality and convenience methods for building, training, and saving models. Some of these are demonstrated in the next section.\nRefer to Intro to modules for details."
  },
  {
    "objectID": "tensorflow/guide/basics.html#training-loops",
    "href": "tensorflow/guide/basics.html#training-loops",
    "title": "Tensorflow Basics",
    "section": "Training loops",
    "text": "Training loops\nNow put this all together to build a basic model and train it from scratch.\nFirst, create some example data. This generates a cloud of points that loosely follows a quadratic curve:\n\nx <- as_tensor(seq(-2, 2, length.out = 201))\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n\n\n\n\nCreate a model:\n\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n\n\nmodel <- Model(64)\n\n\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n\n\n\n\nWrite a basic training loop:\n\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n\nMean squared error: 0.945\n\n\nMean squared error: 0.923\n\n\nMean squared error: 0.907\n\n\nMean squared error: 0.895\n\n\nMean squared error: 0.886\n\n\nMean squared error: 0.880\n\n\nMean squared error: 0.875\n\n\nMean squared error: 0.871\n\n\nMean squared error: 0.868\n\n\nMean squared error: 0.865\n\n\n\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n\n\n\n\nThat’s working, but remember that implementations of common training utilities are available in the tf$keras module. So consider using those before writing your own. To start with, the compile and fit methods for Keras Models implement a training loop for you:\n\nnew_model <- Model(64)\n\n\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n\n\nplot(history, metrics = 'loss', method = \"base\") \n\n\n\n# see ?plot.keras_training_history for more options.\n\nRefer to Basic training loops and the Keras guide for more details."
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html",
    "href": "keras/guides/customizing_what_happens_in_fit.html",
    "title": "Customizing what happens in fit()",
    "section": "",
    "text": "When you’re doing supervised learning, you can use fit() and everything works smoothly.\nWhen you need to write your own training loop from scratch, you can use the GradientTape and take control of every little detail.\nBut what if you need a custom training algorithm, but you still want to benefit from the convenient features of fit(), such as callbacks, built-in distribution support, or step fusing?\nA core principle of Keras is progressive disclosure of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn’t fall off a cliff if the high-level functionality doesn’t exactly match your use case. You should be able to gain more control over the small details while retaining a commensurate amount of high-level convenience.\nWhen you need to customize what fit() does, you should override the training step function of the Model class. This is the function that is called by fit() for every batch of data. You will then be able to call fit() as usual – and it will be running your own learning algorithm.\nNote that this pattern does not prevent you from building models with the Functional API. You can do this whether you’re building Sequential models, Functional API models, or subclassed models.\nLet’s see how that works."
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#setup",
    "href": "keras/guides/customizing_what_happens_in_fit.html#setup",
    "title": "Customizing what happens in fit()",
    "section": "Setup",
    "text": "Setup\nRequires TensorFlow 2.2 or later.\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#a-first-simple-example",
    "href": "keras/guides/customizing_what_happens_in_fit.html#a-first-simple-example",
    "title": "Customizing what happens in fit()",
    "section": "A first simple example",
    "text": "A first simple example\nLet’s start from a simple example:\n\nWe create a new model class by calling new_model_class().\nWe just override the method train_step(data).\nWe return a dictionary mapping metric names (including the loss) to their current value.\n\nThe input argument data is what gets passed to fit as training data:\n\nIf you pass arrays, by calling fit(x, y, ...), then data will be the tuple (x, y)\nIf you pass a tf$data$Dataset, by calling fit(dataset, ...), then data will be what gets yielded by dataset at each batch.\n\nIn the body of the train_step method, we implement a regular training update, similar to what you are already familiar with. Importantly, we compute the loss via self$compiled_loss, which wraps the loss(es) function(s) that were passed to compile().\nSimilarly, we call self$compiled_metrics$update_state(y, y_pred) to update the state of the metrics that were passed in compile(), and we query results from self$metrics at the end to retrieve their current value.\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\nLoaded Tensorflow version 2.8.0\n\n\nLet’s try this out:\n\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)"
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#going-lower-level",
    "href": "keras/guides/customizing_what_happens_in_fit.html#going-lower-level",
    "title": "Customizing what happens in fit()",
    "section": "Going lower-level",
    "text": "Going lower-level\nNaturally, you could just skip passing a loss function in compile(), and instead do everything manually in train_step. Likewise for metrics.\nHere’s a lower-level example, that only uses compile() to configure the optimizer:\n\nWe start by creating Metric instances to track our loss and a MAE score.\nWe implement a custom train_step() that updates the state of these metrics (by calling update_state() on them), then query them (via result()) to return their current average value, to be displayed by the progress bar and to be pass to any callback.\nNote that we would need to call reset_states() on our metrics between each epoch! Otherwise calling result() would return an average since the start of training, whereas we usually work with per-epoch averages. Thankfully, the framework can do that for us: just list any metric you want to reset in the metrics property of the model. The model will call reset_states() on any object listed here at the beginning of each fit() epoch or at the beginning of a call to evaluate().\n\n\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)"
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "href": "keras/guides/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "title": "Customizing what happens in fit()",
    "section": "Supporting sample_weight & class_weight",
    "text": "Supporting sample_weight & class_weight\nYou may have noticed that our first basic example didn’t make any mention of sample weighting. If you want to support the fit() arguments sample_weight and class_weight, you’d simply do the following:\n\nUnpack sample_weight from the data argument\nPass it to compiled_loss & compiled_metrics (of course, you could also just apply it manually if you don’t rely on compile() for losses & metrics)\nThat’s it. That’s the list.\n\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)"
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "href": "keras/guides/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "title": "Customizing what happens in fit()",
    "section": "Providing your own evaluation step",
    "text": "Providing your own evaluation step\nWhat if you want to do the same for calls to model$evaluate()? Then you would override test_step in exactly the same way. Here’s what it looks like:\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n\n     loss       mae \n0.2720022 0.4182713"
  },
  {
    "objectID": "keras/guides/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "href": "keras/guides/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "title": "Customizing what happens in fit()",
    "section": "Wrapping up: an end-to-end GAN example",
    "text": "Wrapping up: an end-to-end GAN example\nLet’s walk through an end-to-end example that leverages everything you just learned.\nLet’s consider:\n\nA generator network meant to generate 28x28x1 images.\nA discriminator network meant to classify 28x28x1 images into two classes (“fake” and “real”).\nOne optimizer for each.\nA loss function to train the discriminator.\n\n\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n\nHere’s a feature-complete GAN class, overriding compile() to use its own signature, and implementing the entire GAN algorithm in 17 lines in train_step:\n\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n\nLet’s test-drive it:\n\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n\nHappy training!"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "",
    "text": "library(magrittr)\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n\nLoaded Tensorflow version 2.8.0\n\n\n[1] '2.8'"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Layer class: a combination of state (weights) and some computation",
    "text": "The Layer class: a combination of state (weights) and some computation\nOne of the central abstractions in Keras is the Layer class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).\nHere’s a densely-connected layer. It has a state: the variables w and b.\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \"float32\"\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \"float32\"),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nYou would use a layer by calling it on some tensor input(s), much like a regular function.\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[0.11195514 0.06101172 0.05288789 0.03446084]\n [0.11195514 0.06101172 0.05288789 0.03446084]], shape=(2, 4), dtype=float32)\n\n\nLinear behaves similarly to a layer present in the Python interface to keras (e.g., keras$layers$Dense).\nHowever, one additional step is needed to make it behave like the builtin layers present in the keras R package (e.g., layer_dense()).\nKeras layers in R are designed to compose nicely with the pipe operator (%>%), so that the layer instance is conveniently created on demand when an existing model or tensor is piped in. In order to make a custom layer similarly compose nicely with the pipe, you can call create_layer_wrapper() on the layer class constructor.\n\nlayer_linear <- create_layer_wrapper(Linear)\n\nNow layer_linear is a layer constructor that composes nicely with %>%, just like the built-in layers:\n\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n\ntf.Tensor(\n[[ 0.09593566 -0.0239684  -0.08020082 -0.20181108]\n [ 0.09593566 -0.0239684  -0.08020082 -0.20181108]], shape=(2, 4), dtype=float32)\n\nmodel\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n linear_1 (Linear)                (2, 4)                        12          \n============================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nBecause the pattern above is so common, there is a convenience function that combines the steps of subclassing keras$layers$Layer and calling create_layer_wrapper on the output: the Layer function. The layer_linear defined below is identical to the layer_linear defined above.\n\nlayer_linear <- Layer(\n  \"Linear\",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\nFor the remainder of this vignette we’ll be using the %py_class% constructor. However, in your own code feel free to use create_layer_wrapper and/or Layer if you prefer.\nNote that the weights w and b are automatically tracked by the layer upon being set as layer attributes:\n\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n\nYou also have access to a quicker shortcut for adding a weight to a layer: the add_weight() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[-0.10461128  0.03076715  0.03548232 -0.02009123]\n [-0.10461128  0.03076715  0.03548232 -0.02009123]], shape=(2, 4), dtype=float32)"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers can have non-trainable weights",
    "text": "Layers can have non-trainable weights\nBesides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\nHere’s how to add and use a non-trainable weight:\n\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 2 2\n\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 4 4\n\n\nIt’s part of layer$weights, but it gets categorized as a non-trainable weight:\n\ncat(\"weights:\", length(my_sum$weights), \"\\n\")\n\nweights: 1 \n\ncat(\"non-trainable weights:\", length(my_sum$non_trainable_weights), \"\\n\")\n\nnon-trainable weights: 1 \n\n# It's not included in the trainable weights:\ncat(\"trainable_weights:\", my_sum$trainable_weights, \"\\n\")\n\ntrainable_weights:"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Best practice: deferring weight creation until the shape of the inputs is known",
    "text": "Best practice: deferring weight creation until the shape of the inputs is known\nOur Linear layer above took an input_dimargument that was used to compute the shape of the weights w and b in initialize():\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nIn many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\nIn the Keras API, we recommend creating layer weights in the build(self, inputs_shape) method of your layer. Like this:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nThe build() method of your layer will automatically run the first time your layer instance is called. You now have a layer that can handle an arbitrary number of input features:\n\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\n\nImplementing build() separately as shown above nicely separates creating weights only once from using weights in every call. However, for some advanced custom layers, it can become impractical to separate the state creation and computation. Layer implementers are allowed to defer weight creation to the first call(), but need to take care that later calls use the same weights. In addition, since call() is likely to be executed for the first time inside a tf_function(), any variable creation that takes place in call() should be wrapped in a tf$init_scope()."
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers are recursively composable",
    "text": "Layers are recursively composable\nIf you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer.\nWe recommend creating such sublayers in the initialize() method and leave it to the first call() to trigger building their weights.\n\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\ncat(\"weights:\", length(mlp$weights), \"\\n\")\n\nweights: 6 \n\ncat(\"trainable weights:\", length(mlp$trainable_weights), \"\\n\")\n\ntrainable weights: 6"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_loss() method",
    "text": "The add_loss() method\nWhen writing the call() method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling self$add_loss(value):\n\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n\nThese losses (including those created by any inner layer) can be retrieved via layer$losses. This property is reset at the start of every call() to the top-level layer, so that layer$losses always contains the loss values created during the last forward pass.\n\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n\nIn addition, the loss property also contains regularization losses created for the weights of any inner layer:\n\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n\n[[1]]\ntf.Tensor(0.0018123012, shape=(), dtype=float32)\n\n\nThese losses are meant to be taken into account when writing training loops, like this:\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n\nFor a detailed guide about writing training loops, see the guide to writing a training loop from scratch.\nThese losses also work seamlessly with fit() (they get automatically summed and added to the main loss, if any):\n\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \"adam\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_metric() method",
    "text": "The add_metric() method\nSimilarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training.\nConsider the following layer: a “logistic endpoint” layer. It takes as inputs predictions and targets, it computes a loss which it tracks via add_loss(), and it computes an accuracy scalar, which it tracks via add_metric().\n\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n\nMetrics tracked in this way are accessible via layer$metrics:\n\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\"layer$metrics: \")\n\nlayer$metrics: \n\nstr(layer$metrics)\n\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n\ncat(\"current accuracy value:\", as.numeric(layer$metrics[[1]]$result()), \"\\n\")\n\ncurrent accuracy value: 1 \n\n\nJust like for add_loss(), these metrics are tracked by fit():\n\ninputs <- layer_input(shape(3), name = \"inputs\")\ntargets <- layer_input(shape(10), name = \"targets\")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "You can optionally enable serialization on your layers",
    "text": "You can optionally enable serialization on your layers\nIf you need your custom layers to be serializable as part of a Functional model, you can optionally implement a get_config() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n\n$units\n[1] 64\n\nnew_layer <- Linear$from_config(config)\n\nNote that the initialize() method of the base Layer class takes some additional named arguments, in particular a name and a dtype. It’s good practice to pass these arguments to the parent class in initialize() and to include them in the layer config:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n\nList of 4\n $ name     : chr \"linear_9\"\n $ trainable: logi TRUE\n $ dtype    : chr \"float32\"\n $ units    : num 64\n\nnew_layer <- Linear$from_config(config)\n\nIf you need more flexibility when deserializing the layer from its config, you can also override the from_config() class method. This is the base implementation of from_config():\n\nfrom_config <- function(cls, config) do.call(cls, config)\n\nTo learn more about serialization and saving, see the complete guide to saving and serializing models."
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged training argument in the call() method",
    "text": "Privileged training argument in the call() method\nSome layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call() method.\nBy exposing this argument in call(), you enable the built-in training and evaluation loops (e.g. fit()) to correctly use the layer in training and inference. Note, the default of NULL means that the training parameter will be inferred by keras from the training context (e.g., it will be TRUE if called from fit(), FALSE if called from predict())\n\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged mask argument in the call() method",
    "text": "Privileged mask argument in the call() method\nThe other privileged argument supported by call() is the mask argument.\nYou will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\nKeras will automatically pass the correct mask argument to call() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\nTo learn more about masking and how to write masking-enabled layers, please check out the guide “understanding padding and masking”."
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Model class",
    "text": "The Model class\nIn general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model – the object you will train.\nFor instance, in a ResNet50 model, you would have several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.\nThe Model class has the same API as Layer, with the following differences:\n\nIt has support for built-in training, evaluation, and prediction methods (fit(), evaluate(), predict()).\nIt exposes the list of its inner layers, via the model$layers property.\nIt exposes saving and serialization APIs (save_model_tf(), save_model_weights_tf(), …)\n\nEffectively, the Layer class corresponds to what we refer to in the literature as a “layer” (as in “convolution layer” or “recurrent layer”) or as a “block” (as in “ResNet block” or “Inception block”).\nMeanwhile, the Model class corresponds to what is referred to in the literature as a “model” (as in “deep learning model”) or as a “network” (as in “deep neural network”).\nSo if you’re wondering, “should I use the Layer class or the Model class?”, ask yourself: will I need to call fit() on it? Will I need to call save() on it? If so, go with Model. If not (either because your class is just a block in a bigger system, or because you are writing training & saving code yourself), use Layer.\nFor instance, we could take our mini-resnet example above, and use it to build a Model that we could train with fit(), and that we could save with save_model_weights_tf():\n\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Putting it all together: an end-to-end example",
    "text": "Putting it all together: an end-to-end example\nHere’s what you’ve learned so far:\n\nA Layer encapsulates a state (created in initialize() or build()), and some computation (defined in call()).\nLayers can be recursively nested to create new, bigger computation blocks.\nLayers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\nThe outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n\nLet’s put all of these things together into an end-to-end example: we’re going to implement a Variational AutoEncoder (VAE). We’ll train it on MNIST digits.\nOur VAE will be a subclass of Model, built as a nested composition of layers that subclass Layer. It will feature a regularization loss (KL divergence).\n\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \"encoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \"Converts z, the encoded digit vector, back into a readable digit.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \"decoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \"Combines the encoder and decoder into an end-to-end model for training.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \"autoencoder\", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n\nLet’s write a simple training loop on MNIST:\n\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\"Start of epoch %d\\n\", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\"step %d: mean loss = %.4f\\n\", step, loss_metric$result()))\n      }\n    }\n  })\n}\n\nStart of epoch 1\nstep 100: mean loss = 0.1247\nstep 200: mean loss = 0.0987\nstep 300: mean loss = 0.0888\nstep 400: mean loss = 0.0839\nstep 500: mean loss = 0.0806\nstep 600: mean loss = 0.0785\nstep 700: mean loss = 0.0770\nstep 800: mean loss = 0.0759\nstep 900: mean loss = 0.0749\nStart of epoch 2\nstep 100: mean loss = 0.0739\nstep 200: mean loss = 0.0734\nstep 300: mean loss = 0.0729\nstep 400: mean loss = 0.0726\nstep 500: mean loss = 0.0722\nstep 600: mean loss = 0.0719\nstep 700: mean loss = 0.0716\nstep 800: mean loss = 0.0714\nstep 900: mean loss = 0.0711\n\n\nNote that since the VAE is subclassing Model, it features built-in training loops. So you could also have trained it like this:\n\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)"
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Beyond object-oriented development: the Functional API",
    "text": "Beyond object-oriented development: the Functional API\nIf you prefer a less object-oriented way of programming, you can also build models using the Functional API. Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\nFor instance, the Functional API example below reuses the same Sampling layer we defined in the example above:\n\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \"encoder_input\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \"z_mean\")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \"z_log_var\")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \"encoder\")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \"z_sampling\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \"sigmoid\")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \"decoder\")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \"vae\")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n\nFor more information, make sure to read the Functional API guide."
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Defining custom layers and models in an R package",
    "text": "Defining custom layers and models in an R package\nUnfortunately you can’t use anything that creates references to Python objects, at the top-level of an R package.\nHere is why: when you build an R package, all the R files in the R/ directory get sourced in an R environment (the package namespace), and then that environment is saved as part of the package bundle. Loading the package means restoring the saved R environment. This means that the R code only gets sourced once, at build time. If you create references to external objects (e.g., Python objects) at package build time, they will be NULL pointers when the package is loaded, because the external objects they pointed to at build time no longer exist at load time.\nThe solution is to delay creating references to Python objects until run time. Fortunately, %py_class%, Layer(), and create_layer_wrapper(R6Class(...)) are all lazy about initializing the Python reference, so they are safe to define and export in an R package.\nIf you’re writing an R package that uses keras and reticulate, this article might be helpful to read over."
  },
  {
    "objectID": "keras/guides/making_new_layers_and_models_via_subclassing.html#summary",
    "href": "keras/guides/making_new_layers_and_models_via_subclassing.html#summary",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Summary",
    "text": "Summary\nIn this guide you learned about creating custom layers and models in keras.\n\nThe constructors available: new_layer_class(), %py_class%, create_layer_wrapper(), R6Class(), Layer().\nWhat methods to you might want to define to your model: initialize(), build(), call(), and get_config().\nWhat convenience methods are available when you subclass keras$layers$Layer: add_weight(), add_loss(), and add_metric()"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html",
    "href": "keras/guides/preprocessing_layers.html",
    "title": "Working with preprocessing layers",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#keras-preprocessing",
    "href": "keras/guides/preprocessing_layers.html#keras-preprocessing",
    "title": "Working with preprocessing layers",
    "section": "Keras preprocessing",
    "text": "Keras preprocessing\nThe Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own."
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#available-preprocessing-layers",
    "href": "keras/guides/preprocessing_layers.html#available-preprocessing-layers",
    "title": "Working with preprocessing layers",
    "section": "Available preprocessing layers",
    "text": "Available preprocessing layers\n\nText preprocessing\n\nlayer_text_vectorization(): turns raw strings into an encoded representation that can be read by a layer_embedding() or layer_dense() layer.\n\n\n\nNumerical features preprocessing\n\nlayer_normalization(): performs feature-wise normalization of input features.\nlayer_discretization(): turns continuous numerical features into integer categorical features.\n\n\n\nCategorical features preprocessing\n\nlayer_category_encoding(): turns integer categorical features into one-hot, multi-hot, or count-based, dense representations.\nlayer_hashing(): performs categorical feature hashing, also known as the “hashing trick”.\nlayer_string_lookup(): turns string categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\nlayer_integer_lookup(): turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\n\n\nImage preprocessing\nThese layers are for standardizing the inputs of an image model.\n\nlayer_resizing(): resizes a batch of images to a target size.\nlayer_rescaling(): rescales and offsets the values of a batch of images (e.g., going from inputs in the [0, 255] range to inputs in the [0, 1] range.\nlayer_center_crop(): returns a center crop of a batch of images.\n\n\n\nImage data augmentation\nThese layers apply random augmentation transforms to a batch of images. They are only active during training.\n\nlayer_random_crop()\nlayer_random_flip()\nlayer_random_flip()\nlayer_random_translation()\nlayer_random_rotation()\nlayer_random_zoom()\nlayer_random_height()\nlayer_random_width()\nlayer_random_contrast()"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#the-adapt-function",
    "href": "keras/guides/preprocessing_layers.html#the-adapt-function",
    "title": "Working with preprocessing layers",
    "section": "The adapt() function",
    "text": "The adapt() function\nSome preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n\nlayer_text_vectorization(): holds a mapping between string tokens and integer indices\nlayer_string_lookup() and layer_integer_lookup(): hold a mapping between input values and integer indices.\nlayer_normalization(): holds the mean and standard deviation of the features.\nlayer_discretization(): holds information about value bucket boundaries.\n\nCrucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by “adapting” them on data.\nYou set the state of a preprocessing layer by exposing it to training data, via adapt():\n\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n\nLoaded Tensorflow version 2.8.0\n\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\"Features mean: %.2f\", mean(normalized_data))\n\n[1] \"Features mean: -0.00\"\n\nsprintf(\"Features std: %.2f\", sd(normalized_data))\n\n[1] \"Features std: 1.06\"\n\n\nadapt() takes either an array or a tf_dataset. In the case of layer_string_lookup() and layer_text_vectorization(), you can also pass a character vector:\n\ndata <- c(\n  \"Congratulations!\",\n  \"Today is your day.\",\n  \"You're off to Great Places!\",\n  \"You're off and away!\",\n  \"You have brains in your head.\",\n  \"You have feet in your shoes.\",\n  \"You can steer yourself\",\n  \"any direction you choose.\",\n  \"You're on your own. And you know what you know.\",\n  \"And YOU are the one who'll decide where to go.\"\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n\n\nIn addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the adapt() call, they can be set without relying on the layer’s internal computation. For instance, if external vocabulary files for the layer_text_vectorization(), layer_string_lookup(), or layer_integer_lookup() layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer’s constructor arguments.\nHere’s an example where we instantiate a layer_string_lookup() layer with precomputed vocabulary:\n\nvocab <- c(\"a\", \"b\", \"c\", \"d\")\ndata <- as_tensor(rbind(c(\"a\", \"c\", \"d\"),\n                        c(\"d\", \"z\", \"b\")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "href": "keras/guides/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing data before the model or inside the model",
    "text": "Preprocessing data before the model or inside the model\nThere are two ways you could be using preprocessing layers:\nOption 1: Make them part of the model, like this:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n\nWith this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you’re training on GPU, this is the best option for the layer_normalization() layer, and for all image preprocessing and data augmentation layers.\nOption 2: apply it to your tf_dataset, so as to obtain a dataset that yields batches of preprocessed data, like this:\n\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\nWith this option, your preprocessing will happen on CPU, asynchronously, and will be buffered before going into the model. In addition, if you call tfdatasets::dataset_prefetch() on your dataset, the preprocessing will happen efficiently in parallel with training:\n\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n\nThis is the best option for layer_text_vectorization(), and all structured data preprocessing layers. It can also be a good option if you’re training on CPU and you use image preprocessing layers."
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "href": "keras/guides/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "title": "Working with preprocessing layers",
    "section": "Benefits of doing preprocessing inside the model at inference time",
    "text": "Benefits of doing preprocessing inside the model at inference time\nEven if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew.\nWhen all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to [-1, +1] or to [0, 1], etc. This is especially powerful if you’re exporting your model to another runtime, such as TensorFlow.js: you won’t have to reimplement your preprocessing pipeline in JavaScript.\nIf you initially put your preprocessing layers in your tf_dataset pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "href": "keras/guides/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing during multi-worker training",
    "text": "Preprocessing during multi-worker training\nPreprocessing layers are compatible with the tf.distribute API for running training across multiple machines.\nIn general, preprocessing layers should be placed inside a strategy$scope() and called either inside or before the model as discussed above.\n\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n\nFor more details, refer to the preprocessing section of the distributed input guide."
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#quick-recipes",
    "href": "keras/guides/preprocessing_layers.html#quick-recipes",
    "title": "Working with preprocessing layers",
    "section": "Quick recipes",
    "text": "Quick recipes\n\nImage data augmentation\nNote that image data augmentation layers are only active during training (similar to the layer_dropout() layer).\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n\nYou can see a similar setup in action in the example image classification from scratch.\n\n\nNormalizing numerical features\n\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"adam\",\n          loss = \"sparse_categorical_crossentropy\")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n\n\n\nEncoding string categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(c(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\"a\", \"b\", \"c\", \"d\", \"e\", \"\")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n\n\nNote that, here, index 0 is reserved for out-of-vocabulary values (values that were not seen during adapt()).\nYou can see the layer_string_lookup() in action in the Structured data classification from scratch example.\n\n\nEncoding integer categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \"int32\")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \"int32\")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n\n\nNote that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_token and oov_token constructor arguments of layer_integer_lookup().\nYou can see the layer_integer_lookup() in action in the example structured data classification from scratch.\n\n\nApplying the hashing trick to an integer categorical feature\nIf you have a categorical feature that can take many different values (on the order of 10e3 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the “hashing trick”: hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing.\n\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \"int64\")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n\nTensorShape([10000, 64])\n\n\n\n\nEncoding text as a sequence of token indices\nThis is how you should preprocess text to be passed to an Embedding layer.\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\",\n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 2 19 14 1 9 2 1\n\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer = \"rmsprop\", loss = \"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output:\", as.array(test_output), \"\\n\")\n\nModel output: 0.1437887 \n\n\nYou can see the layer_text_vectorization() layer in action, combined with an Embedding mode, in the example text classification from scratch.\nNote that when training such a model, for best performance, you should always use the layer_text_vectorization() layer as part of the input pipeline.\n\n\nEncoding text as a dense matrix of ngrams with multi-hot encoding\nThis is how you can preprocess text to be passed to a Dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"multi_hot\" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output); cat(\"\\n\")\n\nModel output: \n\n\ntf.Tensor([[0.31572872]], shape=(1, 1), dtype=float32)\n\n\n\n\nEncoding text as a dense matrix of ngrams with TF-IDF weighting\nThis is an alternative way of preprocessing text before passing it to a layer_dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"tf-idf\" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\"CPU\"), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\"Training model...\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output)\n\nModel output: \n\n\ntf.Tensor([[0.09238248]], shape=(1, 1), dtype=float32)"
  },
  {
    "objectID": "keras/guides/preprocessing_layers.html#important-gotchas",
    "href": "keras/guides/preprocessing_layers.html#important-gotchas",
    "title": "Working with preprocessing layers",
    "section": "Important gotchas",
    "text": "Important gotchas\n\nWorking with lookup layers with very large vocabularies\nYou may find yourself working with a very large vocabulary in a layer_text_vectorization(), a layer_string_lookup() layer, or an layer_integer_lookup() layer. Typically, a vocabulary larger than 500MB would be considered “very large”.\nIn such case, for best performance, you should avoid using adapt(). Instead, pre-compute your vocabulary in advance (you could use Apache Beam or TF Transform for this) and store it in a file. Then load the vocabulary into the layer at construction time by passing the filepath as the vocabulary argument."
  },
  {
    "objectID": "keras/guides/python_subclasses.html",
    "href": "keras/guides/python_subclasses.html",
    "title": "Python Subclasses",
    "section": "",
    "text": "When using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses R6::R6Class as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R."
  },
  {
    "objectID": "keras/guides/python_subclasses.html#examples",
    "href": "keras/guides/python_subclasses.html#examples",
    "title": "Python Subclasses",
    "section": "Examples",
    "text": "Examples\n\nA custom constraint (R6)\nFor demonstration purposes, let’s say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\nNonNegative <- R6::R6Class(\"NonNegative\",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \"__call__\" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n\nLoaded Tensorflow version 2.8.0\n\n\nThe r_to_py method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\nNew class instances are generated by calling the class directly: NonNegative() (not NonNegative$new())\nAll methods (functions) are (potentially) modified to ensure their first argument is self.\nAll methods have in scope __class__, super and the class name (NonNegative).\nFor convenience, some method names are treated as aliases:\n\ninitialize is treated as an alias for __init__()\nfinalize is treated as an alias for __del__()\n\nsuper can be accessed in 3 ways:\n\nR6 style, which supports only single inheritance (the most common type)\n\nsuper$initialize()\n\nPython 2 style, which requires explicitly providing the class generator and instance\n\nsuper(NonNegative, self)$`__init__`()\n\nPython 3 style\n\nsuper()$`__init__`()\nWhen subclassing Keras base classes, it is generally your responsibility to call super$initialize() if you are masking a superclass initializer by providing your own initialize method.\nPassing convert=FALSE to r_to_py() will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with convert=TRUE, namely, modifying some Python objects, like dictionaries or lists, in-place.\nActive bindings (methods supplied to R6Class(active=...)) are converted to Python @property-decorated methods.\nR6 classes with private methods or attributes are not supported.\nThe argument supplied to inherit can be:\n\nmissing or NULL\na Python class generator\nan R6 class generator, as long as it can be converted to a Python class generator as well\na list of Python/R6 classes (for multiple inheritance)\nA list of superclasses, with optional additional keywords (e.g., metaclass=, only for advanced Python use cases)\n\n\n\n\nA custom constraint (%py_class%)\nAs an alternative to r_to_py(R6Class(...)), we also provide %py_class%, a more concise alternative syntax for achieving the same outcome. %py_class% is heavily inspired by the Python class statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using %py_class%:\n\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \"__call__\" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n\nNotice, this is very similar to the equivalent Python code:\n\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n\nSome (potentially surprising) notes about %py_class%:\n\nJust like the Python class statement, it assigns the constructed class in the current scope! (There is no need to write NonNegative <- ...).\nThe left hand side can be:\n\nA bare symbol, ClassName\nA pseudo-call, with superclasses and keywords as arguments: ClassName(Superclass1, Superclass2, metaclass=my_metaclass)\n\nThe right hand side is evaluated in a new environment to form the namespace for the class methods.\n%py_class% objects can be safely defined at the top level of an R package. (see details about delay_load below)\nTwo keywords are treated specially: convert and delay_load.\nIf you want to call r_to_py with convert=FALSE, pass it as a keyword:\n\n\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n\n\nYou can delay creating the python type object until this first time a class instance is created by passing delay_load=TRUE. The default value is FALSE for most contexts, but TRUE if you are in an R package. (The actual test performed is identical(topenv(), globalenv())). If a %py_class% type object is delayed, it will display \"<<R6type>.ClassName> (delayed)\" when printed.\nAn additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the __doc__ attribute of the class or method. The doc string will then be visible to both python and R tools e.g. reticulate::py_help(). See ?py_class for an example.\n\nIn all other regards, %py_class% is equivalent to r_to_py(R6Class()) (indeed, under the hood, they do the same thing).\n\n\nA custom layer (R6)\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \"CustomLayer\",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n\n\n\nA custom layer (%py_class%)\nor using %py_class%:\n\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}"
  },
  {
    "objectID": "keras/guides/transfer_learning.html",
    "href": "keras/guides/transfer_learning.html",
    "title": "Transfer learning and fine-tuning",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#introduction",
    "href": "keras/guides/transfer_learning.html#introduction",
    "title": "Transfer learning and fine-tuning",
    "section": "Introduction",
    "text": "Introduction\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\nTake layers from a previously trained model.\nFreeze them, so as to avoid destroying any of the information they contain during future training rounds.\nAdd some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\nTrain the new layers on your dataset.\n\nA last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\nFirst, we will go over the Keras trainable API in detail, which underlies most transfer learning and fine-tuning workflows.\nThen, we’ll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle “cats vs dogs” classification dataset.\nThis is adapted from Deep Learning with R and the 2016 blog post “building powerful image classification models using very little data”."
  },
  {
    "objectID": "keras/guides/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "href": "keras/guides/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Freezing layers: understanding the trainable attribute",
    "text": "Freezing layers: understanding the trainable attribute\nLayers and models have three weight attributes:\n\nweights is the list of all weights variables of the layer.\ntrainable_weights is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\nnon_trainable_weights is the list of those that aren’t meant to be trained. Typically they are updated by the model during the forward pass.\n\nExample: the Dense layer has 2 trainable weights (kernel and bias)\n\nlayer <- layer_dense(units = 3)\n\nLoaded Tensorflow version 2.8.0\n\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 0\n\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is layer_batch_normalization(). It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the guide to writing new layers from scratch.\nExample: The layer instance returned by layer_batch_normalization() has 2 trainable weights and 2 non-trainable weights\n\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 4\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nLayers and models also feature a boolean attribute trainable. Its value can be changed. Setting layer$trainable to FALSE moves all the layer’s weights from trainable to non-trainable. This is called “freezing” the layer: the state of a frozen layer won’t be updated during training (either when training with fit() or when training with any custom loop that relies on trainable_weights to apply gradient updates).\nExample: setting trainable to False\n\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 0\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \"relu\")\nlayer2 <- layer_dense(units = 3, activation = \"sigmoid\")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n\nDo not confuse the layer$trainable attribute with the training argument in a layer instance’s call signature layer(training =) (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the Keras FAQ."
  },
  {
    "objectID": "keras/guides/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "href": "keras/guides/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Recursive setting of the trainable attribute",
    "text": "Recursive setting of the trainable attribute\nIf you set trainable = FALSE on a model or on any layer that has sublayers, all child layers become non-trainable as well.\nExample:\n\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \"sigmoid\")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#the-typical-transfer-learning-workflow",
    "href": "keras/guides/transfer_learning.html#the-typical-transfer-learning-workflow",
    "title": "Transfer learning and fine-tuning",
    "section": "The typical transfer-learning workflow",
    "text": "The typical transfer-learning workflow\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\nInstantiate a base model and load pre-trained weights into it.\nFreeze all layers in the base model by setting trainable = FALSE.\nCreate a new model on top of the output of one (or several) layers from the base model.\nTrain your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\nInstantiate a base model and load pre-trained weights into it.\nRun your new dataset through it and record the output of one (or several) layers from the base model. This is called feature extraction.\nUse that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it’s a lot faster and cheaper.\nAn issue with that second workflow, though, is that it doesn’t allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\nHere’s what the first workflow looks like in Keras:\nFirst, instantiate a base model with pre-trained weights.\n\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\nThen, freeze the base model.\n\nbase_model$trainable <- FALSE\n\nCreate a new model on top.\n\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n\nTrain the model on new data.\n\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#fine-tuning",
    "href": "keras/guides/transfer_learning.html#fine-tuning",
    "title": "Transfer learning and fine-tuning",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting – keep that in mind.\nIt is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\nIt’s also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\nThis is how to implement fine-tuning of the whole base model:\n\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n\nImportant note about compile() and trainable\nCalling compile() on a model is meant to “freeze” the behavior of that model. This implies that the trainable attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until compile is called again. Hence, if you change any trainable value, make sure to call compile() again on your model for your changes to be taken into account.\nImportant notes about layer_batch_normalization()\nMany image models contain BatchNormalization layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\nBatchNormalization contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\nWhen you set bn_layer$trainable = FALSE, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as weight trainability and inference/training modes are two orthogonal concepts. But the two are tied in the case of the BatchNormalization layer.\nWhen you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = FALSE when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou’ll see this pattern in action in the end-to-end example at the end of this guide."
  },
  {
    "objectID": "keras/guides/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "href": "keras/guides/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "title": "Transfer learning and fine-tuning",
    "section": "Transfer learning and fine-tuning with a custom training loop",
    "text": "Transfer learning and fine-tuning with a custom training loop\nIf instead of fit(), you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list model$trainable_weights when applying gradient updates:\n\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n\nLikewise for fine-tuning."
  },
  {
    "objectID": "keras/guides/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "href": "keras/guides/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "title": "Transfer learning and fine-tuning",
    "section": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset",
    "text": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\nTo solidify these concepts, let’s walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats vs. dogs” classification dataset.\n\nGetting the data\nFirst, let’s fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you’ll probably want to use the utility image_dataset_from_directory() to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n# reticulate::py_install(\"tensorflow_datasets\", pip = TRUE)\ntfds <- reticulate::import(\"tensorflow_datasets\")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\"Number of training samples: %d\", length(train_ds))\n\nNumber of training samples: 9305\n\nprintf(\"Number of validation samples: %d\", length(validation_ds) )\n\nNumber of validation samples: 2326\n\nprintf(\"Number of test samples: %d\", length(test_ds))\n\nNumber of test samples: 2326\n\n\nThese are the first 9 images in the training dataset – as you can see, they’re all different sizes.\n\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\"label: %s   size: %s\",\n                  label, paste(dim(image), collapse = \" x \")))\n  })\n\n\n\n\nWe can also see that label 1 is “dog” and label 0 is “cat”.\n\n\nStandardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn’t a great fit for feeding a neural network. We need to do 2 things:\n\nStandardize to a fixed image size. We pick 150x150.\nNormalize pixel values between -1 and 1. We’ll do this using a layer_normalization() as part of the model itself.\n\nIn general, it’s a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you’ll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\nHere, we’ll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we’ll do the input value scaling as part of the model, when we create it.\nLet’s resize images to 150x150:\n\nlibrary(magrittr, include.only = \"%<>%\")\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n\nBesides, let’s batch the data and use caching and prefetching to optimize loading speed.\n\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n\n\n\nUsing random data augmentation\nWhen you don’t have a large image dataset, it’s a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(.1)\n\nLet’s visualize what the first image of the first batch looks like after various random transformations:\n\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\")\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#build-a-model",
    "href": "keras/guides/transfer_learning.html#build-a-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Build a model",
    "text": "Build a model\nNow let’s build a model that follows the blueprint we’ve explained earlier.\nNote that:\n\nWe add layer_rescaling() to scale input values (initially in the [0, 255] range) to the [-1, 1] range.\nWe add a layer_dropout() before the classification layer, for regularization.\nWe make sure to pass training = FALSE when calling the base model, so that it runs in inference mode, so that batchnorm statistics don’t get updated even after we unfreeze the base model for fine-tuning.\n\n\nbase_model = application_xception(\n  weights = \"imagenet\", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs = layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  N          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n____________________________________________________________________________"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#train-the-top-layer",
    "href": "keras/guides/transfer_learning.html#train-the-top-layer",
    "title": "Transfer learning and fine-tuning",
    "section": "Train the top layer",
    "text": "Train the top layer\n\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 20\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)"
  },
  {
    "objectID": "keras/guides/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "href": "keras/guides/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Do a round of fine-tuning of the entire model",
    "text": "Do a round of fine-tuning of the entire model\nFinally, let’s unfreeze the base model and train the entire model end-to-end with a low learning rate.\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed training = FALSE when calling it when we built the model. This means that the batch normalization layers inside won’t update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  Y          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n____________________________________________________________________________\n\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 10\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here."
  },
  {
    "objectID": "keras/guides/working_with_rnns.html",
    "href": "keras/guides/working_with_rnns.html",
    "title": "Working with RNNs",
    "section": "",
    "text": "Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\nThe Keras RNN API is designed with a focus on:\n\nEase of use: the built-in layer_rnn(), layer_lstm(), layer_gru() layers enable you to quickly build recurrent models without having to make difficult configuration choices.\nEase of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic layer_rnn layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code."
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#setup",
    "href": "keras/guides/working_with_rnns.html#setup",
    "title": "Working with RNNs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "href": "keras/guides/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "title": "Working with RNNs",
    "section": "Built-in RNN layers: a simple example",
    "text": "Built-in RNN layers: a simple example\nThere are three built-in RNN layers in Keras:\n\nlayer_simple_rnn(), a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\nlayer_gru(), first proposed in Cho et al., 2014.\nlayer_lstm(), first proposed in Hochreiter & Schmidhuber, 1997.\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a layer_lstm().\n\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n\nLoaded Tensorflow version 2.8.0\n\nmodel\n\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 64)              64000       \n lstm (LSTM)                      (None, 128)                   98816       \n dense (Dense)                    (None, 10)                    1290        \n============================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nBuilt-in RNNs support a number of useful features:\n\nRecurrent dropout, via the dropout and recurrent_dropout arguments\nAbility to process an input sequence in reverse, via the go_backwards argument\nLoop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the unroll argument\n…and more.\n\nFor more information, see the RNN API documentation."
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#outputs-and-states",
    "href": "keras/guides/working_with_rnns.html#outputs-and-states",
    "title": "Working with RNNs",
    "section": "Outputs and states",
    "text": "Outputs and states\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is (batch_size, units) where units corresponds to the units argument passed to the layer’s constructor.\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences = TRUE. The shape of this output is (batch_size, timesteps, units).\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_1\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding_1 (Embedding)          (None, None, 64)              64000       \n gru (GRU)                        (None, None, 256)             247296      \n simple_rnn (SimpleRNN)           (None, 128)                   49280       \n dense_1 (Dense)                  (None, 10)                    1290        \n============================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\nTo configure a RNN layer to return its internal state, set return_state = TRUE when creating the layer. Note that LSTM has 2 state tensors, but GRU only has one.\nTo configure the initial state of the layer, call the layer instance with the additional named argument initial_state. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\"encoder\")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \"decoder\")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n\nModel: \"model\"\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, None)]  0        []                        \n input_2 (InputLayer)    [(None, None)]  0        []                        \n embedding_2 (Embedding)  (None, None, 6  64000   ['input_1[0][0]']         \n                         4)                                                 \n embedding_3 (Embedding)  (None, None, 6  128000  ['input_2[0][0]']         \n                         4)                                                 \n encoder (LSTM)          [(None, 64),    33024    ['embedding_2[0][0]']     \n                          (None, 64),                                       \n                          (None, 64)]                                       \n decoder (LSTM)          (None, 64)      33024    ['embedding_3[0][0]',     \n                                                   'encoder[0][1]',         \n                                                   'encoder[0][2]']         \n dense_2 (Dense)         (None, 10)      650      ['decoder[0][0]']         \n============================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n____________________________________________________________________________"
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "href": "keras/guides/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "title": "Working with RNNs",
    "section": "RNN layers and RNN cells",
    "text": "RNN layers and RNN cells\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\nThe cell is the inside of the for loop of a RNN layer. Wrapping a cell inside a layer_rnn() layer gives you a layer capable of processing a sequence, e.g. layer_rnn(layer_lstm_cell(10)).\nMathematically, layer_rnn(layer_lstm_cell(10)) produces the same result as layer_lstm(10). In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in layer_gru() and layer_lstm() layers enable the use of CuDNN and you may see better performance.\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\nlayer_simple_rnn_cell() corresponds to the layer_simple_rnn() layer.\nlayer_gru_cell corresponds to the layer_gru layer.\nlayer_lstm_cell corresponds to the layer_lstm layer.\n\nThe cell abstraction, together with the generic layer_rnn() class, makes it very easy to implement custom RNN architectures for your research."
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#cross-batch-statefulness",
    "href": "keras/guides/working_with_rnns.html#cross-batch-statefulness",
    "title": "Working with RNNs",
    "section": "Cross-batch statefulness",
    "text": "Cross-batch statefulness\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of cross-batch statefulness.\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer’s state. That way, the layer can retain information about the entirety of the sequence, even though it’s only seeing one sub-sequence at a time.\nYou can do this by setting stateful = TRUE in the constructor.\nIf you have a sequence s = c(t0, t1, ... t1546, t1547), you would split it into e.g.\n\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n\nThen you would process it via:\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n\nWhen you want to clear the state, you can use layer$reset_states().\n\nNote: In this setup, sample i in a given batch is assumed to be the continuation of sample i in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], the next batch should contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200].\n\nHere is a complete example:\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n\n\nRNN State Reuse\nThe recorded states of the RNN layer are not included in the layer$weights(). If you would like to reuse the state from a RNN layer, you can retrieve the states value by layer$states and use it as the initial state of a new layer instance via the Keras functional API like new_layer(inputs, initial_state = layer$states), or model subclassing.\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)"
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#bidirectional-rnns",
    "href": "keras/guides/working_with_rnns.html#bidirectional-rnns",
    "title": "Working with RNNs",
    "section": "Bidirectional RNNs",
    "text": "Bidirectional RNNs\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\nKeras provides an easy API for you to build such bidirectional RNNs: the bidirectional() wrapper.\n\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_2\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n bidirectional_1 (Bidirectional)  (None, 5, 128)                38400       \n bidirectional (Bidirectional)    (None, 64)                    41216       \n dense_3 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n____________________________________________________________________________\n\n\nUnder the hood, bidirectional() will copy the RNN layer passed in, and flip the go_backwards field of the newly copied layer, so that it will process the inputs in reverse order.\nThe output of the bidirectional RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the merge_mode parameter in the bidirectional wrapper constructor. For more details about bidirectional, please check the API docs."
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "href": "keras/guides/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "title": "Working with RNNs",
    "section": "Performance optimization and CuDNN kernels",
    "text": "Performance optimization and CuDNN kernels\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior layer_cudnn_gru/layer_cudnn_lstm layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\nSince the CuDNN kernel is built with certain assumptions, this means the layer will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers. E.g.:\n\nChanging the activation function from \"tanh\" to something else.\nChanging the recurrent_activation function from \"sigmoid\" to something else.\nUsing recurrent_dropout > 0.\nSetting unroll to TRUE, which forces LSTM/GRU to decompose the inner tf$while_loop into an unrolled for loop.\nSetting use_bias to FALSE.\nUsing masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the LSTM and GRU layers.\n\nUsing CuDNN kernels when available\nLet’s build a simple LSTM model to demonstrate the performance difference.\nWe’ll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we’ll predict the digit’s label.\n\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n\nLet’s load the MNIST dataset:\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n\nLet’s create a model instance and train it.\nWe choose sparse_categorical_crossentropy() as the loss function for the model. The output of the model has shape of (batch_size, 10). The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"sgd\",\n    metrics = \"accuracy\"\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nNow, let’s compare to a model that does not use the CuDNN kernel:\n\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\"sgd\",\n    metrics=\"accuracy\",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The tf$device() annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\nYou simply don’t have to worry about the hardware you’re running on anymore. Isn’t that pretty cool?\n\nwith(tf$device(\"CPU:0\"), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \"Predicted result is: %s, target result is: %s\\n\", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n\nPredicted result is: 3, target result is: 5"
  },
  {
    "objectID": "keras/guides/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "href": "keras/guides/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "title": "Working with RNNs",
    "section": "RNNs with list/dict inputs, or nested inputs",
    "text": "RNNs with list/dict inputs, or nested inputs\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\nDefine a custom cell that supports nested input/output\nSee Making new Layers & Models via subclassing for details on writing your own layers.\n\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \"uniform\",\n      name = \"kernel_1\"\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \"uniform\",\n      name = \"kernel_2_3\"\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\"bij,ijkl->bkl\", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\"unit_1\" = self$unit_1,\n         \"unit_2\" = self$unit_2,\n         \"unit_3\" = self$unit_3)\n  }\n}\n\n\n\nBuild a RNN model with nested input/output\nLet’s build a Keras model that uses a layer_rnn layer and the custom cell we just defined.\n\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"accuracy\")\n\n\n\nTrain the model with randomly generated data\nSince there isn’t a good candidate dataset for this model, we use random data for demonstration.\n\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n\nWith keras::layer_rnn(), you are only expected to define the math logic for an individual step within the sequence, and the layer_rnn() will handle the sequence iteration for you. It’s an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\nFor more details, please visit the API docs."
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html",
    "href": "keras/guides/writing_your_own_callbacks.html",
    "title": "Writing your own callbacks",
    "section": "",
    "text": "A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include callback_tensorboard() to visualize training progress and results with TensorBoard, or callback_model_checkpoint() to periodically save your model during training.\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started."
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#setup",
    "href": "keras/guides/writing_your_own_callbacks.html#setup",
    "title": "Writing your own callbacks",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()"
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#keras-callbacks-overview",
    "href": "keras/guides/writing_your_own_callbacks.html#keras-callbacks-overview",
    "title": "Writing your own callbacks",
    "section": "Keras callbacks overview",
    "text": "Keras callbacks overview\nAll callbacks subclass the keras$callbacks$Callback class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\nYou can pass a list of callbacks (as a named argument callbacks) to the following keras model methods:\n\nfit()\nevaluate()\npredict()"
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "href": "keras/guides/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "title": "Writing your own callbacks",
    "section": "An overview of callback methods",
    "text": "An overview of callback methods\n\nGlobal methods\n\non_(train|test|predict)_begin(self, logs=None)\nCalled at the beginning of fit/evaluate/predict.\n\n\non_(train|test|predict)_end(self, logs=None)\nCalled at the end of fit/evaluate/predict.\n\n\n\nBatch-level methods for training/testing/predicting\n\non_(train|test|predict)_batch_begin(self, batch, logs=None)\nCalled right before processing a batch during training/testing/predicting.\n\n\non_(train|test|predict)_batch_end(self, batch, logs=None)\nCalled at the end of training/testing/predicting a batch. Within this method, logs is a dict containing the metrics results.\n\n\n\nEpoch-level methods (training only)\n\non_epoch_begin(self, epoch, logs=None)\nCalled at the beginning of an epoch during training.\n\n\non_epoch_end(self, epoch, logs=None)\nCalled at the end of an epoch during training."
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#a-basic-example",
    "href": "keras/guides/writing_your_own_callbacks.html#a-basic-example",
    "title": "Writing your own callbacks",
    "section": "A basic example",
    "text": "A basic example\nLet’s take a look at a concrete example. To get started, let’s import tensorflow and define a simple Sequential Keras model:\n\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n\nNow, define a simple custom callback that logs:\n\nWhen fit/evaluate/predict starts & ends\nWhen each epoch starts & ends\nWhen each training batch starts & ends\nWhen each evaluation (test) batch starts & ends\nWhen each inference (prediction) batch starts & ends\n\n\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  str(logs); cat(\"\\n\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n\nLet’s try it out:\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n\n\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n\n\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n\n\nUsage of logs dict\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)"
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "href": "keras/guides/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "title": "Writing your own callbacks",
    "section": "Usage of self$model attribute",
    "text": "Usage of self$model attribute\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: self$model.\nHere are of few of the things you can do with self$model in a callback:\n\nSet self$model$stop_training <- TRUE to immediately interrupt training.\nMutate hyperparameters of the optimizer (available as self$model$optimizer), such as self$model$optimizer$learning_rate.\nSave the model at period intervals.\nRecord the output of predict(model) on a few test samples at the end of each epoch, to use as a sanity check during training.\nExtract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\netc.\n\nLet’s see this in action in a couple of examples."
  },
  {
    "objectID": "keras/guides/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "href": "keras/guides/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "title": "Writing your own callbacks",
    "section": "Examples of Keras callback applications",
    "text": "Examples of Keras callback applications\n\nEarly stopping at minimum loss\nThis first example shows the creation of a Callback that stops training when the minimum of loss has been reached, by setting the attribute self$model$stop_training (boolean). Optionally, you can provide an argument patience to specify how many epochs we should wait before stopping after having reached a local minimum.\nkeras$callbacks$EarlyStopping provides a more complete and general implementation.\n\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n\n\n\nLearning rate scheduling\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\nSee keras$callbacks$LearningRateScheduler for a more general implementations (in RStudio, press F1 while the cursor is over LearningRateScheduler and a browser will open to this page).\n\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n\n\n\nBuilt-in Keras callbacks\nBe sure to check out the existing Keras callbacks by reading the API docs. Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!"
  },
  {
    "objectID": "tensorflow/guide/tensor.html",
    "href": "tensorflow/guide/tensor.html",
    "title": "Tensor",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#basics",
    "href": "tensorflow/guide/tensor.html#basics",
    "title": "Tensor",
    "section": "Basics",
    "text": "Basics\nLet’s create some basic tensors.\nHere is a “scalar” or “rank-0” tensor . A scalar contains a single value, and no “axes”.\n\n# This will be an float64 tensor by default; see \"dtypes\" below.\nrank_0_tensor <- as_tensor(4)\n\nLoaded Tensorflow version 2.9.1\n\nprint(rank_0_tensor)\n\ntf.Tensor([4.], shape=(1), dtype=float64)\n\n\nA “vector” or “rank-1” tensor is like a list of values. A vector has one axis:\n\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3), dtype=float64)\n\n\nA “matrix” or “rank-2” tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]\n [5. 6.]], shape=(3, 2), dtype=float16)\n\n\n\n\n\n\n\n\n\n\nA scalar, shape: []\nA vector, shape: [3]\nA matrix, shape: [3, 2]\n\n\n\n\n\n\n\n\n\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of\n# axes (sometimes called \"dimensions\")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nThere are many ways you might visualize a tensor with more than two axes.\n\n\n\n\n\n\nA 3-axis tensor, shape: [3, 2, 5]\n\n\n\n\n\n\n\n!  \n\n\n\nYou can convert a tensor to an R array using as.array():\n\nas.array(rank_2_tensor)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nTensors often contain floats and ints, but have many other types, including:\n\ncomplex numbers\nstrings\n\nThe base tf$Tensor class requires tensors to be “rectangular”—that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\n\ntf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n\na * b # element-wise multiplication, same as tf$multiply(a, b)\n\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n\ntf$matmul(a, b) # matrix multiplication\n\ntf.Tensor(\n[[3 3]\n [7 7]], shape=(2, 2), dtype=int32)\n\n\nTensors are used in all kinds of operations (ops).\n\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n\ntf.Tensor(10.0, shape=(), dtype=float64)\n\n# Find the index of the largest value\ntf$math$argmax(x) \n\ntf.Tensor([1 0], shape=(2), dtype=int64)\n\ntf$nn$softmax(x) # Compute the softmax\n\ntf.Tensor(\n[[2.68941421e-01 7.31058579e-01]\n [9.99876605e-01 1.23394576e-04]], shape=(2, 2), dtype=float64)"
  },
  {
    "objectID": "tensorflow/guide/tensor.html#about-shapes",
    "href": "tensorflow/guide/tensor.html#about-shapes",
    "title": "Tensor",
    "section": "About shapes",
    "text": "About shapes\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\nAxis or Dimension: A particular dimension of a tensor.\nSize: The total number of items in the tensor, the product of the shape vector’s elements.\n\nNote: Although you may see reference to a “tensor of two dimensions”, a rank-2 tensor does not usually describe a 2D space.\nTensors and tf$TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n\n\n\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\n\n\n\n\n\n\n\n\nmessage(\"Type of every element: \", rank_4_tensor$dtype)\n\nType of every element: <dtype: 'float32'>\n\nmessage(\"Number of axes: \", length(dim(rank_4_tensor)))\n\nNumber of axes: 4\n\nmessage(\"Shape of tensor: \", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\n\nShape of tensor: 3245\n\nmessage(\"Elements along axis 0 of tensor: \", dim(rank_4_tensor)[1])\n\nElements along axis 0 of tensor: 3\n\nmessage(\"Elements along the last axis of tensor: \", dim(rank_4_tensor) |> tail(1)) \n\nElements along the last axis of tensor: 5\n\nmessage(\"Total number of elements (3*2*4*5): \", length(rank_4_tensor)) # can also call tf$size()\n\nTotal number of elements (3*2*4*5): 120\n\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\n\n\n\n\n\nTypical axis order"
  },
  {
    "objectID": "tensorflow/guide/tensor.html#indexing",
    "href": "tensorflow/guide/tensor.html#indexing",
    "title": "Tensor",
    "section": "Indexing",
    "text": "Indexing\n\nSingle-axis indexing\nSee ?`[.tensorflow.tensor` for details\n\n\nMulti-axis indexing\nHigher rank tensors are indexed by passing multiple indices.\nThe exact same rules as in the single-axis case apply to each axis independently.\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#manipulating-shapes",
    "href": "tensorflow/guide/tensor.html#manipulating-shapes",
    "title": "Tensor",
    "section": "Manipulating Shapes",
    "text": "Manipulating Shapes\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n\nTensorShape([1, 3])\n\n\n\n# You can convert this object into an R vector too\nas.integer(x$shape)\n\n[1] 1 3\n\n\nYou can reshape a tensor into a new shape. The tf$reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n\n\nx$shape\n\nTensorShape([1, 3])\n\nreshaped$shape\n\nTensorShape([1, 3])\n\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style “row-major” memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says \"Whatever fits\".\ntf$reshape(rank_3_tensor, c(-1L))\n\ntf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29], shape=(30), dtype=int32)\n\n\nA typical and reasonable use of tf$reshape is to combine or split adjacent axes (or add/remove 1s).\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\n\ntf.Tensor(\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [20 21 22 23 24]\n [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\n\n\n\n\n\n\n\nSome good reshapes.\n\n\n\n\n  \n\n\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png https://www.tensorflow.org/guide/ https://www.tensorflow.org/guide/images/tensor/reshape-good2.png\nReshaping will “work” for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\nSwapping axes in tf$reshape does not work; you need tf$transpose for that.\n\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]\n  [10 11 12 13 14]]\n\n [[15 16 17 18 19]\n  [20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32)\n\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]], shape=(5, 6), dtype=int32)\n\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\n\n\n\n\n\n\n\nSome bad reshapes.\n\n\n\n\n  \n\n\n\nYou may run across not-fully-specified shapes. Either the shape contains a NULL (an axis-length is unknown) or the whole shape is NULL (the rank of the tensor is unknown).\nExcept for tf$RaggedTensor, such shapes will only occur in the context of TensorFlow’s symbolic, graph-building APIs:\n\ntf_function\nThe keras functional API."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#more-on-dtypes",
    "href": "tensorflow/guide/tensor.html#more-on-dtypes",
    "title": "Tensor",
    "section": "More on DTypes",
    "text": "More on DTypes\nTo inspect a tf$Tensor’s data type use the Tensor$dtype property.\nWhen creating a tf$Tensor from a Python object you may optionally specify the datatype.\nIf you don’t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts R integers to tf$int32 and R floating point numbers to tf$float64.\nYou can cast from type to type.\n\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor\n\ntf.Tensor([2 3 4], shape=(3), dtype=uint8)"
  },
  {
    "objectID": "tensorflow/guide/tensor.html#broadcasting",
    "href": "tensorflow/guide/tensor.html#broadcasting",
    "title": "Tensor",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are recycled automatically to fit larger tensors when running combined operations on them.\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * y\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * z\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n\ntf.Tensor(\n[[1.]\n [2.]\n [3.]], shape=(3, 1), dtype=float64)\n\n(y <- tf$range(1, 5,  dtype = \"float64\"))\n\ntf.Tensor([1. 2. 3. 4.], shape=(4), dtype=float64)\n\nx * y\n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\n\n\n\n\n\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\n\n\n\n\\\n\n\n\nHere is the same operation without broadcasting:\n\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\nYou see what broadcasting looks like using tf$broadcast_to.\n\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n\ntf.Tensor(\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]], shape=(3, 3), dtype=float64)\n\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\nIt can get even more complicated. This section of Jake VanderPlas’s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy)."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#tfconvert_to_tensor",
    "href": "tensorflow/guide/tensor.html#tfconvert_to_tensor",
    "title": "Tensor",
    "section": "tf$convert_to_tensor",
    "text": "tf$convert_to_tensor\nMost ops, like tf$matmul and tf$reshape take arguments of class tf$Tensor. However, you’ll notice in the above case, objects shaped like tensors are also accepted.\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy’s ndarray, TensorShape, Python lists, and tf$Variable will all convert automatically.\nSee tf$register_tensor_conversion_function for more details, and if you have your own type you’d like to automatically convert to a tensor."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#ragged-tensors",
    "href": "tensorflow/guide/tensor.html#ragged-tensors",
    "title": "Tensor",
    "section": "Ragged Tensors",
    "text": "Ragged Tensors\nA tensor with variable numbers of elements along some axis is called “ragged”. Use tf$ragged$RaggedTensor for ragged data.\nFor example, This cannot be represented as a regular tensor:\n\n\n\n\n\n\nA tf$RaggedTensor, shape: [4, NULL]\n\n\n\n\n\n\n\n\n\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n\n\ntry(tensor <- as_tensor(ragged_list))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Can't convert non-rectangular Python sequence to Tensor.\n\n\nInstead create a tf$RaggedTensor using tf$ragged$constant:\n\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n\n<tf.RaggedTensor [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0], [6.0, 7.0, 8.0], [9.0]]>\n\n\nThe shape of a tf$RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor$shape)\n\nTensorShape([4, None])"
  },
  {
    "objectID": "tensorflow/guide/tensor.html#string-tensors",
    "href": "tensorflow/guide/tensor.html#string-tensors",
    "title": "Tensor",
    "section": "String tensors",
    "text": "String tensors\ntf$string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\nThe length of the string is not one of the axes of the tensor. See tf$strings for functions to manipulate them.\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\"Gray wolf\"))\n\ntf.Tensor([b'Gray wolf'], shape=(1), dtype=string)\n\n\nAnd a vector of strings:\n\n\n\n\n\n\nA vector of strings, shape: [3,]\n\n\n\n\n\n\n\n\n\ntensor_of_strings <- as_tensor(c(\"Gray wolf\",\n                                 \"Quick brown fox\",\n                                 \"Lazy dog\"))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n\ntf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3), dtype=string)\n\n\nIn the above printout the b prefix indicates that tf$string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\nIf you pass unicode characters they are utf-8 encoded.\n\nas_tensor(\"🥳👍\")\n\ntf.Tensor([b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d'], shape=(1), dtype=string)\n\n\nSome basic functions with strings can be found in tf$strings, including tf$strings$split.\n\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\" \")\n\n<tf.RaggedTensor [[b'Gray', b'wolf']]>\n\n\n\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n\n<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n\n\n\n\n\n\n\n\nThree strings split, shape: [3, NULL]\n\n\n\n\n\n\n\n\nAnd tf$string$to_number:\n\ntext <- as_tensor(\"1 10 100\")\ntf$strings$to_number(tf$strings$split(text, \" \"))\n\n<tf.RaggedTensor [[1.0, 10.0, 100.0]]>\n\n\nAlthough you can’t use tf$cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings <- tf$strings$bytes_split(as_tensor(\"Duck\"))\nbyte_ints <- tf$io$decode_raw(as_tensor(\"Duck\"), tf$uint8)\ncat(\"Byte strings: \"); print(byte_strings)\n\nByte strings: \n\n\n<tf.RaggedTensor [[b'D', b'u', b'c', b'k']]>\n\ncat(\"Bytes: \"); print(byte_ints)\n\nBytes: \n\n\ntf.Tensor([[ 68 117  99 107]], shape=(1, 4), dtype=uint8)\n\n\n\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\"アヒル 🦆\")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \"UTF-8\")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \"UTF-8\")\n\ncat(\"Unicode bytes: \"); unicode_bytes\n\nUnicode bytes: \n\n\ntf.Tensor([b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86'], shape=(1), dtype=string)\n\ncat(\"Unicode chars: \"); unicode_char_bytes\n\nUnicode chars: \n\n\n<tf.RaggedTensor [[b'\\xe3\\x82\\xa2', b'\\xe3\\x83\\x92', b'\\xe3\\x83\\xab', b' ',\n  b'\\xf0\\x9f\\xa6\\x86']]>\n\ncat(\"Unicode values: \"); unicode_values\n\nUnicode values: \n\n\n<tf.RaggedTensor [[12450, 12498, 12523, 32, 129414]]>\n\n\nThe tf$string dtype is used for all raw bytes data in TensorFlow. The tf$io module contains functions for converting data to and from bytes, including decoding images and parsing csv."
  },
  {
    "objectID": "tensorflow/guide/tensor.html#sparse-tensors",
    "href": "tensorflow/guide/tensor.html#sparse-tensors",
    "title": "Tensor",
    "section": "Sparse tensors",
    "text": "Sparse tensors\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf$sparse$SparseTensor and related operations to store sparse data efficiently.\n\n\n\n\n\n\nA tf$SparseTensor, shape: [3, 4]\n\n\n\n\n\n\n\n\n\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n\nSparseTensor(indices=tf.Tensor(\n[[0 0]\n [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2), dtype=int64))\n\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)\n\ntf.Tensor(\n[[1. 0. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)"
  },
  {
    "objectID": "tensorflow/guide/variable.html",
    "href": "tensorflow/guide/variable.html",
    "title": "Variable",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "tensorflow/guide/variable.html#setup",
    "href": "tensorflow/guide/variable.html#setup",
    "title": "Variable",
    "section": "Setup",
    "text": "Setup\nThis notebook discusses variable placement. If you want to see on what device your variables are placed, uncomment this line.\n\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)"
  },
  {
    "objectID": "tensorflow/guide/variable.html#create-a-variable",
    "href": "tensorflow/guide/variable.html#create-a-variable",
    "title": "Variable",
    "section": "Create a variable",
    "text": "Create a variable\nTo create a variable, provide an initial value. The tf$Variable will have the same dtype as the initialization value.\n\nmy_tensor <- as_tensor(1:4, \"float32\", shape = c(2, 2))\n\nLoaded Tensorflow version 2.9.1\n\n(my_variable <- tf$Variable(my_tensor))\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=bool, numpy=array([False, False, False,  True])>\n\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n\n\nA variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf$Tensor. Like tensors, they have a dtype and a shape, and can be exported to regular R arrays.\n\ncat(\"Shape: \"); my_variable$shape\n\nShape: \n\n\nTensorShape([2, 2])\n\ncat(\"DType: \"); my_variable$dtype\n\nDType: \n\n\ntf.float32\n\ncat(\"As R array: \"); str(as.array(my_variable))\n\nAs R array: \n\n\n num [1:2, 1:2] 1 3 2 4\n\n\nMost tensor operations work on variables as expected, although variables cannot be reshaped.\n\nmessage(\"A variable: \")\n\nA variable: \n\nmy_variable\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\nmessage(\"Viewed as a tensor: \")\n\nViewed as a tensor: \n\nas_tensor(my_variable)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\nmessage(\"Index of highest value: \")\n\nIndex of highest value: \n\ntf$math$argmax(my_variable)\n\ntf.Tensor([1 1], shape=(2), dtype=int64)\n\n# This creates a new tensor; it does not reshape the variable.\nmessage(\"Copying and reshaping: \") \n\nCopying and reshaping: \n\ntf$reshape(my_variable, c(1L, 4L))\n\ntf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n\n\nAs noted above, variables are backed by tensors. You can reassign the tensor using tf$Variable$assign. Calling assign does not (usually) allocate a new tensor; instead, the existing tensor’s memory is reused.\n\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n\n\nIf you use a variable like a tensor in operations, you will usually operate on the backing tensor.\nCreating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory.\n\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([5., 6.], dtype=float32)>\n\n# a and b are different\n\nas.array(a)\n\n[1] 5 6\n\nas.array(b)\n\n[1] 2 3\n\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\n\n[1] 7 9\n\nas.array(a$assign_sub(c(7,9))) # c(0, 0)\n\n[1] 0 0"
  },
  {
    "objectID": "tensorflow/guide/variable.html#lifecycles-naming-and-watching",
    "href": "tensorflow/guide/variable.html#lifecycles-naming-and-watching",
    "title": "Variable",
    "section": "Lifecycles, naming, and watching",
    "text": "Lifecycles, naming, and watching\nIn TensorFlow, tf$Variable instance have the same lifecycle as other R objects. When there are no references to a variable it is automatically deallocated (garbage-collected).\nVariables can also be named which can help you track and debug them. You can give two variables the same name.\n\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \"Mark\")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \"Mark\")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n\ntf.Tensor(\n[[False False]\n [False False]], shape=(2, 2), dtype=bool)\n\n\nVariable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.\nAlthough variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter.\n\n(step_counter <- tf$Variable(1L, trainable = FALSE))\n\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>"
  },
  {
    "objectID": "tensorflow/guide/variable.html#placing-variables-and-tensors",
    "href": "tensorflow/guide/variable.html#placing-variables-and-tensors",
    "title": "Variable",
    "section": "Placing variables and tensors",
    "text": "Placing variables and tensors\nFor better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype. This means most variables are placed on a GPU if one is available.\nHowever, you can override this. In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available. By turning on device placement logging (see above), you can see where the variable is placed.\nNote: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation.\nIf you run this notebook on different backends with and without a GPU you will see different logging. Note that logging device placement must be turned on at the start of the session.\n\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \"float32\")\n  c <- tf$matmul(a, b)\n})\n\nc\n\ntf.Tensor(\n[[22. 49.]\n [28. 64.]], shape=(2, 2), dtype=float32)\n\n\nIt’s possible to set the location of a variable or tensor on one device and do the computation on another device. This will introduce delay, as data needs to be copied between the devices.\nYou might do this, however, if you had multiple GPU workers but only want one copy of the variables.\n\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \"float32\")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n\ntf.Tensor(\n[[ 1.  6. 15.]\n [ 2.  8. 18.]], shape=(2, 3), dtype=float32)\n\n\nNote: Because tf$config$set_soft_device_placement() is turned on by default, even if you run this code on a device without a GPU, it will still run. The multiplication step will happen on the CPU.\nFor more on distributed training, refer to the guide."
  },
  {
    "objectID": "tensorflow/guide/variable.html#next-steps",
    "href": "tensorflow/guide/variable.html#next-steps",
    "title": "Variable",
    "section": "Next steps",
    "text": "Next steps\nTo understand how variables are typically used, see our guide on automatic differentiation."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html",
    "href": "tensorflow/guide/autodiff.html",
    "title": "Autodiff",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#automatic-differentiation-and-gradients",
    "href": "tensorflow/guide/autodiff.html#automatic-differentiation-and-gradients",
    "title": "Autodiff",
    "section": "Automatic Differentiation and Gradients",
    "text": "Automatic Differentiation and Gradients\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#setup",
    "href": "tensorflow/guide/autodiff.html#setup",
    "title": "Autodiff",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#computing-gradients",
    "href": "tensorflow/guide/autodiff.html#computing-gradients",
    "title": "Autodiff",
    "section": "Computing gradients",
    "text": "Computing gradients\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#gradient-tapes",
    "href": "tensorflow/guide/autodiff.html#gradient-tapes",
    "title": "Autodiff",
    "section": "Gradient tapes",
    "text": "Gradient tapes\nTensorFlow provides the tf$GradientTape() API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf$Variables. TensorFlow “records” relevant operations executed inside the context of a tf$GradientTape() onto a “tape”. TensorFlow then uses that tape to compute the gradients of a “recorded” computation using reverse mode differentiation.\nHere is a simple example:\n\nx <- tf$Variable(3)\n\nLoaded Tensorflow version 2.9.1\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n\nOnce you’ve recorded some operations, use GradientTape$gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model’s variables):\n\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\nThe above example uses scalars, but tf$GradientTape works as easily on any tensor:\n\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \"float32\", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf$nest).\n\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n\nThe gradient with respect to each source has the shape of the source:\n\nw$shape\n\nTensorShape([3, 2])\n\ndl_dw$shape\n\nTensorShape([3, 2])\n\n\nHere is the gradient calculation again, this time passing a named list of variables:\n\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b\n\ntf.Tensor([-0.39992297 -3.231268  ], shape=(2), dtype=float32)"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#gradients-with-respect-to-a-model",
    "href": "tensorflow/guide/autodiff.html#gradients-with-respect-to-a-model",
    "title": "Autodiff",
    "section": "Gradients with respect to a model",
    "text": "Gradients with respect to a model\nIt’s common to collect tf$Variables into a tf$Module or one of its subclasses (tf$keras$layers$Layer, tf$keras$Model) for checkpointing and exporting.\nIn most cases, you will want to calculate gradients with respect to a model’s trainable variables. Since all subclasses of tf$Module aggregate their variables in the Module$trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \"float32\", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n\n\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}\n\ndense/kernel:0, shape: (3, 2)\ndense/bias:0, shape: (2)"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#controlling-what-the-tape-watches",
    "href": "tensorflow/guide/autodiff.html#controlling-what-the-tape-watches",
    "title": "Autodiff",
    "section": "Controlling what the tape watches",
    "text": "Controlling what the tape watches\nThe default behavior is to record all operations after accessing a trainable tf$Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\nThe tape holds references to intermediate outputs, so you don’t want to record unnecessary operations.\nThe most common use case involves calculating the gradient of a loss with respect to all a model’s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf$Tensor is not “watched” by default, and the tf$Variable is not trainable:\n\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n\nList of 4\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nYou can list the variables being watched by the tape using the GradientTape$watched_variables method:\n\ntape$watched_variables()\n\n[[1]]\n<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n\n\ntf$GradientTape provides hooks that give the user control over what is or is not watched.\nTo record gradients with respect to a tf$Tensor, you need to call GradientTape$watch(x):\n\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n\n[1] 6\n\n\nConversely, to disable the default behavior of watching all tf$Variables, set watch_accessed_variables = FALSE when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n\nSince GradientTape$watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\n\ndy/dx0: \n\ncat('dy/dx1: ', as.array(grad$x1))\n\ndy/dx1:  0.9999546"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#intermediate-results",
    "href": "tensorflow/guide/autodiff.html#intermediate-results",
    "title": "Autodiff",
    "section": "Intermediate results",
    "text": "Intermediate results\nYou can also request gradients of the output with respect to intermediate values computed inside the tf$GradientTape context.\n\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n\n[1] 18\n\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape$gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent = TRUE. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\n\n[1]   4 108\n\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n\n[1] 2 6\n\n\n\nrm(tape)   # Drop the reference to the tape"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#notes-on-performance",
    "href": "tensorflow/guide/autodiff.html#notes-on-performance",
    "title": "Autodiff",
    "section": "Notes on performance",
    "text": "Notes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\nFor efficiency, some ops (like ReLU) don’t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent = TRUE on your tape, nothing is discarded and your peak memory usage will be higher."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#gradients-of-non-scalar-targets",
    "href": "tensorflow/guide/autodiff.html#gradients-of-non-scalar-targets",
    "title": "Autodiff",
    "section": "Gradients of non-scalar targets",
    "text": "Gradients of non-scalar targets\nA gradient is fundamentally an operation on a scalar.\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\n\n[1] 4\n\nas.array(tape$gradient(y1, x))\n\n[1] -0.25\n\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\nThe sum of the gradients of each target.\n\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n\n[1] 3.75\n\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n\n[1] 7\n\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\nIf you need a separate gradient for each item, refer to Jacobians.\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n\n\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \"royalblue\", lwd = 2)\nlines(x, dy_dx, col = \"coral\", lwd=2)\nlegend(\"topleft\", inset = .05,\n       expression(y, dy/dx),\n       col = c(\"royalblue\", \"coral\"), lwd = 2)"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#control-flow",
    "href": "tensorflow/guide/autodiff.html#control-flow",
    "title": "Autodiff",
    "section": "Control flow",
    "text": "Control flow\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\ndv1\n\nNULL\n\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1 ^ 2. The gradient with respect to x is always NULL.\n\n(dx <- tape$gradient(result, x))\n\nNULL"
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#getting-a-gradient-of-null",
    "href": "tensorflow/guide/autodiff.html#getting-a-gradient-of-null",
    "title": "Autodiff",
    "section": "Getting a gradient of NULL",
    "text": "Getting a gradient of NULL\nWhen a target is not connected to a source you will get a gradient of NULL.\n\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\nIn the section on “controlling what the tape watches” you saw that the tape will automatically watch a tf$Variable but not a tf$Tensor.\nOne common error is to inadvertently replace a tf$Variable with a tf$Tensor, instead of using Variable$assign to update the tf$Variable. Here is an example:\n\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \": \")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n\nResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\nEagerTensor : NULL\n\n\n\n\n2. Did calculations outside of TensorFlow\nThe tape can’t record the gradient path if the calculation exits TensorFlow. For example:\n\nnp <- reticulate::import(\"numpy\", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n\nNULL\n\n\n\n\n3. Took gradients through an integer or string\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\nNobody expects strings to be differentiable, but it’s easy to accidentally create an int constant or variable if you don’t specify the dtype.\n\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nTensorFlow doesn’t automatically cast between types, so, in practice, you’ll often get a type error instead of a missing gradient.\n\n\n4. Took gradients through a stateful object\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\nA tf$Tensor is immutable. You can’t change a tensor once it’s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf$matmul only depends on its inputs.\nA tf$Variable has internal state—its value. When you use the variable, the state is read. It’s normal to calculate a gradient with respect to a variable, but the variable’s state blocks gradient calculations from going farther back. For example:\n\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n\nNULL\n\n\nSimilarly, tf$data$Dataset iterators and tf$queues are stateful, and will stop all gradients on tensors that pass through them."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#no-gradient-registered",
    "href": "tensorflow/guide/autodiff.html#no-gradient-registered",
    "title": "Autodiff",
    "section": "No gradient registered",
    "text": "No gradient registered\nSome tf$Operations are registered as being non-differentiable* and will return NULL. Others have no gradient registered**.\nThe tf$raw_ops page shows which low-level ops have gradients registered.\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning NULL. This way you know something has gone wrong.\nFor example, the tf$image$adjust_contrast function wraps raw_ops$AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  LookupError: gradient registry has no entry for: AdjustContrastv2\n\n\nIf you need to differentiate through this op, you’ll either need to implement the gradient and register it (using tf$RegisterGradient) or re-implement the function using other ops."
  },
  {
    "objectID": "tensorflow/guide/autodiff.html#zeros-instead-of-null",
    "href": "tensorflow/guide/autodiff.html#zeros-instead-of-null",
    "title": "Autodiff",
    "section": "Zeros instead of NULL",
    "text": "Zeros instead of NULL\nIn some cases it would be convenient to get 0 instead of NULL for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)\n\ntf.Tensor([0. 0.], shape=(2), dtype=float32)"
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html",
    "href": "tensorflow/guide/intro_to_graphs.html",
    "title": "Intro To_graphs",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#overview",
    "href": "tensorflow/guide/intro_to_graphs.html#overview",
    "title": "Intro To_graphs",
    "section": "Overview",
    "text": "Overview\nThis guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the collection of Keras guides.\nIn this guide, you’ll learn how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.\nNote: For those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.\nThis is a big-picture overview that covers how tf_function() allows you to switch from eager execution to graph execution. For a more complete specification of tf_function(), go to the tf_function() guide.\n\nWhat are graphs?\nIn the previous three guides, you ran TensorFlow eagerly. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\nWhile eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. Graph execution means that tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf$Graph or simply a “graph.”\nGraphs are data structures that contain a set of tf$Operation objects, which represent units of computation; and tf$Tensor objects, which represent the units of data that flow between operations. They are defined in a tf$Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original R code.\nThis is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard.\n\n\n\nA simple TensorFlow g\n\n\n\n\nThe benefits of graphs\nWith a graph, you have a great deal of flexibility. You can use your TensorFlow graph in environments that don’t have an R interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for saved models when it exports them from R.\nGraphs are also easily optimized, allowing the compiler to do transformations like:\n\nStatically infer the value of tensors by folding constant nodes in your computation (“constant folding”).\nSeparate sub-parts of a computation that are independent and split them between threads or devices.\nSimplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system, Grappler, to perform this and other speedups.\nIn short, graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices.\nHowever, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them."
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#setup",
    "href": "tensorflow/guide/intro_to_graphs.html#setup",
    "title": "Intro To_graphs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \"%>%\")"
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#taking-advantage-of-graphs",
    "href": "tensorflow/guide/intro_to_graphs.html#taking-advantage-of-graphs",
    "title": "Intro To_graphs",
    "section": "Taking advantage of graphs",
    "text": "Taking advantage of graphs\nYou create and run a graph in TensorFlow by using tf_function(), either as a direct call or as a decorator. tf_function() takes a regular function as input and returns a Function. A Function is a callable that builds TensorFlow graphs from the R function. You use a Function in the same way as its R equivalent.\n\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \"float64\", shape = c(1, 2))\ny1 <- as_tensor(2:3, \"float64\", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n\nOn the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf$Graphs behind one API. That is how Function is able to give you the benefits of graph execution, like speed and deployability.\ntf_function applies to a function and all other functions it calls:\n\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \"float64\", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \"float64\", shape = c(1, 2))) #%>% as.array()\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you need to define a Placeholder or tf$Session().\n\nConverting Python functions to graphs\nAny function you write with TensorFlow will contain a mixture of built-in TF operations and R control-flow logic, such as if-then clauses, loops, break, return, next, and more. While TensorFlow operations are easily captured by a tf$Graph, R-specific logic needs to undergo an extra step in order to become part of the graph. tf_function() uses a library called {tfautograph} to evaluate the R code in a special way so that it generates a graph.\n\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \"First branch, with graph: \", format(tf_simple_relu(as_tensor(1))), \"\\n\",\n  \"Second branch, with graph: \", format(tf_simple_relu(as_tensor(-1))), \"\\n\",\n  sep = \"\"\n)\n\nThough it is unlikely that you will need to view graphs directly, you can inspect the outputs to check the exact results. These are not easy to read, so no need to look too carefully!\n\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n\nMost of the time, tf_function() will work without special considerations. However, there are some caveats, and the tf_function guide can help here, as well as the tfautograph Getting Started vignette\n\n\nPolymorphism: one Function, many graphs\nA tf$Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()) (i.e, the same memory address).\nEach time you invoke a Function with a set of arguments that can’t be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf$Graph specialized to those new arguments. The type specification of a tf$Graph’s inputs is known as its input signature or just a signature. For more information regarding when a new tf$Graph is generated and how that can be controlled, see the rules of retracing.\nThe Function stores the tf$Graph corresponding to that signature in a ConcreteFunction. A ConcreteFunction is a wrapper around a tf$Graph.\n\nmy_relu <- tf_function(function(x) {\n  message(\"Tracing my_relu(x) with: \", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\nmy_relu(c(1, -1))\nmy_relu(as_tensor(c(3, -3)))\n\nIf the Function has already been called with that signature, Function does not create a new tf$Graph.\n\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n\nBecause it’s backed by multiple graphs, a Function is polymorphic. That enables it to support more input types than a single tf$Graph could represent, as well as to optimize each tf$Graph for better performance.\n\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())"
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#using-tf_function",
    "href": "tensorflow/guide/intro_to_graphs.html#using-tf_function",
    "title": "Intro To_graphs",
    "section": "Using tf_function()",
    "text": "Using tf_function()\nSo far, you’ve learned how to convert a Python function into a graph simply by using tf_function() as function wrapper. But in practice, getting tf_function to work correctly can be tricky! In the following sections, you’ll learn how you can make your code work as expected with tf_function().\n\nGraph execution vs. eager execution\nThe code in a Function can be executed both eagerly and as a graph. By default, Function executes its code as a graph:\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n\n\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\n\nget_MSE(y_true, y_pred)\n\nTo verify that your Function’s graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with tf$config$run_functions_eagerly(TRUE). This is a switch that turns off Function’s ability to create and run graphs, instead executing the code normally.\n\ntf$config$run_functions_eagerly(TRUE)\n\n\nget_MSE(y_true, y_pred)\n\n\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n\nHowever, Function can behave differently under graph and eager execution. The R print() function is one example of how these two modes differ. Let’s check out what happens when you insert a print statement to your function and call it repeatedly.\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\"Calculating MSE!\")\n  mean((y_true - y_pred) ^ 2)\n  })\n\nObserve what is printed:\n\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\nIs the output surprising? get_MSE only printed once even though it was called three times.\nTo explain, the print statement is executed when Function runs the original code in order to create the graph in a process known as “tracing”. Tracing captures the TensorFlow operations into a graph, and print() is not captured in the graph. That graph is then executed for all three calls without ever running the R code again.\nAs a sanity check, let’s turn off graph execution to compare:\n\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n\n\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\n\ntf$config$run_functions_eagerly(FALSE)\n\nprint is an R side effect, and there are other differences that you should be aware of when converting a function into a Function. Learn more in the Limitations section of the Better performance with tf_function guide.\n\n\n\n\n\n\nNote\n\n\n\nNote: If you would like to print values in both eager and graph execution, use tf$print() instead.\n\n\n\n\nNon-strict execution\nGraph execution only executes the operations necessary to produce the observable effects, which includes:\n\nThe return value of the function\nDocumented well-known side-effects such as:\n\nInput/output operations, like tf$print()\nDebugging operations, such as the assert functions in tf$debugging() (also, stopifnot())\nMutations of tf$Variable()\n\n\nThis behavior is usually known as “Non-strict execution”, and differs from eager execution, which steps through all of the program operations, needed or not.\nIn particular, runtime error checking does not count as an observable effect. If an operation is skipped because it is unnecessary, it cannot raise any runtime errors.\nIn the following example, the “unnecessary” operation tf$gather() is skipped during graph execution, so the runtime error InvalidArgumentError is not raised as it would be in eager execution. Do not rely on an error being raised while executing a graph.\n\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n# All operations are run during eager execution so an error is raised.\n\n\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n\n\n\ntf_function() best practices\nIt may take some time to get used to the behavior of Function. To get started quickly, first-time users should play around with wrapping toy functions with tf_function() to get experience with going from eager to graph execution.\nDesigning for tf_function may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n\nToggle between eager and graph execution early and often with tf$config$run_functions_eagerly() to pinpoint if/when the two modes diverge.\nCreate tf$Variables outside the Python function and modify them on the inside. The same goes for objects that use tf$Variable, like keras$layers, keras$Models and tf$optimizers.\nAvoid writing functions that depend on outer Python variables, excluding tf$Variables and Keras objects.\nPrefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful!\nInclude as much computation as possible under a tf_function to maximize the performance gain. For example, wrap a whole training step or the entire training loop."
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#seeing-the-speed-up",
    "href": "tensorflow/guide/intro_to_graphs.html#seeing-the-speed-up",
    "title": "Intro To_graphs",
    "section": "Seeing the speed-up",
    "text": "Seeing the speed-up\ntf_function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:\n\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n\n\nplot(bench::mark(\n  \"Eager execution\" = power(x, 100),\n  \"Graph execution\" = power_as_graph(x, 100)))\n\ntf_function is commonly used to speed up training loops, and you can learn more about it in Writing a training loop from scratch with Keras.\nNote: You can also try tf_function(jit_compile = TRUE) for a more significant performance boost, especially if your code is heavy on TF control flow and uses many small tensors.\n\nPerformance and trade-offs\nGraphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. This investment is usually quickly paid back with the performance boost of subsequent executions, but it’s important to be aware that the first few steps of any large model training can be slower due to tracing.\nNo matter how large your model, you want to avoid tracing frequently. The tf_function() guide discusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it’s a good idea to check if you are retracing accidentally."
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#when-is-a-function-tracing",
    "href": "tensorflow/guide/intro_to_graphs.html#when-is-a-function-tracing",
    "title": "Intro To_graphs",
    "section": "When is a Function tracing?",
    "text": "When is a Function tracing?\nTo figure out when your Function is tracing, add a print or message() statement to its code. As a rule of thumb, Function will execute the message statement every time it traces.\n\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\"Tracing!\") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n\n\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\na_function_with_r_side_effect(3)\n\nNew (non-tensor) R arguments always trigger the creation of a new graph, hence the extra tracing."
  },
  {
    "objectID": "tensorflow/guide/intro_to_graphs.html#next-steps",
    "href": "tensorflow/guide/intro_to_graphs.html#next-steps",
    "title": "Intro To_graphs",
    "section": "Next steps",
    "text": "Next steps\nYou can learn more about tf_function() on the API reference page and by following the Better performance with tf_function guide."
  }
]