[
  {
    "objectID": "examples/addition_rnn.html",
    "href": "examples/addition_rnn.html",
    "title": "addition_rnn",
    "section": "",
    "text": "Input: “535+61”\nOutput: “596”\nPadding is handled by using a repeated sentinel character (space)\nInput may optionally be reversed, shown to increase performance in many tasks in: “Learning to Execute” http://arxiv.org/abs/1410.4615 and “Sequence to Sequence Learning with Neural Networks” http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf Theoretically it introduces shorter term dependencies between source and target.\nTwo digits reversed: One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\nThree digits reversed: One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\nFour digits reversed: One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\nFive digits reversed: One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n\nlibrary(keras)\nlibrary(stringi)\n\n# Function Definitions ----------------------------------------------------\n\n# Creates the char table and sorts them.\nlearn_encoding <- function(chars){\n  sort(chars)\n}\n\n# Encode from a character sequence to a one hot integer representation.\n# > encode(\"22+22\", char_table)\n# [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# +    0    1    0    0    0    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\n# 2    0    0    0    0    1    0    0    0    0     0     0     0\nencode <- function(char, char_table){\n  strsplit(char, \"\") %>%\n    unlist() %>%\n    sapply(function(x){\n      as.numeric(x == char_table)\n    }) %>% \n    t()\n}\n\n# Decode the one hot representation/probabilities representation\n# to their character output.\ndecode <- function(x, char_table){\n  apply(x,1, function(y){\n    char_table[which.max(y)]\n  }) %>% paste0(collapse = \"\")\n}\n\n# Returns a list of questions and expected answers.\ngenerate_data <- function(size, digits, invert = TRUE){\n  \n  max_num <- as.integer(paste0(rep(9, digits), collapse = \"\"))\n  \n  # generate integers for both sides of question\n  x <- sample(1:max_num, size = size, replace = TRUE)\n  y <- sample(1:max_num, size = size, replace = TRUE)\n  \n  # make left side always smaller than right side\n  left_side <- ifelse(x <= y, x, y)\n  right_side <- ifelse(x >= y, x, y)\n  \n  results <- left_side + right_side\n  \n  # pad with spaces on the right\n  questions <- paste0(left_side, \"+\", right_side)\n  questions <- stri_pad(questions, width = 2*digits+1, \n                        side = \"right\", pad = \" \")\n  if(invert){\n    questions <- stri_reverse(questions)\n  }\n  # pad with spaces on the left\n  results <- stri_pad(results, width = digits + 1, \n                      side = \"left\", pad = \" \")\n  \n  list(\n    questions = questions,\n    results = results\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\n# Parameters for the model and dataset\nTRAINING_SIZE <- 50000\nDIGITS <- 2\n\n# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n# int is DIGITS\nMAXLEN <- DIGITS + 1 + DIGITS\n\n# All the numbers, plus sign and space for padding\ncharset <- c(0:9, \"+\", \" \")\nchar_table <- learn_encoding(charset)\n\n\n# Data Preparation --------------------------------------------------------\n\n# Generate Data\nexamples <- generate_data(size = TRAINING_SIZE, digits = DIGITS)\n\n# Vectorization\nx <- array(0, dim = c(length(examples$questions), MAXLEN, length(char_table)))\ny <- array(0, dim = c(length(examples$questions), DIGITS + 1, length(char_table)))\n\nfor(i in 1:TRAINING_SIZE){\n  x[i,,] <- encode(examples$questions[i], char_table)\n  y[i,,] <- encode(examples$results[i], char_table)\n}\n\n# Shuffle\nindices <- sample(1:TRAINING_SIZE, size = TRAINING_SIZE)\nx <- x[indices,,]\ny <- y[indices,,]\n\n\n# Explicitly set apart 10% for validation data that we never train over\nsplit_at <- trunc(TRAINING_SIZE/10)\nx_val <- x[1:split_at,,]\ny_val <- y[1:split_at,,]\nx_train <- x[(split_at + 1):TRAINING_SIZE,,]\ny_train <- y[(split_at + 1):TRAINING_SIZE,,]\n\nprint('Training Data:')\nprint(dim(x_train))\nprint(dim(y_train))\n\nprint('Validation Data:')\nprint(dim(x_val))\nprint(dim(y_val))\n\n\n# Training ----------------------------------------------------------------\n\nHIDDEN_SIZE <- 128\nBATCH_SIZE <- 128\nLAYERS <- 1\n\n# Initialize sequential model\nmodel <- keras_model_sequential() \n\nmodel %>%\n  # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n  # Note: In a situation where your input sequences have a variable length,\n  # use input_shape=(None, num_feature).\n  layer_lstm(HIDDEN_SIZE, input_shape=c(MAXLEN, length(char_table))) %>%\n  # As the decoder RNN's input, repeatedly provide with the last hidden state of\n  # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n  # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n  layer_repeat_vector(DIGITS + 1)\n\n# The decoder RNN could be multiple layers stacked or a single layer.\n# By setting return_sequences to True, return not only the last output but\n# all the outputs so far in the form of (num_samples, timesteps,\n# output_dim). This is necessary as TimeDistributed in the below expects\n# the first dimension to be the timesteps.\nfor(i in 1:LAYERS)\n  model %>% layer_lstm(HIDDEN_SIZE, return_sequences = TRUE)\n\nmodel %>% \n  # Apply a dense layer to the every temporal slice of an input. For each of step\n  # of the output sequence, decide which character should be chosen.\n  time_distributed(layer_dense(units = length(char_table))) %>%\n  layer_activation(\"softmax\")\n\n# Compiling the model\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = \"adam\", \n  metrics = \"accuracy\"\n)\n\n# Get the model summary\nsummary(model)\n\n# Fitting loop\nmodel %>% fit( \n  x = x_train, \n  y = y_train, \n  batch_size = BATCH_SIZE, \n  epochs = 70,\n  validation_data = list(x_val, y_val)\n)\n\n# Predict for a new observation\nnew_obs <- encode(\"55+22\", char_table) %>%\n  array(dim = c(1,5,12))\nresult <- predict(model, new_obs)\nresult <- result[1,,]\ndecode(result, char_table)"
  },
  {
    "objectID": "examples/babi_memnn.html",
    "href": "examples/babi_memnn.html",
    "title": "babi_memnn",
    "section": "",
    "text": "References:\n\nJason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks”, http://arxiv.org/abs/1502.05698\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, “End-To-End Memory Networks”, http://arxiv.org/abs/1503.08895\n\nReaches 98.6% accuracy on task ‘single_supporting_fact_10k’ after 120 epochs. Time per epoch: 3s on CPU (core i7).\n\nlibrary(keras)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(dplyr)\n\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_df(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n  \n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n  \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n\n# Parameters --------------------------------------------------------------\n\nchallenges <- list(\n  # QA1 with 10,000 samples\n  single_supporting_fact_10k = \"%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt\",\n  # QA2 with 10,000 samples\n  two_supporting_facts_10k = \"%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt\"\n)\n\nchallenge_type <- \"single_supporting_fact_10k\"\nchallenge <- challenges[[challenge_type]]\nmax_length <- 999999\n\n\n# Data Preparation --------------------------------------------------------\n\n# Download data\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Reading training and test data\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n# Extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# Vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n\n# Defining the model ------------------------------------------------------\n\n# Placeholders\nsequence <- layer_input(shape = c(story_maxlen))\nquestion <- layer_input(shape = c(query_maxlen))\n\n# Encoders\n# Embed the input sequence into a sequence of vectors\nsequence_encoder_m <- keras_model_sequential()\nsequence_encoder_m %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, embedding_dim)\n\n# Embed the input into a sequence of vectors of size query_maxlen\nsequence_encoder_c <- keras_model_sequential()\nsequence_encoder_c %>%\n  layer_embedding(input_dim = vocab_size, output = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, story_maxlen, query_maxlen)\n\n# Embed the question into a sequence of vectors\nquestion_encoder <- keras_model_sequential()\nquestion_encoder %>%\n  layer_embedding(input_dim = vocab_size, output_dim = 64, \n                  input_length = query_maxlen) %>%\n  layer_dropout(rate = 0.3)\n# output: (samples, query_maxlen, embedding_dim)\n\n# Encode input sequence and questions (which are indices)\n# to sequences of dense vectors\nsequence_encoded_m <- sequence_encoder_m(sequence)\nsequence_encoded_c <- sequence_encoder_c(sequence)\nquestion_encoded <- question_encoder(question)\n\n# Compute a 'match' between the first input vector sequence\n# and the question vector sequence\n# shape: `(samples, story_maxlen, query_maxlen)`\nmatch <- list(sequence_encoded_m, question_encoded) %>%\n  layer_dot(axes = c(2,2)) %>%\n  layer_activation(\"softmax\")\n\n# Add the match matrix with the second input vector sequence\nresponse <- list(match, sequence_encoded_c) %>%\n  layer_add() %>%\n  layer_permute(c(2,1))\n\n# Concatenate the match matrix with the question vector sequence\nanswer <- list(response, question_encoded) %>%\n  layer_concatenate() %>%\n  # The original paper uses a matrix multiplication for this reduction step.\n  # We choose to use an RNN instead.\n  layer_lstm(32) %>%\n  # One regularization layer -- more would probably be needed.\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(vocab_size) %>%\n  # We output a probability distribution over the vocabulary\n  layer_activation(\"softmax\")\n\n# Build the final model\nmodel <- keras_model(inputs = list(sequence, question), answer)\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = 32,\n  epochs = 120,\n  validation_data = list(list(test_vec$stories, test_vec$questions), test_vec$answers)\n)"
  },
  {
    "objectID": "examples/babi_rnn.html",
    "href": "examples/babi_rnn.html",
    "title": "babi_rnn",
    "section": "",
    "text": "The results are comparable to those for an LSTM model provided in Weston et al.: “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks” http://arxiv.org/abs/1502.05698\n\n\n\nTask Number\nFB LSTM Baseline\nKeras QA\n\n\n\n\nQA1 - Single Supporting Fact\n50\n100.0\n\n\nQA2 - Two Supporting Facts\n20\n50.0\n\n\nQA3 - Three Supporting Facts\n20\n20.5\n\n\nQA4 - Two Arg. Relations\n61\n62.9\n\n\nQA5 - Three Arg. Relations\n70\n61.9\n\n\nQA6 - yes/No Questions\n48\n50.7\n\n\nQA7 - Counting\n49\n78.9\n\n\nQA8 - Lists/Sets\n45\n77.2\n\n\nQA9 - Simple Negation\n64\n64.0\n\n\nQA10 - Indefinite Knowledge\n44\n47.7\n\n\nQA11 - Basic Coreference\n72\n74.9\n\n\nQA12 - Conjunction\n74\n76.4\n\n\nQA13 - Compound Coreference\n94\n94.4\n\n\nQA14 - Time Reasoning\n27\n34.8\n\n\nQA15 - Basic Deduction\n21\n32.4\n\n\nQA16 - Basic Induction\n23\n50.6\n\n\nQA17 - Positional Reasoning\n51\n49.1\n\n\nQA18 - Size Reasoning\n52\n90.8\n\n\nQA19 - Path Finding\n8\n9.0\n\n\nQA20 - Agent’s Motivations\n91\n90.7\n\n\n\nFor the resources related to the bAbI project, refer to: https://research.facebook.com/researchers/1543934539189348\nNotes:\n\nWith default word, sentence, and query vector sizes, the GRU model achieves:\n100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU) In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\nThe task does not traditionally parse the question separately. This likely improves accuracy and is a good example of merging two RNNs.\nThe word vector embeddings are not shared between the story and question RNNs.\nSee how the accuracy changes given 10,000 training samples (en-10k) instead of only 1000. 1000 was used in order to be comparable to the original paper.\nExperiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\nThe length and noise (i.e. ‘useless’ story components) impact the ability for LSTMs / GRUs to provide the correct answer. Given only the supporting facts, these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural networks that use attentional processes can efficiently search through this noise to find the relevant statements, improving performance substantially. This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n\n\nlibrary(keras)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(dplyr)\n\n# Function definition -----------------------------------------------------\n\ntokenize_words <- function(x){\n  x <- x %>% \n    str_replace_all('([[:punct:]]+)', ' \\\\1') %>% \n    str_split(' ') %>%\n    unlist()\n  x[x != \"\"]\n}\n\nparse_stories <- function(lines, only_supporting = FALSE){\n  lines <- lines %>% \n    str_split(\" \", n = 2) %>%\n    map_df(~tibble(nid = as.integer(.x[[1]]), line = .x[[2]]))\n  \n  lines <- lines %>%\n    mutate(\n      split = map(line, ~str_split(.x, \"\\t\")[[1]]),\n      q = map_chr(split, ~.x[1]),\n      a = map_chr(split, ~.x[2]),\n      supporting = map(split, ~.x[3] %>% str_split(\" \") %>% unlist() %>% as.integer()),\n      story_id = c(0, cumsum(nid[-nrow(.)] > nid[-1]))\n    ) %>%\n    select(-split)\n  \n  stories <- lines %>%\n    filter(is.na(a)) %>%\n    select(nid_story = nid, story_id, story = q)\n  \n  questions <- lines %>%\n    filter(!is.na(a)) %>%\n    select(-line) %>%\n    left_join(stories, by = \"story_id\") %>%\n    filter(nid_story < nid)\n\n  if(only_supporting){\n    questions <- questions %>%\n      filter(map2_lgl(nid_story, supporting, ~.x %in% .y))\n  }\n    \n  questions %>%\n    group_by(story_id, nid, question = q, answer = a) %>%\n    summarise(story = paste(story, collapse = \" \")) %>%\n    ungroup() %>% \n    mutate(\n      question = map(question, ~tokenize_words(.x)),\n      story = map(story, ~tokenize_words(.x)),\n      id = row_number()\n    ) %>%\n    select(id, question, answer, story)\n}\n\nvectorize_stories <- function(data, vocab, story_maxlen, query_maxlen){\n  \n  questions <- map(data$question, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  stories <- map(data$story, function(x){\n    map_int(x, ~which(.x == vocab))\n  })\n  \n  # \"\" represents padding\n  answers <- sapply(c(\"\", vocab), function(x){\n    as.integer(x == data$answer)\n  })\n  \n\n  list(\n    questions = pad_sequences(questions, maxlen = query_maxlen),\n    stories   = pad_sequences(stories, maxlen = story_maxlen),\n    answers   = answers\n  )\n}\n\n# Parameters --------------------------------------------------------------\n\nmax_length <- 99999\nembed_hidden_size <- 50\nbatch_size <- 32\nepochs <- 40\n\n# Data Preparation --------------------------------------------------------\n\npath <- get_file(\n  fname = \"babi-tasks-v1-2.tar.gz\",\n  origin = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n)\nuntar(path, exdir = str_replace(path, fixed(\".tar.gz\"), \"/\"))\npath <- str_replace(path, fixed(\".tar.gz\"), \"/\")\n\n# Default QA1 with 1000 samples\n# challenge = '%stasks_1-20_v1-2/en/qa1_single-supporting-fact_%s.txt'\n# QA1 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_%s.txt'\n# QA2 with 1000 samples\nchallenge <- \"%stasks_1-20_v1-2/en/qa2_two-supporting-facts_%s.txt\"\n# QA2 with 10,000 samples\n# challenge = '%stasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_%s.txt'\n\ntrain <- read_lines(sprintf(challenge, path, \"train\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\ntest <- read_lines(sprintf(challenge, path, \"test\")) %>%\n  parse_stories() %>%\n  filter(map_int(story, ~length(.x)) <= max_length)\n\n# extract the vocabulary\nall_data <- bind_rows(train, test)\nvocab <- c(unlist(all_data$question), all_data$answer, \n           unlist(all_data$story)) %>%\n  unique() %>%\n  sort()\n\n# Reserve 0 for masking via pad_sequences\nvocab_size <- length(vocab) + 1\nstory_maxlen <- map_int(all_data$story, ~length(.x)) %>% max()\nquery_maxlen <- map_int(all_data$question, ~length(.x)) %>% max()\n\n# vectorized versions of training and test sets\ntrain_vec <- vectorize_stories(train, vocab, story_maxlen, query_maxlen)\ntest_vec <- vectorize_stories(test, vocab, story_maxlen, query_maxlen)\n\n# Defining the model ------------------------------------------------------\n\nsentence <- layer_input(shape = c(story_maxlen), dtype = \"int32\")\nencoded_sentence <- sentence %>% \n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\nquestion <- layer_input(shape = c(query_maxlen), dtype = \"int32\")\nencoded_question <- question %>%\n  layer_embedding(input_dim = vocab_size, output_dim = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_repeat_vector(n = story_maxlen)\n\nmerged <- list(encoded_sentence, encoded_question) %>%\n  layer_add() %>%\n  layer_lstm(units = embed_hidden_size) %>%\n  layer_dropout(rate = 0.3)\n\npreds <- merged %>%\n  layer_dense(units = vocab_size, activation = \"softmax\")\n\nmodel <- keras_model(inputs = list(sentence, question), outputs = preds)\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nmodel\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x = list(train_vec$stories, train_vec$questions),\n  y = train_vec$answers,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split=0.05\n)\n\nevaluation <- model %>% evaluate(\n  x = list(test_vec$stories, test_vec$questions),\n  y = test_vec$answers,\n  batch_size = batch_size\n)\n\nevaluation"
  },
  {
    "objectID": "examples/cifar10_cnn.html",
    "href": "examples/cifar10_cnn.html",
    "title": "cifar10_cnn",
    "section": "",
    "text": "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs, though it is still underfitting at that point.\n\nlibrary(keras)\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 32\nepochs <- 200\ndata_augmentation <- TRUE\n\n\n# Data Preparation --------------------------------------------------------\n\n# See ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n\n# Feature scale RGB values in test and train inputs  \nx_train <- cifar10$train$x/255\nx_test <- cifar10$test$x/255\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n\n# Defining Model ----------------------------------------------------------\n\n# Initialize sequential model\nmodel <- keras_model_sequential()\n\nmodel %>%\n \n  # Start with hidden 2D convolutional layer being fed 32x32 pixel images\n  layer_conv_2d(\n    filter = 32, kernel_size = c(3,3), padding = \"same\", \n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation(\"relu\") %>%\n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling once more\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # Flatten max filtered output into feature vector \n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(512) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10 unit output layer\n  layer_dense(10) %>%\n  layer_activation(\"softmax\")\n\nopt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = opt,\n  metrics = \"accuracy\"\n)\n\n\n# Training ----------------------------------------------------------------\n\nif(!data_augmentation){\n  \n  model %>% fit(\n    x_train, y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, y_test),\n    shuffle = TRUE\n  )\n  \n} else {\n  \n  datagen <- image_data_generator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    horizontal_flip = TRUE\n  )\n  \n  datagen %>% fit_image_data_generator(x_train)\n  \n  model %>% fit_generator(\n    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),\n    steps_per_epoch = as.integer(50000/batch_size), \n    epochs = epochs, \n    validation_data = list(x_test, y_test)\n  )\n  \n}"
  },
  {
    "objectID": "examples/cifar10_densenet.html",
    "href": "examples/cifar10_densenet.html",
    "title": "cifar10_densenet",
    "section": "",
    "text": "DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each dense block). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using less than half the amount of parameters and roughly half the number of FLOPs.\nFinal accuracy on test set was 0.9351 versus 0.9300 reported on the paper.\nBeside the keras package, you will need to install the densenet package. Installation instructions are available here.\n\n# Libraries ---------------------------------------------------------------\nlibrary(keras)\nlibrary(densenet)\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 64\nepochs <- 300\n\n# Data Preparation --------------------------------------------------------\n\n# see ?dataset_cifar10 for more info\ncifar10 <- dataset_cifar10()\n\n# Normalisation\nfor(i in 1:3){\n  mea <- mean(cifar10$train$x[,,,i])\n  sds <- sd(cifar10$train$x[,,,i])\n  \n  cifar10$train$x[,,,i] <- (cifar10$train$x[,,,i] - mea) / sds\n  cifar10$test$x[,,,i] <- (cifar10$test$x[,,,i] - mea) / sds\n}\nx_train <- cifar10$train$x\nx_test <- cifar10$test$x\n\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\n\n# Model Definition -------------------------------------------------------\n\ninput_img <- layer_input(shape = c(32, 32, 3))\nmodel <- application_densenet(include_top = TRUE, input_tensor = input_img, dropout_rate = 0.2)\n\nopt <- optimizer_sgd(lr = 0.1, momentum = 0.9, nesterov = TRUE)\n\nmodel %>% compile(\n  optimizer = opt,\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n# Model fitting -----------------------------------------------------------\n\n# callbacks for weights and learning rate\nlr_schedule <- function(epoch, lr) {\n  \n  if(epoch <= 150) {\n    0.1\n  } else if(epoch > 150 && epoch <= 225){\n    0.01\n  } else {\n    0.001\n  }\n\n}\n\nlr_reducer <- callback_learning_rate_scheduler(lr_schedule)\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  batch_size = batch_size, \n  epochs = epochs, \n  validation_data = list(x_test, y_test), \n  callbacks = list(\n    lr_reducer\n  )\n)\n\nplot(history)\n\nevaluate(model, x_test, y_test)"
  },
  {
    "objectID": "examples/conv_lstm.html",
    "href": "examples/conv_lstm.html",
    "title": "conv_lstm",
    "section": "",
    "text": "# This script demonstrates the use of a convolutional LSTM network.\n# This network is used to predict the next frame of an artificially\n# generated movie which contains moving squares.\n\nlibrary(keras)\nlibrary(abind)\nlibrary(raster)\n\n# Function Definition -----------------------------------------------------\n\ngenerate_movies <- function(n_samples = 1200, n_frames = 15){\n  \n  rows <- 80\n  cols <- 80\n  \n  noisy_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  shifted_movies <- array(0, dim = c(n_samples, n_frames, rows, cols))\n  \n  n <- sample(3:8, 1)\n  \n  for(s in 1:n_samples){\n    for(i in 1:n){\n      # Initial position\n      xstart <- sample(20:60, 1)\n      ystart <- sample(20:60, 1)\n      \n      # Direction of motion\n      directionx <- sample(-1:1, 1)\n      directiony <- sample(-1:1, 1)\n      \n      # Size of the square\n      w <- sample(2:3, 1)\n      \n      x_shift <- xstart + directionx*(0:(n_frames))\n      y_shift <- ystart + directiony*(0:(n_frames))\n      \n      for(t in 1:n_frames){\n        square_x <- (x_shift[t] - w):(x_shift[t] + w)\n        square_y <- (y_shift[t] - w):(y_shift[t] + w)\n        \n        noisy_movies[s, t, square_x, square_y] <- \n          noisy_movies[s, t, square_x, square_y] + 1\n        \n        # Make it more robust by adding noise. The idea is that if \n        # during inference, the value of the pixel is not exactly \n        # one; we need to train the network to be robust and still \n        # consider it as a pixel belonging to a square.\n        if(runif(1) > 0.5){\n          noise_f <- sample(c(-1, 1), 1)\n          \n          square_x_n <- (x_shift[t] - w - 1):(x_shift[t] + w + 1)\n          square_y_n <- (y_shift[t] - w - 1):(y_shift[t] + w + 1)\n          \n          noisy_movies[s, t, square_x_n, square_y_n] <- \n            noisy_movies[s, t, square_x_n, square_y_n] + noise_f*0.1\n          \n        }\n        \n        # Shift the ground truth by 1\n        square_x_s <- (x_shift[t+1] - w):(x_shift[t+1] + w)\n        square_y_s <- (y_shift[t+1] - w):(y_shift[t+1] + w)\n        \n        shifted_movies[s, t, square_x_s, square_y_s] <- \n          shifted_movies[s, t, square_x_s, square_y_s] + 1\n      }\n    }  \n  }\n  \n  # Cut to a 40x40 window\n  noisy_movies <- noisy_movies[,,21:60, 21:60]\n  shifted_movies = shifted_movies[,,21:60, 21:60]\n  \n  noisy_movies[noisy_movies > 1] <- 1\n  shifted_movies[shifted_movies > 1] <- 1\n\n  # Add channel dimension\n  noisy_movies <- array_reshape(noisy_movies, c(dim(noisy_movies), 1))\n  shifted_movies <- array_reshape(shifted_movies, c(dim(shifted_movies), 1))\n  \n  list(\n    noisy_movies = noisy_movies,\n    shifted_movies = shifted_movies\n  )\n}\n\n\n# Data Preparation --------------------------------------------------------\n\n# Artificial data generation:\n  # Generate movies with 3 to 7 moving squares inside.\n  # The squares are of shape 1x1 or 2x2 pixels, which move linearly over time.\n  # For convenience we first create movies with bigger width and height (80x80)\n  # and at the end we select a 40x40 window.\nmovies <- generate_movies(n_samples = 1000, n_frames = 15)\nmore_movies <- generate_movies(n_samples = 200, n_frames = 15)\n\n\n# Model definition --------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>%\n\n  # Begin with 2D convolutional LSTM layer\n  layer_conv_lstm_2d(\n    input_shape = list(NULL,40,40,1), \n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", \n    return_sequences = TRUE\n  ) %>%\n  # Normalize the activations of the previous layer\n  layer_batch_normalization() %>%\n  \n  # Add 3x hidden 2D convolutions LSTM layers, with\n  # batch normalization layers between\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>% \n  layer_conv_lstm_2d(\n    filters = 40, kernel_size = c(3,3),\n    padding = \"same\", return_sequences = TRUE\n  ) %>%\n  layer_batch_normalization() %>%\n  \n  # Add final 3D convolutional output layer \n  layer_conv_3d(\n    filters = 1, kernel_size = c(3,3,3),\n    activation = \"sigmoid\", \n    padding = \"same\", data_format =\"channels_last\"\n  )\n\n# Prepare model for training\nmodel %>% compile(\n  loss = \"binary_crossentropy\", \n  optimizer = \"adadelta\"\n)\n\nmodel\n\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  movies$noisy_movies,\n  movies$shifted_movies,\n  batch_size = 10,\n  epochs = 30, \n  validation_split = 0.05\n)\n\n\n# Visualization  ----------------------------------------------------------------\n\n# Testing the network on one movie\n# feed it with the first 7 positions and then\n# predict the new positions\n\n#Example to visualize on\nwhich <- 100\n\ntrack <- more_movies$noisy_movies[which,1:8,,,1]\ntrack <- array(track, c(1,8,40,40,1))\n\nfor (k in 1:15){\n  if (k<8){ \n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k)) \n    dev.off()\n  } else {\n    \n    # And then compare the predictions to the ground truth\n    png(paste0(k,'_animate.png'))\n    par(mfrow=c(1,2),bg = 'white')\n    (more_movies$noisy_movies[which,k,,,1])  %>% raster() %>% plot() %>% title (main=paste0('Ground_',k))\n    \n    # Make Prediction\n    new_pos <- model %>% predict(track)\n   \n    # Slice the last row  \n    new_pos_loc <- new_pos[1,k,1:40,1:40,1]  \n    new_pos_loc  %>% raster() %>% plot() %>% title (main=paste0('Pred_',k))    \n    \n    # Reshape it\n    new_pos <- array(new_pos_loc, c(1,1, 40,40,1))     \n    \n    # Bind it to the earlier data\n    track <- abind(track,new_pos,along = 2)  \n    dev.off()\n  }\n} \n\n# Can also create a gif by running\nsystem(\"convert -delay 40 *.png animation.gif\")"
  },
  {
    "objectID": "examples/deep_dream.html",
    "href": "examples/deep_dream.html",
    "title": "deep_dream",
    "section": "",
    "text": "library(keras)\n\n\n# Utility functions -------------------------------------------------------\n\n# Util function to open, resize, and format pictures into tensors that Inception V3 can process\npreprocess_image <- function(image_path) {\n  image_load(image_path) %>%\n    image_to_array() %>%\n    array_reshape(dim = c(1, dim(.))) %>%\n    inception_v3_preprocess_input()\n}\n\n# Util function to convert a tensor into a valid image\ndeprocess_image <- function(img) {\n  img <- array_reshape(img, dim = c(dim(img)[[2]], dim(img)[[3]], 3))\n  # Undoes preprocessing that was performed by `imagenet_preprocess_input`\n  img <- img / 2\n  img <- img + 0.5\n  img <- img * 255\n  \n  dims <- dim(img)\n  img <- pmax(0, pmin(img, 255))\n  dim(img) <- dims\n  img\n}\n\nresize_img <- function(img, size) {\n  image_array_resize(img, size[[1]], size[[2]])\n}\n\nsave_img <- function(img, fname) {\n  img <- deprocess_image(img)\n  image_array_save(img, fname)\n}\n\n\n# Model  ----------------------------------------------\n\n# You won't be training the model, so this command disables all training-specific operations.\nk_set_learning_phase(0)\n\n# Builds the Inception V3 network, without its convolutional base. The model will be loaded with pretrained ImageNet weights.\nmodel <- application_inception_v3(weights = \"imagenet\",\n                                  include_top = FALSE)\n\n# Named list mapping layer names to a coefficient quantifying how much the layer's activation contributes to the loss you'll seek to maximize. Note that the layer names are hardcoded in the built-in Inception V3 application. You can list all layer names using `summary(model)`.\nlayer_contributions <- list(\n  mixed2 = 0.2,\n  mixed3 = 3,\n  mixed4 = 2,\n  mixed5 = 1.5\n)\n\n# You'll define the loss by adding layer contributions to this scalar variable\nloss <- k_variable(0)\nfor (layer_name in names(layer_contributions)) {\n  coeff <- layer_contributions[[layer_name]]\n  # Retrieves the layer's output\n  activation <- get_layer(model, layer_name)$output\n  scaling <- k_prod(k_cast(k_shape(activation), \"float32\"))\n  # Retrieves the layer's output\n  loss <- loss + (coeff * k_sum(k_square(activation)) / scaling)\n}\n\n# Retrieves the layer's output\ndream <- model$input\n\n# Computes the gradients of the dream with regard to the loss\ngrads <- k_gradients(loss, dream)[[1]]\n\n# Normalizes the gradients (important trick)\ngrads <- grads / k_maximum(k_mean(k_abs(grads)), 1e-7)\n\noutputs <- list(loss, grads)\n\n# Sets up a Keras function to retrieve the value of the loss and gradients, given an input image\nfetch_loss_and_grads <- k_function(list(dream), outputs)\n\neval_loss_and_grads <- function(x) {\n  outs <- fetch_loss_and_grads(list(x))\n  loss_value <- outs[[1]]\n  grad_values <- outs[[2]]\n  list(loss_value, grad_values)\n}\n\n\n# Run gradient ascent -----------------------------------------------------\n\n# This function runs gradient ascent for a number of iterations.\ngradient_ascent <-\n  function(x, iterations, step, max_loss = NULL) {\n    for (i in 1:iterations) {\n      c(loss_value, grad_values) %<-% eval_loss_and_grads(x)\n      if (!is.null(max_loss) && loss_value > max_loss)\n        break\n      cat(\"...Loss value at\", i, \":\", loss_value, \"\\n\")\n      x <- x + (step * grad_values)\n    }\n    x\n  }\n\n# Playing with these hyperparameters will let you achieve new effects.\n# Gradient ascent step size\nstep <- 0.01\n# Number of scales at which to run gradient ascent\nnum_octave <- 3\n# Size ratio between scales\noctave_scale <- 1.4\n# Number of ascent steps to run at each scale\niterations <- 20\n# If the loss grows larger than 10, we will interrupt the gradient-ascent process to avoid ugly artifacts.\nmax_loss <- 10\n\n# Fill this with the path to the image you want to use.\nbase_image_path <- \"/tmp/mypic.jpg\"\n\n# Loads the base image into an array\nimg <-\n  preprocess_image(base_image_path)\n\n# Prepares a list of shape tuples defining the different scales at which to run gradient ascent\noriginal_shape <- dim(img)[-1]\nsuccessive_shapes <-\n  list(original_shape)\nfor (i in 1:num_octave) {\n  shape <- as.integer(original_shape / (octave_scale ^ i))\n  successive_shapes[[length(successive_shapes) + 1]] <-\n    shape\n}\n# Reverses the list of shapes so they're in increasing order\nsuccessive_shapes <-\n  rev(successive_shapes)\n\noriginal_img <- img\n#  Resizes the array of the image to the smallest scale\nshrunk_original_img <-\n  resize_img(img, successive_shapes[[1]])\n\nfor (shape in successive_shapes) {\n  cat(\"Processing image shape\", shape, \"\\n\")\n  # Scales up the dream image\n  img <- resize_img(img, shape)\n  # Runs gradient ascent, altering the dream\n  img <- gradient_ascent(img,\n                         iterations = iterations,\n                         step = step,\n                         max_loss = max_loss)\n  # Scales up the smaller version of the original image: it will be pixellated\n  upscaled_shrunk_original_img <-\n    resize_img(shrunk_original_img, shape)\n  # Computes the high-quality version of the original image at this size\n  same_size_original <-\n    resize_img(original_img, shape)\n  # The difference between the two is the detail that was lost when scaling up\n  lost_detail <-\n    same_size_original - upscaled_shrunk_original_img\n  # Reinjects lost detail into the dream\n  img <- img + lost_detail\n  shrunk_original_img <-\n    resize_img(original_img, shape)\n  save_img(img, fname = sprintf(\"dream_at_scale_%s.png\",\n                                paste(shape, collapse = \"x\")))\n}"
  },
  {
    "objectID": "examples/eager_dcgan.html",
    "href": "examples/eager_dcgan.html",
    "title": "eager_dcgan",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nuse_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\n\nmnist <- dataset_mnist()\nc(train_images, train_labels) %<-% mnist$train\n\ntrain_images <- train_images %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- (train_images - 127.5) / 127.5\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- (buffer_size / batch_size) %>% round()\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      \n      self$conv1 <-\n        layer_conv_2d_transpose(\n          filters = 64,\n          kernel_size = c(5, 5),\n          strides = c(1, 1),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm2 <- layer_batch_normalization()\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      \n      self$conv2 <-\n        layer_conv_2d_transpose(\n          filters = 32,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm3 <- layer_batch_normalization()\n      self$leaky_relu3 <- layer_activation_leaky_relu()\n      \n      self$conv3 <-\n        layer_conv_2d_transpose(\n          filters = 1,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE,\n          activation = \"tanh\"\n        )\n      \n      function(inputs,\n               mask = NULL,\n               training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          self$leaky_relu1() %>%\n          k_reshape(shape = c(-1, 7, 7, 64)) %>%\n          \n          self$conv1() %>%\n          self$batchnorm2(training = training) %>%\n          self$leaky_relu2() %>%\n          \n          self$conv2() %>%\n          self$batchnorm3(training = training) %>%\n          self$leaky_relu3() %>%\n          \n          self$conv3()\n      }\n    })\n  }\n\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$conv1 <- layer_conv_2d(\n        filters = 64,\n        kernel_size = c(5, 5),\n        strides = c(2, 2),\n        padding = \"same\"\n      )\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$dropout <- layer_dropout(rate = 0.3)\n      \n      self$conv2 <-\n        layer_conv_2d(\n          filters = 128,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\"\n        )\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$flatten <- layer_flatten()\n      self$fc1 <- layer_dense(units = 1)\n      \n      function(inputs,\n               mask = NULL,\n               training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          self$dropout(training = training) %>%\n          self$conv2() %>%\n          self$leaky_relu2() %>%\n          self$flatten() %>%\n          self$fc1()\n        \n      }\n    })\n  }\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = k_ones_like(real_output),\n                                    logits = real_output)\n  generated_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = k_zeros_like(generated_output),\n                                    logits = generated_output)\n  real_loss + generated_loss\n}\n\ngenerator_loss <- function(generated_output) {\n  tf$losses$sigmoid_cross_entropy(tf$ones_like(generated_output), generated_output)\n}\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(1e-4)\ngenerator_optimizer <- tf$train$AdamOptimizer(1e-4)\n\nnum_epochs <- 150\nnoise_dim <- 100\nnum_examples_to_generate <- 25L\n\nrandom_vector_for_generation <-\n  k_random_normal(c(num_examples_to_generate,\n                    noise_dim))\n\ngenerate_and_save_images <- function(model, epoch, test_input) {\n  predictions <- model(test_input, training = FALSE)\n  png(paste0(\"images_epoch_\", epoch, \".png\"))\n  par(mfcol = c(5, 5))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:25) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\ntrain <- function(dataset, epochs, noise_dim) {\n  for (epoch in seq_len(num_epochs)) {\n    start <- Sys.time()\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      noise <- k_random_normal(c(batch_size, noise_dim))\n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          generated_images <- generator(noise)\n          disc_real_output <- discriminator(batch, training = TRUE)\n          disc_generated_output <-\n            discriminator(generated_images, training = TRUE)\n          gen_loss <- generator_loss(disc_generated_output)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n        })\n      })\n      \n      gradients_of_generator <-\n        gen_tape$gradient(gen_loss, generator$variables)\n      gradients_of_discriminator <-\n        disc_tape$gradient(disc_loss, discriminator$variables)\n      \n      generator_optimizer$apply_gradients(purrr::transpose(list(\n        gradients_of_generator, generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_discriminator, discriminator$variables)\n      ))\n      \n      total_loss_gen <- total_loss_gen + gen_loss\n      total_loss_disc <- total_loss_disc + disc_loss\n      \n    })\n    \n    cat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    if (epoch %% 10 == 0)\n      generate_and_save_images(generator,\n                               epoch,\n                               random_vector_for_generation)\n    \n  }\n}\n\ntrain(train_dataset, num_epochs, noise_dim)"
  },
  {
    "objectID": "examples/eager_image_captioning.html",
    "href": "examples/eager_image_captioning.html",
    "title": "eager_image_captioning",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nnp <- import(\"numpy\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(rjson)\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(magick)\n\nmaybecat <- function(context, x) {\n  if (debugshapes) {\n    name <- enexpr(x)\n    dims <- paste0(dim(x), collapse = \" \")\n    cat(context, \": shape of \", name, \": \", dims, \"\\n\", sep = \"\")\n  }\n}\n\ndebugshapes <- FALSE\nrestore_checkpoint <- FALSE\nsaved_features_exist <- FALSE\n\nuse_session_with_seed(7777,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\nannotation_file <- \"train2014/annotations/captions_train2014.json\"\nimage_path <- \"train2014/train2014\"\n\nannotations <- fromJSON(file = annotation_file)\n\nannot_captions <- annotations[[4]]\n# 414113\nnum_captions <- length(annot_captions)\n\nall_captions <- vector(mode = \"list\", length = num_captions)\nall_img_names <- vector(mode = \"list\", length = num_captions)\n\nfor (i in seq_len(num_captions)) {\n  caption <-\n    paste0(\"<start> \", annot_captions[[i]][[\"caption\"]], \" <end>\")\n  image_id <- annot_captions[[i]][[\"image_id\"]]\n  full_coco_image_path <-\n    sprintf(\"train2014/train2014/COCO_train2014_%012d.jpg\", image_id)\n  all_img_names[[i]] <- full_coco_image_path\n  all_captions[[i]] <- caption\n}\n\nnum_examples <- 30000\n\nif (!saved_features_exist) {\n  random_sample <- sample(1:num_captions, size = num_examples)\n  train_indices <-\n    sample(random_sample, size = length(random_sample) * 0.8)\n  validation_indices <-\n    setdiff(random_sample, train_indices)\n  saveRDS(random_sample,\n          paste0(\"random_sample_\", num_examples, \".rds\"))\n  saveRDS(train_indices,\n          paste0(\"train_indices_\", num_examples, \".rds\"))\n  saveRDS(validation_indices,\n          paste0(\"validation_indices_\", num_examples, \".rds\"))\n} else {\n  random_sample <-\n    readRDS(paste0(\"random_sample_\", num_examples, \".rds\"))\n  train_indices <-\n    readRDS(paste0(\"train_indices_\", num_examples, \".rds\"))\n  validation_indices <-\n    readRDS(paste0(\"validation_indices_\", num_examples, \".rds\"))\n}\n\nsample_captions <- all_captions[random_sample]\nsample_images <- all_img_names[random_sample]\ntrain_captions <- all_captions[train_indices]\ntrain_images <- all_img_names[train_indices]\nvalidation_captions <- all_captions[validation_indices]\nvalidation_images <- all_img_names[validation_indices]\n\n\nload_image <- function(image_path) {\n  img <- tf$read_file(image_path) %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize_images(c(299L, 299L)) %>%\n    tf$keras$applications$inception_v3$preprocess_input()\n  list(img, image_path)\n}\n\n\nimage_model <- application_inception_v3(include_top = FALSE,\n                                        weights = \"imagenet\")\n\nif (!saved_features_exist) {\n  preencode <- unique(sample_images) %>% unlist() %>% sort()\n  num_unique <- length(preencode)\n  \n  batch_size_4save <- 1\n  image_dataset <- tensor_slices_dataset(preencode) %>%\n    dataset_map(load_image) %>%\n    dataset_batch(batch_size_4save)\n  \n  save_iter <- make_iterator_one_shot(image_dataset)\n  save_count <- 0\n  \n  until_out_of_range({\n    if (save_count %% 100 == 0) {\n      cat(\"Saving feature:\", save_count, \"of\", num_unique, \"\\n\")\n    }\n    save_count <- save_count + batch_size_4save\n    batch_4save <- save_iter$get_next()\n    img <- batch_4save[[1]]\n    path <- batch_4save[[2]]\n    batch_features <- image_model(img)\n    batch_features <- tf$reshape(batch_features,\n                                 list(dim(batch_features)[1],-1L, dim(batch_features)[4]))\n    for (i in 1:dim(batch_features)[1]) {\n      p <- path[i]$numpy()$decode(\"utf-8\")\n      np$save(p,\n              batch_features[i, ,]$numpy())\n      \n    }\n    \n  })\n}\n\ntop_k <- 5000\ntokenizer <- text_tokenizer(num_words = top_k,\n                            oov_token = \"<unk>\",\n                            filters = '!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~ ')\ntokenizer$fit_on_texts(sample_captions)\ntrain_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(train_captions)\nvalidation_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(validation_captions)\ntokenizer$word_index\n\ntokenizer$word_index[\"<unk>\"]\n\ntokenizer$word_index[\"<pad>\"] <- 0\ntokenizer$word_index[\"<pad>\"]\n\nword_index_df <- data.frame(\n  word = tokenizer$word_index %>% names(),\n  index = tokenizer$word_index %>% unlist(use.names = FALSE),\n  stringsAsFactors = FALSE\n)\n\nword_index_df <- word_index_df %>% arrange(index)\n\ndecode_caption <- function(text) {\n  paste(map(text, function(number)\n    word_index_df %>%\n      filter(index == number) %>%\n      select(word) %>%\n      pull()),\n    collapse = \" \")\n}\n\ncaption_lengths <-\n  map(all_captions[1:num_examples], function(c)\n    str_split(c, \" \")[[1]] %>% length()) %>% unlist()\nfivenum(caption_lengths)\nmax_length <- fivenum(caption_lengths)[5]\n\ntrain_captions_padded <-\n  pad_sequences(\n    train_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\nvalidation_captions_padded <-\n  pad_sequences(\n    validation_captions_tokenized,\n    maxlen = max_length,\n    padding = \"post\",\n    truncating = \"post\"\n  )\n\nlength(train_images)\ndim(train_captions_padded)\n\nbatch_size <- 10\nbuffer_size <- num_examples\nembedding_dim <- 256\ngru_units <- 512\nvocab_size <- top_k\nfeatures_shape <- 2048\nattention_features_shape <- 64\n\ntrain_images_4checking <- train_images[c(4, 10, 30)]\ntrain_captions_4checking <- train_captions_padded[c(4, 10, 30),]\nvalidation_images_4checking <- validation_images[c(7, 10, 12)]\nvalidation_captions_4checking <-\n  validation_captions_padded[c(7, 10, 12),]\n\n\nmap_func <- function(img_name, cap) {\n  p <- paste0(img_name$decode(\"utf-8\"), \".npy\")\n  img_tensor <- np$load(p)\n  img_tensor <- tf$cast(img_tensor, tf$float32)\n  list(img_tensor, cap)\n}\n\ntrain_dataset <-\n  tensor_slices_dataset(list(train_images, train_captions_padded)) %>%\n  dataset_map(function(item1, item2)\n    tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))) %>%\n  # dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size) \n\n\ncnn_encoder <-\n  function(embedding_dim,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$fc <-\n        layer_dense(units = embedding_dim, activation = \"relu\")\n      \n      function(x, mask = NULL) {\n        # input shape: (batch_size, 64, features_shape)\n        # shape after fc: (batch_size, 64, embedding_dim)\n        maybecat(\"encoder input\", x)\n        x <- self$fc(x)\n        maybecat(\"encoder output\", x)\n        x\n      }\n    })\n  }\n\nattention_module <-\n  function(gru_units,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$W1 = layer_dense(units = gru_units)\n      self$W2 = layer_dense(units = gru_units)\n      self$V = layer_dense(units = 1)\n      \n      function(inputs, mask = NULL) {\n        features <- inputs[[1]]\n        hidden <- inputs[[2]]\n        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n        # hidden shape == (batch_size, gru_units)\n        # hidden_with_time_axis shape == (batch_size, 1, gru_units)\n        hidden_with_time_axis <- k_expand_dims(hidden, axis = 2)\n        \n        maybecat(\"attention module\", features)\n        maybecat(\"attention module\", hidden)\n        maybecat(\"attention module\", hidden_with_time_axis)\n        \n        # score shape == (batch_size, 64, 1)\n        score <-\n          self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))\n        # attention_weights shape == (batch_size, 64, 1)\n        attention_weights <- k_softmax(score, axis = 2)\n        # context_vector shape after sum == (batch_size, embedding_dim)\n        context_vector <-\n          k_sum(attention_weights * features, axis = 2)\n        \n        maybecat(\"attention module\", score)\n        maybecat(\"attention module\", attention_weights)\n        maybecat(\"attention module\", context_vector)\n        \n        list(context_vector, attention_weights)\n      }\n    })\n  }\n\nrnn_decoder <-\n  function(embedding_dim,\n           gru_units,\n           vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru_units <- gru_units\n      self$embedding <-\n        layer_embedding(input_dim = vocab_size, output_dim = embedding_dim)\n      self$gru <- if (tf$test$is_gpu_available()) {\n        layer_cudnn_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      } else {\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE,\n          recurrent_initializer = 'glorot_uniform'\n        )\n      }\n      \n      self$fc1 <- layer_dense(units = self$gru_units)\n      self$fc2 <- layer_dense(units = vocab_size)\n      \n      self$attention <- attention_module(self$gru_units)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        features <- inputs[[2]]\n        hidden <- inputs[[3]]\n        \n        maybecat(\"decoder\", x)\n        maybecat(\"decoder\", features)\n        maybecat(\"decoder\", hidden)\n        \n        c(context_vector, attention_weights) %<-% self$attention(list(features, hidden))\n        \n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x <- self$embedding(x)\n        \n        maybecat(\"decoder x after embedding\", x)\n        \n        # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)\n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x))\n        \n        maybecat(\"decoder x after concat\", x)\n        \n        # passing the concatenated vector to the GRU\n        c(output, state) %<-% self$gru(x)\n        \n        maybecat(\"decoder output after gru\", output)\n        maybecat(\"decoder state after gru\", state)\n        \n        # shape == (batch_size, 1, gru_units)\n        x <- self$fc1(output)\n        \n        maybecat(\"decoder output after fc1\", x)\n        \n        # x shape == (batch_size, gru_units)\n        x <- k_reshape(x, c(-1, dim(x)[[3]]))\n        \n        maybecat(\"decoder output after reshape\", x)\n        \n        # output shape == (batch_size, vocab_size)\n        x <- self$fc2(x)\n        \n        maybecat(\"decoder output after fc2\", x)\n        \n        list(x, state, attention_weights)\n        \n      }\n    })\n  }\n\n\nencoder <- cnn_encoder(embedding_dim)\ndecoder <- rnn_decoder(embedding_dim, gru_units, vocab_size)\n\noptimizer = tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- 1 - k_cast(y_true == 0L, dtype = \"float32\")\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true, logits =\n                                                     y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\nget_caption <-\n  function(image) {\n    attention_matrix <-\n      matrix(0, nrow = max_length, ncol = attention_features_shape)\n    # shape=(1, 299, 299, 3)\n    temp_input <- k_expand_dims(load_image(image)[[1]], 1)\n    # shape=(1, 8, 8, 2048),\n    img_tensor_val <- image_model(temp_input)\n    # shape=(1, 64, 2048)\n    img_tensor_val <- k_reshape(img_tensor_val,\n                                list(dim(img_tensor_val)[1],-1, dim(img_tensor_val)[4]))\n    # shape=(1, 64, 256)\n    features <- encoder(img_tensor_val)\n    \n    dec_hidden <- k_zeros(c(1, gru_units))\n    dec_input <-\n      k_expand_dims(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]))\n    \n    result <- \"\"\n    \n    for (t in seq_len(max_length - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, features, dec_hidden))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t, ] <- attention_weights %>% as.double()\n      \n      pred_idx = tf$multinomial(exp(preds), num_samples = 1)[1, 1] %>% as.double()\n      \n      pred_word <-\n        word_index_df[word_index_df$index == pred_idx, \"word\"]\n      \n      if (pred_word == \"<end>\") {\n        result <-\n          paste(result, pred_word)\n        attention_matrix <-\n          attention_matrix[1:length(str_split(result, \" \")[[1]]), , drop = FALSE]\n        return (list(str_trim(result), attention_matrix))\n      } else {\n        result <-\n          paste(result, pred_word)\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    \n    list(str_trim(result), attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           image_name,\n           result,\n           epoch) {\n    image <-\n      image_read(image_name) %>% image_scale(\"299x299!\")\n    result <- str_split(result, \" \")[[1]] %>% as.list()\n    # attention_matrix shape: nrow = max_length, ncol = attention_features_shape\n    for (i in 1:length(result)) {\n      att <- attention_matrix[i, ] %>% np$resize(tuple(8L, 8L))\n      dim(att) <- c(8, 8, 1)\n      att <- image_read(att) %>% image_scale(\"299x299\") %>%\n        image_annotate(\n          result[[i]],\n          gravity = \"northeast\",\n          size = 20,\n          color = \"white\",\n          location = \"+20+40\"\n        )\n      overlay <-\n        image_composite(att, image, operator = \"blend\", compose_args = \"30\")\n      image_write(\n        overlay,\n        paste0(\n          \"attention_plot_epoch_\",\n          epoch,\n          \"_img_\",\n          image_name %>% basename() %>% str_sub(16,-5),\n          \"_word_\",\n          i,\n          \".png\"\n        )\n      )\n    }\n  }\n\n\ncheck_sample_captions <-\n  function(epoch, mode, plot_attention) {\n    images <- switch(mode,\n                     training = train_images_4checking,\n                     validation = validation_images_4checking)\n    captions <- switch(mode,\n                       training = train_captions_4checking,\n                       validation = validation_captions_4checking)\n    cat(\"\\n\", \"Sample checks on \", mode, \" set:\", \"\\n\", sep = \"\")\n    for (i in 1:length(images)) {\n      c(result, attention_matrix) %<-% get_caption(images[[i]])\n      real_caption <-\n        decode_caption(captions[i,]) %>% str_remove_all(\" <pad>\")\n      cat(\"\\nReal caption:\",  real_caption, \"\\n\")\n      cat(\"\\nPredicted caption:\", result, \"\\n\")\n      if (plot_attention)\n        plot_attention(attention_matrix, images[[i]], result, epoch)\n    }\n    \n  }\n\ncheckpoint_dir <- \"./checkpoints_captions\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(optimizer = optimizer,\n                      encoder = encoder,\n                      decoder = decoder)\n\n\nif (restore_checkpoint) {\n  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n}\n\nnum_epochs <- 20\n\nif (!restore_checkpoint) {\n  for (epoch in seq_len(num_epochs)) {\n    cat(\"Starting epoch:\", epoch, \"\\n\")\n    total_loss <- 0\n    progress <- 0\n    train_iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      progress <- progress + 1\n      if (progress %% 10 == 0)\n        cat(\"-\")\n      \n      batch <- iterator_get_next(train_iter)\n      loss <- 0\n\n      img_tensor <- batch[[1]]\n      target_caption <- batch[[2]]\n      \n      dec_hidden <- k_zeros(c(batch_size, gru_units))\n      \n      dec_input <-\n        k_expand_dims(rep(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]), batch_size))\n      \n      with(tf$GradientTape() %as% tape, {\n        features <- encoder(img_tensor)\n        \n        for (t in seq_len(dim(target_caption)[2] - 1)) {\n          c(preds, dec_hidden, weights) %<-%\n            decoder(list(dec_input, features, dec_hidden))\n          loss <- loss + cx_loss(target_caption[, t], preds)\n          dec_input <- k_expand_dims(target_caption[, t])\n        }\n        \n      })\n      total_loss <-\n        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])\n      \n      variables <- c(encoder$variables, decoder$variables)\n      gradients <- tape$gradient(loss, variables)\n      \n      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),\n                                global_step = tf$train$get_or_create_global_step())\n    })\n    cat(paste0(\n      \"\\n\\nTotal loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n      \"\\n\"\n    ))\n    \n    \n    checkpoint$save(file_prefix = checkpoint_prefix)\n    \n    check_sample_captions(epoch, \"training\", plot_attention = FALSE)\n    check_sample_captions(epoch, \"validation\", plot_attention = FALSE)\n    \n  }\n}\n\n\nepoch <- num_epochs\ncheck_sample_captions(epoch, \"training\", plot_attention = TRUE)\ncheck_sample_captions(epoch, \"validation\", plot_attention = TRUE)"
  },
  {
    "objectID": "examples/eager_pix2pix.html",
    "href": "examples/eager_pix2pix.html",
    "title": "eager_pix2pix",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\n\nrestore <- TRUE\n\ndata_dir <- \"facades\"\n\nbuffer_size <- 400\nbatch_size <- 1\nbatches_per_epoch <- buffer_size / batch_size\nimg_width <- 256L\nimg_height <- 256L\n\nload_image <- function(image_file, is_train) {\n\n  image <- tf$read_file(image_file)\n  image <- tf$image$decode_jpeg(image)\n  \n  w <- as.integer(k_shape(image)[2])\n  w2 <- as.integer(w / 2L)\n  real_image <- image[ , 1L:w2, ]\n  input_image <- image[ , (w2 + 1L):w, ]\n  \n  input_image <- k_cast(input_image, tf$float32)\n  real_image <- k_cast(real_image, tf$float32)\n\n  if (is_train) {\n      input_image <-\n      tf$image$resize_images(input_image,\n                             c(286L, 286L),\n                             align_corners = TRUE,\n                             method = 2)\n    real_image <- tf$image$resize_images(real_image,\n                                         c(286L, 286L),\n                                         align_corners = TRUE,\n                                         method = 2)\n    \n    stacked_image <-\n      k_stack(list(input_image, real_image), axis = 1)\n    cropped_image <-\n      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))\n    c(input_image, real_image) %<-% list(cropped_image[1, , , ], cropped_image[2, , , ])\n    \n    if (runif(1) > 0.5) {\n      input_image <- tf$image$flip_left_right(input_image)\n      real_image <- tf$image$flip_left_right(real_image)\n    }\n  } else {\n    input_image <-\n      tf$image$resize_images(\n        input_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n    real_image <-\n      tf$image$resize_images(\n        real_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n  }\n  \n  input_image <- (input_image / 127.5) - 1\n  real_image <- (real_image / 127.5) - 1\n  \n  list(input_image, real_image)\n}\n\ntrain_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"train/*.jpg\")) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"test/*.jpg\")) %>%\n  dataset_map(function(image)\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))) %>%\n  dataset_batch(batch_size)\n\n\ndownsample <- function(filters,\n                       size,\n                       apply_batchnorm = TRUE,\n                       name = \"downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      cat(\"downsample (generator) output: \", x$shape$as_list(), \"\\n\")\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\nupsample <- function(filters,\n                     size,\n                     apply_dropout = FALSE,\n                     name = \"upsample\") {\n  keras_model_custom(name = NULL, function(self) {\n    self$apply_dropout <- apply_dropout\n    self$up_conv <- layer_conv_2d_transpose(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    if (self$apply_dropout) {\n      self$dropout <- layer_dropout(rate = 0.5)\n    }\n    function(xs,\n             mask = NULL,\n             training = TRUE) {\n      c(x1, x2) %<-% xs\n      x <- self$up_conv(x1) %>% self$batchnorm(training = training)\n      if (self$apply_dropout) {\n        x %>% self$dropout(training = training)\n      }\n      x %>% layer_activation(\"relu\")\n      concat <- k_concatenate(list(x, x2))\n      cat(\"upsample (generator) output: \", concat$shape$as_list(), \"\\n\")\n      concat\n    }\n  })\n}\n\ngenerator <- function(name = \"generator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- downsample(64, 4, apply_batchnorm = FALSE)\n    self$down2 <- downsample(128, 4)\n    self$down3 <- downsample(256, 4)\n    self$down4 <- downsample(512, 4)\n    self$down5 <- downsample(512, 4)\n    self$down6 <- downsample(512, 4)\n    self$down7 <- downsample(512, 4)\n    self$down8 <- downsample(512, 4)\n    \n    self$up1 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up2 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up3 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up4 <- upsample(512, 4)\n    self$up5 <- upsample(256, 4)\n    self$up6 <- upsample(128, 4)\n    self$up7 <- upsample(64, 4)\n    self$last <- layer_conv_2d_transpose(\n      filters = 3,\n      kernel_size = 4,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      activation = \"tanh\"\n    )\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      # x shape == (bs, 256, 256, 3)\n      x1 <-\n        x %>% self$down1(training = training)  # (bs, 128, 128, 64)\n      x2 <- self$down2(x1, training = training) # (bs, 64, 64, 128)\n      x3 <- self$down3(x2, training = training) # (bs, 32, 32, 256)\n      x4 <- self$down4(x3, training = training) # (bs, 16, 16, 512)\n      x5 <- self$down5(x4, training = training) # (bs, 8, 8, 512)\n      x6 <- self$down6(x5, training = training) # (bs, 4, 4, 512)\n      x7 <- self$down7(x6, training = training) # (bs, 2, 2, 512)\n      x8 <- self$down8(x7, training = training) # (bs, 1, 1, 512)\n\n      x9 <-\n        self$up1(list(x8, x7), training = training) # (bs, 2, 2, 1024)\n      x10 <-\n        self$up2(list(x9, x6), training = training) # (bs, 4, 4, 1024)\n      x11 <-\n        self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)\n      x12 <-\n        self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)\n      x13 <-\n        self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)\n      x14 <-\n        self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)\n      x15 <-\n        self$up7(list(x14, x1), training = training) # (bs, 128, 128, 128)\n      x16 <- self$last(x15) # (bs, 256, 256, 3)\n      cat(\"generator output: \", x16$shape$as_list(), \"\\n\")\n      x16\n    }\n  })\n}\n\n\ndisc_downsample <- function(filters,\n                            size,\n                            apply_batchnorm = TRUE,\n                            name = \"disc_downsample\") {\n  keras_model_custom(name = name, function(self) {\n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x,\n             mask = NULL,\n             training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n    \n  })\n}\n\ndiscriminator <- function(name = \"discriminator\") {\n  keras_model_custom(name = name, function(self) {\n    self$down1 <- disc_downsample(64, 4, FALSE)\n    self$down2 <- disc_downsample(128, 4)\n    self$down3 <- disc_downsample(256, 4)\n    # we are zero padding here with 1 because we need our shape to\n    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n    self$zero_pad1 <- layer_zero_padding_2d()\n    self$conv <- layer_conv_2d(\n      filters = 512,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    self$zero_pad2 <- layer_zero_padding_2d()\n    self$last <- layer_conv_2d(\n      filters = 1,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal()\n    )\n    \n    function(x,\n             y,\n             mask = NULL,\n             training = TRUE) {\n      x <- k_concatenate(list(x, y)) %>% # (bs, 256, 256, channels*2)\n        self$down1(training = training) %>% # (bs, 128, 128, 64)\n        self$down2(training = training) %>% # (bs, 64, 64, 128)\n        self$down3(training = training) %>% # (bs, 32, 32, 256)\n        self$zero_pad1() %>% # (bs, 34, 34, 256)\n        self$conv() %>% # (bs, 31, 31, 512)\n        self$batchnorm(training = training) %>%\n        layer_activation_leaky_relu() %>%\n        self$zero_pad2() %>% # (bs, 33, 33, 512)\n        self$last() # (bs, 30, 30, 1)\n      cat(\"discriminator output: \", x$shape$as_list(), \"\\n\")\n      x\n    }\n  })\n  \n}\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$ones_like(real_output),\n                                    logits = real_output)\n  generated_loss <-\n    tf$losses$sigmoid_cross_entropy(multi_class_labels = tf$zeros_like(generated_output),\n                                    logits = generated_output)\n  real_loss + generated_loss\n}\n\nlambda <- 100\ngenerator_loss <-\n  function(disc_judgment, generated_output, target) {\n    gan_loss <-\n      tf$losses$sigmoid_cross_entropy(tf$ones_like(disc_judgment), disc_judgment)\n    l1_loss <- tf$reduce_mean(tf$abs(target - generated_output))\n    gan_loss + (lambda * l1_loss)\n  }\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\ngenerator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\n\ncheckpoint_dir <- \"./checkpoints_pix2pix\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer,\n    generator = generator,\n    discriminator = discriminator\n  )\n\ngenerate_images <- function(generator, input, target, id) {\n  prediction <- generator(input, training = TRUE)\n  png(paste0(\"pix2pix_\", id, \".png\"), width = 900, height = 300)\n  par(mfcol = c(1, 3))\n  par(mar = c(0, 0, 0, 0),\n      xaxs = 'i',\n      yaxs = 'i')\n  input <- input[1, , ,]$numpy() * 0.5 + 0.5\n  input[input > 1] <- 1\n  input[input < 0] <- 0\n  plot(as.raster(input, main = \"input image\"))\n  target <- target[1, , ,]$numpy() * 0.5 + 0.5\n  target[target > 1] <- 1\n  target[target < 0] <- 0\n  plot(as.raster(target, main = \"ground truth\"))\n  prediction <- prediction[1, , ,]$numpy() * 0.5 + 0.5\n  prediction[prediction > 1] <- 1\n  prediction[prediction < 0] <- 0\n  plot(as.raster(prediction, main = \"generated\"))\n  dev.off()\n}\n\ntrain <- function(dataset, num_epochs) {\n  for (epoch in 1:num_epochs) {\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      input_image <- batch[[1]]\n      target <- batch[[2]]\n      \n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          gen_output <- generator(input_image, training = TRUE)\n          disc_real_output <-\n            discriminator(input_image, target, training = TRUE)\n          disc_generated_output <-\n            discriminator(input_image, gen_output, training = TRUE)\n          gen_loss <-\n            generator_loss(disc_generated_output, gen_output, target)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n          total_loss_gen <- total_loss_gen + gen_loss\n          total_loss_disc <- total_loss_disc + disc_loss\n        })\n      })\n      generator_gradients <- gen_tape$gradient(gen_loss,\n                                               generator$variables)\n      discriminator_gradients <- disc_tape$gradient(disc_loss,\n                                                    discriminator$variables)\n      \n      generator_optimizer$apply_gradients(transpose(list(\n        generator_gradients,\n        generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(transpose(\n        list(discriminator_gradients,\n             discriminator$variables)\n      ))\n      \n    })\n    cat(\"Epoch \", epoch, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    if (epoch %% 10 == 0) {\n      test_iter <- make_iterator_one_shot(test_dataset)\n      batch <- iterator_get_next(test_iter)\n      input <- batch[[1]]\n      target <- batch[[2]]\n      generate_images(generator, input, target, paste0(\"epoch_\", i))\n    }\n    if (epoch %% 10 == 0) {\n      checkpoint$save(file_prefix = checkpoint_prefix)\n    }\n    \n  }\n}\n\nif (!restore) {\n  train(train_dataset, 200)\n} \n\n\ncheckpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n\ntest_iter <- make_iterator_one_shot(test_dataset)\ni <- 1\nuntil_out_of_range({\n  batch <- iterator_get_next(test_iter)\n  input <- batch[[1]]\n  target <- batch[[2]]\n  generate_images(generator, input, target, paste0(\"test_\", i))\n  i <- i + 1\n})"
  },
  {
    "objectID": "examples/eager_styletransfer.html",
    "href": "examples/eager_styletransfer.html",
    "title": "eager_styletransfer",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-09-09-eager-style-transfer\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nuse_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(purrr)\nlibrary(glue)\n\nimg_shape <- c(128, 128, 3)\ncontent_path <- \"isar.jpg\"\nstyle_path <- \"The_Great_Wave_off_Kanagawa.jpg\"\n\n\nnum_iterations <- 2000\ncontent_weight <- 100\nstyle_weight <- 0.8\ntotal_variation_weight <- 0.01\n\ncontent_image <-\n  image_load(content_path, target_size = img_shape[1:2])\ncontent_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\nstyle_image <-\n  image_load(style_path, target_size = img_shape[1:2])\nstyle_image %>% image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%  plot()\n\n\nload_and_process_image <- function(path) {\n  img <- image_load(path, target_size = img_shape[1:2]) %>%\n    image_to_array() %>%\n    k_expand_dims(axis = 1) %>%\n    imagenet_preprocess_input()\n}\n\ndeprocess_image <- function(x) {\n  x <- x[1, , ,]\n  # Remove zero-center by mean pixel\n  x[, , 1] <- x[, , 1] + 103.939\n  x[, , 2] <- x[, , 2] + 116.779\n  x[, , 3] <- x[, , 3] + 123.68\n  # 'BGR'->'RGB'\n  x <- x[, , c(3, 2, 1)]\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x) / 255\n  x\n}\n\ncontent_layers <- c(\"block5_conv2\")\nstyle_layers = c(\"block1_conv1\",\n                 \"block2_conv1\",\n                 \"block3_conv1\",\n                 \"block4_conv1\",\n                 \"block5_conv1\")\nnum_content_layers <- length(content_layers)\nnum_style_layers <- length(style_layers)\n\nget_model <- function() {\n  vgg <- application_vgg19(include_top = FALSE, weights = \"imagenet\")\n  vgg$trainable <- FALSE\n  style_outputs <-\n    map(style_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  content_outputs <-\n    map(content_layers, function(layer)\n      vgg$get_layer(layer)$output)\n  model_outputs <- c(style_outputs, content_outputs)\n  keras_model(vgg$input, model_outputs)\n}\n\ncontent_loss <- function(content_image, target) {\n  k_sum(k_square(target - content_image))\n}\n\ngram_matrix <- function(x) {\n  features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))\n  gram <- k_dot(features, k_transpose(features))\n  gram\n}\n\nstyle_loss <- function(gram_target, combination) {\n  gram_comb <- gram_matrix(combination)\n  k_sum(k_square(gram_target - gram_comb)) / (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^\n                                                2)\n}\n\ntotal_variation_loss <- function(image) {\n  y_ij  <- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]\n  y_i1j <- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]\n  y_ij1 <- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]\n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\nget_feature_representations <-\n  function(model, content_path, style_path) {\n    # dim == (1, 128, 128, 3)\n    style_image <-\n      load_and_process_image(style_path) %>% k_cast(\"float32\")\n    # dim == (1, 128, 128, 3)\n    content_image <-\n      load_and_process_image(content_path) %>% k_cast(\"float32\")\n    # dim == (2, 128, 128, 3)\n    stack_images <-\n      k_concatenate(list(style_image, content_image), axis = 1)\n    # length(model_outputs) == 6\n    # dim(model_outputs[[1]]) = (2, 128, 128, 64)\n    # dim(model_outputs[[6]]) = (2, 8, 8, 512)\n    model_outputs <- model(stack_images)\n    style_features <- model_outputs[1:num_style_layers] %>%\n      map(function(batch)\n        batch[1, , , ])\n    content_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %>%\n      map(function(batch)\n        batch[2, , , ])\n    list(style_features, content_features)\n  }\n\ncompute_loss <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    c(style_weight, content_weight) %<-% loss_weights\n    model_outputs <- model(init_image)\n    style_output_features <- model_outputs[1:num_style_layers]\n    content_output_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]\n    \n    weight_per_style_layer <- 1 / num_style_layers\n    style_score <- 0\n    # str(style_zip, max.level = 1)\n    # dim(style_zip[[5]][[1]]) == (512, 512)\n    style_zip <-\n      transpose(list(gram_style_features, style_output_features))\n    for (l in 1:length(style_zip)) {\n      # for l == 1:\n      # dim(target_style) == (64, 64)\n      # dim(comb_style) == (1, 128, 128, 64)\n      c(target_style, comb_style) %<-% style_zip[[l]]\n      style_score <-\n        style_score + weight_per_style_layer * style_loss(target_style, comb_style[1, , , ])\n    }\n    \n    weight_per_content_layer <- 1 / num_content_layers\n    content_score <- 0\n    content_zip <-\n      transpose(list(content_features, content_output_features))\n    for (l in 1:length(content_zip)) {\n      # dim(comb_content) ==  (1, 8, 8, 512)\n      # dim(target_content) == (8, 8, 512)\n      c(target_content, comb_content) %<-% content_zip[[l]]\n      content_score <-\n        content_score + weight_per_content_layer * content_loss(comb_content[1, , , ], target_content)\n    }\n    \n    variation_loss <- total_variation_loss(init_image[1, , ,])\n    style_score <- style_score * style_weight\n    content_score <- content_score * content_weight\n    variation_score <- variation_loss * total_variation_weight\n    \n    loss <- style_score + content_score + variation_score\n    list(loss, style_score, content_score, variation_score)\n  }\n\ncompute_grads <-\n  function(model,\n           loss_weights,\n           init_image,\n           gram_style_features,\n           content_features) {\n    with(tf$GradientTape() %as% tape, {\n      scores <-\n        compute_loss(model,\n                     loss_weights,\n                     init_image,\n                     gram_style_features,\n                     content_features)\n    })\n    total_loss <- scores[[1]]\n    list(tape$gradient(total_loss, init_image), scores)\n  }\n\nrun_style_transfer <- function(content_path,\n                               style_path) {\n  model <- get_model()\n  walk(model$layers, function(layer)\n    layer$trainable = FALSE)\n  \n  c(style_features, content_features) %<-% get_feature_representations(model, content_path, style_path)\n  # dim(gram_style_features[[1]]) == (64, 64)\n  # we compute this once, in advance\n  gram_style_features <-\n    map(style_features, function(feature)\n      gram_matrix(feature))\n  \n  init_image <- load_and_process_image(content_path)\n  init_image <-\n    tf$contrib$eager$Variable(init_image, dtype = \"float32\")\n  \n  optimizer <-\n    tf$train$AdamOptimizer(learning_rate = 1,\n                           beta1 = 0.99,\n                           epsilon = 1e-1)\n  \n  c(best_loss, best_image) %<-% list(Inf, NULL)\n  loss_weights <- list(style_weight, content_weight)\n  \n  start_time <- Sys.time()\n  global_start <- Sys.time()\n  \n  norm_means <- c(103.939, 116.779, 123.68)\n  min_vals <- -norm_means\n  max_vals <- 255 - norm_means\n  \n  for (i in seq_len(num_iterations)) {\n    # dim(grads) == (1, 128, 128, 3)\n    c(grads, all_losses) %<-% compute_grads(model,\n                                            loss_weights,\n                                            init_image,\n                                            gram_style_features,\n                                            content_features)\n    c(loss, style_score, content_score, variation_score) %<-% all_losses\n    optimizer$apply_gradients(list(tuple(grads, init_image)))\n    clipped <- tf$clip_by_value(init_image, min_vals, max_vals)\n    init_image$assign(clipped)\n    \n    end_time <- Sys.time()\n    \n    if (k_cast_to_floatx(loss) < best_loss) {\n      best_loss <- k_cast_to_floatx(loss)\n      best_image <- init_image\n    }\n    \n    if (i %% 50 == 0) {\n      glue(\"Iteration: {i}\") %>% print()\n      glue(\n        \"Total loss: {k_cast_to_floatx(loss)}, style loss: {k_cast_to_floatx(style_score)},\n        content loss: {k_cast_to_floatx(content_score)}, total variation loss: {k_cast_to_floatx(variation_score)},\n        time for 1 iteration: {(Sys.time() - start_time) %>% round(2)}\"\n      ) %>% print()\n      \n      if (i %% 100 == 0) {\n        png(paste0(\"style_epoch_\", i, \".png\"))\n        plot_image <- best_image$numpy()\n        plot_image <- deprocess_image(plot_image)\n        plot(as.raster(plot_image), main = glue(\"Iteration {i}\"))\n        dev.off()\n      }\n    }\n  }\n  \n  glue(\"Total time: {Sys.time() - global_start} seconds\") %>% print()\n  list(best_image, best_loss)\n}\n\nc(best_image, best_loss) %<-% run_style_transfer(content_path, style_path)"
  },
  {
    "objectID": "examples/fine_tuning.html",
    "href": "examples/fine_tuning.html",
    "title": "fine_tuning",
    "section": "",
    "text": "It’s preferable to run this example in a GPU.\n\n# Download data -----------------------------------------------------------\n\ndownload.file(\n  \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\", \n  destfile = \"cats-dogs.zip\"\n)\n\n# Pre-processing ----------------------------------------------------------\n\nzip::unzip(\"cats-dogs.zip\", exdir = \"data-raw\")\n\n# We will organize images in the following structure:\n# data/\n#     train/\n#          Cat/\n#          Dog/\n#     validation\n#          Cat/\n#          Dog/\n#     test/\n#          images/\n#\n\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\n# some images are corrupt and we exclude them\n# this will make sure all images can be read.\nfor (im in all_imgs) {\n  out <- try(magick::image_read(im), silent = TRUE)\n  if (inherits(out, \"try-error\")) {\n    fs::file_delete(im)\n    message(\"removed image: \", im)\n  }\n}\n\n# re-list all imgs\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\nset.seed(5)\n\ntraining_imgs <- sample(all_imgs, size = length(all_imgs)/2)\nvalidation_imgs <- sample(all_imgs[!all_imgs %in% training_imgs], size = length(all_imgs)/4)         \ntesting_imgs <- all_imgs[!all_imgs %in% c(training_imgs, validation_imgs)]\n\n# create directory structure\nfs::dir_create(c(\n  \"data/train/Cat\",\n  \"data/train/Dog\",\n  \"data/validation/Cat\",\n  \"data/validation/Dog\",\n  \"data/test/images\"\n))\n\n# copy training images\nfs::file_copy(\n  path = training_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/train\", training_imgs)\n)\n\n# copy valid images\nfs::file_copy(\n  path = validation_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/validation\", validation_imgs)\n)\n\n# copy testing imgs\nfs::file_copy(\n  path = testing_imgs,\n  new_path = gsub(\"data-raw/PetImages/(Dog|Cat)/\", \"data/test/images/\\\\1\", testing_imgs)\n)\n\n# Image flow --------------------------------------------------------------\n\nlibrary(keras)\n\ntraining_image_gen <- image_data_generator(\n  rotation_range = 20,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  horizontal_flip = TRUE,\n  preprocessing_function = imagenet_preprocess_input\n)\n\nvalidation_image_gen <- image_data_generator(\n  preprocessing_function = imagenet_preprocess_input\n)\n\ntraining_image_flow <- flow_images_from_directory(\n  directory = \"data/train/\", \n  generator = training_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n)\n\nvalidation_image_flow <- flow_images_from_directory(\n  directory = \"data/validation/\", \n  generator = validation_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n  shuffle = FALSE\n)\n\n# Model -------------------------------------------------------------------\n\nmob <- application_mobilenet(include_top = FALSE, pooling = \"avg\")\nfreeze_weights(mob)\n\nmodel <- keras_model_sequential() %>% \n  mob() %>% \n  layer_dense(256, activation = \"relu\") %>% \n  layer_dropout(rate = 0.2) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 1, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# now top layers weights are fine, we can unfreeze the lower layer weights.\nunfreeze_weights(mob)\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 3, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# Generate predictions for test data --------------------------------------\n\ntest_flow <- flow_images_from_directory(\n  generator = validation_image_gen,\n  directory = \"data/test\", \n  target_size = c(224, 224),\n  class_mode = NULL,\n  shuffle = FALSE\n)\n\npredictions <- predict_generator(\n  model, \n  test_flow,\n  steps = test_flow$n/test_flow$batch_size\n)\n\nmagick::image_read(testing_imgs[1])\npredictions[1]\n\nmagick::image_read(testing_imgs[6250])\npredictions[6250]"
  },
  {
    "objectID": "examples/imdb_bidirectional_lstm.html",
    "href": "examples/imdb_bidirectional_lstm.html",
    "title": "imdb_bidirectional_lstm",
    "section": "",
    "text": "Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s.\n\nlibrary(keras)\n\n# Define maximum number of input features\nmax_features <- 20000\n\n# Cut texts after this number of words\n# (among top max_features most common words)\nmaxlen <- 100\n\nbatch_size <- 32\n\n# Load imdb dataset \ncat('Loading data...\\n')\nimdb <- dataset_imdb(num_words = max_features)\n\n# Define training and test sets\nx_train <- imdb$train$x\ny_train <- imdb$train$y\nx_test <- imdb$test$x\ny_test <- imdb$test$y\n\n# Output lengths of testing and training sets\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\ncat('Pad sequences (samples x time)\\n')\n\n# Pad training and test inputs\nx_train <- pad_sequences(x_train, maxlen = maxlen)\nx_test <- pad_sequences(x_test, maxlen = maxlen)\n\n# Output dimensions of training and test inputs\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\n# Initialize model\nmodel <- keras_model_sequential()\nmodel %>%\n  # Creates dense embedding layer; outputs 3D tensor\n  # with shape (batch_size, sequence_length, output_dim)\n  layer_embedding(input_dim = max_features, \n                  output_dim = 128, \n                  input_length = maxlen) %>% \n  bidirectional(layer_lstm(units = 64)) %>%\n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\n# Try using different optimizers and different optimizer configs\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\n# Train model over four epochs\ncat('Train...\\n')\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = 4,\n  validation_data = list(x_test, y_test)\n)"
  },
  {
    "objectID": "examples/imdb_cnn.html",
    "href": "examples/imdb_cnn.html",
    "title": "imdb_cnn",
    "section": "",
    "text": "Output after 2 epochs: ~0.89 Time per epoch on CPU (Intel i5 2.4Ghz): 90s Time per epoch on GPU (Tesla K40): 10s\n\nlibrary(keras)\n\n# Set parameters:\nmax_features <- 5000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 250\nkernel_size <- 3\nhidden_dims <- 250\nepochs <- 2\n\n\n# Data Preparation --------------------------------------------------------\n\n# Keras load all data into a list with the following structure:\n# List of 2\n# $ train:List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 0 0 1 0 0 1 0 1 0 ...\n# $ test :List of 2\n# ..$ x:List of 25000\n# .. .. [list output truncated]\n# .. ..- attr(*, \"dim\")= int 25000\n# ..$ y: num [1:25000(1d)] 1 1 1 1 1 0 0 0 1 1 ...\n#\n# The x data includes integer sequences, each integer is a word.\n# The y data includes a set of integer labels (0 or 1).\n# The num_words argument indicates that only the max_fetures most frequent\n# words will be integerized. All other will be ignored.\n# See help(dataset_imdb)\nimdb <- dataset_imdb(num_words = max_features)\n\n# Pad the sequences, so they have all the same length\n# This will convert the dataset into a matrix: each line is a review\n# and each column a word on the sequence. \n# Pad the sequences with 0 to the left.\nx_train <- imdb$train$x %>%\n  pad_sequences(maxlen = maxlen)\nx_test <- imdb$test$x %>%\n  pad_sequences(maxlen = maxlen)\n\n# Defining Model ------------------------------------------------------\n\n#Initialize model\nmodel <- keras_model_sequential()\n\nmodel %>% \n  # Start off with an efficient embedding layer which maps\n  # the vocab indices into embedding_dims dimensions\n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n\n  # Add a Convolution1D, which will learn filters\n    # Word group filters of size filter_length:\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  # Apply max pooling:\n  layer_global_max_pooling_1d() %>%\n\n  # Add a vanilla hidden layer:\n  layer_dense(hidden_dims) %>%\n\n  # Apply 20% layer dropout\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n\n  # Project onto a single unit output layer, and squash it with a sigmoid\n\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile model\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Training ----------------------------------------------------------------\n\nmodel %>%\n  fit(\n    x_train, imdb$train$y,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, imdb$test$y)\n  )"
  },
  {
    "objectID": "examples/imdb_cnn_lstm.html",
    "href": "examples/imdb_cnn_lstm.html",
    "title": "imdb_cnn_lstm",
    "section": "",
    "text": "Achieves 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n\nlibrary(keras)\n\n# Parameters --------------------------------------------------------------\n\n# Embedding\nmax_features = 20000\nmaxlen = 100\nembedding_size = 128\n\n# Convolution\nkernel_size = 5\nfilters = 64\npool_size = 4\n\n# LSTM\nlstm_output_size = 70\n\n# Training\nbatch_size = 30\nepochs = 2\n\n# Data Preparation --------------------------------------------------------\n\n# The x data includes integer sequences, each integer is a word\n# The y data includes a set of integer labels (0 or 1)\n# The num_words argument indicates that only the max_fetures most frequent\n# words will be integerized. All other will be ignored.\n# See help(dataset_imdb)\nimdb <- dataset_imdb(num_words = max_features)\n# Keras load all data into a list with the following structure:\nstr(imdb)\n\n# Pad the sequences to the same length\n  # This will convert our dataset into a matrix: each line is a review\n  # and each column a word on the sequence\n# We pad the sequences with 0s to the left\nx_train <- imdb$train$x %>%\n  pad_sequences(maxlen = maxlen)\nx_test <- imdb$test$x %>%\n  pad_sequences(maxlen = maxlen)\n\n# Defining Model ------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_embedding(max_features, embedding_size, input_length = maxlen) %>%\n  layer_dropout(0.25) %>%\n  layer_conv_1d(\n    filters, \n    kernel_size, \n    padding = \"valid\",\n    activation = \"relu\",\n    strides = 1\n  ) %>%\n  layer_max_pooling_1d(pool_size) %>%\n  layer_lstm(lstm_output_size) %>%\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Training ----------------------------------------------------------------\n\nmodel %>% fit(\n  x_train, imdb$train$y,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, imdb$test$y)\n)"
  },
  {
    "objectID": "examples/imdb_fasttext.html",
    "href": "examples/imdb_fasttext.html",
    "title": "imdb_fasttext",
    "section": "",
    "text": "Based on Joulin et al’s paper: “Bags of Tricks for Efficient Text Classification” https://arxiv.org/abs/1607.01759\nResults on IMDB datasets with uni and bi-gram embeddings: Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 CPU Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M GPU\n\nlibrary(keras)\nlibrary(purrr)\n\n# Function Definitions ----------------------------------------------------\n\ncreate_ngram_set <- function(input_list, ngram_value = 2){\n  indices <- map(0:(length(input_list) - ngram_value), ~1:ngram_value + .x)\n  indices %>%\n    map_chr(~input_list[.x] %>% paste(collapse = \"|\")) %>%\n    unique()\n}\n\nadd_ngram <- function(sequences, token_indice, ngram_range = 2){\n  ngrams <- map(\n    sequences, \n    create_ngram_set, ngram_value = ngram_range\n  )\n  \n  seqs <- map2(sequences, ngrams, function(x, y){\n    tokens <- token_indice$token[token_indice$ngrams %in% y]  \n    c(x, tokens)\n  })\n  \n  seqs\n}\n\n\n# Parameters --------------------------------------------------------------\n\n# ngram_range = 2 will add bi-grams features\nngram_range <- 2\nmax_features <- 20000\nmaxlen <- 400\nbatch_size <- 32\nembedding_dims <- 50\nepochs <- 5\n\n\n# Data Preparation --------------------------------------------------------\n\n# Load data\nimdb_data <- dataset_imdb(num_words = max_features)\n\n# Train sequences\nprint(length(imdb_data$train$x))\nprint(sprintf(\"Average train sequence length: %f\", mean(map_int(imdb_data$train$x, length))))\n\n# Test sequences\nprint(length(imdb_data$test$x)) \nprint(sprintf(\"Average test sequence length: %f\", mean(map_int(imdb_data$test$x, length))))\n\nif(ngram_range > 1) {\n  \n  # Create set of unique n-gram from the training set.\n  ngrams <- imdb_data$train$x %>% \n    map(create_ngram_set) %>%\n    unlist() %>%\n    unique()\n\n  # Dictionary mapping n-gram token to a unique integer\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features\n  token_indice <- data.frame(\n    ngrams = ngrams,\n    token  = 1:length(ngrams) + (max_features), \n    stringsAsFactors = FALSE\n  )\n  \n  # max_features is the highest integer that could be found in the dataset\n  max_features <- max(token_indice$token) + 1\n  \n  # Augmenting x_train and x_test with n-grams features\n  imdb_data$train$x <- add_ngram(imdb_data$train$x, token_indice, ngram_range)\n  imdb_data$test$x <- add_ngram(imdb_data$test$x, token_indice, ngram_range)\n}\n\n# Pad sequences\nimdb_data$train$x <- pad_sequences(imdb_data$train$x, maxlen = maxlen)\nimdb_data$test$x <- pad_sequences(imdb_data$test$x, maxlen = maxlen)\n\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_embedding(\n    input_dim = max_features, output_dim = embedding_dims, \n    input_length = maxlen\n    ) %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n\n# Fitting -----------------------------------------------------------------\n\nmodel %>% fit(\n  imdb_data$train$x, imdb_data$train$y, \n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(imdb_data$test$x, imdb_data$test$y)\n)"
  },
  {
    "objectID": "examples/imdb_lstm.html",
    "href": "examples/imdb_lstm.html",
    "title": "imdb_lstm",
    "section": "",
    "text": "The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg.\nNotes: - RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won’t converge. - LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc.\n\nlibrary(keras)\n\nmax_features <- 20000\nbatch_size <- 32\n\n# Cut texts after this number of words (among top max_features most common words)\nmaxlen <- 80  \n\ncat('Loading data...\\n')\nimdb <- dataset_imdb(num_words = max_features)\nx_train <- imdb$train$x\ny_train <- imdb$train$y\nx_test <- imdb$test$x\ny_test <- imdb$test$y\n\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\ncat('Pad sequences (samples x time)\\n')\nx_train <- pad_sequences(x_train, maxlen = maxlen)\nx_test <- pad_sequences(x_test, maxlen = maxlen)\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\ncat('Build model...\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_embedding(input_dim = max_features, output_dim = 128) %>% \n  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\n# Try using different optimizers and different optimizer configs\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\ncat('Train...\\n')\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = 15,\n  validation_data = list(x_test, y_test)\n)\n\nscores <- model %>% evaluate(\n  x_test, y_test,\n  batch_size = batch_size\n)\n\ncat('Test score:', scores[[1]])\ncat('Test accuracy', scores[[2]])"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Keras Examples",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\naddition_rnn\n\n\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\n\n\n\n\nbabi_memnn\n\n\nTrains a memory network on the bAbI dataset for reading comprehension.\n\n\n\n\nbabi_rnn\n\n\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\n\n\n\n\ncifar10_cnn\n\n\nTrains a simple deep CNN on the CIFAR10 small images dataset.\n\n\n\n\ncifar10_densenet\n\n\nTrains a DenseNet-40-12 on the CIFAR10 small images dataset.\n\n\n\n\nconv_lstm\n\n\nDemonstrates the use of a convolutional LSTM network.\n\n\n\n\ndeep_dream\n\n\nDeep Dreams in Keras.\n\n\n\n\neager_dcgan\n\n\nGenerating digits with generative adversarial networks and eager execution.\n\n\n\n\neager_image_captioning\n\n\nGenerating image captions with Keras and eager execution.\n\n\n\n\neager_pix2pix\n\n\nImage-to-image translation with Pix2Pix, using eager execution.\n\n\n\n\neager_styletransfer\n\n\nNeural style transfer with eager execution.\n\n\n\n\nfine_tuning\n\n\nFine tuning of a image classification model.\n\n\n\n\nimdb_bidirectional_lstm\n\n\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\n\n\n\n\nimdb_cnn\n\n\nDemonstrates the use of Convolution1D for text classification.\n\n\n\n\nimdb_cnn_lstm\n\n\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\n\n\n\n\nimdb_fasttext\n\n\nTrains a FastText model on the IMDB sentiment classification task.\n\n\n\n\nimdb_lstm\n\n\nTrains a LSTM on the IMDB sentiment classification task.\n\n\n\n\nlstm_seq2seq\n\n\nThis script demonstrates how to implement a basic character-level sequence-to-sequence model.\n\n\n\n\nlstm_text_generation\n\n\nGenerates text from Nietzsche’s writings.\n\n\n\n\nmnist_acgan\n\n\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\n\n\n\n\nmnist_antirectifier\n\n\nDemonstrates how to write custom layers for Keras\n\n\n\n\nmnist_cnn\n\n\nTrains a simple convnet on the MNIST dataset.\n\n\n\n\nmnist_cnn_embeddings\n\n\nDemonstrates how to visualize embeddings in TensorBoard.\n\n\n\n\nmnist_hierarchical_rnn\n\n\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\n\n\n\n\nmnist_irnn\n\n\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\n\n\n\n\nmnist_mlp\n\n\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\n\n\n\n\nmnist_tfrecord\n\n\nMNIST dataset with TFRecords, the standard TensorFlow data format.\n\n\n\n\nmnist_transfer_cnn\n\n\nTransfer learning toy example.\n\n\n\n\nneural_style_transfer\n\n\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\n\n\n\n\nnmt_attention\n\n\nNeural machine translation with an attention mechanism.\n\n\n\n\nquora_siamese_lstm\n\n\nClassifying duplicate quesitons from Quora using Siamese Recurrent Architecture.\n\n\n\n\nreuters_mlp\n\n\nTrains and evaluatea a simple MLP on the Reuters newswire topic classification task.\n\n\n\n\nstateful_lstm\n\n\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\n\n\n\n\ntext_explanation_lime\n\n\nHow to use lime to explain text data.\n\n\n\n\ntfprob_vae\n\n\nA variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST.\n\n\n\n\nvariational_autoencoder\n\n\nDemonstrates how to build a variational autoencoder.\n\n\n\n\nvariational_autoencoder_deconv\n\n\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\n\n\n\n\nvq_vae\n\n\nDiscrete Representation Learning with VQ-VAE and TensorFlow Probability.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples/lstm_seq2seq.html",
    "href": "examples/lstm_seq2seq.html",
    "title": "lstm_seq2seq",
    "section": "",
    "text": "This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain.\nAlgorithm\n\nWe start with input sequences from a domain (e.g. English sentences) and correspding target sequences from another domain (e.g. French sentences).\nAn encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).\nA decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called “teacher forcing” in this context. Is uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\nIn inference mode, when we want to decode unknown input sequences, we:\n\nEncode the input sequence into state vectors\nStart with a target sequence of size 1 (just the start-of-sequence character)\nFeed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\nSample the next character using these predictions (we simply use argmax).\nAppend the sampled character to the target sequence\nRepeat until we generate the end-of-sequence character or we hit the character limit.\n\n\nData download\nEnglish to French sentence pairs. http://www.manythings.org/anki/fra-eng.zip\nLots of neat sentence pairs datasets can be found at: http://www.manythings.org/anki/\nReferences\n\nSequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078\n\n\nlibrary(keras)\nlibrary(data.table)\n\nbatch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 10000  # Number of samples to train on.\n\n## Path to the data txt file on disk.\ndata_path = 'fra.txt'\ntext <- fread(data_path, sep=\"\\t\", header=FALSE, nrows=num_samples)\n\n## Vectorize the data.\ninput_texts  <- text[[1]]\ntarget_texts <- paste0('\\t',text[[2]],'\\n')\ninput_texts  <- lapply( input_texts, function(s) strsplit(s, split=\"\")[[1]])\ntarget_texts <- lapply( target_texts, function(s) strsplit(s, split=\"\")[[1]])\n\ninput_characters  <- sort(unique(unlist(input_texts)))\ntarget_characters <- sort(unique(unlist(target_texts)))\nnum_encoder_tokens <- length(input_characters)\nnum_decoder_tokens <- length(target_characters)\nmax_encoder_seq_length <- max(sapply(input_texts,length))\nmax_decoder_seq_length <- max(sapply(target_texts,length))\n\ncat('Number of samples:', length(input_texts),'\\n')\ncat('Number of unique input tokens:', num_encoder_tokens,'\\n')\ncat('Number of unique output tokens:', num_decoder_tokens,'\\n')\ncat('Max sequence length for inputs:', max_encoder_seq_length,'\\n')\ncat('Max sequence length for outputs:', max_decoder_seq_length,'\\n')\n\ninput_token_index  <- 1:length(input_characters)\nnames(input_token_index) <- input_characters\ntarget_token_index <- 1:length(target_characters)\nnames(target_token_index) <- target_characters\nencoder_input_data <- array(\n  0, dim = c(length(input_texts), max_encoder_seq_length, num_encoder_tokens))\ndecoder_input_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\ndecoder_target_data <- array(\n  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))\n\nfor(i in 1:length(input_texts)) {\n  d1 <- sapply( input_characters, function(x) { as.integer(x == input_texts[[i]]) })\n  encoder_input_data[i,1:nrow(d1),] <- d1\n  d2 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]]) })\n  decoder_input_data[i,1:nrow(d2),] <- d2\n  d3 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]][-1]) })\n  decoder_target_data[i,1:nrow(d3),] <- d3\n}\n\n\n## Create the model\n\n\n## Define an input sequence and process it.\nencoder_inputs  <- layer_input(shape=list(NULL,num_encoder_tokens))\nencoder         <- layer_lstm(units=latent_dim, return_state=TRUE)\nencoder_results <- encoder_inputs %>% encoder\n## We discard `encoder_outputs` and only keep the states.\nencoder_states  <- encoder_results[2:3]\n\n## Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs  <- layer_input(shape=list(NULL, num_decoder_tokens))\n## We set up our decoder to return full output sequences,\n## and to return internal states as well. We don't use the\n## return states in the training model, but we will use them in inference.\ndecoder_lstm    <- layer_lstm(units=latent_dim, return_sequences=TRUE,\n                              return_state=TRUE, stateful=FALSE)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense   <- layer_dense(units=num_decoder_tokens, activation='softmax')\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\n\n## Define the model that will turn\n## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel <- keras_model( inputs = list(encoder_inputs, decoder_inputs),\n                      outputs = decoder_outputs )\n\n## Compile model\nmodel %>% compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n## Run model\nmodel %>% fit( list(encoder_input_data, decoder_input_data), decoder_target_data,\n               batch_size=batch_size,\n               epochs=epochs,\n               validation_split=0.2)\n\n## Save model\nsave_model_hdf5(model,'s2s.h5')\nsave_model_weights_hdf5(model,'s2s-wt.h5')\n\n##model <- load_model_hdf5('s2s.h5')\n##load_model_weights_hdf5(model,'s2s-wt.h5')\n\n\n## Next: inference mode (sampling).\n\n\n## Here's the drill:\n## 1) encode input and retrieve initial decoder state\n## 2) run one step of decoder with this initial state\n## and a \"start of sequence\" token as target.\n## Output will be the next target token\n## 3) Repeat with the current target token and current states\n\n## Define sampling models\nencoder_model <-  keras_model(encoder_inputs, encoder_states)\ndecoder_state_input_h <- layer_input(shape=latent_dim)\ndecoder_state_input_c <- layer_input(shape=latent_dim)\ndecoder_states_inputs <- c(decoder_state_input_h, decoder_state_input_c)\ndecoder_results <- decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states  <- decoder_results[2:3]\ndecoder_outputs <- decoder_dense(decoder_results[[1]])\ndecoder_model   <- keras_model(\n  inputs  = c(decoder_inputs, decoder_states_inputs),\n  outputs = c(decoder_outputs, decoder_states))\n\n## Reverse-lookup token index to decode sequences back to\n## something readable.\nreverse_input_char_index  <- as.character(input_characters)\nreverse_target_char_index <- as.character(target_characters)\n\ndecode_sequence <- function(input_seq) {\n  ## Encode the input as state vectors.\n  states_value <- predict(encoder_model, input_seq)\n  \n  ## Generate empty target sequence of length 1.\n  target_seq <- array(0, dim=c(1, 1, num_decoder_tokens))\n  ## Populate the first character of target sequence with the start character.\n  target_seq[1, 1, target_token_index['\\t']] <- 1.\n  \n  ## Sampling loop for a batch of sequences\n  ## (to simplify, here we assume a batch of size 1).\n  stop_condition = FALSE\n  decoded_sentence = ''\n  maxiter = max_decoder_seq_length\n  niter = 1\n  while (!stop_condition && niter < maxiter) {\n    \n    ## output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n    decoder_predict <- predict(decoder_model, c(list(target_seq), states_value))\n    output_tokens <- decoder_predict[[1]]\n    \n    ## Sample a token\n    sampled_token_index <- which.max(output_tokens[1, 1, ])\n    sampled_char <- reverse_target_char_index[sampled_token_index]\n    decoded_sentence <-  paste0(decoded_sentence, sampled_char)\n    decoded_sentence\n    \n    ## Exit condition: either hit max length\n    ## or find stop character.\n    if (sampled_char == '\\n' ||\n        length(decoded_sentence) > max_decoder_seq_length) {\n      stop_condition = TRUE\n    }\n    \n    ## Update the target sequence (of length 1).\n    ## target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[1, 1, ] <- 0\n    target_seq[1, 1, sampled_token_index] <- 1.\n    \n    ## Update states\n    h <- decoder_predict[[2]]\n    c <- decoder_predict[[3]]\n    states_value = list(h, c)\n    niter <- niter + 1\n  }    \n  return(decoded_sentence)\n}\n\nfor (seq_index in 1:100) {\n  ## Take one sequence (part of the training test)\n  ## for trying out decoding.\n  input_seq = encoder_input_data[seq_index,,,drop=FALSE]\n  decoded_sentence = decode_sequence(input_seq)\n  target_sentence <- gsub(\"\\t|\\n\",\"\",paste(target_texts[[seq_index]],collapse=''))\n  input_sentence  <- paste(input_texts[[seq_index]],collapse='')\n  cat('-\\n')\n  cat('Input sentence  : ', input_sentence,'\\n')\n  cat('Target sentence : ', target_sentence,'\\n')\n  cat('Decoded sentence: ', decoded_sentence,'\\n')\n}"
  },
  {
    "objectID": "examples/lstm_text_generation.html",
    "href": "examples/lstm_text_generation.html",
    "title": "lstm_text_generation",
    "section": "",
    "text": "At least 20 epochs are required before the generated text starts sounding coherent.\nIt is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.\nIf you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better.\n\nlibrary(keras)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tokenizers)\n\n# Parameters --------------------------------------------------------------\n\nmaxlen <- 40\n\n# Data Preparation --------------------------------------------------------\n\n# Retrieve text\npath <- get_file(\n  'nietzsche.txt', \n  origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n  )\n\n# Load, collapse, and tokenize text\ntext <- read_lines(path) %>%\n  str_to_lower() %>%\n  str_c(collapse = \"\\n\") %>%\n  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)\n\nprint(sprintf(\"corpus length: %d\", length(text)))\n\nchars <- text %>%\n  unique() %>%\n  sort()\n\nprint(sprintf(\"total chars: %d\", length(chars)))  \n\n# Cut the text in semi-redundant sequences of maxlen characters\ndataset <- map(\n  seq(1, length(text) - maxlen - 1, by = 3), \n  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])\n  )\n\ndataset <- transpose(dataset)\n\n# Vectorization\nx <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))\ny <- array(0, dim = c(length(dataset$sentece), length(chars)))\n\nfor(i in 1:length(dataset$sentece)){\n  \n  x[i,,] <- sapply(chars, function(x){\n    as.integer(x == dataset$sentece[[i]])\n  })\n  \n  y[i,] <- as.integer(chars == dataset$next_char[[i]])\n  \n}\n\n# Model Definition --------------------------------------------------------\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%\n  layer_dense(length(chars)) %>%\n  layer_activation(\"softmax\")\n\noptimizer <- optimizer_rmsprop(lr = 0.01)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = optimizer\n)\n\n# Training & Results ----------------------------------------------------\n\nsample_mod <- function(preds, temperature = 1){\n  preds <- log(preds)/temperature\n  exp_preds <- exp(preds)\n  preds <- exp_preds/sum(exp(preds))\n  \n  rmultinom(1, 1, preds) %>% \n    as.integer() %>%\n    which.max()\n}\n\non_epoch_end <- function(epoch, logs) {\n  \n  cat(sprintf(\"epoch: %02d ---------------\\n\\n\", epoch))\n  \n  for(diversity in c(0.2, 0.5, 1, 1.2)){\n    \n    cat(sprintf(\"diversity: %f ---------------\\n\\n\", diversity))\n    \n    start_index <- sample(1:(length(text) - maxlen), size = 1)\n    sentence <- text[start_index:(start_index + maxlen - 1)]\n    generated <- \"\"\n    \n    for(i in 1:400){\n      \n      x <- sapply(chars, function(x){\n        as.integer(x == sentence)\n      })\n      x <- array_reshape(x, c(1, dim(x)))\n      \n      preds <- predict(model, x)\n      next_index <- sample_mod(preds, diversity)\n      next_char <- chars[next_index]\n      \n      generated <- str_c(generated, next_char, collapse = \"\")\n      sentence <- c(sentence[-1], next_char)\n      \n    }\n    \n    cat(generated)\n    cat(\"\\n\\n\")\n    \n  }\n}\n\nprint_callback <- callback_lambda(on_epoch_end = on_epoch_end)\n\nmodel %>% fit(\n  x, y,\n  batch_size = 128,\n  epochs = 1,\n  callbacks = print_callback\n)"
  },
  {
    "objectID": "examples/mnist_acgan.html",
    "href": "examples/mnist_acgan.html",
    "title": "mnist_acgan",
    "section": "",
    "text": "You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano.\n\n\n\nHardware\nBackend\nTime / Epoch\n\n\n\n\nCPU\nTF\n3 hrs\n\n\nTitan X (maxwell)\nTF\n4 min\n\n\nTitan X (maxwell)\nTH\n7 min\n\n\n\n\nlibrary(keras)\nlibrary(progress)\nlibrary(abind)\nk_set_image_data_format('channels_first')\n\n# Functions ---------------------------------------------------------------\n\nbuild_generator <- function(latent_size){\n  \n  # We will map a pair of (z, L), where z is a latent vector and L is a\n  # label drawn from P_c, to image space (..., 1, 28, 28)\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_dense(1024, input_shape = latent_size, activation = \"relu\") %>%\n    layer_dense(128*7*7, activation = \"relu\") %>%\n    layer_reshape(c(128, 7, 7)) %>%\n    # Upsample to (..., 14, 14)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      256, c(5,5), padding = \"same\", activation = \"relu\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Upsample to (..., 28, 28)\n    layer_upsampling_2d(size = c(2, 2)) %>%\n    layer_conv_2d(\n      128, c(5,5), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    ) %>%\n    # Take a channel axis reduction\n    layer_conv_2d(\n      1, c(2,2), padding = \"same\", activation = \"tanh\",\n      kernel_initializer = \"glorot_normal\"\n    )\n  \n  \n  # This is the z space commonly referred to in GAN papers\n  latent <- layer_input(shape = list(latent_size))\n  \n  # This will be our label\n  image_class <- layer_input(shape = list(1))\n  \n  # 10 classes in MNIST\n  cls <-  image_class %>%\n    layer_embedding(\n      input_dim = 10, output_dim = latent_size, \n      embeddings_initializer='glorot_normal'\n    ) %>%\n    layer_flatten()\n  \n  \n  # Hadamard product between z-space and a class conditional embedding\n  h <- layer_multiply(list(latent, cls))\n  \n  fake_image <- cnn(h)\n  \n  keras_model(list(latent, image_class), fake_image)\n}\n\nbuild_discriminator <- function(){\n  \n  # Build a relatively standard conv net, with LeakyReLUs as suggested in\n  # the reference paper\n  cnn <- keras_model_sequential()\n  \n  cnn %>%\n    layer_conv_2d(\n      32, c(3,3), padding = \"same\", strides = c(2,2),\n      input_shape = c(1, 28, 28)\n    ) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%\n    \n    layer_conv_2d(64, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(128, c(3, 3), padding = \"same\", strides = c(2,2)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_conv_2d(256, c(3, 3), padding = \"same\", strides = c(1,1)) %>%\n    layer_activation_leaky_relu() %>%\n    layer_dropout(0.3) %>%  \n    \n    layer_flatten()\n  \n  \n  \n  image <- layer_input(shape = c(1, 28, 28))\n  features <- cnn(image)\n  \n  # First output (name=generation) is whether or not the discriminator\n  # thinks the image that is being shown is fake, and the second output\n  # (name=auxiliary) is the class that the discriminator thinks the image\n  # belongs to.\n  fake <- features %>% \n    layer_dense(1, activation = \"sigmoid\", name = \"generation\")\n  \n  aux <- features %>%\n    layer_dense(10, activation = \"softmax\", name = \"auxiliary\")\n  \n  keras_model(image, list(fake, aux))\n}\n\n# Parameters --------------------------------------------------------------\n\n# Batch and latent size taken from the paper\nepochs <- 50\nbatch_size <- 100\nlatent_size <- 100\n\n# Adam parameters suggested in https://arxiv.org/abs/1511.06434\nadam_lr <- 0.00005 \nadam_beta_1 <- 0.5\n\n# Model Definition --------------------------------------------------------\n\n# Build the discriminator\ndiscriminator <- build_discriminator()\ndiscriminator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Build the generator\ngenerator <- build_generator(latent_size)\ngenerator %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = \"binary_crossentropy\"\n)\n\nlatent <- layer_input(shape = list(latent_size))\nimage_class <- layer_input(shape = list(1), dtype = \"int32\")\n\nfake <- generator(list(latent, image_class))\n\n# Only want to be able to train generation for the combined model\nfreeze_weights(discriminator)\nresults <- discriminator(fake)\n\ncombined <- keras_model(list(latent, image_class), results)\ncombined %>% compile(\n  optimizer = optimizer_adam(lr = adam_lr, beta_1 = adam_beta_1),\n  loss = list(\"binary_crossentropy\", \"sparse_categorical_crossentropy\")\n)\n\n# Data Preparation --------------------------------------------------------\n\n# Loade mnist data, and force it to be of shape (..., 1, 28, 28) with\n# range [-1, 1]\nmnist <- dataset_mnist()\nmnist$train$x <- (mnist$train$x - 127.5)/127.5\nmnist$test$x <- (mnist$test$x - 127.5)/127.5\nmnist$train$x <- array_reshape(mnist$train$x, c(60000, 1, 28, 28))\nmnist$test$x <- array_reshape(mnist$test$x, c(10000, 1, 28, 28))\n\nnum_train <- dim(mnist$train$x)[1]\nnum_test <- dim(mnist$test$x)[1]\n\n# Training ----------------------------------------------------------------\n\nfor(epoch in 1:epochs){\n  \n  num_batches <- trunc(num_train/batch_size)\n  pb <- progress_bar$new(\n    total = num_batches, \n    format = sprintf(\"epoch %s/%s :elapsed [:bar] :percent :eta\", epoch, epochs),\n    clear = FALSE\n  )\n  \n  epoch_gen_loss <- NULL\n  epoch_disc_loss <- NULL\n  \n  possible_indexes <- 1:num_train\n  \n  for(index in 1:num_batches){\n    \n    pb$tick()\n    \n    # Generate a new batch of noise\n    noise <- runif(n = batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = batch_size, ncol = latent_size)\n    \n    # Get a batch of real images\n    batch <- sample(possible_indexes, size = batch_size)\n    possible_indexes <- possible_indexes[!possible_indexes %in% batch]\n    image_batch <- mnist$train$x[batch,,,,drop = FALSE]\n    label_batch <- mnist$train$y[batch]\n    \n    # Sample some labels from p_c\n    sampled_labels <- sample(0:9, batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Generate a batch of fake images, using the generated labels as a\n    # conditioner. We reshape the sampled labels to be\n    # (batch_size, 1) so that we can feed them into the embedding\n    # layer as a length one sequence\n    generated_images <- predict(generator, list(noise, sampled_labels))\n    \n    X <- abind(image_batch, generated_images, along = 1)\n    y <- c(rep(1L, batch_size), rep(0L, batch_size)) %>% matrix(ncol = 1)\n    aux_y <- c(label_batch, sampled_labels) %>% matrix(ncol = 1)\n    \n    # Check if the discriminator can figure itself out\n    disc_loss <- train_on_batch(\n      discriminator, x = X, \n      y = list(y, aux_y)\n    )\n    \n    epoch_disc_loss <- rbind(epoch_disc_loss, unlist(disc_loss))\n    \n    # Make new noise. Generate 2 * batch size here such that\n    # the generator optimizes over an identical number of images as the\n    # discriminator\n    noise <- runif(2*batch_size*latent_size, min = -1, max = 1) %>%\n      matrix(nrow = 2*batch_size, ncol = latent_size)\n    sampled_labels <- sample(0:9, size = 2*batch_size, replace = TRUE) %>%\n      matrix(ncol = 1)\n    \n    # Want to train the generator to trick the discriminator\n    # For the generator, we want all the {fake, not-fake} labels to say\n    # not-fake\n    trick <- rep(1, 2*batch_size) %>% matrix(ncol = 1)\n    \n    combined_loss <- train_on_batch(\n      combined, \n      list(noise, sampled_labels),\n      list(trick, sampled_labels)\n    )\n    \n    epoch_gen_loss <- rbind(epoch_gen_loss, unlist(combined_loss))\n    \n  }\n  \n  cat(sprintf(\"\\nTesting for epoch %02d:\", epoch))\n  \n  # Evaluate the testing loss here\n  \n  # Generate a new batch of noise\n  noise <- runif(num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = num_test, ncol = latent_size)\n  \n  # Sample some labels from p_c and generate images from them\n  sampled_labels <- sample(0:9, size = num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  generated_images <- predict(generator, list(noise, sampled_labels))\n  \n  X <- abind(mnist$test$x, generated_images, along = 1)\n  y <- c(rep(1, num_test), rep(0, num_test)) %>% matrix(ncol = 1)\n  aux_y <- c(mnist$test$y, sampled_labels) %>% matrix(ncol = 1)\n  \n  # See if the discriminator can figure itself out...\n  discriminator_test_loss <- evaluate(\n    discriminator, X, list(y, aux_y), \n    verbose = FALSE\n  ) %>% unlist()\n  \n  discriminator_train_loss <- apply(epoch_disc_loss, 2, mean)\n  \n  # Make new noise\n  noise <- runif(2*num_test*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 2*num_test, ncol = latent_size)\n  sampled_labels <- sample(0:9, size = 2*num_test, replace = TRUE) %>%\n    matrix(ncol = 1)\n  \n  trick <- rep(1, 2*num_test) %>% matrix(ncol = 1)\n  \n  generator_test_loss = combined %>% evaluate(\n    list(noise, sampled_labels),\n    list(trick, sampled_labels),\n    verbose = FALSE\n  )\n  \n  generator_train_loss <- apply(epoch_gen_loss, 2, mean)\n  \n  \n  # Generate an epoch report on performance\n  row_fmt <- \"\\n%22s : loss %4.2f | %5.2f | %5.2f\"\n  cat(sprintf(\n    row_fmt, \n    \"generator (train)\",\n    generator_train_loss[1],\n    generator_train_loss[2],\n    generator_train_loss[3]\n  ))\n  cat(sprintf(\n    row_fmt, \n    \"generator (test)\",\n    generator_test_loss[1],\n    generator_test_loss[2],\n    generator_test_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (train)\",\n    discriminator_train_loss[1],\n    discriminator_train_loss[2],\n    discriminator_train_loss[3]\n  ))\n  \n  cat(sprintf(\n    row_fmt, \n    \"discriminator (test)\",\n    discriminator_test_loss[1],\n    discriminator_test_loss[2],\n    discriminator_test_loss[3]\n  ))\n  \n  cat(\"\\n\")\n  \n  # Generate some digits to display\n  noise <- runif(10*latent_size, min = -1, max = 1) %>%\n    matrix(nrow = 10, ncol = latent_size)\n  \n  sampled_labels <- 0:9 %>%\n    matrix(ncol = 1)\n  \n  # Get a batch to display\n  generated_images <- predict(\n    generator,    \n    list(noise, sampled_labels)\n  )\n  \n  img <- NULL\n  for(i in 1:10){\n    img <- cbind(img, generated_images[i,,,])\n  }\n  \n  ((img + 1)/2) %>% as.raster() %>%\n    plot()\n  \n}"
  },
  {
    "objectID": "examples/mnist_antirectifier.html",
    "href": "examples/mnist_antirectifier.html",
    "title": "mnist_antirectifier",
    "section": "",
    "text": "We build a custom activation layer called ‘Antirectifier’, which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call.\nNote that the same result can also be achieved via a Lambda layer.\n\nlibrary(keras)\n\n# Data Preparation --------------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 40\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redimension\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Antirectifier Layer -----------------------------------------------------\n\nThis is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input. The result is a tensor of samples that are twice as large as the input samples.\nIt can be used in place of a ReLU. Input shape: 2D tensor of shape (samples, n) Output shape: 2D tensor of shape (samples, 2*n)\nWhen applying ReLU, assuming that the distribution of the previous output is approximately centered around 0., you are discarding half of your input. This is inefficient.\nAntirectifier allows to return all-positive outputs like ReLU, without discarding any data.\nTests on MNIST show that Antirectifier allows to train networks with half the parameters yet with comparable classification accuracy as an equivalent ReLU-based network.\n\n# Custom layer class\nAntirectifierLayer <- R6::R6Class(\"KerasLayer\",\n  \n  inherit = KerasLayer,\n                           \n  public = list(\n   \n    call = function(x, mask = NULL) {\n      x <- x - k_mean(x, axis = 2, keepdims = TRUE)\n      x <- k_l2_normalize(x, axis = 2)\n      pos <- k_relu(x)\n      neg <- k_relu(-x)\n      k_concatenate(c(pos, neg), axis = 2)\n      \n    },\n     \n    compute_output_shape = function(input_shape) {\n      input_shape[[2]] <- input_shape[[2]] * 2L \n      input_shape\n    }\n  )\n)\n\n# Create layer wrapper function\nlayer_antirectifier <- function(object) {\n  create_layer(AntirectifierLayer, object)\n}\n\n\n# Define & Train Model -------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 256, input_shape = c(784)) %>% \n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 256) %>%\n  layer_antirectifier() %>% \n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile the model\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Train the model\nmodel %>% fit(x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data= list(x_test, y_test)\n)"
  },
  {
    "objectID": "examples/mnist_cnn.html",
    "href": "examples/mnist_cnn.html",
    "title": "mnist_cnn",
    "section": "",
    "text": "Gets to 99.25% test accuracy after 12 epochs Note: There is still a large margin for parameter tuning\n16 seconds per epoch on a GRID K520 GPU.\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 12\n\n# Input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redefine  dimension of train/test inputs\nx_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model -----------------------------------------------------------\n\n# Define model\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n                input_shape = input_shape) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n# Train model\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split = 0.2\n)\n\n\n\n\nscores <- model %>% evaluate(\n  x_test, y_test, verbose = 0\n)\n\n# Output metrics\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_cnn_embeddings.html",
    "href": "examples/mnist_cnn_embeddings.html",
    "title": "mnist_cnn_embeddings",
    "section": "",
    "text": "Embeddings in the sense used here don’t necessarily refer to embedding layers. In fact, features (= activations) from other hidden layers can be visualized, as shown in this example for a dense layer.\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 12\n\n# Input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Redefine  dimension of train/test inputs\nx_train <-\n  array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test <-\n  array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n\n# Prepare for logging embeddings --------------------------------------------------\n\nembeddings_dir <- file.path(tempdir(), 'embeddings')\nif (!file.exists(embeddings_dir))\n  dir.create(embeddings_dir)\nembeddings_metadata <- file.path(embeddings_dir, 'metadata.tsv')\n\n# we use the class names from the test set as embeddings_metadata\nreadr::write_tsv(data.frame(y_test), path = embeddings_metadata, col_names = FALSE)\n\ntensorboard_callback <- callback_tensorboard(\n  log_dir = embeddings_dir,\n  batch_size = batch_size,\n  embeddings_freq = 1,\n  # if missing or NULL all embedding layers will be monitored\n  embeddings_layer_names = list('features'),\n  # single file for all embedding layers, could also be a named list mapping\n  # layer names to file names\n  embeddings_metadata = embeddings_metadata,\n  # data to be embedded\n  embeddings_data = x_test\n)\n\n\n# Define Model -----------------------------------------------------------\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define model\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filters = 32,\n    kernel_size = c(3, 3),\n    activation = 'relu',\n    input_shape = input_shape\n  ) %>%\n  layer_conv_2d(filters = 64,\n                kernel_size = c(3, 3),\n                activation = 'relu') %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_flatten() %>%\n  # these are the embeddings (activations) we are going to visualize\n  layer_dense(units = 128, activation = 'relu', name = 'features') %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n# Launch TensorBoard\n#\n# As the model is being fit you will be able to view the embedings in the \n# Projector tab. On the left, use \"color by label\" to see the digits displayed\n# in 10 different colors. Hover over a point to see its label.\ntensorboard(embeddings_dir)\n\n# Train model\nmodel %>% fit(\n  x_train,\n  y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_data = list(x_test, y_test),\n  callbacks = list(tensorboard_callback)\n)\n\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\n\n# Output metrics\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_hierarchical_rnn.html",
    "href": "examples/mnist_hierarchical_rnn.html",
    "title": "mnist_hierarchical_rnn",
    "section": "",
    "text": "HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence. Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors) into a sentence vector. The second recurrent layer then encodes a sequence of such vectors (encoded by the first layer) into a document vector. This document vector is considered to preserve both the word-level and sentence-level structure of the context.\nReferences: - A Hierarchical Neural Autoencoder for Paragraphs and Documents Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. - Hierarchical recurrent neural network for skeleton based action recognition Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers.\nIn the below MNIST example the first LSTM layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final dense layer is added for prediction.\nAfter 5 epochs: train acc: 0.9858, val acc: 0.9864\n\nlibrary(keras)\n\n# Data Preparation -----------------------------------------------------------------\n\n# Training parameters.\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 5\n\n# Embedding dimensions.\nrow_hidden <- 128\ncol_hidden <- 128\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n# Reshapes data to 4D for Hierarchical RNN.\nx_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ndim_x_train <- dim(x_train)\ncat('x_train_shape:', dim_x_train)\ncat(nrow(x_train), 'train samples')\ncat(nrow(x_test), 'test samples')\n\n# Converts class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define input dimensions\nrow <- dim_x_train[[2]]\ncol <- dim_x_train[[3]]\npixel <- dim_x_train[[4]]\n\n# Model input (4D)\ninput <- layer_input(shape = c(row, col, pixel))\n\n# Encodes a row of pixels using TimeDistributed Wrapper\nencoded_rows <- input %>% time_distributed(layer_lstm(units = row_hidden))\n\n# Encodes columns of encoded rows\nencoded_columns <- encoded_rows %>% layer_lstm(units = col_hidden)\n\n# Model output\nprediction <- encoded_columns %>%\n  layer_dense(units = num_classes, activation = 'softmax')\n\n# Define Model ------------------------------------------------------------------------\n\nmodel <- keras_model(input, prediction)\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\n# Training\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n\n# Evaluation\nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('Test loss:', scores[[1]], '\\n')\ncat('Test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_irnn.html",
    "href": "examples/mnist_irnn.html",
    "title": "mnist_irnn",
    "section": "",
    "text": "arxiv:1504.00941v2 [cs.NE] 7 Apr 2015 http://arxiv.org/pdf/1504.00941v2.pdf\nOptimizer is replaced with RMSprop which yields more stable and steady improvement.\nReaches 0.93 train/test accuracy after 900 epochs This corresponds to roughly 1687500 steps in the original paper.\n\nlibrary(keras)\n\n# Data Preparation ---------------------------------------------------------------\n\nbatch_size <- 32\nnum_classes <- 10\nepochs <- 200\nhidden_units <- 100\n\nimg_rows <- 28\nimg_cols <- 28\n\nlearning_rate <- 1e-6\nclip_norm <- 1.0\n\n# The data, shuffled and split between train and test sets\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nx_train <- array_reshape(x_train, c(nrow(x_train), img_rows * img_cols, 1))\nx_test <- array_reshape(x_test, c(nrow(x_test), img_rows * img_cols, 1))\ninput_shape <- c(img_rows, img_cols, 1)\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat('x_train_shape:', dim(x_train), '\\n')\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model ------------------------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_simple_rnn(units = hidden_units,\n                   kernel_initializer = initializer_random_normal(stddev = 0.01),\n                   recurrent_initializer = initializer_identity(gain = 1.0),\n                   activation = 'relu',\n                   input_shape = dim(x_train)[-1]) %>% \n  layer_dense(units = num_classes) %>% \n  layer_activation(activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(lr = learning_rate),\n  metrics = c('accuracy')\n)\n \n# Training & Evaluation ---------------------------------------------------------\n\ncat(\"Evaluate IRNN...\\n\")\nmodel %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_data = list(x_test, y_test)\n)\n  \nscores <- model %>% evaluate(x_test, y_test, verbose = 0)\ncat('IRNN test score:', scores[[1]], '\\n')\ncat('IRNN test accuracy:', scores[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_mlp.html",
    "href": "examples/mnist_mlp.html",
    "title": "mnist_mlp",
    "section": "",
    "text": "Gets to 98.40% test accuracy after 20 epochs (there is a lot of margin for parameter tuning). 2 seconds per epoch on a K520 GPU.\n\nlibrary(keras)\n\n# Data Preparation ---------------------------------------------------\n\nbatch_size <- 128\nnum_classes <- 10\nepochs <- 30\n\n# The data, shuffled and split between train and test sets\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ncat(nrow(x_train), 'train samples\\n')\ncat(nrow(x_test), 'test samples\\n')\n\n# Convert class vectors to binary class matrices\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define Model --------------------------------------------------------------\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nsummary(model)\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n# Training & Evaluation ----------------------------------------------------\n\n# Fit model to data\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_split = 0.2\n)\n\nplot(history)\n  \nscore <- model %>% evaluate(\n  x_test, y_test,\n  verbose = 0\n)\n\n# Output metrics\ncat('Test loss:', score[[1]], '\\n')\ncat('Test accuracy:', score[[2]], '\\n')"
  },
  {
    "objectID": "examples/mnist_tfrecord.html",
    "href": "examples/mnist_tfrecord.html",
    "title": "mnist_tfrecord",
    "section": "",
    "text": "TFRecord is a data format supported throughout TensorFlow. This example demonstrates how to load TFRecord data using Input Tensors. Input Tensors differ from the normal Keras workflow because instead of fitting to data loaded into a a numpy array, data is supplied via a special tensor that reads data from nodes that are wired directly into model graph with the layer_input(tensor=input_tensor) parameter.\nThere are several advantages to using Input Tensors. First, if a dataset is already in TFRecord format you can load and train on that data directly in Keras. Second, extended backend API capabilities such as TensorFlow data augmentation is easy to integrate directly into your Keras training scripts via input tensors. Third, TensorFlow implements several data APIs for TFRecords, some of which provide significantly faster training performance than numpy arrays can provide because they run via the C++ backend. Please note that this example is tailored for brevity and clarity and not to demonstrate performance or augmentation capabilities.\nInput Tensors also have important disadvantages. In particular, Input Tensors are fixed at model construction because rewiring networks is not yet supported. For this reason, changing the data input source means model weights must be saved and the model rebuilt from scratch to connect the new input data. validation cannot currently be performed as training progresses, and must be performed after training completes. This example demonstrates how to train with input tensors, save the model weights, and then evaluate the model using the standard Keras API.\nGets to ~99.1% validation accuracy after 5 epochs (there is still a lot of margin for parameter tuning).\n\nlibrary(keras)\nlibrary(tensorflow)\n\nif (k_backend() != 'tensorflow') {\n  stop('This example can only run with the ',\n       'TensorFlow backend, ',\n       'because it requires TFRecords, which ',\n       'are not supported on other platforms.')\n}\n\n# Define Model -------------------------------------------------------------------\n\ncnn_layers <- function(x_train_input) {\n  x_train_input %>% \n    layer_conv_2d(filters = 32, kernel_size = c(3,3), \n                  activation = 'relu', padding = 'valid') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n    layer_max_pooling_2d(pool_size = c(2,2)) %>% \n    layer_flatten() %>% \n    layer_dense(units = 512, activation = 'relu') %>% \n    layer_dropout(rate = 0.5) %>% \n    layer_dense(units = classes, activation = 'softmax', name = 'x_train_out')\n}\n\nsess <- k_get_session()\n\n# Data Preparation --------------------------------------------------------------\n\nbatch_size <- 128L\nbatch_shape = list(batch_size, 28L, 28L, 1L)\nsteps_per_epoch <- 469L\nepochs <- 5L\nclasses <- 10L\n\n# The capacity variable controls the maximum queue size\n# allowed when prefetching data for training.\ncapacity <- 10000L\n\n# min_after_dequeue is the minimum number elements in the queue\n# after a dequeue, which ensures sufficient mixing of elements.\nmin_after_dequeue <- 3000L\n\n# If `enqueue_many` is `FALSE`, `tensors` is assumed to represent a\n# single example.  An input tensor with shape `(x, y, z)` will be output\n# as a tensor with shape `(batch_size, x, y, z)`.\n#\n# If `enqueue_many` is `TRUE`, `tensors` is assumed to represent a\n# batch of examples, where the first dimension is indexed by example,\n# and all members of `tensors` should have the same size in the\n# first dimension.  If an input tensor has shape `(*, x, y, z)`, the\n# output will have shape `(batch_size, x, y, z)`.\nenqueue_many <- TRUE\n\n# mnist dataset from tf contrib\nmnist <- tf$contrib$learn$datasets$mnist\ndata <- mnist$load_mnist()\n\ntrain_data <- tf$train$shuffle_batch(\n  tensors = list(data$train$images, data$train$labels),\n  batch_size = batch_size,\n  capacity = capacity,\n  min_after_dequeue = min_after_dequeue,\n  enqueue_many = enqueue_many,\n  num_threads = 8L\n)\nx_train_batch <- train_data[[1]]\ny_train_batch <- train_data[[2]]\n\nx_train_batch <- tf$cast(x_train_batch, tf$float32)\nx_train_batch <- tf$reshape(x_train_batch, shape = batch_shape)\n\ny_train_batch <- tf$cast(y_train_batch, tf$int32)\ny_train_batch <- tf$one_hot(y_train_batch, classes)\n\nx_batch_shape <- x_train_batch$get_shape()$as_list()\ny_batch_shape = y_train_batch$get_shape()$as_list()\n\nx_train_input <- layer_input(tensor = x_train_batch, batch_shape = x_batch_shape)\nx_train_out <- cnn_layers(x_train_input)\n\n# Training & Evaluation ---------------------------------------------------------\n\ntrain_model = keras_model(inputs = x_train_input, outputs = x_train_out)\n\n# Pass the target tensor `y_train_batch` to `compile`\n# via the `target_tensors` keyword argument:\ntrain_model %>% compile(\n  optimizer = optimizer_rmsprop(lr = 2e-3, decay = 1e-5),\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy'),\n  target_tensors = y_train_batch\n)\n\nsummary(train_model)\n\n# Fit the model using data from the TFRecord data tensors.\ncoord <- tf$train$Coordinator()\nthreads = tf$train$start_queue_runners(sess, coord)\n\ntrain_model %>% fit(\n  epochs = epochs,\n  steps_per_epoch = steps_per_epoch\n)\n\n# Save the model weights.\ntrain_model %>% save_model_weights_hdf5('saved_wt.h5')\n\n# Clean up the TF session.\ncoord$request_stop()\ncoord$join(threads)\nk_clear_session()\n\n# Second Session to test loading trained model without tensors\nx_test <- data$validation$images\nx_test <- array_reshape(x_test, dim = c(nrow(x_test), 28, 28, 1))\ny_test <- data$validation$labels\nx_test_inp <- layer_input(shape = dim(x_test)[-1])\ntest_out <- cnn_layers(x_test_inp)\ntest_model <- keras_model(inputs = x_test_inp, outputs = test_out)\ntest_model %>% load_model_weights_hdf5('saved_wt.h5')\ntest_model %>% compile(\n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy')\n)\nsummary(test_model)\n\nresult <- test_model %>% evaluate(x_test, to_categorical(y_test, classes))\ncat(sprintf('\\nTest accuracy: %f', result$acc))"
  },
  {
    "objectID": "examples/mnist_transfer_cnn.html",
    "href": "examples/mnist_transfer_cnn.html",
    "title": "mnist_transfer_cnn",
    "section": "",
    "text": "Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\nFreeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].\n\n\nlibrary(keras)\n\nnow <- Sys.time()\n\nbatch_size <- 128\nnum_classes <- 5\nepochs <- 5\n\n# input image dimensions\nimg_rows <- 28\nimg_cols <- 28\n\n# number of convolutional filters to use\nfilters <- 32\n\n# size of pooling area for max pooling\npool_size <- 2\n\n# convolution kernel size\nkernel_size <- c(3, 3)\n\n# input shape\ninput_shape <- c(img_rows, img_cols, 1)\n\n# the data, shuffled and split between train and test sets\ndata <- dataset_mnist()\nx_train <- data$train$x\ny_train <- data$train$y\nx_test <- data$test$x\ny_test <- data$test$y\n\n# create two datasets one with digits below 5 and one with 5 and above\nx_train_lt5 <- x_train[y_train < 5]\ny_train_lt5 <- y_train[y_train < 5]\nx_test_lt5 <- x_test[y_test < 5]\ny_test_lt5 <- y_test[y_test < 5]\n\nx_train_gte5 <- x_train[y_train >= 5]\ny_train_gte5 <- y_train[y_train >= 5] - 5\nx_test_gte5 <- x_test[y_test >= 5]\ny_test_gte5 <- y_test[y_test >= 5] - 5\n\n# define two groups of layers: feature (convolutions) and classification (dense)\nfeature_layers <- \n  layer_conv_2d(filters = filters, kernel_size = kernel_size, \n                input_shape = input_shape) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_conv_2d(filters = filters, kernel_size = kernel_size) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = pool_size) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten()\n  \n\n\n# feature_layers = [\n#   Conv2D(filters, kernel_size,\n#          padding='valid',\n#          input_shape=input_shape),\n#   Activation('relu'),\n#   Conv2D(filters, kernel_size),\n#   Activation('relu'),\n#   MaxPooling2D(pool_size=pool_size),\n#   Dropout(0.25),\n#   Flatten(),\n#   ]\n# \n# classification_layers = [\n#   Dense(128),\n#   Activation('relu'),\n#   Dropout(0.5),\n#   Dense(num_classes),\n#   Activation('softmax')\n#   ]"
  },
  {
    "objectID": "examples/neural_style_transfer.html",
    "href": "examples/neural_style_transfer.html",
    "title": "neural_style_transfer",
    "section": "",
    "text": "It is preferable to run this script on a GPU, for speed.\nExample result: https://twitter.com/fchollet/status/686631033085677568\nStyle transfer consists in generating an image with the same “content” as a base image, but with the “style” of a different picture (typically artistic).\nThis is achieved through the optimization of a loss function that has 3 components: “style loss”, “content loss”, and “total variation loss”:\n\nThe total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence.\nThe style loss is where the deep learning keeps in –that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales –defined by the depth of the layer considered).\nThe content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one.\n\n\nlibrary(keras)\nlibrary(purrr)\nlibrary(R6)\n\n# Parameters --------------------------------------------------------------\n\nbase_image_path <- \"neural-style-base-img.png\"\nstyle_reference_image_path <- \"neural-style-style.jpg\"\niterations <- 10\n\n# these are the weights of the different loss components\ntotal_variation_weight <- 1\nstyle_weight <- 1\ncontent_weight <- 0.025\n\n# dimensions of the generated picture.\nimg <- image_load(base_image_path)\nwidth <- img$size[[1]]\nheight <- img$size[[2]]\nimg_nrows <- 400\nimg_ncols <- as.integer(width * img_nrows / height)\n\n\n# Functions ---------------------------------------------------------------\n\n# util function to open, resize and format pictures into appropriate tensors\npreprocess_image <- function(path){\n  img <- image_load(path, target_size = c(img_nrows, img_ncols)) %>%\n    image_to_array() %>%\n    array_reshape(c(1, dim(.)))\n  imagenet_preprocess_input(img)\n}\n\n# util function to convert a tensor into a valid image\n# also turn BGR into RGB.\ndeprocess_image <- function(x){\n  x <- x[1,,,]\n  # Remove zero-center by mean pixel\n  x[,,1] <- x[,,1] + 103.939\n  x[,,2] <- x[,,2] + 116.779\n  x[,,3] <- x[,,3] + 123.68\n  # BGR -> RGB\n  x <- x[,,c(3,2,1)]\n  # clip to interval 0, 255\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x)/255\n  x\n}\n\n\n# Defining the model ------------------------------------------------------\n\n# get tensor representations of our images\nbase_image <- k_variable(preprocess_image(base_image_path))\nstyle_reference_image <- k_variable(preprocess_image(style_reference_image_path))\n\n# this will contain our generated image\ncombination_image <- k_placeholder(c(1, img_nrows, img_ncols, 3))\n\n# combine the 3 images into a single Keras tensor\ninput_tensor <- k_concatenate(list(base_image, style_reference_image, \n                                   combination_image), axis = 1)\n\n# build the VGG16 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nmodel <- application_vgg16(input_tensor = input_tensor, weights = \"imagenet\", \n                           include_top = FALSE)\n\nprint(\"Model loaded.\")\n\nnms <- map_chr(model$layers, ~.x$name)\noutput_dict <- map(model$layers, ~.x$output) %>% set_names(nms)\n\n# compute the neural style loss\n# first we need to define 4 util functions\n\n# the gram matrix of an image tensor (feature-wise outer product)\n\ngram_matrix <- function(x){\n  \n  features <- x %>%\n    k_permute_dimensions(pattern = c(3, 1, 2)) %>%\n    k_batch_flatten()\n  \n  k_dot(features, k_transpose(features))\n}\n\n# the \"style loss\" is designed to maintain\n# the style of the reference image in the generated image.\n# It is based on the gram matrices (which capture style) of\n# feature maps from the style reference image\n# and from the generated image\n\nstyle_loss <- function(style, combination){\n  S <- gram_matrix(style)\n  C <- gram_matrix(combination)\n  \n  channels <- 3\n  size <- img_nrows*img_ncols\n  \n  k_sum(k_square(S - C)) / (4 * channels^2  * size^2)\n}\n\n# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\n\ncontent_loss <- function(base, combination){\n  k_sum(k_square(combination - base))\n}\n\n# the 3rd loss function, total variation loss,\n# designed to keep the generated image locally coherent\n\ntotal_variation_loss <- function(x){\n  y_ij  <- x[,1:(img_nrows - 1L), 1:(img_ncols - 1L),]\n  y_i1j <- x[,2:(img_nrows), 1:(img_ncols - 1L),]\n  y_ij1 <- x[,1:(img_nrows - 1L), 2:(img_ncols),]\n  \n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\n# combine these loss functions into a single scalar\nloss <- k_variable(0.0)\nlayer_features <- output_dict$block4_conv2\nbase_image_features <- layer_features[1,,,]\ncombination_features <- layer_features[3,,,]\n\nloss <- loss + content_weight*content_loss(base_image_features, \n                                           combination_features)\n\nfeature_layers = c('block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1')\n\nfor(layer_name in feature_layers){\n  layer_features <- output_dict[[layer_name]]\n  style_reference_features <- layer_features[2,,,]\n  combination_features <- layer_features[3,,,]\n  sl <- style_loss(style_reference_features, combination_features)\n  loss <- loss + ((style_weight / length(feature_layers)) * sl)\n}\n\nloss <- loss + (total_variation_weight * total_variation_loss(combination_image))\n\n# get the gradients of the generated image wrt the loss\ngrads <- k_gradients(loss, combination_image)[[1]]\n\nf_outputs <- k_function(list(combination_image), list(loss, grads))\n\neval_loss_and_grads <- function(image){\n  image <- array_reshape(image, c(1, img_nrows, img_ncols, 3))\n  outs <- f_outputs(list(image))\n  list(\n    loss_value = outs[[1]],\n    grad_values = array_reshape(outs[[2]], dim = length(outs[[2]]))\n  )\n}\n\n# Loss and gradients evaluator.\n# \n# This Evaluator class makes it possible\n# to compute loss and gradients in one pass\n# while retrieving them via two separate functions,\n# \"loss\" and \"grads\". This is done because scipy.optimize\n# requires separate functions for loss and gradients,\n# but computing them separately would be inefficient.\nEvaluator <- R6Class(\n  \"Evaluator\",\n  public = list(\n    \n    loss_value = NULL,\n    grad_values = NULL,\n    \n    initialize = function() {\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n    },\n    \n    loss = function(x){\n      loss_and_grad <- eval_loss_and_grads(x)\n      self$loss_value <- loss_and_grad$loss_value\n      self$grad_values <- loss_and_grad$grad_values\n      self$loss_value\n    },\n    \n    grads = function(x){\n      grad_values <- self$grad_values\n      self$loss_value <- NULL\n      self$grad_values <- NULL\n      grad_values\n    }\n    \n  )\n)\n\nevaluator <- Evaluator$new()\n\n# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\ndms <- c(1, img_nrows, img_ncols, 3)\nx <- array(data = runif(prod(dms), min = 0, max = 255) - 128, dim = dms)\n\n# Run optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the loss\nfor(i in 1:iterations){\n\n  # Run L-BFGS\n  opt <- optim(\n    array_reshape(x, dim = length(x)), fn = evaluator$loss, gr = evaluator$grads, \n    method = \"L-BFGS-B\",\n    control = list(maxit = 15)\n  )\n  \n  # Print loss value\n  print(opt$value)\n  \n  # decode the image\n  image <- x <- opt$par\n  image <- array_reshape(image, dms)\n  \n  # plot\n  im <- deprocess_image(image)\n  plot(as.raster(im))\n}"
  },
  {
    "objectID": "examples/nmt_attention.html",
    "href": "examples/nmt_attention.html",
    "title": "nmt_attention",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(tibble)\n\n\n# Preprocessing -----------------------------------------------------------\n\n# Assumes you've downloaded and unzipped one of the bilingual datasets offered at\n# http://www.manythings.org/anki/ and put it into a directory \"data\"\n# This example translates English to Dutch.\n\nfilepath <- file.path(\"data\", \"nld.txt\")\n\nlines <- readLines(filepath, n = 10000)\nsentences <- str_split(lines, \"\\t\")\n\nspace_before_punct <- function(sentence) {\n  str_replace_all(sentence, \"([?.!])\", \" \\\\1\")\n}\n\nreplace_special_chars <- function(sentence) {\n  str_replace_all(sentence, \"[^a-zA-Z?.!,¿]+\", \" \")\n}\n\nadd_tokens <- function(sentence) {\n  paste0(\"<start> \", sentence, \" <stop>\")\n}\nadd_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)\n\npreprocess_sentence <- compose(add_tokens,\n                               str_squish,\n                               replace_special_chars,\n                               space_before_punct)\n\nword_pairs <- map(sentences, preprocess_sentence)\n\ncreate_index <- function(sentences) {\n  unique_words <- sentences %>% unlist() %>% paste(collapse = \" \") %>%\n    str_split(pattern = \" \") %>% .[[1]] %>% unique() %>% sort()\n  index <- data.frame(\n    word = unique_words,\n    index = 1:length(unique_words),\n    stringsAsFactors = FALSE\n  ) %>%\n    add_row(word = \"<pad>\",\n            index = 0,\n            .before = 1)\n  index\n}\n\nword2index <- function(word, index_df) {\n  index_df[index_df$word == word, \"index\"]\n}\nindex2word <- function(index, index_df) {\n  index_df[index_df$index == index, \"word\"]\n}\n\nsrc_index <- create_index(map(word_pairs, ~ .[[1]]))\ntarget_index <- create_index(map(word_pairs, ~ .[[2]]))\nsentence2digits <- function(sentence, index_df) {\n  map((sentence %>% str_split(pattern = \" \"))[[1]], function(word)\n    word2index(word, index_df))\n}\n\nsentlist2diglist <- function(sentence_list, index_df) {\n  map(sentence_list, function(sentence)\n    sentence2digits(sentence, index_df))\n}\n\nsrc_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)\nsrc_maxlen <- map(src_diglist, length) %>% unlist() %>% max()\nsrc_matrix <-\n  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = \"post\")\n\ntarget_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)\ntarget_maxlen <- map(target_diglist, length) %>% unlist() %>% max()\ntarget_matrix <-\n  pad_sequences(target_diglist, maxlen = target_maxlen, padding = \"post\")\n\n\n\n# Train-test-split --------------------------------------------------------\n\ntrain_indices <-\n  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)\n\nvalidation_indices <- setdiff(1:nrow(src_matrix), train_indices)\n\nx_train <- src_matrix[train_indices,]\ny_train <- target_matrix[train_indices,]\n\nx_valid <- src_matrix[validation_indices,]\ny_valid <- target_matrix[validation_indices,]\n\nbuffer_size <- nrow(x_train)\n\n# just for convenience, so we may get a glimpse at translation performance \n# during training\ntrain_sentences <- sentences[train_indices]\nvalidation_sentences <- sentences[validation_indices]\nvalidation_sample <- sample(validation_sentences, 5)\n\n\n\n# Hyperparameters / variables ---------------------------------------------\n\nbatch_size <- 32\nembedding_dim <- 64\ngru_units <- 256\n\nsrc_vocab_size <- nrow(src_index)\ntarget_vocab_size <- nrow(target_index)\n\n\n# Create datasets ---------------------------------------------------------\n\ntrain_dataset <-\n  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nvalidation_dataset <-\n  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n\n# Attention encoder -------------------------------------------------------\n\n\nattention_encoder <-\n  function(gru_units,\n           embedding_dim,\n           src_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$embedding <-\n        layer_embedding(input_dim = src_vocab_size,\n                        output_dim = embedding_dim)\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        \n        x <- self$embedding(x)\n        c(output, state) %<-% self$gru(x, initial_state = hidden)\n        \n        list(output, state)\n      }\n    })\n  }\n\n\n\n# Attention decoder -------------------------------------------------------\n\n\nattention_decoder <-\n  function(object,\n           gru_units,\n           embedding_dim,\n           target_vocab_size,\n           name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      self$embedding <-\n        layer_embedding(input_dim = target_vocab_size, output_dim = embedding_dim)\n      gru_units <- gru_units\n      self$fc <- layer_dense(units = target_vocab_size)\n      self$W1 <- layer_dense(units = gru_units)\n      self$W2 <- layer_dense(units = gru_units)\n      self$V <- layer_dense(units = 1L)\n      \n      function(inputs, mask = NULL) {\n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        encoder_output <- inputs[[3]]\n        \n        hidden_with_time_axis <- k_expand_dims(hidden, 2)\n        \n        score <-\n          self$V(k_tanh(\n            self$W1(encoder_output) + self$W2(hidden_with_time_axis)\n          ))\n        \n        attention_weights <- k_softmax(score, axis = 2)\n        \n        context_vector <- attention_weights * encoder_output\n        context_vector <- k_sum(context_vector, axis = 2)\n        \n        x <- self$embedding(x)\n        \n        x <-\n          k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)\n        \n        c(output, state) %<-% self$gru(x)\n        \n        output <- k_reshape(output, c(-1, gru_units))\n        \n        x <- self$fc(output)\n        \n        list(x, state, attention_weights)\n        \n      }\n      \n    })\n  }\n\n\n# The model ---------------------------------------------------------------\n\nencoder <- attention_encoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  src_vocab_size = src_vocab_size\n)\n\ndecoder <- attention_decoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  target_vocab_size = target_vocab_size\n)\n\noptimizer <- tf$optimizers$Adam()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- ifelse(y_true == 0L, 0, 1)\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,\n                                                   logits = y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\n\n\n# Inference / translation functions ---------------------------------------\n# they are appearing here already in the file because we want to watch how\n# the network learns\n\nevaluate <-\n  function(sentence) {\n    attention_matrix <-\n      matrix(0, nrow = target_maxlen, ncol = src_maxlen)\n    \n    sentence <- preprocess_sentence(sentence)\n    input <- sentence2digits(sentence, src_index)\n    input <-\n      pad_sequences(list(input), maxlen = src_maxlen,  padding = \"post\")\n    input <- k_constant(input)\n    \n    result <- \"\"\n    \n    hidden <- k_zeros(c(1, gru_units))\n    c(enc_output, enc_hidden) %<-% encoder(list(input, hidden))\n    \n    dec_hidden <- enc_hidden\n    dec_input <-\n      k_expand_dims(list(word2index(\"<start>\", target_index)))\n    \n    for (t in seq_len(target_maxlen - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, dec_hidden, enc_output))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t,] <- attention_weights %>% as.double()\n      \n      pred_idx <-\n        tf$compat$v1$multinomial(k_exp(preds), num_samples = 1L)[1, 1] %>% as.double()\n      pred_word <- index2word(pred_idx, target_index)\n      \n      if (pred_word == '<stop>') {\n        result <-\n          paste0(result, pred_word)\n        return (list(result, sentence, attention_matrix))\n      } else {\n        result <-\n          paste0(result, pred_word, \" \")\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    list(str_trim(result), sentence, attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           words_sentence,\n           words_result) {\n    melted <- melt(attention_matrix)\n    ggplot(data = melted, aes(\n      x = factor(Var2),\n      y = factor(Var1),\n      fill = value\n    )) +\n      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +\n      theme(axis.ticks = element_blank()) +\n      xlab(\"\") +\n      ylab(\"\") +\n      scale_x_discrete(labels = words_sentence, position = \"top\") +\n      scale_y_discrete(labels = words_result) +\n      theme(aspect.ratio = 1)\n  }\n\n\ntranslate <- function(sentence) {\n  c(result, sentence, attention_matrix) %<-% evaluate(sentence)\n  print(paste0(\"Input: \",  sentence))\n  print(paste0(\"Predicted translation: \", result))\n  attention_matrix <-\n    attention_matrix[1:length(str_split(result, \" \")[[1]]),\n                     1:length(str_split(sentence, \" \")[[1]])]\n  plot_attention(attention_matrix,\n                 str_split(sentence, \" \")[[1]],\n                 str_split(result, \" \")[[1]])\n}\n\n# Training loop -----------------------------------------------------------\n\n\nn_epochs <- 50\n\nencoder_init_hidden <- k_zeros(c(batch_size, gru_units))\n\nfor (epoch in seq_len(n_epochs)) {\n  total_loss <- 0\n  iteration <- 0\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <- 0\n    x <- batch[[1]]\n    y <- batch[[2]]\n    iteration <- iteration + 1\n\n    with(tf$GradientTape() %as% tape, {\n      c(enc_output, enc_hidden) %<-% encoder(list(x, encoder_init_hidden))\n      \n      dec_hidden <- enc_hidden\n      dec_input <-\n        k_expand_dims(rep(list(\n          word2index(\"<start>\", target_index)\n        ), batch_size))\n      \n      \n      for (t in seq_len(target_maxlen - 1)) {\n        c(preds, dec_hidden, weights) %<-%\n          decoder(list(dec_input, dec_hidden, enc_output))\n        loss <- loss + cx_loss(y[, t], preds)\n        \n        dec_input <- k_expand_dims(y[, t])\n      }\n    })\n    total_loss <-\n      total_loss + loss / k_cast_to_floatx(dim(y)[2])\n    \n    paste0(\n      \"Batch loss (epoch/batch): \",\n      epoch,\n      \"/\",\n      iteration,\n      \": \",\n      (loss / k_cast_to_floatx(dim(y)[2])) %>% as.double() %>% round(4),\n      \"\\n\"\n    ) %>% print()\n    \n    variables <- c(encoder$variables, decoder$variables)\n    gradients <- tape$gradient(loss, variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(gradients, variables)))\n    \n  })\n  \n  paste0(\n    \"Total loss (epoch): \",\n    epoch,\n    \": \",\n    (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n    \"\\n\"\n  ) %>% print()\n  \n  walk(train_sentences[1:5], function(pair)\n    translate(pair[1]))\n  walk(validation_sample, function(pair)\n    translate(pair[1]))\n}\n\n# plot a mask\nexample_sentence <- train_sentences[[1]]\ntranslate(example_sentence)"
  },
  {
    "objectID": "examples/quora_siamese_lstm.html",
    "href": "examples/quora_siamese_lstm.html",
    "title": "quora_siamese_lstm",
    "section": "",
    "text": "Our implementation is inspired by the Siamese Recurrent Architecture, Mueller et al. Siamese recurrent architectures for learning sentence similarity, with small modifications like the similarity measure and the embedding layers (The original paper uses pre-trained word vectors). Using this kind of architecture dates back to 2005 with Le Cun et al and is usefull for verification tasks. The idea is to learn a function that maps input patterns into a target space such that a similarity measure in the target space approximates the “semantic” distance in the input space.\nAfter the competition, Quora also described their approach to this problem in this blog post.\n\nlibrary(readr)\nlibrary(keras)\nlibrary(purrr)\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 0.0001),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\n# Downloading Data --------------------------------------------------------\n\nquora_data <- get_file(\n  \"quora_duplicate_questions.tsv\",\n  \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n)\n\n\n# Pre-processing ----------------------------------------------------------\n\ndf <- read_tsv(quora_data)\n\ntokenizer <- text_tokenizer(num_words = FLAGS$vocab_size)\nfit_text_tokenizer(tokenizer, x = c(df$question1, df$question2))\n\nquestion1 <- texts_to_sequences(tokenizer, df$question1)\nquestion2 <- texts_to_sequences(tokenizer, df$question2)\n\nquestion1 <- pad_sequences(question1, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\nquestion2 <- pad_sequences(question2, maxlen = FLAGS$max_len_padding, value = FLAGS$vocab_size + 1)\n\n\n# Model Definition --------------------------------------------------------\n\ninput1 <- layer_input(shape = c(FLAGS$max_len_padding))\ninput2 <- layer_input(shape = c(FLAGS$max_len_padding))\n\nembedding <- layer_embedding(\n  input_dim = FLAGS$vocab_size + 2, \n  output_dim = FLAGS$embedding_size, \n  input_length = FLAGS$max_len_padding, \n  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\nseq_emb <- layer_lstm(\n  units = FLAGS$seq_embedding_size, \n  recurrent_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\n\nvector1 <- embedding(input1) %>%\n  seq_emb()\nvector2 <- embedding(input2) %>%\n  seq_emb()\n\nout <- layer_dot(list(vector1, vector2), axes = 1) %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel <- keras_model(list(input1, input2), out)\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"binary_crossentropy\", \n  metrics = list(\n    acc = metric_binary_accuracy\n  )\n)\n\n# Model Fitting -----------------------------------------------------------\n\nset.seed(1817328)\nval_sample <- sample.int(nrow(question1), size = 0.1*nrow(question1))\n\nmodel %>%\n  fit(\n    list(question1[-val_sample,], question2[-val_sample,]),\n    df$is_duplicate[-val_sample], \n    batch_size = 128, \n    epochs = 30, \n    validation_data = list(\n      list(question1[val_sample,], question2[val_sample,]), df$is_duplicate[val_sample]\n    ),\n    callbacks = list(\n      callback_early_stopping(patience = 5),\n      callback_reduce_lr_on_plateau(patience = 3)\n    )\n  )\n\nsave_model_hdf5(model, \"model-question-pairs.hdf5\", include_optimizer = TRUE)\nsave_text_tokenizer(tokenizer, \"tokenizer-question-pairs.hdf5\")\n\n\n# Prediction --------------------------------------------------------------\n# In a fresh R session:\n# Load model and tokenizer -\n\nmodel <- load_model_hdf5(\"model-question-pairs.hdf5\", compile = FALSE)\ntokenizer <- load_text_tokenizer(\"tokenizer-question-pairs.hdf5\")\n\n\npredict_question_pairs <- function(model, tokenizer, q1, q2) {\n  \n  q1 <- texts_to_sequences(tokenizer, list(q1))\n  q2 <- texts_to_sequences(tokenizer, list(q2))\n  \n  q1 <- pad_sequences(q1, 20)\n  q2 <- pad_sequences(q2, 20)\n  \n  as.numeric(predict(model, list(q1, q2)))\n}\n\n# Getting predictions\n\npredict_question_pairs(\n  model, tokenizer, \n  q1 = \"What is the main benefit of Quora?\",\n  q2 = \"What are the advantages of using Quora?\"\n)"
  },
  {
    "objectID": "examples/reuters_mlp.html",
    "href": "examples/reuters_mlp.html",
    "title": "reuters_mlp",
    "section": "",
    "text": "library(keras)\n\nmax_words <- 1000\nbatch_size <- 32\nepochs <- 5\n\ncat('Loading data...\\n')\nreuters <- dataset_reuters(num_words = max_words, test_split = 0.2)\nx_train <- reuters$train$x\ny_train <- reuters$train$y\nx_test <- reuters$test$x\ny_test <- reuters$test$y\n\ncat(length(x_train), 'train sequences\\n')\ncat(length(x_test), 'test sequences\\n')\n\nnum_classes <- max(y_train) + 1\ncat(num_classes, '\\n')\n\ncat('Vectorizing sequence data...\\n')\n\ntokenizer <- text_tokenizer(num_words = max_words)\nx_train <- sequences_to_matrix(tokenizer, x_train, mode = 'binary')\nx_test <- sequences_to_matrix(tokenizer, x_test, mode = 'binary')\n\ncat('x_train shape:', dim(x_train), '\\n')\ncat('x_test shape:', dim(x_test), '\\n')\n\ncat('Convert class vector to binary class matrix',\n    '(for use with categorical_crossentropy)\\n')\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\ncat('y_train shape:', dim(y_train), '\\n')\ncat('y_test shape:', dim(y_test), '\\n')\n\ncat('Building model...\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 512, input_shape = c(max_words)) %>% \n  layer_activation(activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = num_classes) %>% \n  layer_activation(activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  validation_split = 0.1\n)\n\nscore <- model %>% evaluate(\n  x_test, y_test,\n  batch_size = batch_size,\n  verbose = 1\n)\n\ncat('Test score:', score[[1]], '\\n')\ncat('Test accuracy', score[[2]], '\\n')"
  },
  {
    "objectID": "examples/stateful_lstm.html",
    "href": "examples/stateful_lstm.html",
    "title": "stateful_lstm",
    "section": "",
    "text": "library(keras)\n\n# since we are using stateful rnn tsteps can be set to 1\ntsteps <- 1\nbatch_size <- 25\nepochs <- 25\n# number of elements ahead that are used to make the prediction\nlahead <- 1\n\n# Generates an absolute cosine time series with the amplitude exponentially decreasing\n# Arguments:\n#   amp: amplitude of the cosine function\n#   period: period of the cosine function\n#   x0: initial x of the time series\n#   xn: final x of the time series\n#   step: step of the time series discretization\n#   k: exponential rate\ngen_cosine_amp <- function(amp = 100, period = 1000, x0 = 0, xn = 50000, step = 1, k = 0.0001) {\n  n <- (xn-x0) * step\n  cos <- array(data = numeric(n), dim = c(n, 1, 1))\n  for (i in 1:length(cos)) {\n    idx <- x0 + i * step\n    cos[[i, 1, 1]] <- amp * cos(2 * pi * idx / period)\n    cos[[i, 1, 1]] <- cos[[i, 1, 1]] * exp(-k * idx)\n  }\n  cos\n}\n\ncat('Generating Data...\\n')\ncos <- gen_cosine_amp()\ncat('Input shape:', dim(cos), '\\n')\n\nexpected_output <- array(data = numeric(length(cos)), dim = c(length(cos), 1))\nfor (i in 1:(length(cos) - lahead)) {\n  expected_output[[i, 1]] <- mean(cos[(i + 1):(i + lahead)])\n}\n\ncat('Output shape:', dim(expected_output), '\\n')\n\ncat('Creating model:\\n')\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_lstm(units = 50, input_shape = c(tsteps, 1), batch_size = batch_size,\n             return_sequences = TRUE, stateful = TRUE) %>% \n  layer_lstm(units = 50, return_sequences = FALSE, stateful = TRUE) %>% \n  layer_dense(units = 1)\nmodel %>% compile(loss = 'mse', optimizer = 'rmsprop')\n\ncat('Training\\n')\nfor (i in 1:epochs) {\n  model %>% fit(cos, expected_output, batch_size = batch_size,\n                epochs = 1, verbose = 1, shuffle = FALSE)\n            \n  model %>% reset_states()\n}\n\ncat('Predicting\\n')\npredicted_output <- model %>% predict(cos, batch_size = batch_size)\n\ncat('Plotting Results\\n')\nop <- par(mfrow=c(2,1))\nplot(expected_output, xlab = '')\ntitle(\"Expected\")\nplot(predicted_output, xlab = '')\ntitle(\"Predicted\")\npar(op)"
  },
  {
    "objectID": "examples/text_explanation_lime.html",
    "href": "examples/text_explanation_lime.html",
    "title": "text_explanation_lime",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(keras)\nlibrary(tidyverse)\n\n# Download and unzip data\n\nactivity_url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip\"\ntemp <- tempfile()\ndownload.file(activity_url, temp)\nunzip(temp, \"drugLibTest_raw.tsv\")\n\n\n# Read dataset\n\ndf <- read_delim('drugLibTest_raw.tsv',delim = '\\t')\nunlink(temp)\n\n# Select only rating and text from the whole dataset\n\ndf = df %>% select(rating,commentsReview) %>% mutate(rating = if_else(rating >= 8, 0, 1))\n\n# This is our text\ntext <- df$commentsReview\n\n# And these are ratings given by customers\ny_train <- df$rating\n\n\n# text_tokenizer helps us to turn each word into integers. By selecting maximum number of features\n# we also keep the most frequent words. Additionally, by default, all punctuation is removed.\n\nmax_features <- 1000\ntokenizer <- text_tokenizer(num_words = max_features)\n\n# Then, we need to fit the tokenizer object to our text data\n\ntokenizer %>% fit_text_tokenizer(text)\n\n# Via tokenizer object you can check word indices, word counts and other interesting properties.\n\ntokenizer$word_counts \ntokenizer$word_index\n\n# Finally, we can replace words in dataset with integers\ntext_seqs <- texts_to_sequences(tokenizer, text)\n\ntext_seqs %>% head(3)\n\n# Define the parameters of the keras model\n\nmaxlen <- 15\nbatch_size <- 32\nembedding_dims <- 50\nfilters <- 64\nkernel_size <- 3\nhidden_dims <- 50\nepochs <- 15\n\n# As a final step, restrict the maximum length of all sequences and create a matrix as input for model\nx_train <- text_seqs %>% pad_sequences(maxlen = maxlen)\n\n# Lets print the first 2 rows and see that max length of first 2 sequences equals to 15\nx_train[1:2,]\n\n# Create a model\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_1d(\n    filters, kernel_size, \n    padding = \"valid\", activation = \"relu\", strides = 1\n  ) %>%\n  layer_global_max_pooling_1d() %>%\n  layer_dense(hidden_dims) %>%\n  layer_dropout(0.2) %>%\n  layer_activation(\"relu\") %>%\n  layer_dense(1) %>%\n  layer_activation(\"sigmoid\")\n\n# Compile\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\n# Run\nhist <- model %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split = 0.1\n  )\n\n# Understanding lime for Keras Embedding Layers\n\n# In order to explain a text with LIME, we should write a preprocess function\n# which will help to turn words into integers. Therefore, above mentioned steps \n# (how to encode a text) should be repeated BUT within a function. \n# As we already have had a tokenizer object, we can apply the same object to train/test or a new text.\n\nget_embedding_explanation <- function(text) {\n  \n  tokenizer %>% fit_text_tokenizer(text)\n  \n  text_to_seq <- texts_to_sequences(tokenizer, text)\n  sentences <- text_to_seq %>% pad_sequences(maxlen = maxlen)\n}\n\n\nlibrary(lime)\n\n# Lets choose some text (3 rows) to explain\nsentence_to_explain <- train_sentences$text[15:17]\nsentence_to_explain\n\n# You could notice that our input is just a plain text. Unlike tabular data, lime function \n# for text classification requires a preprocess fuction. Because it will help to convert a text to integers \n# with provided function. \nexplainer <- lime(sentence_to_explain, model = model, preprocess = get_embedding_explanation)\n\n# Get explanation for the first 10 words\nexplanation <- explain(sentence_to_explain, explainer, n_labels = 1, n_features = 10,n_permutations = 1e4)\n\n\n# Different graphical ways to show the same information\n\nplot_text_explanations(explanation)\n\nplot_features(explanation)\n\ninteractive_text_explanations(explainer)"
  },
  {
    "objectID": "examples/tfprob_vae.html",
    "href": "examples/tfprob_vae.html",
    "title": "tfprob_vae",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n\n\n# Utilities --------------------------------------------------------\n\nnum_examples_to_generate <- 64L\n\ngenerate_random <- function(epoch) {\n  decoder_likelihood <-\n    decoder(latent_prior$sample(num_examples_to_generate))\n  predictions <- decoder_likelihood$mean()\n  # change path according to your preferences\n  png(file.path(\"/tmp\", paste0(\"random_epoch_\", epoch, \".png\")))\n  par(mfcol = c(8, 8))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:64) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\nshow_grid <- function(epoch) {\n  # change path according to your preferences\n  png(file.path(\"/tmp\", paste0(\"grid_epoch_\", epoch, \".png\")))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  n <- 16\n  img_size <- 28\n  grid_x <- seq(-4, 4, length.out = n)\n  grid_y <- seq(-4, 4, length.out = n)\n  rows <- NULL\n  for (i in 1:length(grid_x)) {\n    column <- NULL\n    for (j in 1:length(grid_y)) {\n      z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n      decoder_likelihood <- decoder(k_cast(z_sample, k_floatx()))\n      column <-\n        rbind(column,\n              (decoder_likelihood$mean() %>% as.numeric()) %>% matrix(ncol = img_size))\n    }\n    rows <- cbind(rows, column)\n  }\n  rows %>% as.raster() %>% plot()\n  dev.off()\n}\n\n\n# Setup and preprocessing -------------------------------------------------\n\nnp <- import(\"numpy\")\n\n# assume data have been downloaded from https://github.com/rois-codh/kmnist\n# and stored in /tmp\nkuzushiji <- np$load(\"/tmp/kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\n\n# Params ------------------------------------------------------------------\n\nlatent_dim <- 2\nmixture_components <- 16\n\n\n# Model -------------------------------------------------------------------\n\n# Encoder ------------------------------------------------------------------\n\nencoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense()\n      tfd$MultivariateNormalDiag(loc = x[, 1:latent_dim],\n                                 scale_diag = tf$nn$softplus(x[, (latent_dim + 1):(2 * latent_dim)] + 1e-5))\n    }\n  })\n}\n\n\n# Decoder ------------------------------------------------------------------\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = 3L)\n      \n    }\n  })\n}\n\n# Learnable Prior -------------------------------------------------------------------\n\nlearnable_prior_model <-\n  function(name = NULL, latent_dim, mixture_components) {\n    \n    keras_model_custom(name = name, function(self) {\n      self$loc <-\n        tf$get_variable(\n          name = \"loc\",\n          shape = list(mixture_components, latent_dim),\n          dtype = tf$float32\n        )\n      self$raw_scale_diag <- tf$get_variable(\n        name = \"raw_scale_diag\",\n        shape = c(mixture_components, latent_dim),\n        dtype = tf$float32\n      )\n      self$mixture_logits <-\n        tf$get_variable(\n          name = \"mixture_logits\",\n          shape = c(mixture_components),\n          dtype = tf$float32\n        )\n      \n      function (x, mask = NULL) {\n        tfd$MixtureSameFamily(\n          components_distribution = tfd$MultivariateNormalDiag(\n            loc = self$loc,\n            scale_diag = tf$nn$softplus(self$raw_scale_diag)\n          ),\n          mixture_distribution = tfd$Categorical(logits = self$mixture_logits)\n        )\n      }\n    })\n  }\n\n\n# Loss and optimizer ------------------------------------------------------\n\ncompute_kl_loss <-\n  function(latent_prior,\n           approx_posterior,\n           approx_posterior_sample) {\n    kl_div <- approx_posterior$log_prob(approx_posterior_sample) - latent_prior$log_prob(approx_posterior_sample)\n    avg_kl_div <- tf$reduce_mean(kl_div)\n    avg_kl_div\n  }\n\n\nglobal_step <- tf$train$get_or_create_global_step()\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n\n# Training loop -----------------------------------------------------------\n\nnum_epochs <- 50\n\nencoder <- encoder_model()\ndecoder <- decoder_model()\nlatent_prior_model <-\n  learnable_prior_model(latent_dim = latent_dim, mixture_components = mixture_components)\n\n# change this according to your preferences\ncheckpoint_dir <- \"/tmp/checkpoints\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    optimizer = optimizer,\n    global_step = global_step,\n    encoder = encoder,\n    decoder = decoder,\n    latent_prior_model = latent_prior_model\n  )\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      \n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      latent_prior <- latent_prior_model(NULL)\n      \n      kl_loss <-\n        compute_kl_loss(latent_prior,\n                        approx_posterior,\n                        approx_posterior_sample)\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    prior_gradients <-\n      tape$gradient(loss, latent_prior_model$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      prior_gradients, latent_prior_model$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n})\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n  \n  if (TRUE) {\n    generate_random(epoch)\n    show_grid(epoch)\n  }\n}"
  },
  {
    "objectID": "examples/variational_autoencoder.html",
    "href": "examples/variational_autoencoder.html",
    "title": "variational_autoencoder",
    "section": "",
    "text": "library(keras)\nK <- keras::backend()\n\n# Parameters --------------------------------------------------------------\n\nbatch_size <- 100L\noriginal_dim <- 784L\nlatent_dim <- 2L\nintermediate_dim <- 256L\nepochs <- 50L\nepsilon_std <- 1.0\n\n# Model definition --------------------------------------------------------\n\nx <- layer_input(shape = c(original_dim))\nh <- layer_dense(x, intermediate_dim, activation = \"relu\")\nz_mean <- layer_dense(h, latent_dim)\nz_log_var <- layer_dense(h, latent_dim)\n\nsampling <- function(arg){\n  z_mean <- arg[, 1:(latent_dim)]\n  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]), \n    mean=0.,\n    stddev=epsilon_std\n  )\n  \n  z_mean + k_exp(z_log_var/2)*epsilon\n}\n\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% \n  layer_lambda(sampling)\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_mean <- layer_dense(units = original_dim, activation = \"sigmoid\")\nh_decoded <- decoder_h(z)\nx_decoded_mean <- decoder_mean(h_decoded)\n\n# end-to-end autoencoder\nvae <- keras_model(x, x_decoded_mean)\n\n# encoder, from inputs to latent space\nencoder <- keras_model(x, z_mean)\n\n# generator, from latent space to reconstructed inputs\ndecoder_input <- layer_input(shape = latent_dim)\nh_decoded_2 <- decoder_h(decoder_input)\nx_decoded_mean_2 <- decoder_mean(h_decoded_2)\ngenerator <- keras_model(decoder_input, x_decoded_mean_2)\n\n\nvae_loss <- function(x, x_decoded_mean){\n  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)\n  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)\n  xent_loss + kl_loss\n}\n\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\n\n\n# Data preparation --------------------------------------------------------\n\nmnist <- dataset_mnist()\nx_train <- mnist$train$x/255\nx_test <- mnist$test$x/255\nx_train <- array_reshape(x_train, c(nrow(x_train), 784), order = \"F\")\nx_test <- array_reshape(x_test, c(nrow(x_test), 784), order = \"F\")\n\n\n# Model training ----------------------------------------------------------\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n# Visualizations ----------------------------------------------------------\n\nlibrary(ggplot2)\nlibrary(dplyr)\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\n\nx_test_encoded %>%\n  as_data_frame() %>% \n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n# display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = 28) )\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()"
  },
  {
    "objectID": "examples/variational_autoencoder_deconv.html",
    "href": "examples/variational_autoencoder_deconv.html",
    "title": "variational_autoencoder_deconv",
    "section": "",
    "text": "library(keras)\nK <- keras::backend()\n\n#### Parameterization ####\n\n# input image dimensions\nimg_rows <- 28L\nimg_cols <- 28L\n# color channels (1 = grayscale, 3 = RGB)\nimg_chns <- 1L\n\n# number of convolutional filters to use\nfilters <- 64L\n\n# convolution kernel size\nnum_conv <- 3L\n\nlatent_dim <- 2L\nintermediate_dim <- 128L\nepsilon_std <- 1.0\n\n# training parameters\nbatch_size <- 100L\nepochs <- 5L\n\n\n#### Model Construction ####\n\noriginal_img_size <- c(img_rows, img_cols, img_chns)\n\nx <- layer_input(shape = c(original_img_size))\n\nconv_1 <- layer_conv_2d(\n  x,\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_2 <- layer_conv_2d(\n  conv_1,\n  filters = filters,\n  kernel_size = c(2L, 2L),\n  strides = c(2L, 2L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_3 <- layer_conv_2d(\n  conv_2,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nconv_4 <- layer_conv_2d(\n  conv_3,\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\nflat <- layer_flatten(conv_4)\nhidden <- layer_dense(flat, units = intermediate_dim, activation = \"relu\")\n\nz_mean <- layer_dense(hidden, units = latent_dim)\nz_log_var <- layer_dense(hidden, units = latent_dim)\n\nsampling <- function(args) {\n  z_mean <- args[, 1:(latent_dim)]\n  z_log_var <- args[, (latent_dim + 1):(2 * latent_dim)]\n  \n  epsilon <- k_random_normal(\n    shape = c(k_shape(z_mean)[[1]]),\n    mean = 0.,\n    stddev = epsilon_std\n  )\n  z_mean + k_exp(z_log_var) * epsilon\n}\n\nz <- layer_concatenate(list(z_mean, z_log_var)) %>% layer_lambda(sampling)\n\noutput_shape <- c(batch_size, 14L, 14L, filters)\n\ndecoder_hidden <- layer_dense(units = intermediate_dim, activation = \"relu\")\ndecoder_upsample <- layer_dense(units = prod(output_shape[-1]), activation = \"relu\")\n\ndecoder_reshape <- layer_reshape(target_shape = output_shape[-1])\ndecoder_deconv_1 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_2 <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(num_conv, num_conv),\n  strides = c(1L, 1L),\n  padding = \"same\",\n  activation = \"relu\"\n)\n\ndecoder_deconv_3_upsample <- layer_conv_2d_transpose(\n  filters = filters,\n  kernel_size = c(3L, 3L),\n  strides = c(2L, 2L),\n  padding = \"valid\",\n  activation = \"relu\"\n)\n\ndecoder_mean_squash <- layer_conv_2d(\n  filters = img_chns,\n  kernel_size = c(2L, 2L),\n  strides = c(1L, 1L),\n  padding = \"valid\",\n  activation = \"sigmoid\"\n)\n\nhidden_decoded <- decoder_hidden(z)\nup_decoded <- decoder_upsample(hidden_decoded)\nreshape_decoded <- decoder_reshape(up_decoded)\ndeconv_1_decoded <- decoder_deconv_1(reshape_decoded)\ndeconv_2_decoded <- decoder_deconv_2(deconv_1_decoded)\nx_decoded_relu <- decoder_deconv_3_upsample(deconv_2_decoded)\nx_decoded_mean_squash <- decoder_mean_squash(x_decoded_relu)\n\n# custom loss function\nvae_loss <- function(x, x_decoded_mean_squash) {\n  x <- k_flatten(x)\n  x_decoded_mean_squash <- k_flatten(x_decoded_mean_squash)\n  xent_loss <- 1.0 * img_rows * img_cols *\n    loss_binary_crossentropy(x, x_decoded_mean_squash)\n  kl_loss <- -0.5 * k_mean(1 + z_log_var - k_square(z_mean) -\n                           k_exp(z_log_var), axis = -1L)\n  k_mean(xent_loss + kl_loss)\n}\n\n## variational autoencoder\nvae <- keras_model(x, x_decoded_mean_squash)\nvae %>% compile(optimizer = \"rmsprop\", loss = vae_loss)\nsummary(vae)\n\n## encoder: model to project inputs on the latent space\nencoder <- keras_model(x, z_mean)\n\n## build a digit generator that can sample from the learned distribution\ngen_decoder_input <- layer_input(shape = latent_dim)\ngen_hidden_decoded <- decoder_hidden(gen_decoder_input)\ngen_up_decoded <- decoder_upsample(gen_hidden_decoded)\ngen_reshape_decoded <- decoder_reshape(gen_up_decoded)\ngen_deconv_1_decoded <- decoder_deconv_1(gen_reshape_decoded)\ngen_deconv_2_decoded <- decoder_deconv_2(gen_deconv_1_decoded)\ngen_x_decoded_relu <- decoder_deconv_3_upsample(gen_deconv_2_decoded)\ngen_x_decoded_mean_squash <- decoder_mean_squash(gen_x_decoded_relu)\ngenerator <- keras_model(gen_decoder_input, gen_x_decoded_mean_squash)\n\n\n#### Data Preparation ####\n\nmnist <- dataset_mnist()\ndata <- lapply(mnist, function(m) {\n  array_reshape(m$x / 255, dim = c(dim(m$x)[1], original_img_size))\n})\nx_train <- data$train\nx_test <- data$test\n\n\n#### Model Fitting ####\n\nvae %>% fit(\n  x_train, x_train, \n  shuffle = TRUE, \n  epochs = epochs, \n  batch_size = batch_size, \n  validation_data = list(x_test, x_test)\n)\n\n\n#### Visualizations ####\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n## display a 2D plot of the digit classes in the latent space\nx_test_encoded <- predict(encoder, x_test, batch_size = batch_size)\nx_test_encoded %>%\n  as_data_frame() %>%\n  mutate(class = as.factor(mnist$test$y)) %>%\n  ggplot(aes(x = V1, y = V2, colour = class)) + geom_point()\n\n## display a 2D manifold of the digits\nn <- 15  # figure with 15x15 digits\ndigit_size <- 28\n\n# we will sample n points within [-4, 4] standard deviations\ngrid_x <- seq(-4, 4, length.out = n)\ngrid_y <- seq(-4, 4, length.out = n)\n\nrows <- NULL\nfor(i in 1:length(grid_x)){\n  column <- NULL\n  for(j in 1:length(grid_y)){\n    z_sample <- matrix(c(grid_x[i], grid_y[j]), ncol = 2)\n    column <- rbind(column, predict(generator, z_sample) %>% matrix(ncol = digit_size))\n  }\n  rows <- cbind(rows, column)\n}\nrows %>% as.raster() %>% plot()"
  },
  {
    "objectID": "examples/vq_vae.html",
    "href": "examples/vq_vae.html",
    "title": "vq_vae",
    "section": "",
    "text": "https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nuse_session_with_seed(7778,\n                      disable_gpu = FALSE,\n                      disable_parallel_cpu = FALSE)\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\nlibrary(curry)\n\nmoving_averages <- tf$python$training$moving_averages\n\n\n# Utilities --------------------------------------------------------\n\nvisualize_images <-\n  function(dataset,\n           epoch,\n           reconstructed_images,\n           random_images) {\n    write_png(dataset, epoch, \"reconstruction\", reconstructed_images)\n    write_png(dataset, epoch, \"random\", random_images)\n    \n  }\n\nwrite_png <- function(dataset, epoch, desc, images) {\n  png(paste0(dataset, \"_epoch_\", epoch, \"_\", desc, \".png\"))\n  par(mfcol = c(8, 8))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:64) {\n    img <- images[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n  \n}\n\n\n# Setup and preprocessing -------------------------------------------------\n\nnp <- import(\"numpy\")\n\n# download from: https://github.com/rois-codh/kmnist\nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 64\nnum_examples_to_generate <- batch_size\n\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n# test\niter <- make_iterator_one_shot(train_dataset)\nbatch <-  iterator_get_next(iter)\nbatch %>% dim()\n\n# Params ------------------------------------------------------------------\n\nlearning_rate <- 0.001\nlatent_size <- 1\nnum_codes <- 64L\ncode_size <- 16L\nbase_depth <- 32\nactivation <- \"elu\"\nbeta <- 0.25\ndecay <- 0.99\ninput_shape <- c(28, 28, 1)\n\n# Models -------------------------------------------------------------------\n\ndefault_conv <-\n  set_defaults(layer_conv_2d, list(padding = \"same\", activation = activation))\ndefault_deconv <-\n  set_defaults(layer_conv_2d_transpose,\n               list(padding = \"same\", activation = activation))\n\n# Encoder ------------------------------------------------------------------\n\nencoder_model <- function(name = NULL,\n                          code_size) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <- default_conv(filters = base_depth, kernel_size = 5)\n    self$conv2 <-\n      default_conv(filters = base_depth,\n                   kernel_size = 5,\n                   strides = 2)\n    self$conv3 <-\n      default_conv(filters = 2 * base_depth, kernel_size = 5)\n    self$conv4 <-\n      default_conv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$conv5 <-\n      default_conv(\n        filters = 4 * latent_size,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = latent_size * code_size)\n    self$reshape <-\n      layer_reshape(target_shape = c(latent_size, code_size))\n    \n    function (x, mask = NULL) {\n      x %>%\n        # output shape:  7 28 28 32\n        self$conv1() %>%\n        # output shape:  7 14 14 32\n        self$conv2() %>%\n        # output shape:  7 14 14 64\n        self$conv3() %>%\n        # output shape:  7 7 7 64\n        self$conv4() %>%\n        # output shape:  7 1 1 4\n        self$conv5() %>%\n        # output shape:  7 4\n        self$flatten() %>%\n        # output shape:  7 16\n        self$dense() %>%\n        # output shape:  7 1 16\n        self$reshape()\n    }\n    \n  })\n}\n\n\n# Decoder ------------------------------------------------------------------\n\ndecoder_model <- function(name = NULL,\n                          input_size,\n                          output_shape) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$reshape1 <- layer_reshape(target_shape = c(1, 1, input_size))\n    self$deconv1 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$deconv2 <-\n      default_deconv(filters = 2 * base_depth, kernel_size = 5)\n    self$deconv3 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$deconv4 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$deconv5 <-\n      default_deconv(filters = base_depth,\n                     kernel_size = 5,\n                     strides = 2)\n    self$deconv6 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$conv1 <-\n      default_conv(filters = output_shape[3],\n                   kernel_size = 5,\n                   activation = \"linear\")\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        # output shape:  7 1 1 16\n        self$reshape1() %>%\n        # output shape:  7 7 7 64\n        self$deconv1() %>%\n        # output shape:  7 7 7 64\n        self$deconv2() %>%\n        # output shape:  7 14 14 64\n        self$deconv3() %>%\n        # output shape:  7 14 14 32\n        self$deconv4() %>%\n        # output shape:  7 28 28 32\n        self$deconv5() %>%\n        # output shape:  7 28 28 32\n        self$deconv6() %>%\n        # output shape:  7 28 28 1\n        self$conv1()\n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = length(output_shape))\n    }\n  })\n}\n\n# Vector quantizer -------------------------------------------------------------------\n\nvector_quantizer_model <- \n  function(name = NULL, num_codes, code_size) {\n    \n    keras_model_custom(name = name, function(self) {\n      self$num_codes <- num_codes\n      self$code_size <- code_size\n      self$codebook <- tf$get_variable(\"codebook\",\n                                       shape = c(num_codes, code_size),\n                                       dtype = tf$float32)\n      self$ema_count <- tf$get_variable(\n        name = \"ema_count\",\n        shape = c(num_codes),\n        initializer = tf$constant_initializer(0),\n        trainable = FALSE\n      )\n      self$ema_means = tf$get_variable(\n        name = \"ema_means\",\n        initializer = self$codebook$initialized_value(),\n        trainable = FALSE\n      )\n      \n      function (x, mask = NULL) {\n\n        # bs * 1 * num_codes\n        distances <- tf$norm(tf$expand_dims(x, axis = 2L) -\n                               tf$reshape(self$codebook,\n                                          c(\n                                            1L, 1L, self$num_codes, self$code_size\n                                          )),\n                             axis = 3L)\n        \n        # bs * 1\n        assignments <- tf$argmin(distances, axis = 2L)\n        \n        # bs * 1 * num_codes\n        one_hot_assignments <-\n          tf$one_hot(assignments, depth = self$num_codes)\n        \n        # bs * 1 * code_size\n        nearest_codebook_entries <- tf$reduce_sum(\n          tf$expand_dims(one_hot_assignments,-1L) * # bs, 1, 64, 1\n            tf$reshape(self$codebook, c(\n              1L, 1L, self$num_codes, self$code_size\n            )),\n          axis = 2L # 1, 1, 64, 16\n        )\n        \n        list(nearest_codebook_entries, one_hot_assignments)\n      }\n    })\n  }\n\n\n# Update codebook ------------------------------------------------------\n\nupdate_ema <- function(vector_quantizer,\n                       one_hot_assignments,\n                       codes,\n                       decay) {\n  # shape = 64\n  updated_ema_count <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_count,\n    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # 64 * 16\n  updated_ema_means <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_means,\n    # selects all assigned values (masking out the others) and sums them up over the batch\n    # (will be divided by count later)\n    tf$reduce_sum(\n      tf$expand_dims(codes, 2L) *\n        tf$expand_dims(one_hot_assignments, 3L),\n      axis = c(0L, 1L)\n    ),\n    decay,\n    zero_debias = FALSE\n  )\n  \n  # Add small value to avoid dividing by zero\n  updated_ema_count <- updated_ema_count + 1e-5\n  updated_ema_means <-\n    updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)\n  \n  tf$assign(vector_quantizer$codebook, updated_ema_means)\n}\n\n\n# Training setup -----------------------------------------------------------\n\nencoder <- encoder_model(code_size = code_size)\ndecoder <- decoder_model(input_size = latent_size * code_size,\n                         output_shape = input_shape)\n\nvector_quantizer <-\n  vector_quantizer_model(num_codes = num_codes, code_size = code_size)\n\noptimizer <- tf$train$AdamOptimizer(learning_rate = learning_rate)\n\ncheckpoint_dir <- \"./vq_vae_checkpoints\"\n\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <-\n  tf$train$Checkpoint(\n    optimizer = optimizer,\n    encoder = encoder,\n    decoder = decoder,\n    vector_quantizer_model = vector_quantizer\n  )\n\ncheckpoint$save(file_prefix = checkpoint_prefix)\n\n# Training loop -----------------------------------------------------------\n\nnum_epochs <- 20\n\nfor (epoch in seq_len(num_epochs)) {\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  reconstruction_loss_total <- 0\n  commitment_loss_total <- 0\n  prior_loss_total <- 0\n  \n  until_out_of_range({\n    \n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      codes <- encoder(x)\n      c(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n      codes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\n      decoder_distribution <- decoder(codes_straight_through)\n      \n      reconstruction_loss <-\n        -tf$reduce_mean(decoder_distribution$log_prob(x))\n      \n      commitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n      \n      prior_dist <- tfd$Multinomial(total_count = 1,\n                                    logits = tf$zeros(c(latent_size, num_codes)))\n      prior_loss <- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))\n      \n      loss <-\n        reconstruction_loss + beta * commitment_loss + prior_loss\n      \n    })\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    update_ema(vector_quantizer,\n               one_hot_assignments,\n               codes,\n               decay)\n    \n    total_loss <- total_loss + loss\n    reconstruction_loss_total <-\n      reconstruction_loss_total + reconstruction_loss\n    commitment_loss_total <- commitment_loss_total + commitment_loss\n    prior_loss_total <- prior_loss_total + prior_loss\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Loss (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss)/trunc(buffer_size/batch_size)) %>% round(4)} loss\",\n      \"  {(as.numeric(reconstruction_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} reconstruction_loss\",\n      \"  {(as.numeric(commitment_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} commitment_loss\",\n      \"  {(as.numeric(prior_loss_total)/trunc(buffer_size/batch_size)) %>% round(4)} prior_loss\",\n      \n    ),\n    \"\\n\"\n  )\n  \n  # display example images (choose your frequency)\n  if (TRUE) {\n    reconstructed_images <- decoder_distribution$mean()\n    # (64, 1, 16)\n    prior_samples <- tf$reduce_sum(\n      # selects one of the codes (masking out 63 of 64 codes)\n      # (bs, 1, 64, 1)\n      tf$expand_dims(prior_dist$sample(num_examples_to_generate),-1L) *\n        # (1, 1, 64, 16)\n        tf$reshape(vector_quantizer$codebook,\n                   c(1L, 1L, num_codes, code_size)),\n      axis = 2L\n    )\n    decoded_distribution_given_random_prior <-\n      decoder(prior_samples)\n    random_images <- decoded_distribution_given_random_prior$mean()\n    visualize_images(\"k\", epoch, reconstructed_images, random_images)\n  }\n}"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "link"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html",
    "href": "guides/keras/customizing_what_happens_in_fit.html",
    "title": "Customizing what happens in fit()",
    "section": "",
    "text": "When you’re doing supervised learning, you can use fit() and everything works smoothly.\nWhen you need to write your own training loop from scratch, you can use the GradientTape and take control of every little detail.\nBut what if you need a custom training algorithm, but you still want to benefit from the convenient features of fit(), such as callbacks, built-in distribution support, or step fusing?\nA core principle of Keras is progressive disclosure of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn’t fall off a cliff if the high-level functionality doesn’t exactly match your use case. You should be able to gain more control over the small details while retaining a commensurate amount of high-level convenience.\nWhen you need to customize what fit() does, you should override the training step function of the Model class. This is the function that is called by fit() for every batch of data. You will then be able to call fit() as usual – and it will be running your own learning algorithm.\nNote that this pattern does not prevent you from building models with the Functional API. You can do this whether you’re building Sequential models, Functional API models, or subclassed models.\nLet’s see how that works."
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "href": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "title": "Customizing what happens in fit()",
    "section": "Setup",
    "text": "Setup\nRequires TensorFlow 2.2 or later.\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "title": "Customizing what happens in fit()",
    "section": "A first simple example",
    "text": "A first simple example\nLet’s start from a simple example:\n\nWe create a new model class by calling new_model_class().\nWe just override the method train_step(data).\nWe return a dictionary mapping metric names (including the loss) to their current value.\n\nThe input argument data is what gets passed to fit as training data:\n\nIf you pass arrays, by calling fit(x, y, ...), then data will be the tuple (x, y)\nIf you pass a tf$data$Dataset, by calling fit(dataset, ...), then data will be what gets yielded by dataset at each batch.\n\nIn the body of the train_step method, we implement a regular training update, similar to what you are already familiar with. Importantly, we compute the loss via self$compiled_loss, which wraps the loss(es) function(s) that were passed to compile().\nSimilarly, we call self$compiled_metrics$update_state(y, y_pred) to update the state of the metrics that were passed in compile(), and we query results from self$metrics at the end to retrieve their current value.\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\nLet’s try this out:\n\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "href": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "title": "Customizing what happens in fit()",
    "section": "Going lower-level",
    "text": "Going lower-level\nNaturally, you could just skip passing a loss function in compile(), and instead do everything manually in train_step. Likewise for metrics.\nHere’s a lower-level example, that only uses compile() to configure the optimizer:\n\nWe start by creating Metric instances to track our loss and a MAE score.\nWe implement a custom train_step() that updates the state of these metrics (by calling update_state() on them), then query them (via result()) to return their current average value, to be displayed by the progress bar and to be pass to any callback.\nNote that we would need to call reset_states() on our metrics between each epoch! Otherwise calling result() would return an average since the start of training, whereas we usually work with per-epoch averages. Thankfully, the framework can do that for us: just list any metric you want to reset in the metrics property of the model. The model will call reset_states() on any object listed here at the beginning of each fit() epoch or at the beginning of a call to evaluate().\n\n\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "href": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "title": "Customizing what happens in fit()",
    "section": "Supporting sample_weight & class_weight",
    "text": "Supporting sample_weight & class_weight\nYou may have noticed that our first basic example didn’t make any mention of sample weighting. If you want to support the fit() arguments sample_weight and class_weight, you’d simply do the following:\n\nUnpack sample_weight from the data argument\nPass it to compiled_loss & compiled_metrics (of course, you could also just apply it manually if you don’t rely on compile() for losses & metrics)\nThat’s it. That’s the list.\n\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "href": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "title": "Customizing what happens in fit()",
    "section": "Providing your own evaluation step",
    "text": "Providing your own evaluation step\nWhat if you want to do the same for calls to model$evaluate()? Then you would override test_step in exactly the same way. Here’s what it looks like:\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "title": "Customizing what happens in fit()",
    "section": "Wrapping up: an end-to-end GAN example",
    "text": "Wrapping up: an end-to-end GAN example\nLet’s walk through an end-to-end example that leverages everything you just learned.\nLet’s consider:\n\nA generator network meant to generate 28x28x1 images.\nA discriminator network meant to classify 28x28x1 images into two classes (“fake” and “real”).\nOne optimizer for each.\nA loss function to train the discriminator.\n\n\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n\nHere’s a feature-complete GAN class, overriding compile() to use its own signature, and implementing the entire GAN algorithm in 17 lines in train_step:\n\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n\nLet’s test-drive it:\n\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n\nHappy training!"
  },
  {
    "objectID": "guides/keras/functional_api.html",
    "href": "guides/keras/functional_api.html",
    "title": "The Functional API",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/functional_api.html#introduction",
    "href": "guides/keras/functional_api.html#introduction",
    "title": "The Functional API",
    "section": "Introduction",
    "text": "Introduction\nThe Keras functional API is a way to create models that are more flexible than the sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\nThe main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\nConsider the following model:\n\n(input: 784-dimensional vectors)\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (10 units, softmax activation)]\n       ↧\n(output: logits of a probability distribution over 10 classes)\n\nThis is a basic graph with three layers. To build this model using the functional API, start by creating an input node:\n\ninputs <- layer_input(shape = c(784))\n\nThe shape of the data is set as a 784-dimensional vector. The batch size is always omitted since only the shape of each sample is specified.\nIf, for example, you have an image input with a shape of (32, 32, 3), you would use:\n\n# Just for demonstration purposes.\nimg_inputs <- layer_input(shape = c(32, 32, 3))\n\nThe inputs that is returned contains information about the shape and dtype of the input data that you feed to your model. Here’s the shape:\n\ninputs$shape\n\nHere’s the dtype:\n\ninputs$dtype\n\nYou create a new node in the graph of layers by calling a layer on this inputs object:\n\ndense <- layer_dense(units = 64, activation = \"relu\")\nx <- dense(inputs)\n\nThe “layer call” action is like drawing an arrow from “inputs” to this layer you created. You’re “passing” the inputs to the dense layer, and you get x as the output.\nYou can also conveniently create the layer and compose it with inputs in one step, like this:\n\nx <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\") \n\nLet’s add a few more layers to the graph of layers:\n\noutputs <- x %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10)\n\nAt this point, you can create a Model by specifying its inputs and outputs in the graph of layers:\n\nmodel <- keras_model(inputs = inputs, outputs = outputs, \n                     name = \"mnist_model\")\n\nLet’s check out what the model summary looks like:\n\nmodel\n\nYou can also plot the model as a graph:\n\nplot(model)\n\nAnd, optionally, display the input and output shapes of each layer in the plotted graph:\n\nplot(model, show_shapes = TRUE)\n\nThis figure and the code are almost identical. In the code version, the connection arrows are replaced by %>% operator.\nA “graph of layers” is an intuitive mental image for a deep learning model, and the functional API is a way to create models that closely mirrors this."
  },
  {
    "objectID": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "href": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "title": "The Functional API",
    "section": "Training, evaluation, and inference",
    "text": "Training, evaluation, and inference\nTraining, evaluation, and inference work exactly in the same way for models built using the functional API as for Sequential models.\nThe Model class offers a built-in training loop (the fit() method) and a built-in evaluation loop (the evaluate() method). Note that you can easily customize these loops to implement training routines beyond supervised learning (e.g. GANs).\nHere, load the MNIST image data, reshape it into vectors, fit the model on the data (while monitoring performance on a validation split), then evaluate the model on the test data:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nx_train <- array_reshape(x_train, c(60000, 784)) / 255\nx_test <-  array_reshape(x_test, c(10000, 784)) / 255 \n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_rmsprop(),\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2)\n\ntest_scores <- model %>% evaluate(x_test, y_test, verbose = 2)\nprint(test_scores)\n\nFor further reading, see the training and evaluation guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#save-and-serialize",
    "href": "guides/keras/functional_api.html#save-and-serialize",
    "title": "The Functional API",
    "section": "Save and serialize",
    "text": "Save and serialize\nSaving the model and serialization work the same way for models built using the functional API as they do for Sequential models. The standard way to save a functional model is to call save_model_tf() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available.\nThis saved file includes the: - model architecture - model weight values (that were learned during training) - model training config, if any (as passed to compile) - optimizer and its state, if any (to restart training where you left off)\n\npath_to_my_model <- tempfile()\nsave_model_tf(model, path_to_my_model)\n\nrm(model)\n# Recreate the exact same model purely from the file:\nmodel <- load_model_tf(path_to_my_model)\n\nFor details, read the model serialization & saving guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "href": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "title": "The Functional API",
    "section": "Use the same graph of layers to define multiple models",
    "text": "Use the same graph of layers to define multiple models\nIn the functional API, models are created by specifying their inputs and outputs in a graph of layers. That means that a single graph of layers can be used to generate multiple models.\nIn the example below, you use the same stack of layers to instantiate two models: an encoder model that turns image inputs into 16-dimensional vectors, and an end-to-end autoencoder model for training.\n\nencoder_input <- layer_input(shape = c(28, 28, 1), \n                             name = \"img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, \n                       name = \"encoder\")\nencoder\n\ndecoder_output <- encoder_output %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\nautoencoder <- keras_model(encoder_input, decoder_output, \n                           name = \"autoencoder\")\nautoencoder\n\nHere, the decoding architecture is strictly symmetrical to the encoding architecture, so the output shape is the same as the input shape (28, 28, 1).\nThe reverse of a Conv2D layer is a Conv2DTranspose layer, and the reverse of a MaxPooling2D layer is an UpSampling2D layer."
  },
  {
    "objectID": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "href": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "title": "The Functional API",
    "section": "All models are callable, just like layers",
    "text": "All models are callable, just like layers\nYou can treat any model as if it were a layer by invoking it on an Input or on the output of another layer. By calling a model you aren’t just reusing the architecture of the model, you’re also reusing its weights.\nTo see this in action, here’s a different take on the autoencoder example that creates an encoder model, a decoder model, and chains them in two calls to obtain the autoencoder model:\n\nencoder_input <- layer_input(shape = c(28, 28, 1), name = \"original_img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, name = \"encoder\")\nencoder\n\ndecoder_input <- layer_input(shape = c(16), name = \"encoded_img\")\ndecoder_output <- decoder_input %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\ndecoder <- keras_model(decoder_input, decoder_output, \n                       name = \"decoder\")\ndecoder\n\nautoencoder_input <- layer_input(shape = c(28, 28, 1), name = \"img\")\nencoded_img <- encoder(autoencoder_input)\ndecoded_img <- decoder(encoded_img)\nautoencoder <- keras_model(autoencoder_input, decoded_img, \n                           name = \"autoencoder\")\nautoencoder\n\nAs you can see, the model can be nested: a model can contain sub-models (since a model is just like a layer). A common use case for model nesting is ensembling. For example, here’s how to ensemble a set of models into a single model that averages their predictions:\n\nget_model <- function() {\n  inputs <- layer_input(shape = c(128))\n  outputs <- inputs %>% layer_dense(1)\n  keras_model(inputs, outputs)\n}\n\nmodel1 <- get_model()\nmodel2 <- get_model()\nmodel3 <- get_model()\n\ninputs <- layer_input(shape = c(128))\ny1 <- model1(inputs)\ny2 <- model2(inputs)\ny3 <- model3(inputs)\noutputs <- layer_average(list(y1, y2, y3))\nensemble_model <- keras_model(inputs = inputs, outputs = outputs)"
  },
  {
    "objectID": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "href": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "title": "The Functional API",
    "section": "Manipulate complex graph topologies",
    "text": "Manipulate complex graph topologies\n\nModels with multiple inputs and outputs\nThe functional API makes it easy to manipulate multiple inputs and outputs. This cannot be handled with the Sequential API.\nFor example, if you’re building a system for ranking customer issue tickets by priority and routing them to the correct department, then the model will have three inputs:\n\nthe title of the ticket (text input),\nthe text body of the ticket (text input), and\nany tags added by the user (categorical input)\n\nThis model will have two outputs:\n\nthe priority score between 0 and 1 (scalar sigmoid output), and\nthe department that should handle the ticket (softmax output over the set of departments).\n\nYou can build this model in a few lines with the functional API:\n\nnum_tags <- 12  # Number of unique issue tags\nnum_words <- 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments <- 4  # Number of departments for predictions\n\ntitle_input <- layer_input(shape = c(NA), name = \"title\")  # Variable-length sequence of ints\nbody_input <- layer_input(shape = c(NA), name = \"body\")  # Variable-length sequence of ints\ntags_input <- layer_input(shape = c(num_tags), name = \"tags\")  # Binary vectors of size `num_tags`\n\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features <- title_input %>% layer_embedding(num_words, 64)\n\n# Embed each word in the text into a 64-dimensional vector\nbody_features <- body_input %>% layer_embedding(num_words, 64)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features <- title_features %>% layer_lstm(128)\n\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features <- body_features %>% layer_lstm(32)\n\n# Merge all available features into a single large vector via concatenation\nx <- layer_concatenate(title_features, body_features, tags_input)\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred <- x %>% layer_dense(1, name = \"priority\")\n\n# Stick a department classifier on top of the features\ndepartment_pred <- x %>% layer_dense(num_departments, name = \"department\")\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel <- keras_model(\n  inputs <- list(title_input, body_input, tags_input),\n  outputs <- list(priority_pred, department_pred)\n)\n\nNow plot the model:\n\nplot(model, show_shapes = TRUE)\n\nWhen compiling this model, you can assign different losses to each output. You can even assign different weights to each loss – to modulate their contribution to the total training loss.\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_binary_crossentropy(from_logits = TRUE),\n    loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights <- c(1, 0.2)\n)\n\nSince the output layers have different names, you could also specify the losses and loss weights with the corresponding layer names:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    priority = loss_binary_crossentropy(from_logits = TRUE),\n    department = loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights = c(priority =  1.0, department = 0.2),\n)\n\nTrain the model by passing lists of NumPy arrays of inputs and targets:\n\n# some helpers to generate dummy input data\nrandom_uniform_array <- function(dim) \n  array(runif(prod(dim)), dim)\n\nrandom_vectorized_array <- function(num_words, dim)\n  array(sample(0:(num_words - 1), prod(dim), replace = TRUE), dim)\n\n# Dummy input data\ntitle_data <- random_vectorized_array(num_words, c(1280, 10))\nbody_data <- random_vectorized_array(num_words, c(1280, 100))\ntags_data <- random_vectorized_array(2, c(1280, num_tags))\n# storage.mode(tags_data) <- \"double\" # from integer\n\n# Dummy target data\npriority_targets <- random_uniform_array(c(1280, 1))\ndept_targets <- random_vectorized_array(2, c(1280, num_departments))\n\nmodel %>% fit(\n  list(title = title_data, body = body_data, tags = tags_data),\n  list(priority = priority_targets, department = dept_targets),\n  epochs = 2,\n  batch_size = 32\n)\n\nWhen calling fit with a tfdataset object, it should yield either a tuple of lists like tuple(list(title_data, body_data, tags_data), list(priority_targets, dept_targets)) or a tuple of named lists like tuple(list(title = title_data, body = body_data, tags = tags_data), list(priority= priority_targets, department= dept_targets)).\nFor more detailed explanation, refer to the training and evaluation guide.\n\n\nA toy ResNet model\nIn addition to models with multiple inputs and outputs, the functional API makes it easy to manipulate non-linear connectivity topologies – these are models with layers that are not connected sequentially, which the Sequential API cannot handle.\nA common use case for this is residual connections. Let’s build a toy ResNet model for CIFAR10 to demonstrate this:\n\ninputs <- layer_input(shape = c(32, 32, 3), name = \"img\")\nblock_1_output <- inputs %>% \n  layer_conv_2d(32, 3, activation = \"relu\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\") %>% \n  layer_max_pooling_2d(3)\n\nblock_2_output <- block_1_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_1_output)\n\nblock_3_output <- block_2_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_2_output) \n\noutputs <- block_3_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\") %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(10)\n\nmodel <- keras_model(inputs, outputs, name = \"toy_resnet\")\nmodel\n\nPlot the model:\n\nplot(model, show_shapes = TRUE)\n\nNow train the model:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()  \n\nx_train <- x_train / 255\nx_test <- x_test / 255\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = loss_categorical_crossentropy(from_logits = TRUE),\n  metrics = \"acc\"\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# for this guide. Try to train on the entire dataset until convergence!\nmodel %>% fit(\n  x_train[1:1000, , , ],\n  y_train[1:1000, ],\n  batch_size = 64,\n  epochs = 1,\n  validation_split = 0.2\n)"
  },
  {
    "objectID": "guides/keras/functional_api.html#shared-layers",
    "href": "guides/keras/functional_api.html#shared-layers",
    "title": "The Functional API",
    "section": "Shared layers",
    "text": "Shared layers\nAnother good use for the functional API are models that use shared layers. Shared layers are layer instances that are reused multiple times in the same model – they learn features that correspond to multiple paths in the graph-of-layers.\nShared layers are often used to encode inputs from similar spaces (say, two different pieces of text that feature similar vocabulary). They enable sharing of information across these different inputs, and they make it possible to train such a model on less data. If a given word is seen in one of the inputs, that will benefit the processing of all inputs that pass through the shared layer.\nTo share a layer in the functional API, call the same layer instance multiple times. For instance, here’s an Embedding layer shared across two different text inputs:\n\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)\n\n# Variable-length sequence of integers\ntext_input_a <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Variable-length sequence of integers\ntext_input_b <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a <- shared_embedding(text_input_a)\nencoded_input_b <- shared_embedding(text_input_b)"
  },
  {
    "objectID": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "href": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "title": "The Functional API",
    "section": "Extract and reuse nodes in the graph of layers",
    "text": "Extract and reuse nodes in the graph of layers\nBecause the graph of layers you are manipulating is a static data structure, it can be accessed and inspected. And this is how you are able to plot functional models as images.\nThis also means that you can access the activations of intermediate layers (“nodes” in the graph) and reuse them elsewhere – which is very useful for something like feature extraction.\nLet’s look at an example. This is a VGG19 model with weights pretrained on ImageNet:\n\nvgg19 <- application_vgg19()\n\nAnd these are the intermediate activations of the model, obtained by querying the graph data structure:\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\n\nUse these features to create a new feature-extraction model that returns the values of the intermediate layer activations:\n\nfeat_extraction_model <-  keras_model(inputs = vgg19$input, \n                                      outputs = features_list)\n\nimg <- random_uniform_array(c(1, 224, 224, 3))\nextracted_features <- feat_extraction_model(img)\n\nThis comes in handy for tasks like neural style transfer, among other things."
  },
  {
    "objectID": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "href": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "title": "The Functional API",
    "section": "Extend the API using custom layers",
    "text": "Extend the API using custom layers\ntf$keras includes a wide range of built-in layers, for example:\n\nConvolutional layers: Conv1D, Conv2D, Conv3D, Conv2DTranspose\nPooling layers: MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D\nRNN layers: GRU, LSTM, ConvLSTM2D\nBatchNormalization, Dropout, Embedding, etc.\n\nBut if you don’t find what you need, it’s easy to extend the API by creating your own layers. All layers subclass the Layer class and implement:\n\ncall method, that specifies the computation done by the layer.\nbuild method, that creates the weights of the layer (this is just a style convention since you can create weights in __init__, as well).\n\nTo learn more about creating layers from scratch, read custom layers and models guide.\nThe following is a basic implementation of layer_dense():\n\nlibrary(tensorflow)\nlibrary(keras)\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units = as.integer(units)\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\n\nFor serialization support in your custom layer, define a get_config method that returns the constructor arguments of the layer instance:\n\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units <- as.integer(units)\n  },\n  \n  build = function(input_shape) {\n    self$w <-\n      self$add_weight(\n        shape = shape(tail(input_shape, 1), self$units),\n        initializer = \"random_normal\",\n        trainable = TRUE\n      )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  \n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  \n  get_config = function() {\n    list(units = self$units)\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\nconfig <- model %>% get_config()\n\nnew_model <- from_config(config, custom_objects = list(layer_custom_dense))\n\nOptionally, implement the class method from_config(class_constructor, config) which is used when recreating a layer instance given its config. The default implementation of from_config is approximately:\n\nfrom_config <- function(layer_constructor, config) \n  do.call(layer_constructor, config)"
  },
  {
    "objectID": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "href": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "title": "The Functional API",
    "section": "When to use the functional API",
    "text": "When to use the functional API\nShould you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier and safer, and has a number of features that subclassed models do not support.\nHowever, model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass Model directly.\nFor an in-depth look at the differences between the functional API and model subclassing, read What are Symbolic and Imperative APIs in TensorFlow 2.0?.\n\nFunctional API strengths:\nThe following properties are also true for Sequential models (which are also data structures), but are not true for subclassed models (which are R code, not data structures).\n\nLess verbose\nThere is no super$initialize(...), no call <- function(...) {   }, etc.\nCompare:\n\ninputs <- layer_input(shape = c(32))\noutputs <- inputs %>% \n  layer_dense(64, activation = 'relu') %>% \n  layer_dense(10)\nmlp <- keras_model(inputs, outputs)\n\nWith the subclassed version:\n\nMLP <- new_model_class(\n  classname = \"MLP\",\n  \n  initialize = function(...) {\n    super$initialize(...)\n    self$dense_1 <- layer_dense(units = 64, activation = 'relu')\n    self$dense_2 <- layer_dense(units = 10)\n  },\n  \n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2()\n  }\n)\n\n# Instantiate the model.\nmlp <- MLP()\n\n# Necessary to create the model's state.\n# The model doesn't have a state until it's called at least once.\ninvisible(mlp(tf$zeros(shape(1, 32))))\n\n\n\nModel validation while defining its connectivity graph\nIn the functional API, the input specification (shape and dtype) is created in advance (using layer_input). Every time you call a layer, the layer checks that the specification passed to it matches its assumptions, and it will raise a helpful error message if not.\nThis guarantees that any model you can build with the functional API will run. All debugging – other than convergence-related debugging – happens statically during the model construction and not at execution time. This is similar to type checking in a compiler.\n\n\nA functional model is plottable and inspectable\nYou can plot the model as a graph, and you can easily access intermediate nodes in this graph. For example, to extract and reuse the activations of intermediate layers (as seen in a previous example):\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\nfeat_extraction_model <- keras_model(inputs = vgg19$input,\n                                     outputs = features_list)\n\n\n\nA functional model can be serialized or cloned\nBecause a functional model is a data structure rather than a piece of code, it is safely serializable and can be saved as a single file that allows you to recreate the exact same model without having access to any of the original code. See the serialization & saving guide.\nTo serialize a subclassed model, it is necessary for the implementer to specify a get_config() and from_config() method at the model level.\n\n\n\nFunctional API weakness:\n\nIt does not support dynamic architectures\nThe functional API treats models as DAGs of layers. This is true for most deep learning architectures, but not all – for example, recursive networks or Tree RNNs do not follow this assumption and cannot be implemented in the functional API."
  },
  {
    "objectID": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "href": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "title": "The Functional API",
    "section": "Mix-and-match API styles",
    "text": "Mix-and-match API styles\nChoosing between the functional API or Model subclassing isn’t a binary decision that restricts you into one category of models. All models in the tf$keras API can interact with each other, whether they’re Sequential models, functional models, or subclassed models that are written from scratch.\nYou can always use a functional model or Sequential model as part of a subclassed model or layer:\n\nunits <- 32L\ntimesteps <- 10L\ninput_dim <- 5L\n\n# Define a Functional model\n\ninputs <- layer_input(c(NA, units))\noutputs <- inputs %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\n\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <-\n      layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <-\n      layer_dense(units = units, activation = \"tanh\")\n    # Our previously-defined Functional model\n    self$classifier <- model\n  },\n  \n  call = function(inputs) {\n    message(\"inputs shape: \", format(inputs$shape))\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis is 1-based\n    message(\"features shape: \", format(features$shape))\n    self$classifier(features)\n  }\n)\n\nlayer_custom_rnn(tf$zeros(shape(1, timesteps, input_dim)))\n\nYou can use any subclassed layer or model in the functional API as long as it implements a call method that follows one of the following patterns:\n\ncall(inputs, ..., training = NULL, mask = NULL) – Where inputs is a tensor or a nested structure of tensors (e.g. a list of tensors), and where optional named arguments training and mask can be present.\nare non-tensor arguments (non-inputs).\ncall(self, inputs, training = NULL, **kwargs) – Where training is a boolean indicating whether the layer should behave in training mode and inference mode.\ncall(self, inputs, mask = NULL, **kwargs) – Where mask is a boolean mask tensor (useful for RNNs, for instance).\ncall(self, inputs, training = NULL, mask = NULL, **kwargs) – Of course, you can have both masking and training-specific behavior at the same time.\n\nAdditionally, if you implement the get_config method on your custom Layer or model, the functional models you create will still be serializable and cloneable.\nHere’s a quick example of a custom RNN, written from scratch, being used in a functional model:\n\nunits <- 32 \ntimesteps <- 10 \ninput_dim <- 5 \nbatch_size <- 16\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units  \n    self$projection_1 <- layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <- layer_dense(units = units, activation = \"tanh\")\n    self$classifier <- layer_dense(units = 1)\n  },\n  \n  call = function(inputs) {\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis arg is 1-based\n    self$classifier(features)\n  }\n)\n    \n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs <- layer_input(batch_shape = c(batch_size, timesteps, input_dim))\noutputs <- inputs %>% \n  layer_conv_1d(32, 3) %>% \n  layer_custom_rnn()\n\nmodel <- keras_model(inputs, outputs)\nmodel(tf$zeros(shape(1, 10, 5)))"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "",
    "text": "library(magrittr)\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Layer class: a combination of state (weights) and some computation",
    "text": "The Layer class: a combination of state (weights) and some computation\nOne of the central abstractions in Keras is the Layer class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).\nHere’s a densely-connected layer. It has a state: the variables w and b.\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \"float32\"\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \"float32\"),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nYou would use a layer by calling it on some tensor input(s), much like a regular function.\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\nLinear behaves similarly to a layer present in the Python interface to keras (e.g., keras$layers$Dense).\nHowever, one additional step is needed to make it behave like the builtin layers present in the keras R package (e.g., layer_dense()).\nKeras layers in R are designed to compose nicely with the pipe operator (%>%), so that the layer instance is conveniently created on demand when an existing model or tensor is piped in. In order to make a custom layer similarly compose nicely with the pipe, you can call create_layer_wrapper() on the layer class constructor.\n\nlayer_linear <- create_layer_wrapper(Linear)\n\nNow layer_linear is a layer constructor that composes nicely with %>%, just like the built-in layers:\n\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n\nmodel\n\nBecause the pattern above is so common, there is a convenience function that combines the steps of subclassing keras$layers$Layer and calling create_layer_wrapper on the output: the Layer function. The layer_linear defined below is identical to the layer_linear defined above.\n\nlayer_linear <- Layer(\n  \"Linear\",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\nFor the remainder of this vignette we’ll be using the %py_class% constructor. However, in your own code feel free to use create_layer_wrapper and/or Layer if you prefer.\nNote that the weights w and b are automatically tracked by the layer upon being set as layer attributes:\n\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n\nYou also have access to a quicker shortcut for adding a weight to a layer: the add_weight() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers can have non-trainable weights",
    "text": "Layers can have non-trainable weights\nBesides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\nHere’s how to add and use a non-trainable weight:\n\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\ny <- my_sum(x)\nprint(as.numeric(y))\n\nIt’s part of layer$weights, but it gets categorized as a non-trainable weight:\n\ncat(\"weights:\", length(my_sum$weights), \"\\n\")\ncat(\"non-trainable weights:\", length(my_sum$non_trainable_weights), \"\\n\")\n\n# It's not included in the trainable weights:\ncat(\"trainable_weights:\", my_sum$trainable_weights, \"\\n\")"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Best practice: deferring weight creation until the shape of the inputs is known",
    "text": "Best practice: deferring weight creation until the shape of the inputs is known\nOur Linear layer above took an input_dimargument that was used to compute the shape of the weights w and b in initialize():\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nIn many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\nIn the Keras API, we recommend creating layer weights in the build(self, inputs_shape) method of your layer. Like this:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nThe build() method of your layer will automatically run the first time your layer instance is called. You now have a layer that can handle an arbitrary number of input features:\n\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n\nImplementing build() separately as shown above nicely separates creating weights only once from using weights in every call. However, for some advanced custom layers, it can become impractical to separate the state creation and computation. Layer implementers are allowed to defer weight creation to the first call(), but need to take care that later calls use the same weights. In addition, since call() is likely to be executed for the first time inside a tf_function(), any variable creation that takes place in call() should be wrapped in a tf$init_scope()."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers are recursively composable",
    "text": "Layers are recursively composable\nIf you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer.\nWe recommend creating such sublayers in the initialize() method and leave it to the first call() to trigger building their weights.\n\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\ncat(\"weights:\", length(mlp$weights), \"\\n\")\ncat(\"trainable weights:\", length(mlp$trainable_weights), \"\\n\")"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_loss() method",
    "text": "The add_loss() method\nWhen writing the call() method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling self$add_loss(value):\n\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n\nThese losses (including those created by any inner layer) can be retrieved via layer$losses. This property is reset at the start of every call() to the top-level layer, so that layer$losses always contains the loss values created during the last forward pass.\n\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n\nIn addition, the loss property also contains regularization losses created for the weights of any inner layer:\n\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n\nThese losses are meant to be taken into account when writing training loops, like this:\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n\nFor a detailed guide about writing training loops, see the guide to writing a training loop from scratch.\nThese losses also work seamlessly with fit() (they get automatically summed and added to the main loss, if any):\n\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \"adam\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_metric() method",
    "text": "The add_metric() method\nSimilarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training.\nConsider the following layer: a “logistic endpoint” layer. It takes as inputs predictions and targets, it computes a loss which it tracks via add_loss(), and it computes an accuracy scalar, which it tracks via add_metric().\n\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n\nMetrics tracked in this way are accessible via layer$metrics:\n\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\"layer$metrics: \")\nstr(layer$metrics)\ncat(\"current accuracy value:\", as.numeric(layer$metrics[[1]]$result()), \"\\n\")\n\nJust like for add_loss(), these metrics are tracked by fit():\n\ninputs <- layer_input(shape(3), name = \"inputs\")\ntargets <- layer_input(shape(10), name = \"targets\")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "You can optionally enable serialization on your layers",
    "text": "You can optionally enable serialization on your layers\nIf you need your custom layers to be serializable as part of a Functional model, you can optionally implement a get_config() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\nnew_layer <- Linear$from_config(config)\n\nNote that the initialize() method of the base Layer class takes some additional named arguments, in particular a name and a dtype. It’s good practice to pass these arguments to the parent class in initialize() and to include them in the layer config:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\nnew_layer <- Linear$from_config(config)\n\nIf you need more flexibility when deserializing the layer from its config, you can also override the from_config() class method. This is the base implementation of from_config():\n\nfrom_config <- function(cls, config) do.call(cls, config)\n\nTo learn more about serialization and saving, see the complete guide to saving and serializing models."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged training argument in the call() method",
    "text": "Privileged training argument in the call() method\nSome layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call() method.\nBy exposing this argument in call(), you enable the built-in training and evaluation loops (e.g. fit()) to correctly use the layer in training and inference. Note, the default of NULL means that the training parameter will be inferred by keras from the training context (e.g., it will be TRUE if called from fit(), FALSE if called from predict())\n\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged mask argument in the call() method",
    "text": "Privileged mask argument in the call() method\nThe other privileged argument supported by call() is the mask argument.\nYou will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\nKeras will automatically pass the correct mask argument to call() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\nTo learn more about masking and how to write masking-enabled layers, please check out the guide “understanding padding and masking”."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Model class",
    "text": "The Model class\nIn general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model – the object you will train.\nFor instance, in a ResNet50 model, you would have several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.\nThe Model class has the same API as Layer, with the following differences:\n\nIt has support for built-in training, evaluation, and prediction methods (fit(), evaluate(), predict()).\nIt exposes the list of its inner layers, via the model$layers property.\nIt exposes saving and serialization APIs (save_model_tf(), save_model_weights_tf(), …)\n\nEffectively, the Layer class corresponds to what we refer to in the literature as a “layer” (as in “convolution layer” or “recurrent layer”) or as a “block” (as in “ResNet block” or “Inception block”).\nMeanwhile, the Model class corresponds to what is referred to in the literature as a “model” (as in “deep learning model”) or as a “network” (as in “deep neural network”).\nSo if you’re wondering, “should I use the Layer class or the Model class?”, ask yourself: will I need to call fit() on it? Will I need to call save() on it? If so, go with Model. If not (either because your class is just a block in a bigger system, or because you are writing training & saving code yourself), use Layer.\nFor instance, we could take our mini-resnet example above, and use it to build a Model that we could train with fit(), and that we could save with save_model_weights_tf():\n\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Putting it all together: an end-to-end example",
    "text": "Putting it all together: an end-to-end example\nHere’s what you’ve learned so far:\n\nA Layer encapsulates a state (created in initialize() or build()), and some computation (defined in call()).\nLayers can be recursively nested to create new, bigger computation blocks.\nLayers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\nThe outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n\nLet’s put all of these things together into an end-to-end example: we’re going to implement a Variational AutoEncoder (VAE). We’ll train it on MNIST digits.\nOur VAE will be a subclass of Model, built as a nested composition of layers that subclass Layer. It will feature a regularization loss (KL divergence).\n\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \"encoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \"Converts z, the encoded digit vector, back into a readable digit.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \"decoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \"Combines the encoder and decoder into an end-to-end model for training.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \"autoencoder\", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n\nLet’s write a simple training loop on MNIST:\n\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\"Start of epoch %d\\n\", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\"step %d: mean loss = %.4f\\n\", step, loss_metric$result()))\n      }\n    }\n  })\n}\n\nNote that since the VAE is subclassing Model, it features built-in training loops. So you could also have trained it like this:\n\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Beyond object-oriented development: the Functional API",
    "text": "Beyond object-oriented development: the Functional API\nIf you prefer a less object-oriented way of programming, you can also build models using the Functional API. Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\nFor instance, the Functional API example below reuses the same Sampling layer we defined in the example above:\n\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \"encoder_input\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \"z_mean\")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \"z_log_var\")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \"encoder\")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \"z_sampling\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \"sigmoid\")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \"decoder\")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \"vae\")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n\nFor more information, make sure to read the Functional API guide."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Defining custom layers and models in an R package",
    "text": "Defining custom layers and models in an R package\nUnfortunately you can’t use anything that creates references to Python objects, at the top-level of an R package.\nHere is why: when you build an R package, all the R files in the R/ directory get sourced in an R environment (the package namespace), and then that environment is saved as part of the package bundle. Loading the package means restoring the saved R environment. This means that the R code only gets sourced once, at build time. If you create references to external objects (e.g., Python objects) at package build time, they will be NULL pointers when the package is loaded, because the external objects they pointed to at build time no longer exist at load time.\nThe solution is to delay creating references to Python objects until run time. Fortunately, %py_class%, Layer(), and create_layer_wrapper(R6Class(...)) are all lazy about initializing the Python reference, so they are safe to define and export in an R package.\nIf you’re writing an R package that uses keras and reticulate, this article might be helpful to read over."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Summary",
    "text": "Summary\nIn this guide you learned about creating custom layers and models in keras.\n\nThe constructors available: new_layer_class(), %py_class%, create_layer_wrapper(), R6Class(), Layer().\nWhat methods to you might want to define to your model: initialize(), build(), call(), and get_config().\nWhat convenience methods are available when you subclass keras$layers$Layer: add_weight(), add_loss(), and add_metric()"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html",
    "href": "guides/keras/preprocessing_layers.html",
    "title": "Working with preprocessing layers",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "href": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "title": "Working with preprocessing layers",
    "section": "Keras preprocessing",
    "text": "Keras preprocessing\nThe Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "href": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "title": "Working with preprocessing layers",
    "section": "Available preprocessing layers",
    "text": "Available preprocessing layers\n\nText preprocessing\n\nlayer_text_vectorization(): turns raw strings into an encoded representation that can be read by a layer_embedding() or layer_dense() layer.\n\n\n\nNumerical features preprocessing\n\nlayer_normalization(): performs feature-wise normalization of input features.\nlayer_discretization(): turns continuous numerical features into integer categorical features.\n\n\n\nCategorical features preprocessing\n\nlayer_category_encoding(): turns integer categorical features into one-hot, multi-hot, or count-based, dense representations.\nlayer_hashing(): performs categorical feature hashing, also known as the “hashing trick”.\nlayer_string_lookup(): turns string categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\nlayer_integer_lookup(): turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\n\n\nImage preprocessing\nThese layers are for standardizing the inputs of an image model.\n\nlayer_resizing(): resizes a batch of images to a target size.\nlayer_rescaling(): rescales and offsets the values of a batch of images (e.g., going from inputs in the [0, 255] range to inputs in the [0, 1] range.\nlayer_center_crop(): returns a center crop of a batch of images.\n\n\n\nImage data augmentation\nThese layers apply random augmentation transforms to a batch of images. They are only active during training.\n\nlayer_random_crop()\nlayer_random_flip()\nlayer_random_flip()\nlayer_random_translation()\nlayer_random_rotation()\nlayer_random_zoom()\nlayer_random_height()\nlayer_random_width()\nlayer_random_contrast()"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "href": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "title": "Working with preprocessing layers",
    "section": "The adapt() function",
    "text": "The adapt() function\nSome preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n\nlayer_text_vectorization(): holds a mapping between string tokens and integer indices\nlayer_string_lookup() and layer_integer_lookup(): hold a mapping between input values and integer indices.\nlayer_normalization(): holds the mean and standard deviation of the features.\nlayer_discretization(): holds information about value bucket boundaries.\n\nCrucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by “adapting” them on data.\nYou set the state of a preprocessing layer by exposing it to training data, via adapt():\n\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\"Features mean: %.2f\", mean(normalized_data))\nsprintf(\"Features std: %.2f\", sd(normalized_data))\n\nadapt() takes either an array or a tf_dataset. In the case of layer_string_lookup() and layer_text_vectorization(), you can also pass a character vector:\n\ndata <- c(\n  \"Congratulations!\",\n  \"Today is your day.\",\n  \"You're off to Great Places!\",\n  \"You're off and away!\",\n  \"You have brains in your head.\",\n  \"You have feet in your shoes.\",\n  \"You can steer yourself\",\n  \"any direction you choose.\",\n  \"You're on your own. And you know what you know.\",\n  \"And YOU are the one who'll decide where to go.\"\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n\nIn addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the adapt() call, they can be set without relying on the layer’s internal computation. For instance, if external vocabulary files for the layer_text_vectorization(), layer_string_lookup(), or layer_integer_lookup() layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer’s constructor arguments.\nHere’s an example where we instantiate a layer_string_lookup() layer with precomputed vocabulary:\n\nvocab <- c(\"a\", \"b\", \"c\", \"d\")\ndata <- as_tensor(rbind(c(\"a\", \"c\", \"d\"),\n                        c(\"d\", \"z\", \"b\")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing data before the model or inside the model",
    "text": "Preprocessing data before the model or inside the model\nThere are two ways you could be using preprocessing layers:\nOption 1: Make them part of the model, like this:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n\nWith this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you’re training on GPU, this is the best option for the layer_normalization() layer, and for all image preprocessing and data augmentation layers.\nOption 2: apply it to your tf_dataset, so as to obtain a dataset that yields batches of preprocessed data, like this:\n\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\nWith this option, your preprocessing will happen on CPU, asynchronously, and will be buffered before going into the model. In addition, if you call tfdatasets::dataset_prefetch() on your dataset, the preprocessing will happen efficiently in parallel with training:\n\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n\nThis is the best option for layer_text_vectorization(), and all structured data preprocessing layers. It can also be a good option if you’re training on CPU and you use image preprocessing layers."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "href": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "title": "Working with preprocessing layers",
    "section": "Benefits of doing preprocessing inside the model at inference time",
    "text": "Benefits of doing preprocessing inside the model at inference time\nEven if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew.\nWhen all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to [-1, +1] or to [0, 1], etc. This is especially powerful if you’re exporting your model to another runtime, such as TensorFlow.js: you won’t have to reimplement your preprocessing pipeline in JavaScript.\nIf you initially put your preprocessing layers in your tf_dataset pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing during multi-worker training",
    "text": "Preprocessing during multi-worker training\nPreprocessing layers are compatible with the tf.distribute API for running training across multiple machines.\nIn general, preprocessing layers should be placed inside a strategy$scope() and called either inside or before the model as discussed above.\n\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n\nFor more details, refer to the preprocessing section of the distributed input guide."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#quick-recipes",
    "href": "guides/keras/preprocessing_layers.html#quick-recipes",
    "title": "Working with preprocessing layers",
    "section": "Quick recipes",
    "text": "Quick recipes\n\nImage data augmentation\nNote that image data augmentation layers are only active during training (similar to the layer_dropout() layer).\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n\nYou can see a similar setup in action in the example image classification from scratch.\n\n\nNormalizing numerical features\n\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"adam\",\n          loss = \"sparse_categorical_crossentropy\")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n\n\n\nEncoding string categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(c(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\"a\", \"b\", \"c\", \"d\", \"e\", \"\")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n\nNote that, here, index 0 is reserved for out-of-vocabulary values (values that were not seen during adapt()).\nYou can see the layer_string_lookup() in action in the Structured data classification from scratch example.\n\n\nEncoding integer categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \"int32\")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \"int32\")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n\nNote that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_token and oov_token constructor arguments of layer_integer_lookup().\nYou can see the layer_integer_lookup() in action in the example structured data classification from scratch.\n\n\nApplying the hashing trick to an integer categorical feature\nIf you have a categorical feature that can take many different values (on the order of 10e3 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the “hashing trick”: hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing.\n\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \"int64\")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n\n\n\nEncoding text as a sequence of token indices\nThis is how you should preprocess text to be passed to an Embedding layer.\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\",\n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\nmodel %>%\n  compile(optimizer = \"rmsprop\", loss = \"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output:\", as.array(test_output), \"\\n\")\n\nYou can see the layer_text_vectorization() layer in action, combined with an Embedding mode, in the example text classification from scratch.\nNote that when training such a model, for best performance, you should always use the layer_text_vectorization() layer as part of the input pipeline.\n\n\nEncoding text as a dense matrix of ngrams with multi-hot encoding\nThis is how you can preprocess text to be passed to a Dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"multi_hot\" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\n\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output); cat(\"\\n\")\n\n\n\nEncoding text as a dense matrix of ngrams with TF-IDF weighting\nThis is an alternative way of preprocessing text before passing it to a layer_dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"tf-idf\" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\"CPU\"), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\"Training model...\")\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#important-gotchas",
    "href": "guides/keras/preprocessing_layers.html#important-gotchas",
    "title": "Working with preprocessing layers",
    "section": "Important gotchas",
    "text": "Important gotchas\n\nWorking with lookup layers with very large vocabularies\nYou may find yourself working with a very large vocabulary in a layer_text_vectorization(), a layer_string_lookup() layer, or an layer_integer_lookup() layer. Typically, a vocabulary larger than 500MB would be considered “very large”.\nIn such case, for best performance, you should avoid using adapt(). Instead, pre-compute your vocabulary in advance (you could use Apache Beam or TF Transform for this) and store it in a file. Then load the vocabulary into the layer at construction time by passing the filepath as the vocabulary argument."
  },
  {
    "objectID": "guides/keras/python_subclasses.html",
    "href": "guides/keras/python_subclasses.html",
    "title": "Python Subclasses",
    "section": "",
    "text": "When using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses R6::R6Class as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R."
  },
  {
    "objectID": "guides/keras/python_subclasses.html#examples",
    "href": "guides/keras/python_subclasses.html#examples",
    "title": "Python Subclasses",
    "section": "Examples",
    "text": "Examples\n\nA custom constraint (R6)\nFor demonstration purposes, let’s say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\nNonNegative <- R6::R6Class(\"NonNegative\",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \"__call__\" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n\nThe r_to_py method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\nNew class instances are generated by calling the class directly: NonNegative() (not NonNegative$new())\nAll methods (functions) are (potentially) modified to ensure their first argument is self.\nAll methods have in scope __class__, super and the class name (NonNegative).\nFor convenience, some method names are treated as aliases:\n\ninitialize is treated as an alias for __init__()\nfinalize is treated as an alias for __del__()\n\nsuper can be accessed in 3 ways:\n\nR6 style, which supports only single inheritance (the most common type)\n\nsuper$initialize()\n\nPython 2 style, which requires explicitly providing the class generator and instance\n\nsuper(NonNegative, self)$`__init__`()\n\nPython 3 style\n\nsuper()$`__init__`()\nWhen subclassing Keras base classes, it is generally your responsibility to call super$initialize() if you are masking a superclass initializer by providing your own initialize method.\nPassing convert=FALSE to r_to_py() will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with convert=TRUE, namely, modifying some Python objects, like dictionaries or lists, in-place.\nActive bindings (methods supplied to R6Class(active=...)) are converted to Python @property-decorated methods.\nR6 classes with private methods or attributes are not supported.\nThe argument supplied to inherit can be:\n\nmissing or NULL\na Python class generator\nan R6 class generator, as long as it can be converted to a Python class generator as well\na list of Python/R6 classes (for multiple inheritance)\nA list of superclasses, with optional additional keywords (e.g., metaclass=, only for advanced Python use cases)\n\n\n\n\nA custom constraint (%py_class%)\nAs an alternative to r_to_py(R6Class(...)), we also provide %py_class%, a more concise alternative syntax for achieving the same outcome. %py_class% is heavily inspired by the Python class statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using %py_class%:\n\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \"__call__\" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n\nNotice, this is very similar to the equivalent Python code:\n\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n\nSome (potentially surprising) notes about %py_class%:\n\nJust like the Python class statement, it assigns the constructed class in the current scope! (There is no need to write NonNegative <- ...).\nThe left hand side can be:\n\nA bare symbol, ClassName\nA pseudo-call, with superclasses and keywords as arguments: ClassName(Superclass1, Superclass2, metaclass=my_metaclass)\n\nThe right hand side is evaluated in a new environment to form the namespace for the class methods.\n%py_class% objects can be safely defined at the top level of an R package. (see details about delay_load below)\nTwo keywords are treated specially: convert and delay_load.\nIf you want to call r_to_py with convert=FALSE, pass it as a keyword:\n\n\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n\n\nYou can delay creating the python type object until this first time a class instance is created by passing delay_load=TRUE. The default value is FALSE for most contexts, but TRUE if you are in an R package. (The actual test performed is identical(topenv(), globalenv())). If a %py_class% type object is delayed, it will display \"<<R6type>.ClassName> (delayed)\" when printed.\nAn additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the __doc__ attribute of the class or method. The doc string will then be visible to both python and R tools e.g. reticulate::py_help(). See ?py_class for an example.\n\nIn all other regards, %py_class% is equivalent to r_to_py(R6Class()) (indeed, under the hood, they do the same thing).\n\n\nA custom layer (R6)\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \"CustomLayer\",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n\n\n\nA custom layer (%py_class%)\nor using %py_class%:\n\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}"
  },
  {
    "objectID": "guides/keras/sequential_model.html",
    "href": "guides/keras/sequential_model.html",
    "title": "The Sequential model",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "href": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "title": "The Sequential model",
    "section": "When to use a Sequential model",
    "text": "When to use a Sequential model\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\nSchematically, the following Sequential model:\n\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>% \n  layer_dense(2, activation = \"relu\", name = \"layer1\") %>% \n  layer_dense(3, activation = \"relu\", name = \"layer2\") %>% \n  layer_dense(4, name = \"layer3\")\n\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n\nis equivalent to this function:\n\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \"relu\", name = \"layer1\")\nlayer2 <- layer_dense(units = 3, activation = \"relu\", name = \"layer2\")\nlayer3 <- layer_dense(units = 4, name = \"layer3\")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n\nA Sequential model is not appropriate when:\n\nYour model has multiple inputs or multiple outputs\nAny of your layers has multiple inputs or multiple outputs\nYou need to do layer sharing\nYou want non-linear topology (e.g. a residual connection, a multi-branch model)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "href": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "title": "The Sequential model",
    "section": "Creating a Sequential model",
    "text": "Creating a Sequential model\nYou can create a Sequential model by piping a model through a series layers.\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(4)\n\nIts layers are accessible via the layers attribute:\n\nmodel$layers\n\nYou can also create a Sequential model incrementally:\n\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \"relu\")\nmodel %>% layer_dense(3, activation = \"relu\")\nmodel %>% layer_dense(4)\n\nNote that there’s also a corresponding pop() method to remove layers: a Sequential model behaves very much like a stack of layers.\n\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n\nAlso note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nmodel <- keras_model_sequential(name = \"my_sequential\")\nmodel %>% layer_dense(2, activation = \"relu\", name = \"layer1\")\nmodel %>% layer_dense(3, activation = \"relu\", name = \"layer2\")\nmodel %>% layer_dense(4, name = \"layer3\")"
  },
  {
    "objectID": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "href": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "title": "The Sequential model",
    "section": "Specifying the input shape in advance",
    "text": "Specifying the input shape in advance\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn’t “built”: it has no weights (and calling model$weights results in an error stating just this). The weights are created when the model first sees some input data:\n\nmodel <- keras_model_sequential() %>% \n        layer_dense(2, activation = \"relu\") %>% \n        layer_dense(3, activation = \"relu\") %>% \n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n\n\n# The model summary is also not available:\nsummary(model)\n\n\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\"Number of weights after calling the model:\", length(model$weights), \"\\n\")  # 6\n\nOnce a model is “built”, you can call its summary() method to display its contents (the summary() method is also called by the default print() method:\n\nsummary(model)\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an input_shape argument to your model, so that it knows its input shape from the start:\n\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \"relu\")\n\nmodel\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\nIn general, it’s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is."
  },
  {
    "objectID": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "href": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "title": "The Sequential model",
    "section": "A common debugging workflow: %>% + summary()",
    "text": "A common debugging workflow: %>% + summary()\nWhen building a new Sequential architecture, it’s useful to incrementally stack layers and print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n  \nmodel %>% \n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) \n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(2) \n\n# And now?\nmodel\n\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n\nVery practical, right?"
  },
  {
    "objectID": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "href": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "title": "The Sequential model",
    "section": "What to do once you have a model",
    "text": "What to do once you have a model\nOnce your model architecture is ready, you will want to:\n\nTrain your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\nSave your model to disk and restore it. See our guide to serialization & saving.\nSpeed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training."
  },
  {
    "objectID": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Feature extraction with a Sequential model",
    "text": "Feature extraction with a Sequential model\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n\nHere’s a similar example that only extract features from one layer:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\", name = \"my_intermediate_layer\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \"my_intermediate_layer\")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Transfer learning with a Sequential model",
    "text": "Transfer learning with a Sequential model\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren’t familiar with it, make sure to read our guide to transfer learning.\nHere are two common transfer learning blueprint involving Sequential models.\nFirst, let’s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model$layers and set layer$trainable = FALSE on each layer, except the last one. Like this:\n\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:"
  },
  {
    "objectID": "guides/keras/transfer_learning.html",
    "href": "guides/keras/transfer_learning.html",
    "title": "Transfer learning and fine-tuning",
    "section": "",
    "text": "library(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#introduction",
    "href": "guides/keras/transfer_learning.html#introduction",
    "title": "Transfer learning and fine-tuning",
    "section": "Introduction",
    "text": "Introduction\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\nTake layers from a previously trained model.\nFreeze them, so as to avoid destroying any of the information they contain during future training rounds.\nAdd some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\nTrain the new layers on your dataset.\n\nA last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\nFirst, we will go over the Keras trainable API in detail, which underlies most transfer learning and fine-tuning workflows.\nThen, we’ll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle “cats vs dogs” classification dataset.\nThis is adapted from Deep Learning with R and the 2016 blog post “building powerful image classification models using very little data”."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Freezing layers: understanding the trainable attribute",
    "text": "Freezing layers: understanding the trainable attribute\nLayers and models have three weight attributes:\n\nweights is the list of all weights variables of the layer.\ntrainable_weights is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\nnon_trainable_weights is the list of those that aren’t meant to be trained. Typically they are updated by the model during the forward pass.\n\nExample: the Dense layer has 2 trainable weights (kernel and bias)\n\nlayer <- layer_dense(units = 3)\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is layer_batch_normalization(). It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the guide to writing new layers from scratch.\nExample: The layer instance returned by layer_batch_normalization() has 2 trainable weights and 2 non-trainable weights\n\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nLayers and models also feature a boolean attribute trainable. Its value can be changed. Setting layer$trainable to FALSE moves all the layer’s weights from trainable to non-trainable. This is called “freezing” the layer: the state of a frozen layer won’t be updated during training (either when training with fit() or when training with any custom loop that relies on trainable_weights to apply gradient updates).\nExample: setting trainable to False\n\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\"weights: %s\", length(layer$weights))\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \"relu\")\nlayer2 <- layer_dense(units = 3, activation = \"sigmoid\")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n\nDo not confuse the layer$trainable attribute with the training argument in a layer instance’s call signature layer(training =) (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the Keras FAQ."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Recursive setting of the trainable attribute",
    "text": "Recursive setting of the trainable attribute\nIf you set trainable = FALSE on a model or on any layer that has sublayers, all child layers become non-trainable as well.\nExample:\n\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \"sigmoid\")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "href": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "title": "Transfer learning and fine-tuning",
    "section": "The typical transfer-learning workflow",
    "text": "The typical transfer-learning workflow\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\nInstantiate a base model and load pre-trained weights into it.\nFreeze all layers in the base model by setting trainable = FALSE.\nCreate a new model on top of the output of one (or several) layers from the base model.\nTrain your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\nInstantiate a base model and load pre-trained weights into it.\nRun your new dataset through it and record the output of one (or several) layers from the base model. This is called feature extraction.\nUse that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it’s a lot faster and cheaper.\nAn issue with that second workflow, though, is that it doesn’t allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\nHere’s what the first workflow looks like in Keras:\nFirst, instantiate a base model with pre-trained weights.\n\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\nThen, freeze the base model.\n\nbase_model$trainable <- FALSE\n\nCreate a new model on top.\n\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n\nTrain the model on new data.\n\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#fine-tuning",
    "href": "guides/keras/transfer_learning.html#fine-tuning",
    "title": "Transfer learning and fine-tuning",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting – keep that in mind.\nIt is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\nIt’s also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\nThis is how to implement fine-tuning of the whole base model:\n\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n\nImportant note about compile() and trainable\nCalling compile() on a model is meant to “freeze” the behavior of that model. This implies that the trainable attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until compile is called again. Hence, if you change any trainable value, make sure to call compile() again on your model for your changes to be taken into account.\nImportant notes about layer_batch_normalization()\nMany image models contain BatchNormalization layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\nBatchNormalization contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\nWhen you set bn_layer$trainable = FALSE, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as weight trainability and inference/training modes are two orthogonal concepts. But the two are tied in the case of the BatchNormalization layer.\nWhen you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = FALSE when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou’ll see this pattern in action in the end-to-end example at the end of this guide."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "href": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "title": "Transfer learning and fine-tuning",
    "section": "Transfer learning and fine-tuning with a custom training loop",
    "text": "Transfer learning and fine-tuning with a custom training loop\nIf instead of fit(), you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list model$trainable_weights when applying gradient updates:\n\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n\nLikewise for fine-tuning."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "href": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "title": "Transfer learning and fine-tuning",
    "section": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset",
    "text": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\nTo solidify these concepts, let’s walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats vs. dogs” classification dataset.\n\nGetting the data\nFirst, let’s fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you’ll probably want to use the utility image_dataset_from_directory() to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n# reticulate::py_install(\"tensorflow_datasets\", pip = TRUE)\ntfds <- reticulate::import(\"tensorflow_datasets\")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\"Number of training samples: %d\", length(train_ds))\nprintf(\"Number of validation samples: %d\", length(validation_ds) )\nprintf(\"Number of test samples: %d\", length(test_ds))\n\nThese are the first 9 images in the training dataset – as you can see, they’re all different sizes.\n\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\"label: %s   size: %s\",\n                  label, paste(dim(image), collapse = \" x \")))\n  })\n\nWe can also see that label 1 is “dog” and label 0 is “cat”.\n\n\nStandardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn’t a great fit for feeding a neural network. We need to do 2 things:\n\nStandardize to a fixed image size. We pick 150x150.\nNormalize pixel values between -1 and 1. We’ll do this using a layer_normalization() as part of the model itself.\n\nIn general, it’s a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you’ll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\nHere, we’ll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we’ll do the input value scaling as part of the model, when we create it.\nLet’s resize images to 150x150:\n\nlibrary(magrittr, include.only = \"%<>%\")\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n\nBesides, let’s batch the data and use caching and prefetching to optimize loading speed.\n\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n\n\n\nUsing random data augmentation\nWhen you don’t have a large image dataset, it’s a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(.1)\n\nLet’s visualize what the first image of the first batch looks like after various random transformations:\n\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\")\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#build-a-model",
    "href": "guides/keras/transfer_learning.html#build-a-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Build a model",
    "text": "Build a model\nNow let’s build a model that follows the blueprint we’ve explained earlier.\nNote that:\n\nWe add layer_rescaling() to scale input values (initially in the [0, 255] range) to the [-1, 1] range.\nWe add a layer_dropout() before the classification layer, for regularization.\nWe make sure to pass training = FALSE when calling the base model, so that it runs in inference mode, so that batchnorm statistics don’t get updated even after we unfreeze the base model for fine-tuning.\n\n\nbase_model = application_xception(\n  weights = \"imagenet\", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs <- layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#train-the-top-layer",
    "href": "guides/keras/transfer_learning.html#train-the-top-layer",
    "title": "Transfer learning and fine-tuning",
    "section": "Train the top layer",
    "text": "Train the top layer\n\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 2\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "href": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Do a round of fine-tuning of the entire model",
    "text": "Do a round of fine-tuning of the entire model\nFinally, let’s unfreeze the base model and train the entire model end-to-end with a low learning rate.\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed training = FALSE when calling it when we built the model. This means that the batch normalization layers inside won’t update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 1\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html",
    "href": "guides/keras/working_with_rnns.html",
    "title": "Working with RNNs",
    "section": "",
    "text": "Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\nThe Keras RNN API is designed with a focus on:\n\nEase of use: the built-in layer_rnn(), layer_lstm(), layer_gru() layers enable you to quickly build recurrent models without having to make difficult configuration choices.\nEase of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic layer_rnn layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#setup",
    "href": "guides/keras/working_with_rnns.html#setup",
    "title": "Working with RNNs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "href": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "title": "Working with RNNs",
    "section": "Built-in RNN layers: a simple example",
    "text": "Built-in RNN layers: a simple example\nThere are three built-in RNN layers in Keras:\n\nlayer_simple_rnn(), a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\nlayer_gru(), first proposed in Cho et al., 2014.\nlayer_lstm(), first proposed in Hochreiter & Schmidhuber, 1997.\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a layer_lstm().\n\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n\nmodel\n\nBuilt-in RNNs support a number of useful features:\n\nRecurrent dropout, via the dropout and recurrent_dropout arguments\nAbility to process an input sequence in reverse, via the go_backwards argument\nLoop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the unroll argument\n…and more.\n\nFor more information, see the RNN API documentation."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#outputs-and-states",
    "href": "guides/keras/working_with_rnns.html#outputs-and-states",
    "title": "Working with RNNs",
    "section": "Outputs and states",
    "text": "Outputs and states\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is (batch_size, units) where units corresponds to the units argument passed to the layer’s constructor.\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences = TRUE. The shape of this output is (batch_size, timesteps, units).\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\nTo configure a RNN layer to return its internal state, set return_state = TRUE when creating the layer. Note that LSTM has 2 state tensors, but GRU only has one.\nTo configure the initial state of the layer, call the layer instance with the additional named argument initial_state. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\"encoder\")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \"decoder\")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "href": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "title": "Working with RNNs",
    "section": "RNN layers and RNN cells",
    "text": "RNN layers and RNN cells\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\nThe cell is the inside of the for loop of a RNN layer. Wrapping a cell inside a layer_rnn() layer gives you a layer capable of processing a sequence, e.g. layer_rnn(layer_lstm_cell(10)).\nMathematically, layer_rnn(layer_lstm_cell(10)) produces the same result as layer_lstm(10). In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in layer_gru() and layer_lstm() layers enable the use of CuDNN and you may see better performance.\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\nlayer_simple_rnn_cell() corresponds to the layer_simple_rnn() layer.\nlayer_gru_cell corresponds to the layer_gru layer.\nlayer_lstm_cell corresponds to the layer_lstm layer.\n\nThe cell abstraction, together with the generic layer_rnn() class, makes it very easy to implement custom RNN architectures for your research."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "href": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "title": "Working with RNNs",
    "section": "Cross-batch statefulness",
    "text": "Cross-batch statefulness\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of cross-batch statefulness.\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer’s state. That way, the layer can retain information about the entirety of the sequence, even though it’s only seeing one sub-sequence at a time.\nYou can do this by setting stateful = TRUE in the constructor.\nIf you have a sequence s = c(t0, t1, ... t1546, t1547), you would split it into e.g.\n\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n\nThen you would process it via:\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n\nWhen you want to clear the state, you can use layer$reset_states().\n\nNote: In this setup, sample i in a given batch is assumed to be the continuation of sample i in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], the next batch should contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200].\n\nHere is a complete example:\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n\n\nRNN State Reuse\nThe recorded states of the RNN layer are not included in the layer$weights(). If you would like to reuse the state from a RNN layer, you can retrieve the states value by layer$states and use it as the initial state of a new layer instance via the Keras functional API like new_layer(inputs, initial_state = layer$states), or model subclassing.\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "href": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "title": "Working with RNNs",
    "section": "Bidirectional RNNs",
    "text": "Bidirectional RNNs\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\nKeras provides an easy API for you to build such bidirectional RNNs: the bidirectional() wrapper.\n\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n\nUnder the hood, bidirectional() will copy the RNN layer passed in, and flip the go_backwards field of the newly copied layer, so that it will process the inputs in reverse order.\nThe output of the bidirectional RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the merge_mode parameter in the bidirectional wrapper constructor. For more details about bidirectional, please check the API docs."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "href": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "title": "Working with RNNs",
    "section": "Performance optimization and CuDNN kernels",
    "text": "Performance optimization and CuDNN kernels\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior layer_cudnn_gru/layer_cudnn_lstm layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\nSince the CuDNN kernel is built with certain assumptions, this means the layer will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers. E.g.:\n\nChanging the activation function from \"tanh\" to something else.\nChanging the recurrent_activation function from \"sigmoid\" to something else.\nUsing recurrent_dropout > 0.\nSetting unroll to TRUE, which forces LSTM/GRU to decompose the inner tf$while_loop into an unrolled for loop.\nSetting use_bias to FALSE.\nUsing masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the LSTM and GRU layers.\n\nUsing CuDNN kernels when available\nLet’s build a simple LSTM model to demonstrate the performance difference.\nWe’ll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we’ll predict the digit’s label.\n\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n\nLet’s load the MNIST dataset:\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n\nLet’s create a model instance and train it.\nWe choose sparse_categorical_crossentropy() as the loss function for the model. The output of the model has shape of (batch_size, 10). The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"sgd\",\n    metrics = \"accuracy\"\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nNow, let’s compare to a model that does not use the CuDNN kernel:\n\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\"sgd\",\n    metrics=\"accuracy\",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The tf$device() annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\nYou simply don’t have to worry about the hardware you’re running on anymore. Isn’t that pretty cool?\n\nwith(tf$device(\"CPU:0\"), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \"Predicted result is: %s, target result is: %s\\n\", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "href": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "title": "Working with RNNs",
    "section": "RNNs with list/dict inputs, or nested inputs",
    "text": "RNNs with list/dict inputs, or nested inputs\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\nDefine a custom cell that supports nested input/output\nSee Making new Layers & Models via subclassing for details on writing your own layers.\n\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \"uniform\",\n      name = \"kernel_1\"\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \"uniform\",\n      name = \"kernel_2_3\"\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\"bij,ijkl->bkl\", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\"unit_1\" = self$unit_1,\n         \"unit_2\" = self$unit_2,\n         \"unit_3\" = self$unit_3)\n  }\n}\n\n\n\nBuild a RNN model with nested input/output\nLet’s build a Keras model that uses a layer_rnn layer and the custom cell we just defined.\n\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"accuracy\")\n\n\n\nTrain the model with randomly generated data\nSince there isn’t a good candidate dataset for this model, we use random data for demonstration.\n\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n\nWith keras::layer_rnn(), you are only expected to define the math logic for an individual step within the sequence, and the layer_rnn() will handle the sequence iteration for you. It’s an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\nFor more details, please visit the API docs."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html",
    "href": "guides/keras/writing_your_own_callbacks.html",
    "title": "Writing your own callbacks",
    "section": "",
    "text": "A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include callback_tensorboard() to visualize training progress and results with TensorBoard, or callback_model_checkpoint() to periodically save your model during training.\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#setup",
    "href": "guides/keras/writing_your_own_callbacks.html#setup",
    "title": "Writing your own callbacks",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "href": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "title": "Writing your own callbacks",
    "section": "Keras callbacks overview",
    "text": "Keras callbacks overview\nAll callbacks subclass the keras$callbacks$Callback class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\nYou can pass a list of callbacks (as a named argument callbacks) to the following keras model methods:\n\nfit()\nevaluate()\npredict()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "href": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "title": "Writing your own callbacks",
    "section": "An overview of callback methods",
    "text": "An overview of callback methods\n\nGlobal methods\n\non_(train|test|predict)_begin(self, logs=None)\nCalled at the beginning of fit/evaluate/predict.\n\n\non_(train|test|predict)_end(self, logs=None)\nCalled at the end of fit/evaluate/predict.\n\n\n\nBatch-level methods for training/testing/predicting\n\non_(train|test|predict)_batch_begin(self, batch, logs=None)\nCalled right before processing a batch during training/testing/predicting.\n\n\non_(train|test|predict)_batch_end(self, batch, logs=None)\nCalled at the end of training/testing/predicting a batch. Within this method, logs is a dict containing the metrics results.\n\n\n\nEpoch-level methods (training only)\n\non_epoch_begin(self, epoch, logs=None)\nCalled at the beginning of an epoch during training.\n\n\non_epoch_end(self, epoch, logs=None)\nCalled at the end of an epoch during training."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "href": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "title": "Writing your own callbacks",
    "section": "A basic example",
    "text": "A basic example\nLet’s take a look at a concrete example. To get started, let’s import tensorflow and define a simple Sequential Keras model:\n\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n\nNow, define a simple custom callback that logs:\n\nWhen fit/evaluate/predict starts & ends\nWhen each epoch starts & ends\nWhen each training batch starts & ends\nWhen each evaluation (test) batch starts & ends\nWhen each inference (prediction) batch starts & ends\n\n\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  str(logs); cat(\"\\n\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n\nLet’s try it out:\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n\n\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n\n\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n\n\nUsage of logs dict\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "href": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "title": "Writing your own callbacks",
    "section": "Usage of self$model attribute",
    "text": "Usage of self$model attribute\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: self$model.\nHere are of few of the things you can do with self$model in a callback:\n\nSet self$model$stop_training <- TRUE to immediately interrupt training.\nMutate hyperparameters of the optimizer (available as self$model$optimizer), such as self$model$optimizer$learning_rate.\nSave the model at period intervals.\nRecord the output of predict(model) on a few test samples at the end of each epoch, to use as a sanity check during training.\nExtract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\netc.\n\nLet’s see this in action in a couple of examples."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "href": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "title": "Writing your own callbacks",
    "section": "Examples of Keras callback applications",
    "text": "Examples of Keras callback applications\n\nEarly stopping at minimum loss\nThis first example shows the creation of a Callback that stops training when the minimum of loss has been reached, by setting the attribute self$model$stop_training (boolean). Optionally, you can provide an argument patience to specify how many epochs we should wait before stopping after having reached a local minimum.\nkeras$callbacks$EarlyStopping provides a more complete and general implementation.\n\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n\n\n\nLearning rate scheduling\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\nSee keras$callbacks$LearningRateScheduler for a more general implementations (in RStudio, press F1 while the cursor is over LearningRateScheduler and a browser will open to this page).\n\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n\n\n\nBuilt-in Keras callbacks\nBe sure to check out the existing Keras callbacks by reading the API docs. Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html",
    "href": "guides/tensorflow/autodiff.html",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "href": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Automatic Differentiation and Gradients",
    "text": "Automatic Differentiation and Gradients\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#setup",
    "href": "guides/tensorflow/autodiff.html#setup",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#computing-gradients",
    "href": "guides/tensorflow/autodiff.html#computing-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Computing gradients",
    "text": "Computing gradients\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradient-tapes",
    "href": "guides/tensorflow/autodiff.html#gradient-tapes",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradient tapes",
    "text": "Gradient tapes\nTensorFlow provides the tf$GradientTape() API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf$Variables. TensorFlow “records” relevant operations executed inside the context of a tf$GradientTape() onto a “tape”. TensorFlow then uses that tape to compute the gradients of a “recorded” computation using reverse mode differentiation.\nHere is a simple example:\n\nx <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n\nOnce you’ve recorded some operations, use GradientTape$gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model’s variables):\n\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n\nThe above example uses scalars, but tf$GradientTape works as easily on any tensor:\n\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \"float32\", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf$nest).\n\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n\nThe gradient with respect to each source has the shape of the source:\n\nw$shape\ndl_dw$shape\n\nHere is the gradient calculation again, this time passing a named list of variables:\n\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "href": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients with respect to a model",
    "text": "Gradients with respect to a model\nIt’s common to collect tf$Variables into a tf$Module or one of its subclasses (tf$keras$layers$Layer, tf$keras$Model) for checkpointing and exporting.\nIn most cases, you will want to calculate gradients with respect to a model’s trainable variables. Since all subclasses of tf$Module aggregate their variables in the Module$trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \"float32\", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n\n\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "href": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Controlling what the tape watches",
    "text": "Controlling what the tape watches\nThe default behavior is to record all operations after accessing a trainable tf$Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\nThe tape holds references to intermediate outputs, so you don’t want to record unnecessary operations.\nThe most common use case involves calculating the gradient of a loss with respect to all a model’s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf$Tensor is not “watched” by default, and the tf$Variable is not trainable:\n\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n\nYou can list the variables being watched by the tape using the GradientTape$watched_variables method:\n\ntape$watched_variables()\n\ntf$GradientTape provides hooks that give the user control over what is or is not watched.\nTo record gradients with respect to a tf$Tensor, you need to call GradientTape$watch(x):\n\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n\nConversely, to disable the default behavior of watching all tf$Variables, set watch_accessed_variables = FALSE when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n\nSince GradientTape$watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\ncat('dy/dx1: ', as.array(grad$x1))"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#intermediate-results",
    "href": "guides/tensorflow/autodiff.html#intermediate-results",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Intermediate results",
    "text": "Intermediate results\nYou can also request gradients of the output with respect to intermediate values computed inside the tf$GradientTape context.\n\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape$gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent = TRUE. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n\n\nrm(tape)   # Drop the reference to the tape"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#notes-on-performance",
    "href": "guides/tensorflow/autodiff.html#notes-on-performance",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Notes on performance",
    "text": "Notes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\nFor efficiency, some ops (like ReLU) don’t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent = TRUE on your tape, nothing is discarded and your peak memory usage will be higher."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "href": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients of non-scalar targets",
    "text": "Gradients of non-scalar targets\nA gradient is fundamentally an operation on a scalar.\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\nas.array(tape$gradient(y1, x))\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\nThe sum of the gradients of each target.\n\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\nIf you need a separate gradient for each item, refer to Jacobians.\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n\n\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \"royalblue\", lwd = 2)\nlines(x, dy_dx, col = \"coral\", lwd=2)\nlegend(\"topleft\", inset = .05,\n       expression(y, dy/dx),\n       col = c(\"royalblue\", \"coral\"), lwd = 2)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#control-flow",
    "href": "guides/tensorflow/autodiff.html#control-flow",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Control flow",
    "text": "Control flow\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\ndv1\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1 ^ 2. The gradient with respect to x is always NULL.\n\n(dx <- tape$gradient(result, x))"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "href": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Getting a gradient of NULL",
    "text": "Getting a gradient of NULL\nWhen a target is not connected to a source you will get a gradient of NULL.\n\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\nIn the section on “controlling what the tape watches” you saw that the tape will automatically watch a tf$Variable but not a tf$Tensor.\nOne common error is to inadvertently replace a tf$Variable with a tf$Tensor, instead of using Variable$assign to update the tf$Variable. Here is an example:\n\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \": \")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n\n\n\n2. Did calculations outside of TensorFlow\nThe tape can’t record the gradient path if the calculation exits TensorFlow. For example:\n\nnp <- reticulate::import(\"numpy\", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n\n\n\n3. Took gradients through an integer or string\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\nNobody expects strings to be differentiable, but it’s easy to accidentally create an int constant or variable if you don’t specify the dtype.\n\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nTensorFlow doesn’t automatically cast between types, so, in practice, you’ll often get a type error instead of a missing gradient.\n\n\n4. Took gradients through a stateful object\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\nA tf$Tensor is immutable. You can’t change a tensor once it’s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf$matmul only depends on its inputs.\nA tf$Variable has internal state—its value. When you use the variable, the state is read. It’s normal to calculate a gradient with respect to a variable, but the variable’s state blocks gradient calculations from going farther back. For example:\n\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n\nSimilarly, tf$data$Dataset iterators and tf$queues are stateful, and will stop all gradients on tensors that pass through them."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "href": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "No gradient registered",
    "text": "No gradient registered\nSome tf$Operations are registered as being non-differentiable* and will return NULL. Others have no gradient registered**.\nThe tf$raw_ops page shows which low-level ops have gradients registered.\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning NULL. This way you know something has gone wrong.\nFor example, the tf$image$adjust_contrast function wraps raw_ops$AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n\nIf you need to differentiate through this op, you’ll either need to implement the gradient and register it (using tf$RegisterGradient) or re-implement the function using other ops."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "href": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Zeros instead of NULL",
    "text": "Zeros instead of NULL\nIn some cases it would be convenient to get 0 instead of NULL for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)"
  },
  {
    "objectID": "guides/tensorflow/basics.html",
    "href": "guides/tensorflow/basics.html",
    "title": "Tensorflow Basics",
    "section": "",
    "text": "This guide provides a quick overview of TensorFlow basics. Each section of this doc is an overview of a larger topic—you can find links to full guides at the end of each section.\nTensorFlow is an end-to-end platform for machine learning. It supports the following:\n\nMultidimensional-array based numeric computation (similar to Numpy\nGPU and distributed processing\nAutomatic differentiation\nModel construction, training, and export\nAnd more"
  },
  {
    "objectID": "guides/tensorflow/basics.html#tensors",
    "href": "guides/tensorflow/basics.html#tensors",
    "title": "Tensorflow Basics",
    "section": "Tensors",
    "text": "Tensors\nTensorFlow operates on multidimensional arrays or tensors represented as tensorflow.tensor objects. Here is a two-dimensional tensor:\n\nlibrary(tensorflow)\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n\nx\nx$shape\nx$dtype\n\nThe most important attributes of a tensor are its shape and dtype:\n\ntensor$shape: tells you the size of the tensor along each of its axes.\ntensor$dtype: tells you the type of all the elements in the tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as well as many operations specialized for machine learning.\nFor example:\n\nx + x\n\n\n5 * x\n\n\ntf$matmul(x, t(x)) \n\n\ntf$concat(list(x, x, x), axis = 0L)\n\n\ntf$nn$softmax(x, axis = -1L)\n\n\nsum(x) # same as tf$reduce_sum(x)\n\nRunning large calculations on CPU can be slow. When properly configured, TensorFlow can use accelerator hardware like GPUs to execute operations very quickly.\n\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n\nRefer to the Tensor guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#variables",
    "href": "guides/tensorflow/basics.html#variables",
    "title": "Tensorflow Basics",
    "section": "Variables",
    "text": "Variables\nNormal tensor objects are immutable. To store model weights (or other mutable state) in TensorFlow use a tf$Variable.\n\nvar <- tf$Variable(c(0, 0, 0))\nvar\n\n\nvar$assign(c(1, 2, 3))\n\n\nvar$assign_add(c(1, 1, 1))\n\nRefer to the Variables guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#automatic-differentiation",
    "href": "guides/tensorflow/basics.html#automatic-differentiation",
    "title": "Tensorflow Basics",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nGradient descent and related algorithms are a cornerstone of modern machine learning.\nTo enable this, TensorFlow implements automatic differentiation (autodiff), which uses calculus to compute gradients. Typically you’ll use this to calculate the gradient of a model’s error or loss with respect to its weights.\n\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n\n\nf(x)\n\nAt x = 1.0, y = f(x) = (1^2 + 2*1 - 5) = -2.\nThe derivative of y is y' = f'(x) = (2*x + 2) = 4. TensorFlow can calculate this automatically:\n\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n\nThis simplified example only takes the derivative with respect to a single scalar (x), but TensorFlow can compute the gradient with respect to any number of non-scalar tensors simultaneously.\nRefer to the Autodiff guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "href": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "title": "Tensorflow Basics",
    "section": "Graphs and tf_function",
    "text": "Graphs and tf_function\nWhile you can use TensorFlow interactively like any R library, TensorFlow also provides tools for:\n\nPerformance optimization: to speed up training and inference.\nExport: so you can save your model when it’s done training.\n\nThese require that you use tf_function() to separate your pure-TensorFlow code from R.\n\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n\nThe first time you run the tf_function, although it executes in R, it captures a complete, optimized graph representing the TensorFlow computations done within the function.\n\nx <- as_tensor(1:3)\nmy_func(x)\n\nOn subsequent calls TensorFlow only executes the optimized graph, skipping any non-TensorFlow steps. Below, note that my_func doesn’t print \"Tracing.\" since message is an R function, not a TensorFlow function.\n\nx <- as_tensor(10:8)\nmy_func(x)\n\nA graph may not be reusable for inputs with a different signature (shape and dtype), so a new graph is generated instead:\n\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n\nThese captured graphs provide two benefits:\n\nIn many cases they provide a significant speedup in execution (though not this trivial example).\nYou can export these graphs, using tf$saved_model, to run on other systems like a server or a mobile device, no Python installation required.\n\nRefer to Intro to graphs for more details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#modules-layers-and-models",
    "href": "guides/tensorflow/basics.html#modules-layers-and-models",
    "title": "Tensorflow Basics",
    "section": "Modules, layers, and models",
    "text": "Modules, layers, and models\ntf$Module is a class for managing your tf$Variable objects, and the tf_function objects that operate on them. The tf$Module class is necessary to support two significant features:\n\nYou can save and restore the values of your variables using tf$train$Checkpoint. This is useful during training as it is quick to save and restore a model’s state.\nYou can import and export the tf$Variable values and the tf$function graphs using tf$saved_model. This allows you to run your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple tf$Module object:\n\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n\n\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3)))\n\nSave the Module:\n\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n\nThe resulting SavedModel is independent of the code that created it. You can load a SavedModel from R, Python, other language bindings, or TensorFlow Serving. You can also convert it to run with TensorFlow Lite or TensorFlow JS.\n\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3)))\n\nThe tf$keras$layers$Layer and tf$keras$Model classes build on tf$Module providing additional functionality and convenience methods for building, training, and saving models. Some of these are demonstrated in the next section.\nRefer to Intro to modules for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#training-loops",
    "href": "guides/tensorflow/basics.html#training-loops",
    "title": "Tensorflow Basics",
    "section": "Training loops",
    "text": "Training loops\nNow put this all together to build a basic model and train it from scratch.\nFirst, create some example data. This generates a cloud of points that loosely follows a quadratic curve:\n\nx <- as_tensor(seq(-2, 2, length.out = 201))\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n\nCreate a model:\n\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n\n\nmodel <- Model(64)\n\n\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n\nWrite a basic training loop:\n\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n\n\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n\nThat’s working, but remember that implementations of common training utilities are available in the tf$keras module. So consider using those before writing your own. To start with, the compile and fit methods for Keras Models implement a training loop for you:\n\nnew_model <- Model(64)\n\n\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n\n\n\n\n\nplot(history, metrics = 'loss', method = \"base\") \n# see ?plot.keras_training_history for more options.\n\nRefer to Basic training loops and the Keras guide for more details."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html",
    "href": "guides/tensorflow/intro_to_graphs.html",
    "title": "Intro To_graphs",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#overview",
    "href": "guides/tensorflow/intro_to_graphs.html#overview",
    "title": "Intro To_graphs",
    "section": "Overview",
    "text": "Overview\nThis guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the collection of Keras guides.\nIn this guide, you’ll learn how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.\nNote: For those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.\nThis is a big-picture overview that covers how tf_function() allows you to switch from eager execution to graph execution. For a more complete specification of tf_function(), go to the tf_function() guide.\n\nWhat are graphs?\nIn the previous three guides, you ran TensorFlow eagerly. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\nWhile eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. Graph execution means that tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf$Graph or simply a “graph.”\nGraphs are data structures that contain a set of tf$Operation objects, which represent units of computation; and tf$Tensor objects, which represent the units of data that flow between operations. They are defined in a tf$Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original R code.\nThis is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard.\n\n\n\nA simple TensorFlow g\n\n\n\n\nThe benefits of graphs\nWith a graph, you have a great deal of flexibility. You can use your TensorFlow graph in environments that don’t have an R interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for saved models when it exports them from R.\nGraphs are also easily optimized, allowing the compiler to do transformations like:\n\nStatically infer the value of tensors by folding constant nodes in your computation (“constant folding”).\nSeparate sub-parts of a computation that are independent and split them between threads or devices.\nSimplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system, Grappler, to perform this and other speedups.\nIn short, graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices.\nHowever, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#setup",
    "href": "guides/tensorflow/intro_to_graphs.html#setup",
    "title": "Intro To_graphs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \"%>%\")"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "href": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "title": "Intro To_graphs",
    "section": "Taking advantage of graphs",
    "text": "Taking advantage of graphs\nYou create and run a graph in TensorFlow by using tf_function(), either as a direct call or as a decorator. tf_function() takes a regular function as input and returns a Function. A Function is a callable that builds TensorFlow graphs from the R function. You use a Function in the same way as its R equivalent.\n\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \"float64\", shape = c(1, 2))\ny1 <- as_tensor(2:3, \"float64\", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n\nOn the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf$Graphs behind one API. That is how Function is able to give you the benefits of graph execution, like speed and deployability.\ntf_function applies to a function and all other functions it calls:\n\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \"float64\", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \"float64\", shape = c(1, 2))) #%>% as.array()\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you need to define a Placeholder or tf$Session().\n\nConverting Python functions to graphs\nAny function you write with TensorFlow will contain a mixture of built-in TF operations and R control-flow logic, such as if-then clauses, loops, break, return, next, and more. While TensorFlow operations are easily captured by a tf$Graph, R-specific logic needs to undergo an extra step in order to become part of the graph. tf_function() uses a library called {tfautograph} to evaluate the R code in a special way so that it generates a graph.\n\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \"First branch, with graph: \", format(tf_simple_relu(as_tensor(1))), \"\\n\",\n  \"Second branch, with graph: \", format(tf_simple_relu(as_tensor(-1))), \"\\n\",\n  sep = \"\"\n)\n\nThough it is unlikely that you will need to view graphs directly, you can inspect the outputs to check the exact results. These are not easy to read, so no need to look too carefully!\n\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n\nMost of the time, tf_function() will work without special considerations. However, there are some caveats, and the tf_function guide can help here, as well as the tfautograph Getting Started vignette\n\n\nPolymorphism: one Function, many graphs\nA tf$Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()) (i.e, the same memory address).\nEach time you invoke a Function with a set of arguments that can’t be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf$Graph specialized to those new arguments. The type specification of a tf$Graph’s inputs is known as its input signature or just a signature. For more information regarding when a new tf$Graph is generated and how that can be controlled, see the rules of retracing.\nThe Function stores the tf$Graph corresponding to that signature in a ConcreteFunction. A ConcreteFunction is a wrapper around a tf$Graph.\n\nmy_relu <- tf_function(function(x) {\n  message(\"Tracing my_relu(x) with: \", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\nmy_relu(c(1, -1))\nmy_relu(as_tensor(c(3, -3)))\n\nIf the Function has already been called with that signature, Function does not create a new tf$Graph.\n\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n\nBecause it’s backed by multiple graphs, a Function is polymorphic. That enables it to support more input types than a single tf$Graph could represent, as well as to optimize each tf$Graph for better performance.\n\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "href": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "title": "Intro To_graphs",
    "section": "Using tf_function()",
    "text": "Using tf_function()\nSo far, you’ve learned how to convert a Python function into a graph simply by using tf_function() as function wrapper. But in practice, getting tf_function to work correctly can be tricky! In the following sections, you’ll learn how you can make your code work as expected with tf_function().\n\nGraph execution vs. eager execution\nThe code in a Function can be executed both eagerly and as a graph. By default, Function executes its code as a graph:\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n\n\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\n\nget_MSE(y_true, y_pred)\n\nTo verify that your Function’s graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with tf$config$run_functions_eagerly(TRUE). This is a switch that turns off Function’s ability to create and run graphs, instead executing the code normally.\n\ntf$config$run_functions_eagerly(TRUE)\n\n\nget_MSE(y_true, y_pred)\n\n\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n\nHowever, Function can behave differently under graph and eager execution. The R print() function is one example of how these two modes differ. Let’s check out what happens when you insert a print statement to your function and call it repeatedly.\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\"Calculating MSE!\")\n  mean((y_true - y_pred) ^ 2)\n  })\n\nObserve what is printed:\n\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\nIs the output surprising? get_MSE only printed once even though it was called three times.\nTo explain, the print statement is executed when Function runs the original code in order to create the graph in a process known as “tracing”. Tracing captures the TensorFlow operations into a graph, and print() is not captured in the graph. That graph is then executed for all three calls without ever running the R code again.\nAs a sanity check, let’s turn off graph execution to compare:\n\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n\n\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\n\ntf$config$run_functions_eagerly(FALSE)\n\nprint is an R side effect, and there are other differences that you should be aware of when converting a function into a Function. Learn more in the Limitations section of the Better performance with tf_function guide.\n\n\n\n\n\n\nNote\n\n\n\nNote: If you would like to print values in both eager and graph execution, use tf$print() instead.\n\n\n\n\nNon-strict execution\nGraph execution only executes the operations necessary to produce the observable effects, which includes:\n\nThe return value of the function\nDocumented well-known side-effects such as:\n\nInput/output operations, like tf$print()\nDebugging operations, such as the assert functions in tf$debugging() (also, stopifnot())\nMutations of tf$Variable()\n\n\nThis behavior is usually known as “Non-strict execution”, and differs from eager execution, which steps through all of the program operations, needed or not.\nIn particular, runtime error checking does not count as an observable effect. If an operation is skipped because it is unnecessary, it cannot raise any runtime errors.\nIn the following example, the “unnecessary” operation tf$gather() is skipped during graph execution, so the runtime error InvalidArgumentError is not raised as it would be in eager execution. Do not rely on an error being raised while executing a graph.\n\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n# All operations are run during eager execution so an error is raised.\n\n\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n\n\n\ntf_function() best practices\nIt may take some time to get used to the behavior of Function. To get started quickly, first-time users should play around with wrapping toy functions with tf_function() to get experience with going from eager to graph execution.\nDesigning for tf_function may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n\nToggle between eager and graph execution early and often with tf$config$run_functions_eagerly() to pinpoint if/when the two modes diverge.\nCreate tf$Variables outside the Python function and modify them on the inside. The same goes for objects that use tf$Variable, like keras$layers, keras$Models and tf$optimizers.\nAvoid writing functions that depend on outer Python variables, excluding tf$Variables and Keras objects.\nPrefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful!\nInclude as much computation as possible under a tf_function to maximize the performance gain. For example, wrap a whole training step or the entire training loop."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "href": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "title": "Intro To_graphs",
    "section": "Seeing the speed-up",
    "text": "Seeing the speed-up\ntf_function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:\n\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n\n\nplot(bench::mark(\n  \"Eager execution\" = power(x, 100),\n  \"Graph execution\" = power_as_graph(x, 100)))\n\ntf_function is commonly used to speed up training loops, and you can learn more about it in Writing a training loop from scratch with Keras.\nNote: You can also try tf_function(jit_compile = TRUE) for a more significant performance boost, especially if your code is heavy on TF control flow and uses many small tensors.\n\nPerformance and trade-offs\nGraphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. This investment is usually quickly paid back with the performance boost of subsequent executions, but it’s important to be aware that the first few steps of any large model training can be slower due to tracing.\nNo matter how large your model, you want to avoid tracing frequently. The tf_function() guide discusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it’s a good idea to check if you are retracing accidentally."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "href": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "title": "Intro To_graphs",
    "section": "When is a Function tracing?",
    "text": "When is a Function tracing?\nTo figure out when your Function is tracing, add a print or message() statement to its code. As a rule of thumb, Function will execute the message statement every time it traces.\n\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\"Tracing!\") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n\n\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\na_function_with_r_side_effect(3)\n\nNew (non-tensor) R arguments always trigger the creation of a new graph, hence the extra tracing."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "href": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "title": "Intro To_graphs",
    "section": "Next steps",
    "text": "Next steps\nYou can learn more about tf_function() on the API reference page and by following the Better performance with tf_function guide."
  },
  {
    "objectID": "guides/tensorflow/tensor.html",
    "href": "guides/tensorflow/tensor.html",
    "title": "Introduction to Tensors",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nlibrary(tensorflow)\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes with names(tf$dtypes).\nIf you’re familiar with R array or NumPy, tensors are (kind of) like R or NumPy arrays.\nAll tensors are immutable: you can never update the contents of a tensor, only create a new one."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#basics",
    "href": "guides/tensorflow/tensor.html#basics",
    "title": "Introduction to Tensors",
    "section": "Basics",
    "text": "Basics\nLet’s create some basic tensors.\nHere is a “scalar” or “rank-0” tensor . A scalar contains a single value, and no “axes”.\n\n# This will be an float64 tensor by default; see \"dtypes\" below.\nrank_0_tensor <- as_tensor(4)\nprint(rank_0_tensor)\n\nA “vector” or “rank-1” tensor is like a list of values. A vector has one axis:\n\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n\nA “matrix” or “rank-2” tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n\n\n\n\n\n\n\n\n\nA scalar, shape: []\nA vector, shape: [3]\nA matrix, shape: [3, 2]\n\n\n\n\n\n\n\n\n\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of\n# axes (sometimes called \"dimensions\")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n\nThere are many ways you might visualize a tensor with more than two axes.\n\n\n\n\n\n\nA 3-axis tensor, shape: [3, 2, 5]\n\n\n\n\n\n\n\n!  \n\n\n\nYou can convert a tensor to an R array using as.array():\n\nas.array(rank_2_tensor)\n\nTensors often contain floats and ints, but have many other types, including:\n\ncomplex numbers\nstrings\n\nThe base tf$Tensor class requires tensors to be “rectangular”—that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\na * b # element-wise multiplication, same as tf$multiply(a, b)\ntf$matmul(a, b) # matrix multiplication\n\nTensors are used in all kinds of operations (ops).\n\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n\n# Find the index of the largest value\ntf$math$argmax(x) \n\ntf$nn$softmax(x) # Compute the softmax"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#about-shapes",
    "href": "guides/tensorflow/tensor.html#about-shapes",
    "title": "Introduction to Tensors",
    "section": "About shapes",
    "text": "About shapes\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\nAxis or Dimension: A particular dimension of a tensor.\nSize: The total number of items in the tensor, the product of the shape vector’s elements.\n\nNote: Although you may see reference to a “tensor of two dimensions”, a rank-2 tensor does not usually describe a 2D space.\nTensors and tf$TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n\n\n\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\n\n\n\n\n\n\n\n\nmessage(\"Type of every element: \", rank_4_tensor$dtype)\nmessage(\"Number of axes: \", length(dim(rank_4_tensor)))\nmessage(\"Shape of tensor: \", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\nmessage(\"Elements along axis 0 of tensor: \", dim(rank_4_tensor)[1])\nmessage(\"Elements along the last axis of tensor: \", dim(rank_4_tensor) |> tail(1)) \nmessage(\"Total number of elements (3*2*4*5): \", length(rank_4_tensor)) # can also call tf$size()\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\n\n\n\n\n\nTypical axis order"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#indexing",
    "href": "guides/tensorflow/tensor.html#indexing",
    "title": "Introduction to Tensors",
    "section": "Indexing",
    "text": "Indexing\n\nSingle-axis indexing\nSee ?`[.tensorflow.tensor` for details\n\n\nMulti-axis indexing\nHigher rank tensors are indexed by passing multiple indices.\nThe exact same rules as in the single-axis case apply to each axis independently.\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#manipulating-shapes",
    "href": "guides/tensorflow/tensor.html#manipulating-shapes",
    "title": "Introduction to Tensors",
    "section": "Manipulating Shapes",
    "text": "Manipulating Shapes\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n\n\n# You can convert this object into an R vector too\nas.integer(x$shape)\n\nYou can reshape a tensor into a new shape. The tf$reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n\n\nx$shape\nreshaped$shape\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style “row-major” memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nrank_3_tensor\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says \"Whatever fits\".\ntf$reshape(rank_3_tensor, c(-1L))\n\nA typical and reasonable use of tf$reshape is to combine or split adjacent axes (or add/remove 1s).\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n\n\n\n\n\n\n\nSome good reshapes.\n\n\n\n\n  \n\n\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png https://www.tensorflow.org/guide/ https://www.tensorflow.org/guide/images/tensor/reshape-good2.png\nReshaping will “work” for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\nSwapping axes in tf$reshape does not work; you need tf$transpose for that.\n\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n\n\n\n\n\n\n\nSome bad reshapes.\n\n\n\n\n  \n\n\n\nYou may run across not-fully-specified shapes. Either the shape contains a NULL (an axis-length is unknown) or the whole shape is NULL (the rank of the tensor is unknown).\nExcept for tf$RaggedTensor, such shapes will only occur in the context of TensorFlow’s symbolic, graph-building APIs:\n\ntf_function\nThe keras functional API."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#more-on-dtypes",
    "href": "guides/tensorflow/tensor.html#more-on-dtypes",
    "title": "Introduction to Tensors",
    "section": "More on DTypes",
    "text": "More on DTypes\nTo inspect a tf$Tensor’s data type use the Tensor$dtype property.\nWhen creating a tf$Tensor from a Python object you may optionally specify the datatype.\nIf you don’t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts R integers to tf$int32 and R floating point numbers to tf$float64.\nYou can cast from type to type.\n\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#broadcasting",
    "href": "guides/tensorflow/tensor.html#broadcasting",
    "title": "Introduction to Tensors",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are recycled automatically to fit larger tensors when running combined operations on them.\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\nx * y\nx * z\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n(y <- tf$range(1, 5,  dtype = \"float64\"))\n\nx * y\n\n\n\n\n\n\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\n\n\n\n\\\n\n\n\nHere is the same operation without broadcasting:\n\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\nYou see what broadcasting looks like using tf$broadcast_to.\n\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\nIt can get even more complicated. This section of Jake VanderPlas’s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy)."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "href": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "title": "Introduction to Tensors",
    "section": "tf$convert_to_tensor",
    "text": "tf$convert_to_tensor\nMost ops, like tf$matmul and tf$reshape take arguments of class tf$Tensor. However, you’ll notice in the above case, objects shaped like tensors are also accepted.\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy’s ndarray, TensorShape, Python lists, and tf$Variable will all convert automatically.\nSee tf$register_tensor_conversion_function for more details, and if you have your own type you’d like to automatically convert to a tensor."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#ragged-tensors",
    "href": "guides/tensorflow/tensor.html#ragged-tensors",
    "title": "Introduction to Tensors",
    "section": "Ragged Tensors",
    "text": "Ragged Tensors\nA tensor with variable numbers of elements along some axis is called “ragged”. Use tf$ragged$RaggedTensor for ragged data.\nFor example, This cannot be represented as a regular tensor:\n\n\n\n\n\n\nA tf$RaggedTensor, shape: [4, NULL]\n\n\n\n\n\n\n\n\n\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n\n\ntry(tensor <- as_tensor(ragged_list))\n\nInstead create a tf$RaggedTensor using tf$ragged$constant:\n\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n\nThe shape of a tf$RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor$shape)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#string-tensors",
    "href": "guides/tensorflow/tensor.html#string-tensors",
    "title": "Introduction to Tensors",
    "section": "String tensors",
    "text": "String tensors\ntf$string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\nThe length of the string is not one of the axes of the tensor. See tf$strings for functions to manipulate them.\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\"Gray wolf\"))\n\nAnd a vector of strings:\n\n\n\n\n\n\nA vector of strings, shape: [3,]\n\n\n\n\n\n\n\n\n\ntensor_of_strings <- as_tensor(c(\"Gray wolf\",\n                                 \"Quick brown fox\",\n                                 \"Lazy dog\"))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n\nIn the above printout the b prefix indicates that tf$string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\nIf you pass unicode characters they are utf-8 encoded.\n\nas_tensor(\"🥳👍\")\n\nSome basic functions with strings can be found in tf$strings, including tf$strings$split.\n\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\" \")\n\n\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n\n\n\n\n\n\n\nThree strings split, shape: [3, NULL]\n\n\n\n\n\n\n\n\nAnd tf$string$to_number:\n\ntext <- as_tensor(\"1 10 100\")\ntf$strings$to_number(tf$strings$split(text, \" \"))\n\nAlthough you can’t use tf$cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings <- tf$strings$bytes_split(as_tensor(\"Duck\"))\nbyte_ints <- tf$io$decode_raw(as_tensor(\"Duck\"), tf$uint8)\ncat(\"Byte strings: \"); print(byte_strings)\ncat(\"Bytes: \"); print(byte_ints)\n\n\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\"アヒル 🦆\")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \"UTF-8\")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \"UTF-8\")\n\ncat(\"Unicode bytes: \"); unicode_bytes\ncat(\"Unicode chars: \"); unicode_char_bytes\ncat(\"Unicode values: \"); unicode_values\n\nThe tf$string dtype is used for all raw bytes data in TensorFlow. The tf$io module contains functions for converting data to and from bytes, including decoding images and parsing csv."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#sparse-tensors",
    "href": "guides/tensorflow/tensor.html#sparse-tensors",
    "title": "Introduction to Tensors",
    "section": "Sparse tensors",
    "text": "Sparse tensors\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf$sparse$SparseTensor and related operations to store sparse data efficiently.\n\n\n\n\n\n\nA tf$SparseTensor, shape: [3, 4]\n\n\n\n\n\n\n\n\n\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)"
  },
  {
    "objectID": "guides/tensorflow/variable.html",
    "href": "guides/tensorflow/variable.html",
    "title": "Introduction to Variables",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nA TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of tf$Variable in TensorFlow.\nVariables are created and tracked via the tf$Variable class. A tf$Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like tf$keras use tf$Variable to store model parameters."
  },
  {
    "objectID": "guides/tensorflow/variable.html#setup",
    "href": "guides/tensorflow/variable.html#setup",
    "title": "Introduction to Variables",
    "section": "Setup",
    "text": "Setup\nThis notebook discusses variable placement. If you want to see on what device your variables are placed, uncomment this line.\n\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)"
  },
  {
    "objectID": "guides/tensorflow/variable.html#create-a-variable",
    "href": "guides/tensorflow/variable.html#create-a-variable",
    "title": "Introduction to Variables",
    "section": "Create a variable",
    "text": "Create a variable\nTo create a variable, provide an initial value. The tf$Variable will have the same dtype as the initialization value.\n\nmy_tensor <- as_tensor(1:4, \"float32\", shape = c(2, 2))\n(my_variable <- tf$Variable(my_tensor))\n\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n\nA variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf$Tensor. Like tensors, they have a dtype and a shape, and can be exported to regular R arrays.\n\ncat(\"Shape: \"); my_variable$shape\ncat(\"DType: \"); my_variable$dtype\ncat(\"As R array: \"); str(as.array(my_variable))\n\nMost tensor operations work on variables as expected, although variables cannot be reshaped.\n\nmessage(\"A variable: \")\nmy_variable\n\nmessage(\"Viewed as a tensor: \")\nas_tensor(my_variable)\n\nmessage(\"Index of highest value: \")\ntf$math$argmax(my_variable)\n\n# This creates a new tensor; it does not reshape the variable.\nmessage(\"Copying and reshaping: \") \ntf$reshape(my_variable, c(1L, 4L))\n\nAs noted above, variables are backed by tensors. You can reassign the tensor using tf$Variable$assign. Calling assign does not (usually) allocate a new tensor; instead, the existing tensor’s memory is reused.\n\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n\nIf you use a variable like a tensor in operations, you will usually operate on the backing tensor.\nCreating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory.\n\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n\n# a and b are different\n\nas.array(a)\nas.array(b)\n\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\nas.array(a$assign_sub(c(7,9))) # c(0, 0)"
  },
  {
    "objectID": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "href": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "title": "Introduction to Variables",
    "section": "Lifecycles, naming, and watching",
    "text": "Lifecycles, naming, and watching\nIn TensorFlow, tf$Variable instance have the same lifecycle as other R objects. When there are no references to a variable it is automatically deallocated (garbage-collected).\nVariables can also be named which can help you track and debug them. You can give two variables the same name.\n\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \"Mark\")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \"Mark\")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n\nVariable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.\nAlthough variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter.\n\n(step_counter <- tf$Variable(1L, trainable = FALSE))"
  },
  {
    "objectID": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "href": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "title": "Introduction to Variables",
    "section": "Placing variables and tensors",
    "text": "Placing variables and tensors\nFor better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype. This means most variables are placed on a GPU if one is available.\nHowever, you can override this. In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available. By turning on device placement logging (see above), you can see where the variable is placed.\nNote: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation.\nIf you run this notebook on different backends with and without a GPU you will see different logging. Note that logging device placement must be turned on at the start of the session.\n\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \"float32\")\n  c <- tf$matmul(a, b)\n})\n\nc\n\nIt’s possible to set the location of a variable or tensor on one device and do the computation on another device. This will introduce delay, as data needs to be copied between the devices.\nYou might do this, however, if you had multiple GPU workers but only want one copy of the variables.\n\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \"float32\")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n\nNote: Because tf$config$set_soft_device_placement() is turned on by default, even if you run this code on a device without a GPU, it will still run. The multiplication step will happen on the CPU.\nFor more on distributed training, refer to the guide."
  },
  {
    "objectID": "guides/tensorflow/variable.html#next-steps",
    "href": "guides/tensorflow/variable.html#next-steps",
    "title": "Introduction to Variables",
    "section": "Next steps",
    "text": "Next steps\nTo understand how variables are typically used, see our guide on automatic differentiation."
  },
  {
    "objectID": "install/custom.html",
    "href": "install/custom.html",
    "title": "Custom Installation",
    "section": "",
    "text": "The install_tensorflow() function is provided as a convenient way to get started, but is not required. If you have an existing installation of TensorFlow or just prefer your own custom installation that’s fine too.\nThe full instructions for installing TensorFlow on various platforms are here: https://www.tensorflow.org/install/. After installing, please refer to the sections below on locating TensorFlow and meeting additional dependencies to ensure that the tensorflow for R package functions correctly with your installation."
  },
  {
    "objectID": "install/custom.html#supported-platforms",
    "href": "install/custom.html#supported-platforms",
    "title": "Custom Installation",
    "section": "Supported Platforms",
    "text": "Supported Platforms\nNote that binary installations of TensorFlow are provided for Windows, OS X, and Ubuntu 16.04 or higher. It’s possible that binary installations will work on other Linux variants but Ubuntu is the only platform tested and supported.\nIn particular, if you are running on RedHat or CentOS you will need to install from source then follow the instructions in the [Custom Installation] section to ensure that your installation of TensorFlow can be used with the tensorflow R package."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html",
    "href": "install/gpu/cloud_desktop_gpu/index.html",
    "title": "Cloud Desktop GPUs",
    "section": "",
    "text": "Cloud desktops with various GPU configurations are available from Paperspace. With Paperspace, you can access a full Linux desktop running Ubuntu 16.04 all from within a web browser. An SSH interface is also available, as is a browser based RStudio Server interface (via SSH tunnel).\nPaperspace offers an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0) pre-installed, along with the GPU version of TensorFlow v1.4 and the R keras, tfestimators, and tensorflow packages. Follow the instructions below to get started with using RStudio on Paperspace."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#getting-started",
    "href": "install/gpu/cloud_desktop_gpu/index.html#getting-started",
    "title": "Cloud Desktop GPUs",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, sign up for a Paperspace account here: https://www.paperspace.com/account/signup (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\n\nAfter you’ve signed up and verified your account email, you will be taken to a Create Machine page. Here you’ll select various options including your compute region and machine template. You should select the RStudio template:\n\nBe sure to select one of the GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\nAfter your machine is provisioned (this can take a few minutes) you are ready to access it via a web browser. Hover over the machine in the Paperspace Console and click the “Launch” link:\n\nAfter the machine is launched you’ll see your Linux desktop within the browser you launched it from. You may need to use the Scaling Settings to adjust the desktop to a comfortable resolution:\n\nYou should also change your default password using the passwd utility (your default password should have been sent to you in an email titled “Your new Paperspace Linux machine is ready”):\n\nYou now have a Linux desktop equipped ready to use with TensorFlow for R! Go ahead and run RStudio from the application bar:\n\nNVIDIA GPU libraries (CUDA 9 and cuDNN 7) are pre-installed, along with the GPU version of TensorFlow v1.7. The R keras, tfestimators, and tensorflow packages are also pre-installed, as are all of the packages from the [tidyverse[(https://www.tidyverse.org/)] (dplyr, ggplot2, etc.).\nAn important note about the pre-installed dependencies: Since the NVIDIA CUDA libraries, TensorFlow, and Keras are all pre-installed on the Paperspace instances, you should not use the install_tensorflow() or install_keras() functions, but rather rely on the existing, pre-configured versions of these libraries. Installing or updating other versions of these libraries will likely not work at all!"
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "href": "install/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "title": "Cloud Desktop GPUs",
    "section": "Automatic Shutdown",
    "text": "Automatic Shutdown\nYou can set Paperspace machines to automatically shutdown when they have not been used for a set period of time (this is especially important since machine time is billed by the hour). You can access this setting from the Paperspace console for your machine:\n\nHere the auto-shutdown time is set to 1 day, however you can also choose shorter or longer intervals."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "href": "install/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "title": "Cloud Desktop GPUs",
    "section": "Terminal Access",
    "text": "Terminal Access\n\nWeb Terminal\nYou can use the Open Terminal command on the Paperspace console for your machine to open a web based terminal to your machine:\n\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\n\nSSH Login\nYou can also login to your Paperspace instance using a standard SSH client. This requires that you first Assign a public IP address to your machine (note that public IP addresses cost an additional $3/month).\nOnce you have your public IP address, you can SSH into your machine as follows:\n$ ssh paperspace@<public IP>\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created."
  },
  {
    "objectID": "install/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "href": "install/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "title": "Cloud Desktop GPUs",
    "section": "RStudio Server",
    "text": "RStudio Server\nYou may prefer using the RStudio Server browser-based interface to the virtual Linux desktop provided by Paperspace (especially when on slower internet connections). This section describes how to access your Paperspace machine using an SSH tunnel.\nTo start with, follow the instructions for SSH Login immediately above and ensure that you can login to your machine remotely via SSH.\nOnce you’ve verified this, you should also be able to setup an SSH tunnel to RStudio Server as follows:\n$ ssh -N -L 8787:127.0.0.1:8787 paperspace@<public-ip>\nYou can access RStudio Server by navigating to port 8787 on your local machine and logging in using the paperspace account and either the default password emailed to you when you created the machine or to the new password which you subsequently created.\nhttp://localhost:8787"
  },
  {
    "objectID": "install/gpu/cloud_server_gpu/index.html",
    "href": "install/gpu/cloud_server_gpu/index.html",
    "title": "Cloud Server GPUs",
    "section": "",
    "text": "Cloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally."
  },
  {
    "objectID": "install/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "href": "install/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "title": "Cloud Server GPUs",
    "section": "Amazon EC2",
    "text": "Amazon EC2\nRStudio has AWS Marketplace offerings that are designed to provide stable, secure, and high performance execution environments for deep learning applications running on Amazon EC2. The tensorflow, tfestimators, and keras R packages (along with their pre-requisites, including the GPU version of TensorFlow) are installed as part of the image.\n\nLaunching the Server\nThere are AMIs on the Amazon Cloud Marketplace for both the open-source and Professional versions of RStudio Server. You can find them here:\n\nOpen Source: https://aws.amazon.com/marketplace/pp/B0785SXYB2\nProfessional: https://aws.amazon.com/marketplace/pp/B07B8G3FZP\n\nYou should launch these AMIs on the p2.xlarge instance type. This type includes a single GPU whereas other GPU-based images include up to 16 GPUs (however they are commensurately much more expensive). Note that you may need to select a different region than your default to be able to launch p2.xlarge instances (for example, selecting “US East (Ohio)” rather than “US East (N Virginia)”).\n\n\n\nAccessing the Server\nAfter you’ve launched the server you can access an instance of RStudio Server running on port 8787. For example:\nhttp://ec2-18-217-204-43.us-east-2.compute.amazonaws.com:8787\nNote that the above server address needs to be substituted for the public IP of the server you launched, which you can find in the EC2 Dashboard.\nThe first time you access the server you will be presented with a login screen:\n\nLogin with user id “rstudio-user” and password the instance ID of your AWS server (for example “i-0a8ea329c18892dfa”, your specific ID is available via the EC2 dashboard).\nThen, use the RStudio Terminal to change the default password using the passwd utility:\n\nYour EC2 deep learning instance is now ready to use (the tensorflow, tfestimators, and keras R packages along with their pre-requisites, including the GPU version of TensorFlow, are installed as part of the image).\nSee the sections below for discussion of various ways in which you can make your EC2 instance more secure.\n\n\nLimiting Inbound Traffic\nThe EC2 instance is by default configured to allow access to SSH and HTTP traffic from all IP addresses on the internet, whereas it would be more desirable to restrict this to IP addresses that you know you will access the server from (this can however be challenging if you plan on accessing the server from a variety of public networks).\nYou can see these settings in the Security Group of your EC2 instance:\n\nEdit the Source for the SSH and HTTP protocols to limit access to specific blocks of IP addresses.\n\n\nUsing HTTPS\nBy default the EC2 instance which you launched is accessed over HTTP, a non-encrypted channel. This means that data transmitted to the instance (including your username and password) can potentially be compromised during transmission.\nThere are many ways to add HTTPS support to a server including AWS Elastic Load Balancing, CloudFlare SSL, and setting up reverse proxy from an Nginx or Apache web server configured with SSL support.\nThe details of adding HTTPS support to your server are beyond the scope of this article (see the links above to learn more). An alternative to this is to prohibit external HTTP connections entirely and access the server over an SSH Tunnel, this option is covered in the next section.\n\n\nSSH Tunnel\nUsing an SSH Tunnel to access your EC2 instance provides a number of benefits, including:\n\nUse of the SSH authentication protocol to identify and authorize remote users\nEncrypting traffic that would otherwise be sent in the clear\n\nNote that SSH tunnel access as described below works only for Linux and OS X clients.\n\nSecurity Group\nTo use an SSH Tunnel with your EC2 instance, first configure the Security Group of your instance to only accept SSH traffic (removing any HTTP entry that existed previously):\n\nNote that you may also want to restrict the Source of SSH traffic to the specific block of IP addresses you plan to access the server from.\n\n\nServer Configuration\nNext, connect to your instance over SSH (click the Connect button in the EC2 console for instructions specific to your server):\nssh -i \"my-security-key.pem\" ubuntu@my-ec2-server-address\nNote that if you copied and pasted the command from the EC2 console you may see this error message:\nPlease login as the user \"ubuntu\" rather than the user \"root\".\nIn that case be sure that you use ubuntu@my-ec2-server-address rather than root@my-ec2-server-address.\nExecute the following commands to configure RStudio Server to only accept local connections:\n# Configure RStudio to only allow local connections \nsudo /bin/bash -c \"echo 'www-address=127.0.0.1' >> /etc/rstudio/rserver.conf\"\n\n# Restart RStudio with new settings\nsudo rstudio-server restart\n\n\nConnecting to the Server\nYou should now be able to connect to the server via SSH tunnel as follows:\nssh -N -L 8787:localhost:8787 -i my-security-key.pem ubuntu@my-ec2-server-address\n(where my-security-key.pem and my-ec2-server-address are specific to your server configuration).\nOnce the SSH connection is established, RStudio Server will be available at http://localhost:8787/"
  },
  {
    "objectID": "install/gpu/index.html",
    "href": "install/gpu/index.html",
    "title": "Overview",
    "section": "",
    "text": "If your local workstation doesn’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple, low-cost way for you to get started without having to buy any additional hardware. See the documentation below for details on using both local and cloud GPUs.\n\n\n\n\n\n\n\nLocal GPU\nFor systems that have a recent, high-end NVIDIA® GPU, TensorFlow is available in a GPU version that takes advantage of the CUDA and cuDNN libraries to accelerate training performance. Note that the GPU version of TensorFlow is currently only supported on Windows and Linux (there is no GPU version available for Mac OS X since NVIDIA GPUs are not commonly available on that platform).\n\n\nCloudML\nGoogle CloudML is a managed service that provides on-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA. CloudML also provides hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\n\n\nCloud Server\nCloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally.\n\n\nCloud Desktop\nVirtual cloud desktops with GPUs are available from Paperspace. This provides an Ubuntu 16.04 desktop environment that you can access entirely within a web browser (note that this requires a reasonbly fast internet connection to be usable)."
  },
  {
    "objectID": "install/gpu/local_gpu/index.html",
    "href": "install/gpu/local_gpu/index.html",
    "title": "Local GPU",
    "section": "",
    "text": "TensorFlow can be configured to run on either CPUs or GPUs. The CPU version is much easier to install and configure so is the best starting place especially when you are first learning how to use TensorFlow. Here’s the guidance on CPU vs. GPU versions from the TensorFlow website:\n\nTensorFlow with CPU support only. If your system does not have a NVIDIA® GPU, you must install this version. Note that this version of TensorFlow is typically much easier to install (typically, in 5 or 10 minutes), so even if you have an NVIDIA GPU, we recommend installing this version first.\nTensorFlow with GPU support. TensorFlow programs typically run significantly faster on a GPU than on a CPU. Therefore, if your system has a NVIDIA® GPU meeting the prerequisites shown below and you need to run performance-critical applications, you should ultimately install this version.\n\nSo if you are just getting started with TensorFlow you may want to stick with the CPU version to start out, then install the GPU version once your training becomes more computationally demanding.\nThe prerequisites for the GPU version of TensorFlow on each platform are covered below. Once you’ve met the prerequisites installing the GPU version in a single-user / desktop environment is as simple as:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf you are using Keras you can install both Keras and the GPU version of TensorFlow with:\n\nlibrary(keras)\ninstall_keras(tensorflow = \"gpu\")\n\nNote that on all platforms you must be running an NVIDIA® GPU with CUDA® Compute Capability 3.5 or higher in order to run the GPU version of TensorFlow. See the list of CUDA-enabled GPU cards."
  },
  {
    "objectID": "install/gpu/local_gpu/index.html#prerequisties",
    "href": "install/gpu/local_gpu/index.html#prerequisties",
    "title": "Local GPU",
    "section": "Prerequisties",
    "text": "Prerequisties\n\nWindows\nThis article describes how to detect whether your graphics card uses an NVIDIA® GPU:\nhttp://nvidia.custhelp.com/app/answers/detail/a_id/2040/~/identifying-the-graphics-card-model-and-device-id-in-a-pc\nOnce you’ve confirmed that you have an NVIDIA® GPU, the following article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/gpu#hardware_requirements\nNote that the documentation on installation of the last component (cuDNN v7.4.1) is a bit sparse. Once you join the NVIDIA® developer program and download the zip file containing cuDNN you need to extract the zip file and add the location where you extracted it to your system PATH.\n\n\nUbuntu\nThis article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support\nThe specifics of installing required software differ by Linux version so please review the NVIDIA® documentation carefully to ensure you install everything correctly.\nThe following section provides as example of the installation commands you might use on Ubuntu 16.04.\n\nUbuntu 16.04 Example\nFirst, install the NVIDIA drivers:\n# Add NVIDIA package repositories\n# Add HTTPS support for apt-key\nsudo apt-get install gnupg-curl\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\nsudo apt-get update\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt-get update\n\n# Install NVIDIA driver\n# Issue with driver install requires creating /usr/lib/nvidia\nsudo mkdir /usr/lib/nvidia\nsudo apt-get install --no-install-recommends nvidia-410\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\nNext install CUDA Toolkit v10.0 and cuDNN v7.4.1 with:\n# Install development and runtime libraries (~4GB)\nsudo apt-get install --no-install-recommends \\\n    cuda-10-0 \\\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\nNote that it’s important to download CUDA 10.0 (rather than CUDA 10.1, which may be the choice initially presented) as v10.0 is what TensorFlow is built against.\nYou can see more for the installation here.\n\n\nEnvironment Variables\nOn Linux, part of the setup for CUDA libraries is adding the path to the CUDA binaries to your PATH and LD_LIBRARY_PATH as well as setting the CUDA_HOME environment variable. You will set these variables in distinct ways depending on whether you are installing TensorFlow on a single-user workstation or on a multi-user server. If you are running RStudio Server there is some additional setup required which is also covered below.\nIn all cases these are the environment variables that need to be set/modified in order for TensorFlow to find the required CUDA libraries. For example (paths will change depending on your specific installation of CUDA):\nexport CUDA_HOME=/usr/local/cuda\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${CUDA_HOME}/lib64 \nPATH=${CUDA_HOME}/bin:${PATH} \nexport PATH\n\n\nSingle-User Installation\nIn a single-user environment (e.g. a desktop system) you should define the environment variables within your ~/.profile file. It’s necessary to use ~/.profile rather than ~/.bashrc, because ~/.profile is read by desktop applications (e.g. RStudio) as well as terminal sessions whereas ~/.bashrc applies only to terminal sessions.\nNote that you need to restart your system after editing the ~/.profile file for the changes to take effect. Note also that the ~/.profile file will not be read by bash if you have either a ~/.bash_profile or ~/.bash_login file.\nTo summarize the recommendations above:\n\nDefine CUDA related environment variables in ~/.profile rather than ~/.bashrc;\nEnsure that you don’t have either a ~/.bash_profile or ~/.bash_login file (as these will prevent bash from seeing the variables you’ve added into ~/.profile);\nRestart your system after editing ~/.profile so that the changes take effect.\n\n\n\nMulti-User Installation\nIn a multi-user installation (e.g. a server) you should define the environment variables within the system-wide bash startup file (/etc/profile) so all users have access to them.\nIf you are running RStudio Server you need to also provide these variable definitions in an R / RStudio specific fashion (as RStudio Server doesn’t execute system profile scripts for R sessions).\nTo modify the LD_LIBRARY_PATH you use the rsession-ld-library-path in the /etc/rstudio/rserver.conf configuration file\n/etc/rstudio/rserver.conf\nrsession-ld-library-path=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nYou should set the CUDA_HOME and PATH variables in the /usr/lib/R/etc/Rprofile.site configuration file:\n/usr/lib/R/etc/Rprofile.site\nSys.setenv(CUDA_HOME=\"/usr/local/cuda\")\nSys.setenv(PATH=paste(Sys.getenv(\"PATH\"), \"/usr/local/cuda/bin\", sep = \":\"))\nIn a server environment you might also find it more convenient to install TensorFlow into a system-wide location where all users of the server can share access to it. Details on doing this are covered in the multi-user installation section below.\n\n\n\nMac OS X\nAs of version 1.2 of TensorFlow, GPU support is no longer available on Mac OS X. If you want to use a GPU on Mac OS X you will need to install TensorFlow v1.1 as follows:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"1.1-gpu\")\n\nHowever, before you install you should ensure that you have an NVIDIA® GPU and that you have the required CUDA libraries on your system.\nWhile some older Macs include NVIDIA® GPU’s, most Macs (especially newer ones) do not, so you should check the type of graphics card you have in your Mac before proceeding.\nHere is a list of Mac systems which include built in NVIDIA GPU’s:\nhttps://support.apple.com/en-us/HT204349\nYou can check which graphics card your Mac has via the System Report button found within the About This Mac dialog:\n\nThe MacBook Pro system displayed above does not have an NVIDIA® GPU installed (rather it has an Intel Iris Pro).\nIf you do have an NVIDIA® GPU, the following article describes how to install the base CUDA libraries:\nhttp://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\nYou also need to intall the cuDNN library 5.1 library for OS X from here:\nhttps://developer.nvidia.com/cudnn\nAfter installing these components, you need to ensure that both CUDA and cuDNN are available to your R session via the DYLD_LIBRARY_PATH. This typically involves setting environment variables in your .bash_profile as described in the NVIDIA documentation for CUDA and cuDNN.\nNote that environment variables set in .bash_profile will not be available by default to OS X desktop applications like R GUI and RStudio. To use CUDA within those environments you should start the application from a system terminal as follows:\nopen -a R         # R GUI\nopen -a RStudio   # RStudio"
  },
  {
    "objectID": "install/gpu/local_gpu/index.html#installation",
    "href": "install/gpu/local_gpu/index.html#installation",
    "title": "Local GPU",
    "section": "Installation",
    "text": "Installation\n\nSingle User\nIn a single-user desktop environment you can install TensorFlow with GPU support via:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf this version doesn’t load successfully you should review the prerequisites above and ensure that you’ve provided definitions of CUDA environment variables as recommended above.\nSee the main installation article for details on other available options (e.g. virtualenv vs. conda installation, installing development versions, etc.).\n\n\nMultiple Users\nIn a multi-user server environment you may want to install a system-wide version of TensorFlow with GPU support so all users can share the same configuration. To do this, start by following the directions for native pip installation of the GPU version of TensorFlow here:\nhttps://www.tensorflow.org/install/install_linux#InstallingNativePip\nThere are some components of TensorFlow (e.g. the Keras library) which have dependencies on additional Python packages.\nYou can install Keras and it’s optional dependencies with the following command (ensuring you have the correct privilege to write to system library locations as required via sudo, etc.):\npip install keras h5py pyyaml requests Pillow scipy\nIf you have any trouble with locating the system-wide version of TensorFlow from within R please see the section on locating TensorFlow."
  },
  {
    "objectID": "install/index.html",
    "href": "install/index.html",
    "title": "Quick start",
    "section": "",
    "text": "Prior to using the tensorflow R package you need to install a version of TensorFlow on your system. Below we describe how to install TensorFlow as well the various options available for customizing your installation.\nNote that this article principally covers the use of the R install_tensorflow() function, which provides an easy to use wrapper for the various steps required to install TensorFlow.\nYou can also choose to install TensorFlow manually (as described at https://www.tensorflow.org/install/). In that case the Custom Installation section covers how to arrange for the tensorflow R package to use the version you installed.\nTensorFlow is tested and supported on the following 64-bit systems:"
  },
  {
    "objectID": "install/index.html#installation",
    "href": "install/index.html#installation",
    "title": "Quick start",
    "section": "Installation",
    "text": "Installation\nFirst, install the tensorflow R package from GitHub as follows:\n\ninstall.packages(\"tensorflow\")\n\nNext, configure R (reticulate) with a Python installation it can use, like this:\n\nlibrary(reticulate)\npath_to_python <- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n\nThen, use the install_tensorflow() function to install TensorFlow.\n\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-reticulate\")\n\nYou can confirm that the installation succeeded with:\n\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow\")\n\nThis will provide you with a default installation of TensorFlow suitable for use with the tensorflow R package. Read on if you want to learn about additional installation options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed."
  },
  {
    "objectID": "install/index.html#installation-methods",
    "href": "install/index.html#installation-methods",
    "title": "Quick start",
    "section": "Installation methods",
    "text": "Installation methods\nTensorFlow is distributed as a Python package and so needs to be installed within a Python environment on your system. By default, the install_tensorflow() function attempts to install TensorFlow within an isolated Python environment (“r-reticulate”).\nThese are the available methods and their behavior:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nauto\nAutomatically choose an appropriate default for the current platform.\n\n\nvirtualenv\nInstall into a Python virtual environment at ~/.virtualenvs/r-reticulate\n\n\nconda\nInstall into an Anaconda Python environment named r-reticulate\n\n\nsystem\nInstall into the system Python environment\n\n\n\ninstall_tensorflow is a wrapper around reticulate::py_install. Please refer to ‘Installing Python Packages’ for more information."
  },
  {
    "objectID": "install/index.html#alternate-versions",
    "href": "install/index.html#alternate-versions",
    "title": "Quick start",
    "section": "Alternate Versions",
    "text": "Alternate Versions\nBy default, install_tensorflow() install the latest release version of TensorFlow. You can override this behavior by specifying the version parameter. For example:\n\ninstall_tensorflow(version = \"2.7\")\n\nNote that you can provide a full major.minor.patch version specification, or just a major.minor specification, in which case the latest patch is automatically selected.\nYou can install the nightly build of TensorFlow (CPU or GPU version) with:\n\ninstall_tensorflow(version = \"nightly\")      # cpu+gpu version\ninstall_tensorflow(version = \"nightly-cpu\")  # cpu version\n\nYou can install any other build of TensorFlow by specifying a URL to a TensorFlow binary. For example:\n\ninstall_tensorflow(version = \"https://files.pythonhosted.org/packages/c2/c1/a035e377cf5a5b90eff27f096448fa5c5a90cbcf13b7eb0673df888f2c2d/tf_nightly-1.12.0.dev20180918-cp36-cp36m-manylinux1_x86_64.whl\")"
  },
  {
    "objectID": "deploy/docker.html",
    "href": "deploy/docker.html",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using TensorFlow serving.\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\nUsing the Docker container is a an easy way to test the API locally and then deploy it to any cloud provider."
  },
  {
    "objectID": "deploy/docker.html#building-the-model",
    "href": "deploy/docker.html#building-the-model",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/docker.html#running-locally",
    "href": "deploy/docker.html#running-locally",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Running locally",
    "text": "Running locally\nYou can run the tensorflow/serving Docker image locally using the great stevedore package. For example:\n\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside `/models`\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n\nNow we have initialized the container serving the model. You can see the container logs with:\n\ncontainer$logs()\n\nNow you can make POST requests no the following endpoint : http://localhost:8501/v1/models/model/versions/1:predict. The input data must be passed in a special format 0 - see the format definition here, which may seem unnatural for R users. Here is an example:\n\ninstances <- purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %>% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nThis is how you can serve TensorFlow models with TF serving locally. Additionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\nWhen done, you can stop the container with:\n\ncontainer$stop()"
  },
  {
    "objectID": "deploy/docker.html#deploying-to-google-cloud-run",
    "href": "deploy/docker.html#deploying-to-google-cloud-run",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Deploying to Google Cloud Run",
    "text": "Deploying to Google Cloud Run\nTHe first thing you need to do is to follow the section Before you begin in this page.\nNow let’s create a Dockerfile that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\nHere’s an example - create a file called Dockerfile in the same root folder as your SavedModel and paste the following:\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\", \"--rest_api_port=8080\"]\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io. Run the following in your terminal:\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\nYou can get your PROJECT-ID by running:\ngcloud config get-value project\nNext, we can create the service in Google Cloud Run using:\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly you will get a url like https://cnn-mnist-ld4lzfalyq-ue.a.run.app which you can now use to make requests to your model. For example:\n\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API."
  },
  {
    "objectID": "deploy/index.html",
    "href": "deploy/index.html",
    "title": "Overview",
    "section": "",
    "text": "Plumber API: Create a REST API using Plumber to deploy your TensorFlow model. With Plumber you will still depend on having an R runtime which be useful when you want to make the data pre-processing in R.\nShiny: Create a Shiny app that uses a TensorFlow model to generate outputs.\nTensorFlow Serving: This is the most performant way of deploying TensorFlow models since it’s based only inn the TensorFlow serving C++ server. With TF serving you don’t depend on an R runtime, so all pre-processing must be done in the TensorFlow graph.\nRStudio Connect: RStudio Connect makes it easy to deploy TensorFlow models and uses TensorFlow serving in the backend.\n\nThere are many other options to deploy TensorFlow models built with R that are not covered in this section. For example:\n\nDeploy it using a Python runtime.\nDeploy using a JavaScript runtime.\nDeploy to a mobile phone app using TensorFlow Lite.\nDeploy to a iOS app using Apple’s Core ML tool.\nUse plumber and Docker to deploy your TensorFlow model (by T-Mobile)."
  },
  {
    "objectID": "deploy/plumber.html",
    "href": "deploy/plumber.html",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using a plumber API.\nIn this example we will build an endpoint that takes POST requests sending images containing handwritten digits and returning the predicted number."
  },
  {
    "objectID": "deploy/plumber.html#building-the-model",
    "href": "deploy/plumber.html#building-the-model",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. W We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/plumber.html#plumber-api",
    "href": "deploy/plumber.html#plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Plumber API",
    "text": "Plumber API\nA plumber API is defined by a .R file with a few annotations. Here’s is how we can write our api.R file:\n\nlibrary(keras)\n\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n#* Predicts the number in an image\n#* @param enc a base64  encoded 28x28 image\n#* @post /cnn-mnist\nfunction(enc) {\n  # decode and read the jpeg image\n  img <- jpeg::readJPEG(source = base64enc::base64decode(enc))\n  \n  # reshape\n  img <- img %>% \n    array_reshape(., dim = c(1, dim(.), 1))\n  \n  # make the prediction\n  predict_classes(model, img)\n}\n\nMake sure to have the your SavedModel in the same folder as api.R and call:\n\np <- plumber::plumb(\"api.R\")\np$run(port = 8000)\n\nYou can now make requests to the http://lcoalhost:8000/cnn-minist/ endpoint. For example, let’s verify we can make a POST request to the API sending the first image from the test set:\n\nimg <- mnist$test$x[1,,,]\nmnist$test$y[1]\n\nFirst let’s encode the image:\n\nencoded_img <- img %>% \n  jpeg::writeJPEG() %>% \n  base64enc::base64encode()\nencoded_img\n\n\nreq <- httr::POST(\"http://localhost:8000/cnn-mnist\",\n           body = list(enc = encoded_img), \n           encode = \"json\")\nhttr::content(req)\n\n[[1]]\n[1] 7\nYou can also access the Swagger interface by accessing http://127.0.0.1:8000/swagger/ and paste the encoded string in the UI to visualize the result."
  },
  {
    "objectID": "deploy/plumber.html#more-advanced-models",
    "href": "deploy/plumber.html#more-advanced-models",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/plumber.html#hosting-the-plumber-api",
    "href": "deploy/plumber.html#hosting-the-plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Hosting the plumber API",
    "text": "Hosting the plumber API\nPlumber is very flexible and allows multiple hosting options. See the plumber Hostinng documentation for more information."
  },
  {
    "objectID": "deploy/rsconnect.html",
    "href": "deploy/rsconnect.html",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model to RStudio Connect. RStudio Connect uses TensorFlow Serving for performance but makes it much easier for R users to manage their deployment."
  },
  {
    "objectID": "deploy/rsconnect.html#building-the-model",
    "href": "deploy/rsconnect.html#building-the-model",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y, verbose = 0)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "href": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Deployiong to RStudio Connect",
    "text": "Deployiong to RStudio Connect\nOnce the model is saved to the SavedModel format, the model can be deployed with a single line of code:\n\nrsconnect::deployTFModel(\"cnn-mnist/\")\n\nWhen the deployment is complete you will be redirected to your browser with some instructions on how to call the REST endpoint:"
  },
  {
    "objectID": "deploy/shiny.html",
    "href": "deploy/shiny.html",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model inside a Shiny app. We will build a model that can classify handwritten digits in images, then we will build a Shiny app that let’s you upload an image and get predictions from this model."
  },
  {
    "objectID": "deploy/shiny.html#building-the-model",
    "href": "deploy/shiny.html#building-the-model",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/shiny.html#shiny-app",
    "href": "deploy/shiny.html#shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Shiny app",
    "text": "Shiny app\nA simple shiny app can be define in an app.R file with a few conventions. Here’s how we can structure our Shiny app.\n\nlibrary(shiny)\nlibrary(keras)\n\n# Load the model\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n# Define the UI\nui <- fluidPage(\n  # App title ----\n  titlePanel(\"Hello TensorFlow!\"),\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n      # Input: File upload\n      fileInput(\"image_path\", label = \"Input a JPEG image\")\n    ),\n    # Main panel for displaying outputs ----\n    mainPanel(\n      # Output: Histogram ----\n      textOutput(outputId = \"prediction\"),\n      plotOutput(outputId = \"image\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram ----\nserver <- function(input, output) {\n  \n  image <- reactive({\n    req(input$image_path)\n    jpeg::readJPEG(input$image_path$datapath)\n  })\n  \n  output$prediction <- renderText({\n    \n    img <- image() %>% \n      array_reshape(., dim = c(1, dim(.), 1))\n    \n    paste0(\"The predicted class number is \", predict_classes(model, img))\n  })\n  \n  output$image <- renderPlot({\n    plot(as.raster(image()))\n  })\n  \n}\n\nshinyApp(ui, server)\n\nThis app can be used locally or deployed using any Shiny deployment option. If you are deploying to RStudio Connect or Shinnyapps.io, don’t forget to set the RETICULATE_PYTHON environment variable so rsconnect can detect what python packages are required to reproduce your local environment. See the F.A.Q. for more information.\n\nYou can see a live version of this app here. Note that to keep the code simple, it will only accept JPEG images with 28x28 pixels. You can download this file if you want to try the app."
  },
  {
    "objectID": "deploy/shiny.html#more-advanced-models",
    "href": "deploy/shiny.html#more-advanced-models",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/shiny.html#hosting-the-shiny-app",
    "href": "deploy/shiny.html#hosting-the-shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Hosting the shiny app",
    "text": "Hosting the shiny app\nThis Shiny app can be hosted in any server using the Shiny Server. If you are managing the complete infrastructure, make sure that you have Python and all required Python packages installed in the server.\nIf you are using Shinyapps.io or RStudio Connect the dependencies will be infered when deploying the app. In this case, don’t forget to set the RETICULATE_PYTHON environment variable.\nYou can find more examples of using reticulate in RStudio products here and learn more about Python in RStudio Connect best practices here."
  },
  {
    "objectID": "v1/guide/keras/applications.html",
    "href": "v1/guide/keras/applications.html",
    "title": "Using Pre-Trained Models",
    "section": "",
    "text": "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\nWeights are downloaded automatically when instantiating a model. They are stored at ~/.keras/models/.\nThe following image classification models (with weights trained on ImageNet) are available:\n\nXception\nVGG16\nVGG19\nResNet50\nInceptionV3\nInceptionResNetV2\nMobileNet\nMobileNetV2\nDenseNet\nNASNet\n\nAll of these architectures are compatible with all the backends (TensorFlow, Theano, and CNTK), and upon instantiation the models will be built according to the image data format set in your Keras configuration file at ~/.keras/keras.json. For instance, if you have set image_data_format=channels_last, then any model loaded from this repository will get built according to the TensorFlow data format convention, “Height-Width-Depth”.\n\nFor Keras < 2.2.0, The Xception model is only available for TensorFlow, due to its reliance on SeparableConvolution layers.\nFor Keras < 2.1.5, The MobileNet model is only available for TensorFlow, due to its reliance on DepthwiseConvolution layers."
  },
  {
    "objectID": "v1/guide/keras/applications.html#usage-examples",
    "href": "v1/guide/keras/applications.html#usage-examples",
    "title": "Using Pre-Trained Models",
    "section": "Usage Examples",
    "text": "Usage Examples\n\nClassify ImageNet classes with ResNet50\n\n# instantiate the model\nmodel <- application_resnet50(weights = 'imagenet')\n\n# load the image\nimg_path <- \"elephant.jpg\"\nimg <- image_load(img_path, target_size = c(224,224))\nx <- image_to_array(img)\n\n# ensure we have a 4d tensor with single element in the batch dimension,\n# the preprocess the input for prediction using resnet50\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenet_preprocess_input(x)\n\n# make predictions then decode and print them\npreds <- model %>% predict(x)\nimagenet_decode_predictions(preds, top = 3)[[1]]\n\n  class_name class_description      score\n1  n02504013   Indian_elephant 0.90117526\n2  n01871265            tusker 0.08774310\n3  n02504458  African_elephant 0.01046011\n\n\nExtract features with VGG16\n\nmodel <- application_vgg16(weights = 'imagenet', include_top = FALSE)\n\nimg_path <- \"elephant.jpg\"\nimg <- image_load(img_path, target_size = c(224,224))\nx <- image_to_array(img)\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenet_preprocess_input(x)\n\nfeatures <- model %>% predict(x)\n\n\n\nExtract features from an arbitrary intermediate layer with VGG19\n\nbase_model <- application_vgg19(weights = 'imagenet')\nmodel <- keras_model(inputs = base_model$input, \n                     outputs = get_layer(base_model, 'block4_pool')$output)\n\nimg_path <- \"elephant.jpg\"\nimg <- image_load(img_path, target_size = c(224,224))\nx <- image_to_array(img)\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenet_preprocess_input(x)\n\nblock4_pool_features <- model %>% predict(x)\n\n\n\nFine-tune InceptionV3 on a new set of classes\n\n# create the base pre-trained model\nbase_model <- application_inception_v3(weights = 'imagenet', include_top = FALSE)\n\n# add our custom layers\npredictions <- base_model$output %>% \n  layer_global_average_pooling_2d() %>% \n  layer_dense(units = 1024, activation = 'relu') %>% \n  layer_dense(units = 200, activation = 'softmax')\n\n# this is the model we will train\nmodel <- keras_model(inputs = base_model$input, outputs = predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfreeze_weights(base_model)\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel %>% compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel %>% fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nlayers <- base_model$layers\nfor (i in 1:length(layers))\n  cat(i, layers[[i]]$name, \"\\n\")\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 172 layers and unfreeze the rest:\nfreeze_weights(base_model, from = 1, to = 172)\nunfreeze_weights(base_model, from = 173)\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nmodel %>% compile(\n  optimizer = optimizer_sgd(lr = 0.0001, momentum = 0.9), \n  loss = 'categorical_crossentropy'\n)\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel %>% fit_generator(...)\n\n\n\nBuild InceptionV3 over a custom input tensor\n\n# this could also be the output a different Keras model or layer\ninput_tensor <- layer_input(shape = c(224, 224, 3))\n\nmodel <- application_inception_V3(input_tensor = input_tensor, \n                                  weights='imagenet', \n                                  include_top = TRUE)\n\n\n\nAdditional examples\nThe VGG16 model is the basis for the Deep dream Keras example script."
  },
  {
    "objectID": "v1/guide/keras/index.html",
    "href": "v1/guide/keras/index.html",
    "title": "Getting Started with Keras",
    "section": "",
    "text": "Keras is a high-level neural networks API developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Keras has the following key features:\n\nAllows the same code to run on CPU or on GPU, seamlessly.\nUser-friendly API which makes it easy to quickly prototype deep learning models.\nBuilt-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\n\nThis website provides documentation for the R interface to Keras. See the main Keras website at https://keras.io for additional information on the project."
  },
  {
    "objectID": "v1/guide/keras/index.html#installation",
    "href": "v1/guide/keras/index.html#installation",
    "title": "Getting Started with Keras",
    "section": "Installation",
    "text": "Installation\nFirst, install the keras R package with:\n\ninstall.packages(\"keras\")\n\nor install the development version with:\n\ndevtools::install_github(\"rstudio/keras\")\n\nThe Keras R interface uses the TensorFlow backend engine by default.\n\ninstall.packages(\"keras\")\ninstall_keras()\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras() and the installation section."
  },
  {
    "objectID": "v1/guide/keras/index.html#mnist-example",
    "href": "v1/guide/keras/index.html#mnist-example",
    "title": "Getting Started with Keras",
    "section": "MNIST Example",
    "text": "MNIST Example\nWe can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\n\nPreparing the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:\n\nlibrary(keras)\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nThe x data is a 3-d array (images,width,height) of grayscale values . To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:\n\n# reshape\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n# rescale\nx_train <- x_train / 255\nx_test <- x_test / 255\n\nNote that we use the array_reshape() function rather than the dim<-() function to reshape the array. This is so that the data is re-interpreted using row-major semantics (as opposed to R’s default column-major semantics), which is in turn compatible with the way that the numerical libraries called by Keras interpret array dimensions.\nThe y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\n\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers.\nWe begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.\nUse the summary() function to print the details of the model:\n\nsummary(model)\n\nModel\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 256)                     200960      \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 128)                     32896       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\nNext, compile the model with appropriate loss function, optimizer, and metrics:\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n\n\nTraining and Evaluation\nUse the fit() function to train the model for 30 epochs using batches of 128 images:\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nThe history object returned by fit() includes loss and accuracy metrics which we can plot:\n\nplot(history)\n\n\nEvaluate the model’s performance on the test data:\n\nmodel %>% evaluate(x_test, y_test)\n\n$loss\n[1] 0.1149\n\n$acc\n[1] 0.9807\nGenerate predictions on new data:\n\nmodel %>% predict_classes(x_test)\n\n  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2\n [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2\n [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9\n [ reached getOption(\"max.print\") -- omitted 9900 entries ]\nKeras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.\n\n\nDeep Learning with R Book\nIf you want a more comprehensive introduction to both Keras and the concepts and practice of deep learning, we recommend the Deep Learning with R book from Manning. This book is a collaboration between François Chollet, the creator of Keras, and J.J. Allaire, who wrote the R interface to Keras.\nThe book presumes no significant knowledge of machine learning and deep learning, and goes all the way from basic theory to advanced practical applications, all using the R interface to Keras."
  },
  {
    "objectID": "v1/guide/keras/index.html#why-this-name-keras",
    "href": "v1/guide/keras/index.html#why-this-name-keras",
    "title": "Getting Started with Keras",
    "section": "Why this name, Keras?",
    "text": "Why this name, Keras?\nKeras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It’s a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n“Oneiroi are beyond our unravelling –who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.” Homer, Odyssey 19. 562 ff (Shewring translation)."
  },
  {
    "objectID": "v1/deploy/docker.html",
    "href": "v1/deploy/docker.html",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using TensorFlow serving.\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\nUsing the Docker container is a an easy way to test the API locally and then deploy it to any cloud provider."
  },
  {
    "objectID": "v1/deploy/docker.html#building-the-model",
    "href": "v1/deploy/docker.html#building-the-model",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "v1/deploy/docker.html#running-locally",
    "href": "v1/deploy/docker.html#running-locally",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Running locally",
    "text": "Running locally\nYou can run the tensorflow/serving Docker image locally using the great stevedore package. For example:\n\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside `/models`\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n\nNow we have initialized the container serving the model. You can see the container logs with:\n\ncontainer$logs()\n\nNow you can make POST requests no the following endpoint : http://localhost:8501/v1/models/model/versions/1:predict. The input data must be passed in a special format 0 - see the format definition here, which may seem unnatural for R users. Here is an example:\n\ninstances <- purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %>% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nThis is how you can serve TensorFlow models with TF serving locally. Additionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\nWhen done, you can stop the container with:\n\ncontainer$stop()"
  },
  {
    "objectID": "v1/deploy/docker.html#deploying-to-google-cloud-run",
    "href": "v1/deploy/docker.html#deploying-to-google-cloud-run",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Deploying to Google Cloud Run",
    "text": "Deploying to Google Cloud Run\nTHe first thing you need to do is to follow the section Before you begin in this page.\nNow let’s create a Dockerfile that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\nHere’s an example - create a file called Dockerfile in the same root folder as your SavedModel and paste the following:\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\", \"--rest_api_port=8080\"]\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io. Run the following in your terminal:\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\nYou can get your PROJECT-ID by running:\ngcloud config get-value project\nNext, we can create the service in Google Cloud Run using:\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly you will get a url like https://cnn-mnist-ld4lzfalyq-ue.a.run.app which you can now use to make requests to your model. For example:\n\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API."
  },
  {
    "objectID": "v1/deploy/index.html",
    "href": "v1/deploy/index.html",
    "title": "Overview",
    "section": "",
    "text": "Plumber API: Create a REST API using Plumber to deploy your TensorFlow model. With Plumber you will still depend on having an R runtime which be useful when you want to make the data pre-processing in R.\nShiny: Create a Shiny app that uses a TensorFlow model to generate outputs.\nTensorFlow Serving: This is the most performant way of deploying TensorFlow models since it’s based only inn the TensorFlow serving C++ server. With TF serving you don’t depend on an R runtime, so all pre-processing must be done in the TensorFlow graph.\nRStudio Connect: RStudio Connect makes it easy to deploy TensorFlow models and uses TensorFlow serving in the backend.\n\nThere are many other options to deploy TensorFlow models built with R that are not covered in this section. For example:\n\nDeploy it using a Python runtime.\nDeploy using a JavaScript runtime.\nDeploy to a mobile phone app using TensorFlow Lite.\nDeploy to a iOS app using Apple’s Core ML tool.\nUse plumber and Docker to deploy your TensorFlow model (by T-Mobile)."
  },
  {
    "objectID": "v1/deploy/plumber.html",
    "href": "v1/deploy/plumber.html",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using a plumber API.\nIn this example we will build an endpoint that takes POST requests sending images containing handwritten digits and returning the predicted number."
  },
  {
    "objectID": "v1/deploy/plumber.html#building-the-model",
    "href": "v1/deploy/plumber.html#building-the-model",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. W We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "v1/deploy/plumber.html#plumber-api",
    "href": "v1/deploy/plumber.html#plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Plumber API",
    "text": "Plumber API\nA plumber API is defined by a .R file with a few annotations. Here’s is how we can write our api.R file:\n\nlibrary(keras)\n\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n#* Predicts the number in an image\n#* @param enc a base64  encoded 28x28 image\n#* @post /cnn-mnist\nfunction(enc) {\n  # decode and read the jpeg image\n  img <- jpeg::readJPEG(source = base64enc::base64decode(enc))\n  \n  # reshape\n  img <- img %>% \n    array_reshape(., dim = c(1, dim(.), 1))\n  \n  # make the prediction\n  predict_classes(model, img)\n}\n\nMake sure to have the your SavedModel in the same folder as api.R and call:\n\np <- plumber::plumb(\"api.R\")\np$run(port = 8000)\n\nYou can now make requests to the http://lcoalhost:8000/cnn-minist/ endpoint. For example, let’s verify we can make a POST request to the API sending the first image from the test set:\n\nimg <- mnist$test$x[1,,,]\nmnist$test$y[1]\n\nFirst let’s encode the image:\n\nencoded_img <- img %>% \n  jpeg::writeJPEG() %>% \n  base64enc::base64encode()\nencoded_img\n\n\nreq <- httr::POST(\"http://localhost:8000/cnn-mnist\",\n           body = list(enc = encoded_img), \n           encode = \"json\")\nhttr::content(req)\n\n[[1]]\n[1] 7\nYou can also access the Swagger interface by accessing http://127.0.0.1:8000/swagger/ and paste the encoded string in the UI to visualize the result."
  },
  {
    "objectID": "v1/deploy/plumber.html#more-advanced-models",
    "href": "v1/deploy/plumber.html#more-advanced-models",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "v1/deploy/plumber.html#hosting-the-plumber-api",
    "href": "v1/deploy/plumber.html#hosting-the-plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Hosting the plumber API",
    "text": "Hosting the plumber API\nPlumber is very flexible and allows multiple hosting options. See the plumber Hostinng documentation for more information."
  },
  {
    "objectID": "v1/deploy/rsconnect.html",
    "href": "v1/deploy/rsconnect.html",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model to RStudio Connect. RStudio Connect uses TensorFlow Serving for performance but makes it much easier for R users to manage their deployment."
  },
  {
    "objectID": "v1/deploy/rsconnect.html#building-the-model",
    "href": "v1/deploy/rsconnect.html#building-the-model",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y, verbose = 0)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "v1/deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "href": "v1/deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Deployiong to RStudio Connect",
    "text": "Deployiong to RStudio Connect\nOnce the model is saved to the SavedModel format, the model can be deployed with a single line of code:\n\nrsconnect::deployTFModel(\"cnn-mnist/\")\n\nWhen the deployment is complete you will be redirected to your browser with some instructions on how to call the REST endpoint:"
  },
  {
    "objectID": "v1/deploy/shiny.html",
    "href": "v1/deploy/shiny.html",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model inside a Shiny app. We will build a model that can classify handwritten digits in images, then we will build a Shiny app that let’s you upload an image and get predictions from this model."
  },
  {
    "objectID": "v1/deploy/shiny.html#building-the-model",
    "href": "v1/deploy/shiny.html#building-the-model",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "v1/deploy/shiny.html#shiny-app",
    "href": "v1/deploy/shiny.html#shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Shiny app",
    "text": "Shiny app\nA simple shiny app can be define in an app.R file with a few conventions. Here’s how we can structure our Shiny app.\n\nlibrary(shiny)\nlibrary(keras)\n\n# Load the model\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n# Define the UI\nui <- fluidPage(\n  # App title ----\n  titlePanel(\"Hello TensorFlow!\"),\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n      # Input: File upload\n      fileInput(\"image_path\", label = \"Input a JPEG image\")\n    ),\n    # Main panel for displaying outputs ----\n    mainPanel(\n      # Output: Histogram ----\n      textOutput(outputId = \"prediction\"),\n      plotOutput(outputId = \"image\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram ----\nserver <- function(input, output) {\n  \n  image <- reactive({\n    req(input$image_path)\n    jpeg::readJPEG(input$image_path$datapath)\n  })\n  \n  output$prediction <- renderText({\n    \n    img <- image() %>% \n      array_reshape(., dim = c(1, dim(.), 1))\n    \n    paste0(\"The predicted class number is \", predict_classes(model, img))\n  })\n  \n  output$image <- renderPlot({\n    plot(as.raster(image()))\n  })\n  \n}\n\nshinyApp(ui, server)\n\nThis app can be used locally or deployed using any Shiny deployment option. If you are deploying to RStudio Connect or Shinnyapps.io, don’t forget to set the RETICULATE_PYTHON environment variable so rsconnect can detect what python packages are required to reproduce your local environment. See the F.A.Q. for more information.\n\nYou can see a live version of this app here. Note that to keep the code simple, it will only accept JPEG images with 28x28 pixels. You can download this file if you want to try the app."
  },
  {
    "objectID": "v1/deploy/shiny.html#more-advanced-models",
    "href": "v1/deploy/shiny.html#more-advanced-models",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "v1/deploy/shiny.html#hosting-the-shiny-app",
    "href": "v1/deploy/shiny.html#hosting-the-shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Hosting the shiny app",
    "text": "Hosting the shiny app\nThis Shiny app can be hosted in any server using the Shiny Server. If you are managing the complete infrastructure, make sure that you have Python and all required Python packages installed in the server.\nIf you are using Shinyapps.io or RStudio Connect the dependencies will be infered when deploying the app. In this case, don’t forget to set the RETICULATE_PYTHON environment variable.\nYou can find more examples of using reticulate in RStudio products here and learn more about Python in RStudio Connect best practices here."
  },
  {
    "objectID": "v1/guide/index.html",
    "href": "v1/guide/index.html",
    "title": "Overview",
    "section": "",
    "text": "In the guide you will find documentation about specific functionality of the R packages in the TensorFlow ecosystem."
  },
  {
    "objectID": "v1/guide/index.html#essential-documentation",
    "href": "v1/guide/index.html#essential-documentation",
    "title": "Overview",
    "section": "Essential documentation",
    "text": "Essential documentation\nIf you are just getting started started with TensorFlow for R, we recommend reading the following selected topics in the guide:\n\nInstalling TensorFlow: Install TensorFlow R packages and its dependencies.\nGet started with Keras: First time using Keras? Read ir!\nRead more about Keras: More information about customizing your Keras models.\nGet started with TensorFlow Datasets: Overview of the TensorFlow Datasets API.\nEager execution: Understand TensorFlow’s eager execution."
  },
  {
    "objectID": "v1/guide/index.html#customization",
    "href": "v1/guide/index.html#customization",
    "title": "Overview",
    "section": "Customization",
    "text": "Customization\nRead the following guides for more information on how to customize your model with TensorFlow and Keras:\n\nCustom Layers: Create custom layers for your Keras models.\nCallbacks: Using callbacks to customize model training.\nRagged Tensors: Data structure useful for sequences of variable length.\nFeature Spec API: Use the feature spec interface to build models for tabular data.\n\nSee also the deployment guide."
  },
  {
    "objectID": "v1/guide/keras/custom_layers.html",
    "href": "v1/guide/keras/custom_layers.html",
    "title": "Writing Custom Keras Layers",
    "section": "",
    "text": "If the existing Keras layers don’t meet your requirements you can create a custom layer. For simple, stateless custom operations, you are probably better off using layer_lambda() layers. But for any custom operation that has trainable weights, you should implement your own layer.\nThe example below illustrates the skeleton of a Keras custom layer. The mnist_antirectifier example includes another demonstration of creating a custom layer."
  },
  {
    "objectID": "v1/guide/keras/custom_layers.html#keraslayer-r6-class",
    "href": "v1/guide/keras/custom_layers.html#keraslayer-r6-class",
    "title": "Writing Custom Keras Layers",
    "section": "KerasLayer R6 Class",
    "text": "KerasLayer R6 Class\nTo create a custom Keras layer, you create an R6 class derived from KerasLayer. There are three methods to implement (only one of which, call(), is required for all types of layer):\n\nbuild(input_shape): This is where you will define your weights. Note that if your layer doesn’t define trainable weights then you need not implemented this method.\ncall(x): This is where the layer’s logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call: the input tensor.\ncompute_output_shape(input_shape): In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don’t modify the shape of the input then you need not implement this method.\n\n\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    output_dim = NULL,\n    \n    kernel = NULL,\n    \n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n    \n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel', \n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n    \n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n)\n\nNote that tensor operations are executed using the Keras backend(). See the Keras Backend article for details on the various functions available from Keras backends."
  },
  {
    "objectID": "v1/guide/keras/custom_layers.html#layer-wrapper-function",
    "href": "v1/guide/keras/custom_layers.html#layer-wrapper-function",
    "title": "Writing Custom Keras Layers",
    "section": "Layer Wrapper Function",
    "text": "Layer Wrapper Function\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\n# define layer wrapper function\nlayer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {\n  create_layer(CustomLayer, object, list(\n    output_dim = as.integer(output_dim),\n    name = name,\n    trainable = trainable\n  ))\n}\n\n# use it in a model\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 32, input_shape = c(32,32)) %>% \n  layer_custom(output_dim = 32)\n\nSome important things to note about the layer wrapper function:\n\nIt accepts object as its first parameter (the object will either be a Keras sequential model or another Keras layer). The object parameter enables the layer to be composed with other layers using the magrittr pipe (%>%) operator.\nIt converts it’s output_dim to integer using the as.integer() function. This is done as convenience to the user because Keras variables are strongly typed (you can’t pass a float if an integer is expected). This enables users of the function to write output_dim = 32 rather than output_dim = 32L.\nSome additional parameters not used by the layer (name and trainable) are in the function signature. Custom layer functions can include any of the core layer function arguments (input_shape, batch_input_shape, batch_size, dtype, name, trainable, and weights) and they will be automatically forwarded to the Layer base class.\n\nSee the mnist_antirectifier example for another demonstration of creating a custom layer."
  },
  {
    "objectID": "v1/guide/keras/custom_models.html",
    "href": "v1/guide/keras/custom_models.html",
    "title": "Writing Custom Keras Models",
    "section": "",
    "text": "In addition to sequential models and models created with the functional API, you may also define models by defining a custom call() (forward pass) operation.\nTo create a custom Keras model, you call the keras_model_custom() function, passing it an R function which in turn returns another R function that implements the custom call() (forward pass) operation. The R function you pass takes a model argument, which provides access to the underlying Keras model object should you need it.\nTypically, you’ll wrap your call to keras_model_custom() in yet another function that enables callers to easily instantiate your custom model."
  },
  {
    "objectID": "v1/guide/keras/custom_models.html#creating-a-custom-model",
    "href": "v1/guide/keras/custom_models.html#creating-a-custom-model",
    "title": "Writing Custom Keras Models",
    "section": "Creating a Custom Model",
    "text": "Creating a Custom Model\nThis example demonstrates the implementation of a simple custom model that implements a multi-layer-perceptron with optional dropout and batch normalization:\n\nlibrary(keras)\n\nkeras_model_simple_mlp <- function(num_classes, \n                                   use_bn = FALSE, use_dp = FALSE, \n                                   name = NULL) {\n  \n  # define and return a custom model\n  keras_model_custom(name = name, function(self) {\n    \n    # create layers we'll need for the call (this code executes once)\n    self$dense1 <- layer_dense(units = 32, activation = \"relu\")\n    self$dense2 <- layer_dense(units = num_classes, activation = \"softmax\")\n    if (use_dp)\n      self$dp <- layer_dropout(rate = 0.5)\n    if (use_bn)\n      self$bn <- layer_batch_normalization(axis = -1)\n    \n    # implement call (this code executes during training & inference)\n    function(inputs, mask = NULL, training = FALSE) {\n      x <- self$dense1(inputs)\n      if (use_dp)\n        x <- self$dp(x)\n      if (use_bn)\n        x <- self$bn(x)\n      self$dense2(x)\n    }\n  })\n}\n\nNote that we include a name parameter so that users can optionally provide a human readable name for the model.\nNote also that when we create layers to be used in our forward pass we set them onto the self object so they are tracked appropriately by Keras.\nIn call(), you may specify custom losses by calling self$add_loss(). You can also access any other members of the Keras model you need (or even add fields to the model) by using self$."
  },
  {
    "objectID": "v1/guide/keras/custom_models.html#using-a-custom-model",
    "href": "v1/guide/keras/custom_models.html#using-a-custom-model",
    "title": "Writing Custom Keras Models",
    "section": "Using a Custom Model",
    "text": "Using a Custom Model\nTo use a custom model, just call your model’s high-level wrapper function. For example:\n\nlibrary(keras)\n\n# create the model \nmodel <- keras_model_simple_mlp(num_classes = 10, use_dp = TRUE)\n\n# compile graph\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n# Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)\n\n# Convert labels to categorical one-hot encoding\none_hot_labels <- to_categorical(labels, num_classes = 10)\n\n# Train the model\nmodel %>% fit(data, one_hot_labels, epochs=10, batch_size=32)"
  },
  {
    "objectID": "v1/guide/keras/examples/addition_rnn.html",
    "href": "v1/guide/keras/examples/addition_rnn.html",
    "title": "addition_rnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/addition_rnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/babi_memnn.html",
    "href": "v1/guide/keras/examples/babi_memnn.html",
    "title": "babi_memnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/babi_memnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/babi_rnn.html",
    "href": "v1/guide/keras/examples/babi_rnn.html",
    "title": "babi_rnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/babi_rnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/cifar10_cnn.html",
    "href": "v1/guide/keras/examples/cifar10_cnn.html",
    "title": "cifar10_cnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_cnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/cifar10_densenet.html",
    "href": "v1/guide/keras/examples/cifar10_densenet.html",
    "title": "cifar10_densenet",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_densenet.R"
  },
  {
    "objectID": "v1/guide/keras/examples/cifar10_resnet.html",
    "href": "v1/guide/keras/examples/cifar10_resnet.html",
    "title": "cifar10_resnet",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_resnet.R"
  },
  {
    "objectID": "v1/guide/keras/examples/conv_filter_visualization.html",
    "href": "v1/guide/keras/examples/conv_filter_visualization.html",
    "title": "conv_filter_visualization",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/conv_filter_visualization.R"
  },
  {
    "objectID": "v1/guide/keras/examples/conv_lstm.html",
    "href": "v1/guide/keras/examples/conv_lstm.html",
    "title": "conv_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/conv_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/deep_dream.html",
    "href": "v1/guide/keras/examples/deep_dream.html",
    "title": "deep_dream",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/deep_dream.R"
  },
  {
    "objectID": "v1/guide/keras/examples/eager_cvae.html",
    "href": "v1/guide/keras/examples/eager_cvae.html",
    "title": "eager_cvae",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_cvae.R"
  },
  {
    "objectID": "v1/guide/keras/examples/eager_dcgan.html",
    "href": "v1/guide/keras/examples/eager_dcgan.html",
    "title": "eager_dcgan",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_dcgan.R"
  },
  {
    "objectID": "v1/guide/keras/examples/eager_image_captioning.html",
    "href": "v1/guide/keras/examples/eager_image_captioning.html",
    "title": "eager_image_captioning",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_image_captioning.R"
  },
  {
    "objectID": "v1/guide/keras/examples/eager_pix2pix.html",
    "href": "v1/guide/keras/examples/eager_pix2pix.html",
    "title": "eager_pix2pix",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_pix2pix.R"
  },
  {
    "objectID": "v1/guide/keras/examples/eager_styletransfer.html",
    "href": "v1/guide/keras/examples/eager_styletransfer.html",
    "title": "eager_styletransfer",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_styletransfer.R"
  },
  {
    "objectID": "v1/guide/keras/examples/fine_tuning.html",
    "href": "v1/guide/keras/examples/fine_tuning.html",
    "title": "fine_tuning",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/fine_tuning.R"
  },
  {
    "objectID": "v1/guide/keras/examples/image_ocr.html",
    "href": "v1/guide/keras/examples/image_ocr.html",
    "title": "image_ocr",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/image_ocr.R"
  },
  {
    "objectID": "v1/guide/keras/examples/imdb_bidirectional_lstm.html",
    "href": "v1/guide/keras/examples/imdb_bidirectional_lstm.html",
    "title": "imdb_bidirectional_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_bidirectional_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/imdb_cnn.html",
    "href": "v1/guide/keras/examples/imdb_cnn.html",
    "title": "imdb_cnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/imdb_cnn_lstm.html",
    "href": "v1/guide/keras/examples/imdb_cnn_lstm.html",
    "title": "imdb_cnn_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/imdb_fasttext.html",
    "href": "v1/guide/keras/examples/imdb_fasttext.html",
    "title": "imdb_fasttext",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_fasttext.R"
  },
  {
    "objectID": "v1/guide/keras/examples/imdb_lstm.html",
    "href": "v1/guide/keras/examples/imdb_lstm.html",
    "title": "imdb_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/index.html",
    "href": "v1/guide/keras/examples/index.html",
    "title": "Keras Examples",
    "section": "",
    "text": "Example\nDescription\n\n\n\n\naddition_rnn\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\n\n\nbabi_memnn\nTrains a memory network on the bAbI dataset for reading comprehension.\n\n\nbabi_rnn\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\n\n\ncifar10_cnn\nTrains a simple deep CNN on the CIFAR10 small images dataset.\n\n\ncifar10_densenet\nTrains a DenseNet-40-12 on the CIFAR10 small images dataset.\n\n\nconv_lstm\nDemonstrates the use of a convolutional LSTM network.\n\n\ndeep_dream\nDeep Dreams in Keras.\n\n\neager_dcgan\nGenerating digits with generative adversarial networks and eager execution.\n\n\neager_image_captioning\nGenerating image captions with Keras and eager execution.\n\n\neager_pix2pix\nImage-to-image translation with Pix2Pix, using eager execution.\n\n\neager_styletransfer\nNeural style transfer with eager execution.\n\n\nfine_tuning\nFine tuning of a image classification model.\n\n\nimdb_bidirectional_lstm\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\n\n\nimdb_cnn\nDemonstrates the use of Convolution1D for text classification.\n\n\nimdb_cnn_lstm\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\n\n\nimdb_fasttext\nTrains a FastText model on the IMDB sentiment classification task.\n\n\nimdb_lstm\nTrains a LSTM on the IMDB sentiment classification task.\n\n\nlstm_text_generation\nGenerates text from Nietzsche’s writings.\n\n\nlstm_seq2seq\nThis script demonstrates how to implement a basic character-level sequence-to-sequence model.\n\n\nmnist_acgan\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\n\n\nmnist_antirectifier\nDemonstrates how to write custom layers for Keras\n\n\nmnist_cnn\nTrains a simple convnet on the MNIST dataset.\n\n\nmnist_cnn_embeddings\nDemonstrates how to visualize embeddings in TensorBoard.\n\n\nmnist_irnn\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\n\n\nmnist_mlp\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\n\n\nmnist_hierarchical_rnn\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\n\n\nmnist_tfrecord\nMNIST dataset with TFRecords, the standard TensorFlow data format.\n\n\nmnist_transfer_cnn\nTransfer learning toy example.\n\n\nneural_style_transfer\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\n\n\nnmt_attention\nNeural machine translation with an attention mechanism.\n\n\nquora_siamese_lstm\nClassifying duplicate quesitons from Quora using Siamese Recurrent Architecture.\n\n\nreuters_mlp\nTrains and evaluatea a simple MLP on the Reuters newswire topic classification task.\n\n\nstateful_lstm\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\n\n\ntext_explanation_lime\nHow to use lime to explain text data.\n\n\nvariational_autoencoder\nDemonstrates how to build a variational autoencoder.\n\n\nvariational_autoencoder_deconv\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\n\n\ntfprob_vae\nA variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST.\n\n\nvq_vae\nDiscrete Representation Learning with VQ-VAE and TensorFlow Probability."
  },
  {
    "objectID": "v1/guide/keras/examples/lstm_benchmark.html",
    "href": "v1/guide/keras/examples/lstm_benchmark.html",
    "title": "lstm_benchmark",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.R"
  },
  {
    "objectID": "v1/guide/keras/examples/lstm_seq2seq.html",
    "href": "v1/guide/keras/examples/lstm_seq2seq.html",
    "title": "lstm_seq2seq",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.R"
  },
  {
    "objectID": "v1/guide/keras/examples/lstm_stateful.html",
    "href": "v1/guide/keras/examples/lstm_stateful.html",
    "title": "lstm_stateful",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_stateful.R"
  },
  {
    "objectID": "v1/guide/keras/examples/lstm_text_generation.html",
    "href": "v1/guide/keras/examples/lstm_text_generation.html",
    "title": "lstm_text_generation",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_text_generation.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mmd_cvae.html",
    "href": "v1/guide/keras/examples/mmd_cvae.html",
    "title": "mmd_cvae",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_cvae.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_acgan.html",
    "href": "v1/guide/keras/examples/mnist_acgan.html",
    "title": "mnist_acgan",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_acgan.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_antirectifier.html",
    "href": "v1/guide/keras/examples/mnist_antirectifier.html",
    "title": "mnist_antirectifier",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_antirectifier.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_cnn.html",
    "href": "v1/guide/keras/examples/mnist_cnn.html",
    "title": "mnist_cnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_cnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_cnn_embeddings.html",
    "href": "v1/guide/keras/examples/mnist_cnn_embeddings.html",
    "title": "mnist_cnn_embeddings",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_cnn_embeddings.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_dataset_api.html",
    "href": "v1/guide/keras/examples/mnist_dataset_api.html",
    "title": "mnist_dataset_api",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_dataset_api.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_hierarchical_rnn.html",
    "href": "v1/guide/keras/examples/mnist_hierarchical_rnn.html",
    "title": "mnist_hierarchical_rnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_hierarchical_rnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_irnn.html",
    "href": "v1/guide/keras/examples/mnist_irnn.html",
    "title": "mnist_irnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_irnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_mlp.html",
    "href": "v1/guide/keras/examples/mnist_mlp.html",
    "title": "mnist_mlp",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_mlp.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_net2net.html",
    "href": "v1/guide/keras/examples/mnist_net2net.html",
    "title": "mnist_net2net",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_net2net.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_siamese_graph.html",
    "href": "v1/guide/keras/examples/mnist_siamese_graph.html",
    "title": "mnist_siamese_graph",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_siamese_graph.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_swwae.html",
    "href": "v1/guide/keras/examples/mnist_swwae.html",
    "title": "mnist_swwae",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_swwae.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_tfrecord.html",
    "href": "v1/guide/keras/examples/mnist_tfrecord.html",
    "title": "mnist_tfrecord",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_tfrecord.R"
  },
  {
    "objectID": "v1/guide/keras/examples/mnist_transfer_cnn.html",
    "href": "v1/guide/keras/examples/mnist_transfer_cnn.html",
    "title": "mnist_transfer_cnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_transfer_cnn.R"
  },
  {
    "objectID": "v1/guide/keras/examples/neural_style_transfer.html",
    "href": "v1/guide/keras/examples/neural_style_transfer.html",
    "title": "neural_style_transfer",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/neural_style_transfer.R"
  },
  {
    "objectID": "v1/guide/keras/examples/nmt_attention.html",
    "href": "v1/guide/keras/examples/nmt_attention.html",
    "title": "nmt_attention",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/nmt_attention.R"
  },
  {
    "objectID": "v1/guide/keras/examples/nueral_doodle.html",
    "href": "v1/guide/keras/examples/nueral_doodle.html",
    "title": "nueral_doodle",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/nueral_doodle.R"
  },
  {
    "objectID": "v1/guide/keras/examples/pretrained_word_embeddings.html",
    "href": "v1/guide/keras/examples/pretrained_word_embeddings.html",
    "title": "pretrained_word_embeddings",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R"
  },
  {
    "objectID": "v1/guide/keras/examples/quora_siamese_lstm.html",
    "href": "v1/guide/keras/examples/quora_siamese_lstm.html",
    "title": "quora_siamese_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/quora_siamese_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/reuters_mlp.html",
    "href": "v1/guide/keras/examples/reuters_mlp.html",
    "title": "reuters_mlp",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/reuters_mlp.R"
  },
  {
    "objectID": "v1/guide/keras/examples/reuters_mlp_relu_vs_selu.html",
    "href": "v1/guide/keras/examples/reuters_mlp_relu_vs_selu.html",
    "title": "reuters_mlp_relu_vs_selu",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/reuters_mlp_relu_vs_selu.R"
  },
  {
    "objectID": "v1/guide/keras/examples/stateful_lstm.html",
    "href": "v1/guide/keras/examples/stateful_lstm.html",
    "title": "stateful_lstm",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/stateful_lstm.R"
  },
  {
    "objectID": "v1/guide/keras/examples/text_explanation_lime.html",
    "href": "v1/guide/keras/examples/text_explanation_lime.html",
    "title": "text_explanation_lime",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/text_explanation_lime.R"
  },
  {
    "objectID": "v1/guide/keras/examples/tfprob_vae.html",
    "href": "v1/guide/keras/examples/tfprob_vae.html",
    "title": "tfprob_vae",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/tfprob_vae.R"
  },
  {
    "objectID": "v1/guide/keras/examples/unet.html",
    "href": "v1/guide/keras/examples/unet.html",
    "title": "unet",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/unet.R"
  },
  {
    "objectID": "v1/guide/keras/examples/unet_linux.html",
    "href": "v1/guide/keras/examples/unet_linux.html",
    "title": "unet_linux",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/unet_linux.R"
  },
  {
    "objectID": "v1/guide/keras/examples/variational_autoencoder.html",
    "href": "v1/guide/keras/examples/variational_autoencoder.html",
    "title": "variational_autoencoder",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder.R"
  },
  {
    "objectID": "v1/guide/keras/examples/variational_autoencoder_deconv.html",
    "href": "v1/guide/keras/examples/variational_autoencoder_deconv.html",
    "title": "variational_autoencoder_deconv",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder_deconv.R"
  },
  {
    "objectID": "v1/guide/keras/examples/vq_vae.html",
    "href": "v1/guide/keras/examples/vq_vae.html",
    "title": "vq_vae",
    "section": "",
    "text": "Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/vq_vae.R"
  },
  {
    "objectID": "v1/guide/keras/faq.html",
    "href": "v1/guide/keras/faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Please cite Keras in your publications if it helps your research. Here is an example BibTeX entry:\n@misc{chollet2017kerasR,\n  title={R Interface to Keras},\n  author={Chollet, Fran\\c{c}ois and Allaire, JJ and others},\n  year={2017},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/rstudio/keras}},\n}"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-run-keras-on-a-gpu",
    "href": "v1/guide/keras/faq.html#how-can-i-run-keras-on-a-gpu",
    "title": "Frequently Asked Questions",
    "section": "How can I run Keras on a GPU?",
    "text": "How can I run Keras on a GPU?\nNote that installation and configuration of the GPU-based backends can take considerably more time and effort. So if you are just getting started with Keras you may want to stick with the CPU version initially, then install the appropriate GPU version once your training becomes more computationally demanding.\nBelow are instructions for installing and enabling GPU support for the various supported backends.\n\nTensorFlow\nIf your system has an NVIDIA® GPU and you have the GPU version of TensorFlow installed then your Keras code will automatically run on the GPU.\nAdditional details on GPU installation can be found here: https://tensorflow.rstudio.com/installation_gpu.html.\n\n\nTheano\nIf you are running on the Theano backend, you can set the THEANO_FLAGS environment variable to indicate you’d like to execute tensor operations on the GPU. For example:\nSys.setenv(KERAS_BACKEND = \"keras\")\nSys.setenv(THEANO_FLAGS = \"device=gpu,floatX=float32\")\nlibrary(keras)\nThe name ‘gpu’ might have to be changed depending on your device’s identifier (e.g. gpu0, gpu1, etc).\n\n\nCNTK\nIf you have the GPU version of CNTK installed then your Keras code will automatically run on the GPU.\nAdditional information on installing the GPU version of CNTK can be found here: https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-run-a-keras-model-on-multiple-gpus",
    "href": "v1/guide/keras/faq.html#how-can-i-run-a-keras-model-on-multiple-gpus",
    "title": "Frequently Asked Questions",
    "section": "How can I run a Keras model on multiple GPUs?",
    "text": "How can I run a Keras model on multiple GPUs?\nWe recommend doing so using the TensorFlow backend. There are two ways to run a single model on multiple GPUs: data parallelism and device parallelism.\nIn most cases, what you need is most likely data parallelism.\n\nData parallelism\nData parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data. Keras has a built-in utility, multi_gpu_model(), which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.\nFor more information, see the documentation for multi_gpu_model. Here is a quick example:\n\n# Replicates `model` on 8 GPUs.\n# This assumes that your machine has 8 available GPUs.\nparallel_model <- multi_gpu_model(model, gpus=8)\nparallel_model %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"rmsprop\"\n)\n\n# This `fit` call will be distributed on 8 GPUs.\n# Since the batch size is 256, each GPU will process 32 samples.\nparallel_model %>% fit(x, y, epochs = 20, batch_size = 256)\n\n\n\nDevice parallelism\nDevice parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.\nThis can be achieved by using TensorFlow device scopes. Here is a quick example:\n\n# Model where a shared LSTM is used to encode two different sequences in parallel\ninput_a <- layer_input(shape = c(140, 256))\ninput_b <- layer_input(shape = c(140, 256))\n\nshared_lstm <- layer_lstm(units = 64)\n\n# Process the first sequence on one GPU\nlibrary(tensorflow)\nwith(tf$device_scope(\"/gpu:0\", {\n  encoded_a <- shared_lstm(tweet_a)\n}):\n    \n# Process the next sequence on another GPU\nwith(tf$device_scope(\"/gpu:1\", {\n  encoded_b <- shared_lstm(tweet_b)\n}):\n\n# Concatenate results on CPU\nwith(tf$device_scope(\"/cpu:0\", {\n  merged_vector <- layer_concatenate(list(encoded_a, encoded_b))\n}):"
  },
  {
    "objectID": "v1/guide/keras/faq.html#what-does-sample-batch-epoch-mean",
    "href": "v1/guide/keras/faq.html#what-does-sample-batch-epoch-mean",
    "title": "Frequently Asked Questions",
    "section": "What does “sample”, “batch”, “epoch” mean?",
    "text": "What does “sample”, “batch”, “epoch” mean?\nBelow are some common definitions that are necessary to know and understand to correctly utilize Keras:\n\nSample: one element of a dataset.\n\nExample: one image is a sample in a convolutional network\nExample: one audio file is a sample for a speech recognition model\n\nBatch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.\n\nA batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\n\nEpoch: an arbitrary cutoff, generally defined as “one pass over the entire dataset”, used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n\nWhen using evaluation_data or evaluation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.\nWithin Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving)."
  },
  {
    "objectID": "v1/guide/keras/faq.html#why-are-keras-objects-modified-in-place",
    "href": "v1/guide/keras/faq.html#why-are-keras-objects-modified-in-place",
    "title": "Frequently Asked Questions",
    "section": "Why are Keras objects modified in place?",
    "text": "Why are Keras objects modified in place?\nUnlike most R objects, Keras objects are “mutable”. That means that when you modify an object you’re modifying it “in place”, and you don’t need to assign the updated object back to the original name. For example, to add layers to a Keras model you might use this code:\n\nmodel %>% \n  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>% \n  layer_dense(units = 10, activation = 'softmax')\n\nRather than this code:\n\nmodel <- model %>% \n  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>% \n  layer_dense(units = 10, activation = 'softmax')\n\nYou need to be aware of this because it makes the Keras API a little different than most other pipelines you may have used, but it’s necessary to match the data structures and behavior of the underlying Keras library."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-save-a-keras-model",
    "href": "v1/guide/keras/faq.html#how-can-i-save-a-keras-model",
    "title": "Frequently Asked Questions",
    "section": "How can I save a Keras model?",
    "text": "How can I save a Keras model?\n\nSaving/loading whole models (architecture + weights + optimizer state)\nYou can use save_model_hdf5() to save a Keras model into a single HDF5 file which will contain:\n\nthe architecture of the model, allowing to re-create the model\nthe weights of the model\nthe training configuration (loss, optimizer)\nthe state of the optimizer, allowing to resume training exactly where you left off.\n\nYou can then use load_model_hdf5() to reinstantiate your model. load_model_hdf5() will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place).\nExample:\n\nsave_model_hdf5(model, 'my_model.h5')\nmodel <- load_model_hdf5('my_model.h5')\n\n\n\nSaving/loading only a model’s architecture\nIf you only need to save the architecture of a model, and not its weights or its training configuration, you can do:\n\njson_string <- model_to_json(model)\nyaml_string <- model_to_yaml(model)\n\nThe generated JSON / YAML files are human-readable and can be manually edited if needed.\nYou can then build a fresh model from this data:\n\nmodel <- model_from_json(json_string)\nmodel <- model_from_yaml(yaml_string)\n\n\n\nSaving/loading only a model’s weights\nIf you need to save the weights of a model, you can do so in HDF5 with the code below.\n\nsave_model_weights_hdf5('my_model_weights.h5')\n\nAssuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture:\n\nmodel %>% load_model_weights_hdf5('my_model_weights.h5')\n\nIf you need to load weights into a different architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by layer name:\n\nmodel %>% load_model_weights_hdf5('my_model_weights.h5', by_name = TRUE)\n\nFor example:\n\n# assuming the original model looks like this:\n#   model <- keras_model_sequential()\n#   model %>% \n#     layer_dense(units = 2, input_dim = 3, name = \"dense 1\") %>% \n#     layer_dense(units = 3, name = \"dense_3\") %>% \n#     ...\n#   save_model_weights(model, fname)\n\n# new model\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 2, input_dim = 3, name = \"dense 1\") %>%  # will be loaded\n  layer_dense(units = 3, name = \"dense_3\")                     # will not be loaded\n\n# load weights from first model; will only affect the first layer, dense_1.\nload_model_weights(fname, by_name = TRUE)"
  },
  {
    "objectID": "v1/guide/keras/faq.html#why-is-the-training-loss-much-higher-than-the-testing-loss",
    "href": "v1/guide/keras/faq.html#why-is-the-training-loss-much-higher-than-the-testing-loss",
    "title": "Frequently Asked Questions",
    "section": "Why is the training loss much higher than the testing loss?",
    "text": "Why is the training loss much higher than the testing loss?\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\nBesides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-obtain-the-output-of-an-intermediate-layer",
    "href": "v1/guide/keras/faq.html#how-can-i-obtain-the-output-of-an-intermediate-layer",
    "title": "Frequently Asked Questions",
    "section": "How can I obtain the output of an intermediate layer?",
    "text": "How can I obtain the output of an intermediate layer?\nOne simple way is to create a new Model that will output the layers that you are interested in:\n\nmodel <- ...  # create the original model\n\nlayer_name <- 'my_layer'\nintermediate_layer_model <- keras_model(inputs = model$input,\n                                        outputs = get_layer(model, layer_name)$output)\nintermediate_output <- predict(intermediate_layer_model, data)"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory",
    "href": "v1/guide/keras/faq.html#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory",
    "title": "Frequently Asked Questions",
    "section": "How can I use Keras with datasets that don’t fit in memory?",
    "text": "How can I use Keras with datasets that don’t fit in memory?\n\nGenerator Functions\nTo provide training or evaluation data incrementally you can write an R generator function that yields batches of training data then pass the function to the fit_generator() function (or related functions evaluate_generator() and predict_generator().\nThe output of generator functions must be a list of one of these forms:\n\n(inputs, targets)\n(inputs, targets, sample_weights)\n\nAll arrays should contain the same number of samples. The generator is expected to loop over its data indefinitely. For example, here’s simple generator function that yields randomly sampled batches of data:\n\nsampling_generator <- function(X_data, Y_data, batch_size) {\n  function() {\n    rows <- sample(1:nrow(X_data), batch_size, replace = TRUE)\n    list(X_data[rows,], Y_data[rows,])\n  }\n}\n\nmodel %>% \n  fit_generator(sampling_generator(X_train, Y_train, batch_size = 128), \n                steps_per_epoch = nrow(X_train) / 128, epochs = 10)\n\nThe steps_per_epoch parameter indicates the number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples if your dataset divided by the batch size.\n\n\nExternal Data Generators\nThe above example doesn’t however address the use case of datasets that don’t fit in memory. Typically to do that you’ll write a generator that reads from another source (e.g. a sparse matrix or file(s) on disk) and maintains an offset into that data as it’s called repeatedly. For example, imagine you have a set of text files in a directory you want to read from:\n\ndata_files_generator <- function(dir) {\n  \n  files < list.files(dir)\n  next_file <- 0\n  \n  function() {\n    \n    # move to the next file (note the <<- assignment operator)\n    next_file <<- next_file + 1\n    \n    # if we've exhausted all of the files then start again at the\n    # beginning of the list (keras generators need to yield\n    # data infinitely -- termination is controlled by the epochs\n    # and steps_per_epoch arguments to fit_generator())\n    if (next_file > length(files))\n      next_file <<- 1\n    \n    # determine the file name\n    file <- files[[next_file]]\n    \n    # process and return the data in the file. note that in a \n    # real example you'd further subdivide the data within the\n    # file into appropriately sized training batches. this \n    # would make this function much more complicated so we\n    # don't demonstrated it here\n    file_to_training_data(file)\n  }\n}\n\nThe above function is an example of a stateful generator—the function maintains information across calls to keep track of which data to provide next. This is accomplished by defining shared state outside the generator function body and using the <<- operator to assign to it from within the generator.\n\n\nImage Generators\nYou can also use the flow_images_from_directory() and flow_images_from_data() functions along with fit_generator() for training on sets of images stored on disk (with optional image augmentation/normalization via image_data_generator()).\nYou can see batch image training in action in our CIFAR10 example.\n\n\nBatch Functions\nYou can also do batch training using the train_on_batch() and test_on_batch() functions. These functions enable you to write a training loop that reads into memory only the data required for each batch."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore",
    "href": "v1/guide/keras/faq.html#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore",
    "title": "Frequently Asked Questions",
    "section": "How can I interrupt training when the validation loss isn’t decreasing anymore?",
    "text": "How can I interrupt training when the validation loss isn’t decreasing anymore?\nYou can use an early stopping callback:\n\nearly_stopping <- callback_early_stopping(monitor = 'val_loss', patience = 2)\nmodel %>% fit(X, y, validation_split = 0.2, callbacks = c(early_stopping))\n\nFind out more in the callbacks documentation."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-is-the-validation-split-computed",
    "href": "v1/guide/keras/faq.html#how-is-the-validation-split-computed",
    "title": "Frequently Asked Questions",
    "section": "How is the validation split computed?",
    "text": "How is the validation split computed?\nIf you set the validation_split argument in fit to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn’t shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed.\nThe same validation set is used for all epochs (within a same call to fit)."
  },
  {
    "objectID": "v1/guide/keras/faq.html#is-the-data-shuffled-during-training",
    "href": "v1/guide/keras/faq.html#is-the-data-shuffled-during-training",
    "title": "Frequently Asked Questions",
    "section": "Is the data shuffled during training?",
    "text": "Is the data shuffled during training?\nYes, if the shuffle argument in fit is set to TRUE (which is the default), the training data will be randomly shuffled at each epoch.\nValidation data is never shuffled."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch",
    "href": "v1/guide/keras/faq.html#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch",
    "title": "Frequently Asked Questions",
    "section": "How can I record the training / validation loss / accuracy at each epoch?",
    "text": "How can I record the training / validation loss / accuracy at each epoch?\nThe model.fit method returns an History callback, which has a history attribute containing the lists of successive losses and other metrics.\n\nhist <- model %>% fit(X, y, validation_split=0.2)\nhist$history"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-freeze-keras-layers",
    "href": "v1/guide/keras/faq.html#how-can-i-freeze-keras-layers",
    "title": "Frequently Asked Questions",
    "section": "How can I “freeze” Keras layers?",
    "text": "How can I “freeze” Keras layers?\nTo “freeze” a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.\nYou can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable:\n\nfrozen_layer <- layer_dense(units = 32, trainable = FALSE)\n\nAdditionally, you can set the trainable property of a layer to TRUE or FALSE after instantiation. For this to take effect, you will need to call compile() on your model after modifying the trainable property. Here’s an example:\n\nx <- layer_input(shape = c(32))\nlayer <- layer_dense(units = 32)\nlayer$trainable <- FALSE\ny <- x %>% layer\n\nfrozen_model <- keras_model(x, y)\n# in the model below, the weights of `layer` will not be updated during training\nfrozen_model %>% compile(optimizer = 'rmsprop', loss = 'mse')\n\nlayer$trainable <- TRUE\ntrainable_model <- keras_model(x, y)\n# with this model the weights of the layer will be updated during training\n# (which will also affect the above model since it uses the same layer instance)\ntrainable_model %>% compile(optimizer = 'rmsprop', loss = 'mse')\n\nfrozen_model %>% fit(data, labels)  # this does NOT update the weights of `layer`\ntrainable_model %>% fit(data, labels)  # this updates the weights of `layer`\n\nFinally, you can freeze or unfreeze the weights for an entire model (or a range of layers within the model) using the freeze_weights() and unfreeze_weights() functions. For example:\n\n# instantiate a VGG16 model\nconv_base <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(150, 150, 3)\n)\n\n# freeze it's weights\nfreeze_weights(conv_base)\n\n# create a composite model that includes the base + more layers\nmodel <- keras_model_sequential() %>% \n  conv_base %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n# compile\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\n# unfreeze weights from \"block5_conv1\" on\nunfreeze_weights(conv_base, from = \"block5_conv1\")\n\n# compile again since we froze or unfroze layers\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-stateful-rnns",
    "href": "v1/guide/keras/faq.html#how-can-i-use-stateful-rnns",
    "title": "Frequently Asked Questions",
    "section": "How can I use stateful RNNs?",
    "text": "How can I use stateful RNNs?\nMaking a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\nWhen using stateful RNNs, it is therefore assumed that:\n\nall batches have the same number of samples\nIf X1 and X2 are successive batches of samples, then X2[[i]] is the follow-up sequence to X1[[i], for every i.\n\nTo use statefulness in RNNs, you need to:\n\nexplicitly specify the batch size you are using, by passing a batch_size argument to the first layer in your model. E.g. batch_size=32 for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.\nset stateful=TRUE in your RNN layer(s).\nspecify shuffle=FALSE when calling fit().\n\nTo reset the states accumulated in either a single layer or an entire model use the reset_states() function.\nNotes that the methods predict(), fit(), train_on_batch(), predict_classes(), etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-remove-a-layer-from-a-sequential-model",
    "href": "v1/guide/keras/faq.html#how-can-i-remove-a-layer-from-a-sequential-model",
    "title": "Frequently Asked Questions",
    "section": "How can I remove a layer from a Sequential model?",
    "text": "How can I remove a layer from a Sequential model?\nYou can remove the last added layer in a Sequential model by calling pop_layer():\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>% \n  layer_dense(units = 32, activation = 'relu') %>% \n  layer_dense(units = 32, activation = 'relu')\n\nlength(model$layers)     # \"3\"\nmodel %>% pop_layer()\nlength(model$layers)     # \"2\""
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-pre-trained-models-in-keras",
    "href": "v1/guide/keras/faq.html#how-can-i-use-pre-trained-models-in-keras",
    "title": "Frequently Asked Questions",
    "section": "How can I use pre-trained models in Keras?",
    "text": "How can I use pre-trained models in Keras?\nCode and pre-trained weights are available for the following image classification models:\n\nXception\nVGG16\nVGG19\nResNet50\nInceptionV3\nInceptionResNetV2\nMobileNet\nMobileNetV2\nDenseNet\nNASNet\n\nFor example:\n\nmodel <- application_vgg16(weights = 'imagenet', include_top = TRUE)\n\nFor a few simple usage examples, see the documentation for the Applications module.\nThe VGG16 model is also the basis for the Deep dream Keras example script."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-other-keras-backends",
    "href": "v1/guide/keras/faq.html#how-can-i-use-other-keras-backends",
    "title": "Frequently Asked Questions",
    "section": "How can I use other Keras backends?",
    "text": "How can I use other Keras backends?\nBy default the Keras Python and R packages use the TensorFlow backend. Other available backends include Theano or CNTK. To learn more about using alternatate backends (e.g. Theano or CNTK) see the article on Keras backends."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-the-plaidml-backend",
    "href": "v1/guide/keras/faq.html#how-can-i-use-the-plaidml-backend",
    "title": "Frequently Asked Questions",
    "section": "How can I use the PlaidML backend?",
    "text": "How can I use the PlaidML backend?\nPlaidML is an open source portable deep learning engine that runs on most existing PC hardware with OpenCL-capable GPUs from NVIDIA, AMD, or Intel. PlaidML includes a Keras backend which you can use as described below.\nFirst, build and install PlaidML as described on the project website. You must be sure that PlaidML is correctly installed, setup, and working before proceeding further!\nThen, to use Keras with the PlaidML backend you do the following:\n\nlibrary(keras)\nuse_backend(\"plaidml\")\n\nThis should automatically discover and use the Python environment where plaidml and plaidml-keras were installed. If this doesn’t work as expected you can also force the selection of a particular Python environment. For example, if you installed PlaidML in conda environment named “plaidml” you would do this:\n\nlibrary(keras)\nuse_condaenv(\"plaidml\") \nuse_backend(\"plaidml\")"
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-use-keras-in-another-r-package",
    "href": "v1/guide/keras/faq.html#how-can-i-use-keras-in-another-r-package",
    "title": "Frequently Asked Questions",
    "section": "How can I use Keras in another R package?",
    "text": "How can I use Keras in another R package?\n\nTesting on CRAN\nThe main consideration in using Keras within another R package is to ensure that your package can be tested in an environment where Keras is not available (e.g. the CRAN test servers). To do this, arrange for your tests to be skipped when Keras isn’t available using the is_keras_available() function.\nFor example, here’s a testthat utility function that can be used to skip a test when Keras isn’t available:\n\n# testthat utilty for skipping tests when Keras isn't available\nskip_if_no_keras <- function(version = NULL) {\n  if (!is_keras_available(version))\n    skip(\"Required keras version not available for testing\")\n}\n\n# use the function within a test\ntest_that(\"keras function works correctly\", {\n  skip_if_no_keras()\n  # test code here\n})\n\nYou can pass the version argument to check for a specific version of Keras.\n\n\nKeras Module\nAnother consideration is gaining access to the underlying Keras python module. You might need to do this if you require lower level access to Keras than is provided for by the Keras R package.\nSince the Keras R package can bind to multiple different implementations of Keras (either the original Keras or the TensorFlow implementation of Keras), you should use the keras::implementation() function to obtain access to the correct python module. You can use this function within the .onLoad function of a package to provide global access to the module within your package. For example:\n\n# Keras python module\nkeras <- NULL\n\n# Obtain a reference to the module from the keras R package\n.onLoad <- function(libname, pkgname) {\n  keras <<- keras::implementation() \n}\n\n\n\nCustom Layers\nIf you create custom layers in R or import other Python packages which include custom Keras layers, be sure to wrap them using the create_layer() function so that they are composable using the magrittr pipe operator. See the documentation on layer wrapper functions for additional details."
  },
  {
    "objectID": "v1/guide/keras/faq.html#how-can-i-obtain-reproducible-results-using-keras-during-development",
    "href": "v1/guide/keras/faq.html#how-can-i-obtain-reproducible-results-using-keras-during-development",
    "title": "Frequently Asked Questions",
    "section": "How can I obtain reproducible results using Keras during development?",
    "text": "How can I obtain reproducible results using Keras during development?\nDuring development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample.\nThe use_session_with_seed() function establishes a common random seed for R, Python, NumPy, and TensorFlow. It furthermore disables hash randomization, GPU computations, and CPU parallelization, which can be additional sources of non-reproducibility.\nTo use the function, call it immediately after you load the keras package:\n\nlibrary(keras)\nuse_session_with_seed(42)\n\n# ...rest of code follows...\n\nThis function takes all measures known to promote reproducible results from Keras sessions, however it’s possible that various individual features or libraries used by the backend escape its effects. If you encounter non-reproducible results please investigate the possible sources of the problem. The source code for use_session_with_seed() is here: https://github.com/rstudio/tensorflow/blob/master/R/seed.R. Contributions via pull request are very welcome!\nPlease note again that use_session_with_seed() disables GPU computations and CPU parallelization by default (as both can lead to non-deterministic computations) so should generally not be used when model training time is paramount. You can re-enable GPU computations and/or CPU parallelism using the disable_gpu and disable_parallel_cpu arguments. For example:\n\nlibrary(keras)\nuse_session_with_seed(42, disable_gpu = FALSE, disable_parallel_cpu = FALSE)"
  },
  {
    "objectID": "v1/guide/keras/faq.html#where-is-the-keras-configuration-filed-stored",
    "href": "v1/guide/keras/faq.html#where-is-the-keras-configuration-filed-stored",
    "title": "Frequently Asked Questions",
    "section": "Where is the Keras configuration filed stored?",
    "text": "Where is the Keras configuration filed stored?\nThe default directory where all Keras data is stored is:\n~/.keras/\nIn case Keras cannot create the above directory (e.g. due to permission issues), /tmp/.keras/ is used as a backup.\nThe Keras configuration file is a JSON file stored at $HOME/.keras/keras.json. The default configuration file looks like this:\n{\n    \"image_data_format\": \"channels_last\",\n    \"epsilon\": 1e-07,\n    \"floatx\": \"float32\",\n    \"backend\": \"tensorflow\"\n}\nIt contains the following fields:\n\nThe image data format to be used as default by image processing layers and utilities (either channels_last or channels_first).\nThe epsilon numerical fuzz factor to be used to prevent division by zero in some operations.\nThe default float data type.\nThe default backend (this will always be “tensorflow” in the R interface to Keras)\n\nLikewise, cached dataset files, such as those downloaded with get_file(), are stored by default in $HOME/.keras/datasets/."
  },
  {
    "objectID": "v1/guide/keras/functional_api.html",
    "href": "v1/guide/keras/functional_api.html",
    "title": "Guide to the Functional API",
    "section": "",
    "text": "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\nThis guide assumes that you are already familiar with the Sequential model.\nLet’s start with something simple."
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#first-example-a-densely-connected-network",
    "href": "v1/guide/keras/functional_api.html#first-example-a-densely-connected-network",
    "title": "Guide to the Functional API",
    "section": "First example: a densely-connected network",
    "text": "First example: a densely-connected network\nThe Sequential model is probably a better choice to implement such a network, but it helps to start with something really simple.\nTo use the functional API, build your input and output layers and then pass them to the model() function. This model can be trained just like Keras sequential models.\n\nlibrary(keras)\n\n# input layer\ninputs <- layer_input(shape = c(784))\n \n# outputs compose input + dense layers\npredictions <- inputs %>%\n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 10, activation = 'softmax')\n\n# create and compile model\nmodel <- keras_model(inputs = inputs, outputs = predictions)\nmodel %>% compile(\n  optimizer = 'rmsprop',\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\nNote that Keras objects are modified in place which is why it’s not necessary for model to be assigned back to after it is compiled."
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "href": "v1/guide/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "title": "Guide to the Functional API",
    "section": "All models are callable, just like layers",
    "text": "All models are callable, just like layers\nWith the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer. Note that you aren’t just reusing the architecture of the model, you are also reusing its weights.\n\nx <- layer_input(shape = c(784))\n# This works, and returns the 10-way softmax we defined above.\ny <- x %>% model\n\nThis can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line:\n\n# Input tensor for sequences of 20 timesteps,\n# each containing a 784-dimensional vector\ninput_sequences <- layer_input(shape = c(20, 784))\n\n# This applies our previous model to the input sequence\nprocessed_sequences <- input_sequences %>%\n  time_distributed(model)"
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#multi-input-and-multi-output-models",
    "href": "v1/guide/keras/functional_api.html#multi-input-and-multi-output-models",
    "title": "Guide to the Functional API",
    "section": "Multi-input and multi-output models",
    "text": "Multi-input and multi-output models\nHere’s a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\nLet’s consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\nHere’s what our model looks like:\n\nLet’s implement it with the functional API.\nThe main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.\nWe’ll include an\n\nlibrary(keras)\n\nmain_input <- layer_input(shape = c(100), dtype = 'int32', name = 'main_input')\n\nlstm_out <- main_input %>% \n  layer_embedding(input_dim = 10000, output_dim = 512, input_length = 100) %>% \n  layer_lstm(units = 32)\n\nHere we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model:\n\nauxiliary_output <- lstm_out %>% \n  layer_dense(units = 1, activation = 'sigmoid', name = 'aux_output')\n\nAt this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output, stacking a deep densely-connected network on top and adding the main logistic regression layer\n\nauxiliary_input <- layer_input(shape = c(5), name = 'aux_input')\n\nmain_output <- layer_concatenate(c(lstm_out, auxiliary_input)) %>%  \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 1, activation = 'sigmoid', name = 'main_output')\n\nThis defines a model with two inputs and two outputs:\n\nmodel <- keras_model(\n  inputs = c(main_input, auxiliary_input), \n  outputs = c(main_output, auxiliary_output)\n)\n\n\nsummary(model)\n\nModel\n__________________________________________________________________________________________\nLayer (type)                 Output Shape        Param #    Connected to                  \n==========================================================================================\nmain_input (InputLayer)      (None, 100)         0                                        \n__________________________________________________________________________________________\nembedding_1 (Embedding)      (None, 100, 512)    5120000                                  \n__________________________________________________________________________________________\nlstm_1 (LSTM)                (None, 32)          69760                                    \n__________________________________________________________________________________________\naux_input (InputLayer)       (None, 5)           0                                        \n__________________________________________________________________________________________\nconcatenate_1 (Concatenate)  (None, 37)          0                                        \n__________________________________________________________________________________________\ndense_1 (Dense)              (None, 64)          2432                                     \n__________________________________________________________________________________________\ndense_2 (Dense)              (None, 64)          4160                                     \n__________________________________________________________________________________________\ndense_3 (Dense)              (None, 64)          4160                                     \n__________________________________________________________________________________________\nmain_output (Dense)          (None, 1)           65                                       \n__________________________________________________________________________________________\naux_output (Dense)           (None, 1)           33                                       \n==========================================================================================\nTotal params: 5,200,610\nTrainable params: 5,200,610\nNon-trainable params: 0\n__________________________________________________________________________________________\nWe compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.\n\nmodel %>% compile(\n  optimizer = 'rmsprop',\n  loss = 'binary_crossentropy',\n  loss_weights = c(1.0, 0.2)\n)\n\nWe can train the model by passing it lists of input arrays and target arrays:\n\nmodel %>% fit(\n  x = list(headline_data, additional_data),\n  y = list(labels, labels),\n  epochs = 50,\n  batch_size = 32\n)\n\nSince our inputs and outputs are named (we passed them a “name” argument), We could also have compiled the model via:\n\nmodel %>% compile(\n  optimizer = 'rmsprop',\n  loss = list(main_output = 'binary_crossentropy', aux_output = 'binary_crossentropy'),\n  loss_weights = list(main_output = 1.0, aux_output = 0.2)\n)\n\n# And trained it via:\nmodel %>% fit(\n  x = list(main_input = headline_data, aux_input = additional_data),\n  y = list(main_output = labels, aux_output = labels),\n  epochs = 50,\n  batch_size = 32\n)"
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#shared-layers",
    "href": "v1/guide/keras/functional_api.html#shared-layers",
    "title": "Guide to the Functional API",
    "section": "Shared layers",
    "text": "Shared layers\nAnother good use for the functional API are models that use shared layers. Let’s take a look at shared layers.\nLet’s consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\nOne way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\nBecause the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\nLet’s build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256), i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).\n\nlibrary(keras)\n\ntweet_a <- layer_input(shape = c(280, 256))\ntweet_b <- layer_input(shape = c(280, 256))\n\nTo share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:\n\n# This layer can take as input a matrix and will return a vector of size 64\nshared_lstm <- layer_lstm(units = 64)\n\n# When we reuse the same layer instance multiple times, the weights of the layer are also\n# being reused (it is effectively *the same* layer)\nencoded_a <- tweet_a %>% shared_lstm\nencoded_b <- tweet_b %>% shared_lstm\n\n# We can then concatenate the two vectors and add a logistic regression on top\npredictions <- layer_concatenate(c(encoded_a, encoded_b), axis=-1) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\n# We define a trainable model linking the tweet inputs to the predictions\nmodel <- keras_model(inputs = c(tweet_a, tweet_b), outputs = predictions)\n\nmodel %>% compile(\n  optimizer = 'rmsprop',\n  loss = 'binary_crossentropy',\n  metrics = c('accuracy')\n)\n\nmodel %>% fit(list(data_a, data_b), labels, epochs = 10)"
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#the-concept-of-layer-node",
    "href": "v1/guide/keras/functional_api.html#the-concept-of-layer-node",
    "title": "Guide to the Functional API",
    "section": "The concept of layer “node”",
    "text": "The concept of layer “node”\nWhenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a “node” to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 1, 2, 2…\nYou can obtain the output tensor of a layer via layer$output, or its output shape via layer$output_shape. But what if a layer is connected to multiple inputs?\nAs long as a layer is only connected to one input, there is no confusion, and $output will return the one output of the layer:\n\na <- layer_input(shape = c(280, 256))\n\nlstm <- layer_lstm(units = 32)\n\nencoded_a <- a %>% lstm\n\nlstm$output\n\nNot so if the layer has multiple inputs:\n\na <- layer_input(shape = c(280, 256))\nb <- layer_input(shape = c(280, 256))\n\nlstm <- layer_lstm(units = 32)\n\nencoded_a <- a %>% lstm\nencoded_b <- b %>% lstm\n\nlstm$output\n\nAttributeError: Layer lstm_4 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.\nOkay then. The following works:\n\nget_output_at(lstm, 1)\nget_output_at(lstm, 2)\n\nSimple enough, right?\nThe same is true for the properties input_shape and output_shape: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of “layer output/input shape” is well defined, and that one shape will be returned by layer$output_shape/layer$input_shape. But if, for instance, you apply the same layer_conv_2d() layer to an input of shape (32, 32, 3), and then to an input of shape (64, 64, 3), the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:\n\na <- layer_input(shape = c(32, 32, 3))\nb <- layer_input(shape = c(64, 64, 3))\n\nconv <- layer_conv_2d(filters = 16, kernel_size = c(3,3), padding = 'same')\n\nconved_a <- a %>% conv\n\n# only one input so far, the following will work\nconv$input_shape\n\nconved_b <- b %>% conv\n# now the `$input_shape` property wouldn't work, but this does:\nget_input_shape_at(conv, 1)\nget_input_shape_at(conv, 2)"
  },
  {
    "objectID": "v1/guide/keras/functional_api.html#more-examples",
    "href": "v1/guide/keras/functional_api.html#more-examples",
    "title": "Guide to the Functional API",
    "section": "More examples",
    "text": "More examples\nCode examples are still the best way to get started, so here are a few more.\n\nInception module\nFor more information about the Inception architecture, see Going Deeper with Convolutions.\n\nlibrary(keras)\n\ninput_img <- layer_input(shape = c(256, 256, 3))\n\ntower_1 <- input_img %>% \n  layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu') %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding='same', activation='relu')\n\ntower_2 <- input_img %>% \n  layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu') %>% \n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), padding='same', activation='relu')\n\ntower_3 <- input_img %>% \n  layer_max_pooling_2d(pool_size = c(3, 3), strides = c(1, 1), padding = 'same') %>% \n  layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu')\n\noutput <- layer_concatenate(c(tower_1, tower_2, tower_3), axis = 1)\n\n\n\nResidual connection on a convolution layer\nFor more information about residual networks, see Deep Residual Learning for Image Recognition.\n\n# input tensor for a 3-channel 256x256 image\nx <- layer_input(shape = c(256, 256, 3))\n# 3x3 conv with 3 output channels (same as input channels)\ny <- x %>% layer_conv_2d(filters = 3, kernel_size =c(3, 3), padding = 'same')\n# this returns x + y.\nz <- layer_add(c(x, y))\n\n\n\nShared vision model\nThis model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.\n\n# First, define the vision model\ndigit_input <- layer_input(shape = c(27, 27, 1))\nout <- digit_input %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_flatten()\n\nvision_model <- keras_model(digit_input, out)\n\n# Then define the tell-digits-apart model\ndigit_a <- layer_input(shape = c(27, 27, 1))\ndigit_b <- layer_input(shape = c(27, 27, 1))\n\n# The vision model will be shared, weights and all\nout_a <- digit_a %>% vision_model\nout_b <- digit_b %>% vision_model\n\nout <- layer_concatenate(c(out_a, out_b)) %>% \n  layer_dense(units = 1, activation = 'sigmoid')\n\nclassification_model <- keras_model(inputs = c(digit_a, digit_b), out)\n\n\n\nVisual question answering model\nThis model can select the correct one-word answer when asked a natural-language question about a picture.\nIt works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.\n\n# First, let's define a vision model using a Sequential model.\n# This model will encode an image into a vector.\nvision_model <- keras_model_sequential() \nvision_model %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu', padding = 'same',\n                input_shape = c(224, 224, 3)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = 'relu', padding = 'same') %>% \n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = 'relu', padding = 'same') %>% \n  layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = 'relu') %>% \n  layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_flatten()\n\n# Now let's get a tensor with the output of our vision model:\nimage_input <- layer_input(shape = c(224, 224, 3))\nencoded_image <- image_input %>% vision_model\n\n# Next, let's define a language model to encode the question into a vector.\n# Each question will be at most 100 word long,\n# and we will index words as integers from 1 to 9999.\nquestion_input <- layer_input(shape = c(100), dtype = 'int32')\nencoded_question <- question_input %>% \n  layer_embedding(input_dim = 10000, output_dim = 256, input_length = 100) %>% \n  layer_lstm(units = 256)\n\n# Let's concatenate the question vector and the image vector then\n# train a logistic regression over 1000 words on top\noutput <- layer_concatenate(c(encoded_question, encoded_image)) %>% \n  layer_dense(units = 1000, activation='softmax')\n\n# This is our final model:\nvqa_model <- keras_model(inputs = c(image_input, question_input), outputs = output)\n\n\n\nVideo question answering model\nNow that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. “what sport is the boy playing?” -> “football”).\n\nvideo_input <- layer_input(shape = c(100, 224, 224, 3))\n\n# This is our video encoded via the previously trained vision_model (weights are reused)\nencoded_video <- video_input %>% \n  time_distributed(vision_model) %>% \n  layer_lstm(units = 256)\n\n# This is a model-level representation of the question encoder, reusing the same weights as before:\nquestion_encoder <- keras_model(inputs = question_input, outputs = encoded_question)\n\n# Let's use it to encode the question:\nvideo_question_input <- layer_input(shape = c(100), dtype = 'int32')\nencoded_video_question <- video_question_input %>% question_encoder\n\n# And this is our video question answering model:\noutput <- layer_concatenate(c(encoded_video, encoded_video_question)) %>% \n  layer_dense(units = 1000, activation = 'softmax')\n\nvideo_qa_model <- keras_model(inputs= c(video_input, video_question_input), outputs = output)"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html",
    "href": "v1/guide/keras/guide_keras.html",
    "title": "Guide to Keras Basics",
    "section": "",
    "text": "Keras is a high-level API to build and train deep learning models. It’s used for fast prototyping, advanced research, and production, with three key advantages:"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#import-keras",
    "href": "v1/guide/keras/guide_keras.html#import-keras",
    "title": "Guide to Keras Basics",
    "section": "Import keras",
    "text": "Import keras\nTo get started, load the keras library:\n\nlibrary(keras)"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#build-a-simple-model",
    "href": "v1/guide/keras/guide_keras.html#build-a-simple-model",
    "title": "Guide to Keras Basics",
    "section": "Build a simple model",
    "text": "Build a simple model\n\nSequential model\nIn Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers: the sequential model.\nTo build a simple, fully-connected network (i.e., a multi-layer perceptron):\n\nmodel <- keras_model_sequential()\n\nmodel %>% \n  \n  # Adds a densely-connected layer with 64 units to the model:\n  layer_dense(units = 64, activation = 'relu') %>%\n  \n  # Add another:\n  layer_dense(units = 64, activation = 'relu') %>%\n  \n  # Add a softmax layer with 10 output units:\n  layer_dense(units = 10, activation = 'softmax')\n\n\n\nConfigure the layers\nThere are many layers available with some common constructor parameters:\n\nactivation: Set the activation function for the layer. By default, no activation is applied.\nkernel_initializer and bias_initializer: The initialization schemes that create the layer’s weights (kernel and bias). This defaults to the Glorot uniform initializer.\nkernel_regularizer and bias_regularizer: The regularization schemes that apply to the layer’s weights (kernel and bias), such as L1 or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates dense layers using constructor arguments:\n\n# Create a sigmoid layer:\nlayer_dense(units = 64, activation ='sigmoid')\n\n# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))\n\n# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))\n\n# A linear layer with a kernel initialized to a random orthogonal matrix:\nlayer_dense(units = 64, kernel_initializer = 'orthogonal')\n\n# A linear layer with a bias vector initialized to 2.0:\nlayer_dense(units = 64, bias_initializer = initializer_constant(2.0))"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#train-and-evaluate",
    "href": "v1/guide/keras/guide_keras.html#train-and-evaluate",
    "title": "Guide to Keras Basics",
    "section": "Train and evaluate",
    "text": "Train and evaluate\n\nSet up training\nAfter the model is constructed, configure its learning process by calling the compile method:\n\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\ncompile takes three important arguments:\n\noptimizer: This object specifies the training procedure. Commonly used optimizers are e.g.\nadam, rmsprop, or sgd.\nloss: The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, and binary_crossentropy.\nmetrics: Used to monitor training. In classification, this usually is accuracy.\n\nThe following shows a few examples of configuring a model for training:\n\n# Configure a model for mean-squared error regression.\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'mse',           # mean squared error\n  metrics = list('mae')   # mean absolute error\n)\n\n# Configure a model for categorical classification.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.01),\n  loss = \"categorical_crossentropy\",\n  metrics = list(\"categorical_accuracy\")\n)\n\n\n\nInput data\nYou can train keras models directly on R matrices and arrays (possibly created from R data.frames). A model is fit to the training data using the fit method:\n\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32\n)\n\nfit takes three important arguments:\n\nepochs: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\nbatch_size: When passed matrix or array data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\nvalidation_data: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument — a list of inputs and labels — allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.\n\nHere’s an example using validation_data:\n\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nval_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)\nval_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(val_data, val_labels)\n)\n\n\n\nEvaluate and predict\nSame as fit, the evaluate and predict methods can use raw R data as well as a dataset.\nTo evaluate the inference-mode loss and metrics for the data provided:\n\nmodel %>% evaluate(test_data, test_labels, batch_size = 32)\n\nmodel %>% evaluate(test_dataset, steps = 30)\n\nAnd to predict the output of the last layer in inference for the data provided, again as R data as well as a dataset:\n\nmodel %>% predict(test_data, batch_size = 32)\n    \nmodel %>% predict(test_dataset, steps = 30)"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#build-advanced-models",
    "href": "v1/guide/keras/guide_keras.html#build-advanced-models",
    "title": "Guide to Keras Basics",
    "section": "Build advanced models",
    "text": "Build advanced models\n\nFunctional API\nThe sequential model is a simple stack of layers that cannot represent arbitrary models. Use the Keras functional API to build complex model topologies such as:\n\nmulti-input models,\nmulti-output models,\nmodels with shared layers (the same layer called several times),\nmodels with non-sequential data flows (e.g., residual connections).\n\nBuilding a model with the functional API works like this:\n\nA layer instance is callable and returns a tensor.\nInput tensors and output tensors are used to define a keras_model instance.\nThis model is trained just like the sequential model.\n\nThe following example uses the functional API to build a simple, fully-connected network:\n\ninputs <- layer_input(shape = (32))  # Returns a placeholder tensor\n\npredictions <- inputs %>% \n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 10, activation = 'softmax')\n\n# Instantiate the model given inputs and outputs.\nmodel <- keras_model(inputs = inputs, outputs = predictions)\n\n# The compile step specifies the training configuration.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n\n\n\nCustom layers\nTo create a custom Keras layer, you create an R6 class derived from KerasLayer. There are three methods to implement (only one of which, call(), is required for all types of layer):\n\nbuild(input_shape): This is where you will define your weights. Note that if your layer doesn’t define trainable weights then you need not implement this method.\ncall(x): This is where the layer’s logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call: the input tensor.\ncompute_output_shape(input_shape): In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don’t modify the shape of the input then you need not implement this method.\n\nHere is an example custom layer that performs a matrix multiplication:\n\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    output_dim = NULL,\n    \n    kernel = NULL,\n    \n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n    \n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel', \n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n    \n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n)\n\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\n# define layer wrapper function\nlayer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {\n  create_layer(CustomLayer, object, list(\n    output_dim = as.integer(output_dim),\n    name = name,\n    trainable = trainable\n  ))\n}\n\nYou can now use the layer in a model as usual:\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 32, input_shape = c(32,32)) %>% \n  layer_custom(output_dim = 32)\n\n\n\nCustom models\nIn addition to creating custom layers, you can also create a custom model. This might be necessary if you wanted to use TensorFlow eager execution in combination with an imperatively written forward pass.\nIn cases where this is not needed, but flexibility in building the architecture is required, it is recommended to just stick with the functional API.\nA custom model is defined by calling keras_model_custom() passing a function that specifies the layers to be created and the operations to be executed on forward pass.\n\nmy_model <- function(input_dim, output_dim, name = NULL) {\n  \n  # define and return a custom model\n  keras_model_custom(name = name, function(self) {\n    \n    # create layers we'll need for the call (this code executes once)\n    # note: the layers have to be created on the self object!\n    self$dense1 <- layer_dense(units = 64, activation = 'relu', input_shape = input_dim)\n    self$dense2 <- layer_dense(units = 64, activation = 'relu')\n    self$dense3 <- layer_dense(units = 10, activation = 'softmax')\n    \n    # implement call (this code executes during training & inference)\n    function(inputs, mask = NULL) {\n      x <- inputs %>%\n        self$dense1() %>%\n        self$dense2() %>% \n        self$dense3()\n      x\n    }\n  })\n}\n\nmodel <- my_model(input_dim = 32, output_dim = 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#callbacks",
    "href": "v1/guide/keras/guide_keras.html#callbacks",
    "title": "Guide to Keras Basics",
    "section": "Callbacks",
    "text": "Callbacks\nA callback is an object passed to a model to customize and extend its behavior during training. You can write your own custom callback, or use the built-in callbacks that include:\n\ncallback_model_checkpoint: Save checkpoints of your model at regular intervals.\ncallback_learning_rate_scheduler: Dynamically change the learning rate.\ncallback_early_stopping: Interrupt training when validation performance has stopped improving.\ncallbacks_tensorboard: Monitor the model’s behavior using TensorBoard.\n\nTo use a callback, pass it to the model’s fit method:\n\ncallbacks <- list(\n  callback_early_stopping(patience = 2, monitor = 'val_loss'),\n  callback_tensorboard(log_dir = './logs')\n)\n\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5,\n  callbacks = callbacks,\n  validation_data = list(val_data, val_labels)\n)"
  },
  {
    "objectID": "v1/guide/keras/guide_keras.html#save-and-restore",
    "href": "v1/guide/keras/guide_keras.html#save-and-restore",
    "title": "Guide to Keras Basics",
    "section": "Save and restore",
    "text": "Save and restore\n\nWeights only\nSave and load the weights of a model using save_model_weights_hdf5 and load_model_weights_hdf5, respectively:\n\n# save in SavedModel format\nmodel %>% save_model_weights_tf('my_model/')\n\n# Restore the model's state,\n# this requires a model with the same architecture.\nmodel %>% load_model_weights_tf('my_model/')\n\n\n\nConfiguration only\nA model’s configuration can be saved - this serializes the model architecture without any weights. A saved configuration can recreate and initialize the same model, even without the code that defined the original model. Keras supports JSON and YAML serialization formats:\n\n# Serialize a model to JSON format\njson_string <- model %>% model_to_json()\n\n# Recreate the model (freshly initialized)\nfresh_model <- model_from_json(json_string)\n\n# Serializes a model to YAML format\nyaml_string <- model %>% model_to_yaml()\n\n# Recreate the model\nfresh_model <- model_from_yaml(yaml_string)\n\nCaution: Custom models are not serializable because their architecture is defined by the R code in the function passed to keras_model_custom.\n\n\nEntire model\nThe entire model can be saved to a file that contains the weight values, the model’s configuration, and even the optimizer’s configuration. This allows you to checkpoint a model and resume training later —from the exact same state —without access to the original code.\n\n# Save entire model to the SavedModel format\nmodel %>% save_model_tf('my_model/')\n\n# Recreate the exact same model, including weights and optimizer.\nmodel <- load_model_tf('my_model/')"
  },
  {
    "objectID": "v1/guide/keras/saving_serializing.html",
    "href": "v1/guide/keras/saving_serializing.html",
    "title": "Saving and serializing models",
    "section": "",
    "text": "This tutorial is an R translation of this page available in the official TensorFlow documentation.\nThe first part of this guide covers saving and serialization for Sequential models and models built using the Functional API. The saving and serialization APIs are the exact same for both of these types of models.\nSaving for custom subclasses of Model is covered in the section “Saving Subclassed Models”. The APIs in this case are slightly different than for Sequential or Functional models."
  },
  {
    "objectID": "v1/guide/keras/saving_serializing.html#overview",
    "href": "v1/guide/keras/saving_serializing.html#overview",
    "title": "Saving and serializing models",
    "section": "Overview",
    "text": "Overview\nFor Sequential Models and models built using the Functional API use:\n\nsave_model_hdf5()/load_model_hdf5() to save the entire model to disk, including the optimizer state. You can also use save_model_tf/load_model_tf to save the entire model to the SavedModel format.\nget_config()/from_config() to load only the model architecture into an R object.\nmodel_to_json()/model_from_json() to save only the architecture of the model to a single string - useful for saving the architecture to disk. You can also use model_to_yaml()/model_from_yaml() to save the architecture.\nsave_model_weights_hdf5()/load_model_weights_hdf5() if you want to save only the model weights to disk in the hdf5 format. You can also use save_model_weights_tf()/load_model_weights_tf() to save the weights in the SavedModel format.\n\nNote you can use a combination of model_to_json() and save_model_weights_hdf5() to save both the architecture and the weights. In this case the optimizer state is not saved.\nFor custom models use:\n\nsave_model_weights_tf() or save_model_weights_hdf5() to save the model weights. Usually for custom models, the architecture must be recreated using code."
  },
  {
    "objectID": "v1/guide/keras/saving_serializing.html#setup",
    "href": "v1/guide/keras/saving_serializing.html#setup",
    "title": "Saving and serializing models",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)"
  },
  {
    "objectID": "v1/guide/keras/saving_serializing.html#saving-sequential-models-or-functional-models",
    "href": "v1/guide/keras/saving_serializing.html#saving-sequential-models-or-functional-models",
    "title": "Saving and serializing models",
    "section": "Saving Sequential Models or Functional models",
    "text": "Saving Sequential Models or Functional models\n\ninputs <- layer_input(shape = 784, name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\nmodel <- keras_model(inputs, outputs) \nsummary(model)\n\nOptionally, let’s train this model, just so it has weight values to save, as well as an an optimizer state. Of course, you can save models you’ve never trained, too, but obviously that’s less interesting.\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\nx_train <- x_train %>% array_reshape(dim = c(60000, 784))/255\nx_test <- x_test %>% array_reshape(dim = c(10000, 784))/255\n\nmodel %>% compile(loss = \"sparse_categorical_crossentropy\",\n                  optimizer = optimizer_rmsprop())\n\nhistory <- model %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\n\n# Save predictions for future checks\npredictions <- predict(model, x_test)\n\n\nWhole-model saving\nYou can save a model built with the Functional API into a single file. You can later recreate the same model from this file, even if you no longer have access to the code that created the model.\nThis file includes:\n\nThe model’s architecture\nThe model’s weight values (which were learned during training)\nThe model’s training config (what you passed to compile), if any\nThe optimizer and its state, if any (this enables you to restart training where you left off)\n\n\n# Save the model\nsave_model_hdf5(model, \"model.h5\")\n\n# Recreate the exact same model purely from the file\nnew_model <- load_model_hdf5(\"model.h5\")\n\n\n# Check that the state is preserved\nnew_predictions <- predict(new_model, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer state is preserved as well so you can resume training where you left off.\n\n\nExport to SavedModel\nYou can also export a whole model to the TensorFlow SavedModel format. SavedModel is a standalone serialization format for Tensorflow objects, supported by TensorFlow serving as well as TensorFlow implementations other than Python. Note that save_model_tf is only available for TensorFlow version greater than 1.14.\n\n# Export the model to a SavedModel\nsave_model_tf(model, \"model/\")\n\n# Recreate the exact same model\nnew_model <- load_model_tf(\"model/\")\n\n# Check that the state is preserved\nnew_predictions <- predict(new_model, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer state is preserved as well so you can resume training where you left off.\nThe SavedModel files that were created contain:\n\nA TensorFlow checkpoint containing the model weights.\nA SavedModel proto containing the underlying Tensorflow graph. Separate graphs are saved for prediction (serving), train, and evaluation. If the model wasn’t compiled before, then only the inference graph gets exported.\nThe model’s architecture config, if available.\n\nYou can also use the export_savedmodel function to export models but those models can not be loaded as Keras models again. Models exported using exported_savedmodels can be used for prediction though.\n\nexport_savedmodel(model, \"savedmodel/\")\nnew_predictions <- tfdeploy::predict_savedmodel(x_test, \"savedmodel/\")\n\nNote Exporting with export_savedmodel sets learning phase to 0 so you need to restart R and re-build the model before doing additional training.\n\n\nArchitecture-only saving\nSometimes, you are only interested in the architecture of the model, and you don’t need to save the weight values or the optimizer. In this case, you can retrieve the “config” of the model via the get_config() method. The config is a named list that enables you to recreate the same model – initialized from scratch, without any of the information learned previously during training.\n\nconfig <- get_config(model)\nreinitialized_model <- from_config(config)\n\n\n# Note that the model state is not preserved! We only saved the architecture.\nnew_predictions <- predict(reinitialized_model, x_test)\nall.equal(predictions, new_predictions)\n\nYou can alternatively use model_to_json() and model_from_json(), which uses a JSON string to store the config instead of a named list. This is useful to save the config to disk.\n\njson_config <- model_to_json(model)\nreinitialized_model <- model_from_json(json_config)\n\n\n\nWeights-only saving\nSometimes, you are only interested in the state of the model – its weights values – and not in the architecture. In this case, you can retrieve the weights values as a list of arrays via get_weights(), and set the state of the model via set_weights:\n\nweights <- get_weights(model)\nset_weights(reinitialized_model, weights)\n\nnew_predictions <- predict(reinitialized_model, x_test)\nall.equal(predictions, new_predictions)\n\nYou can combine get_config()/from_config() and get_weights()/set_weights() to recreate your model in the same state. However, unlike save_model_hdf5, this will not include the training config and the optimizer. You would have to call compile() again before using the model for training.\n\nconfig <- get_config(model)\nweights <- get_weights(model)\n\nnew_model <- from_config(config)\nset_weights(new_model, weights)\n\n# Check that the state is preserved\nnew_predictions <- predict(new_model, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer was not preserved, so the model should be compiled anew before training (and the optimizer will start from a blank state).\nThe save-to-disk alternative to get_weights() and set_weights(weights) is save_weights(fpath) and load_weights(fpath).\n\n# Save JSON config to disk\njson_config <- model_to_json(model)\nwriteLines(json_config, \"model_config.json\")\n\n# Save weights to disk\nsave_model_weights_hdf5(model, \"model_weights.h5\")\n\n# Reload the model from the 2 files we saved\njson_config <- readLines(\"model_config.json\")\nnew_model <- model_from_json(json_config)\nload_model_weights_hdf5(new_model, \"model_weights.h5\")\n\n# Check that the state is preserved\nnew_predictions <- predict(new_model, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer was not preserved. But remember that the simplest, recommended way is just this:\n\nsave_model_hdf5(model, \"model.h5\")\nnew_model <- load_model_hdf5(\"model.h5\")\n\n\n\nWeights-only saving in SavedModel format\nNote that save_weights can create files either in the Keras HDF5 format, or in the TensorFlow SavedModel format.\n\nsave_model_weights_tf(model, \"model_weights\")"
  },
  {
    "objectID": "v1/guide/keras/saving_serializing.html#saving-subclassed-models",
    "href": "v1/guide/keras/saving_serializing.html#saving-subclassed-models",
    "title": "Saving and serializing models",
    "section": "Saving Subclassed Models",
    "text": "Saving Subclassed Models\nSequential models and Functional models are data structures that represent a DAG of layers. As such, they can be safely serialized and deserialized.\nA subclassed model differs in that it’s not a data structure, it’s a piece of code. The architecture of the model is defined via the body of the call method. This means that the architecture of the model cannot be safely serialized. To load a model, you’ll need to have access to the code that created it (the code of the model subclass). Alternatively, you could be serializing this code as bytecode (e.g. via pickling), but that’s unsafe and generally not portable.\nFor more information about these differences, see the article “What are Symbolic and Imperative APIs in TensorFlow 2.0?”.\nLet’s consider the following subclassed model, which follows the same structure as the model from the first section:\n\nkeras_model_simple_mlp <- function(num_classes, \n                                   use_bn = FALSE, use_dp = FALSE, \n                                   name = NULL) {\n  \n  # define and return a custom model\n  keras_model_custom(name = name, function(self) {\n    \n    # create layers we'll need for the call (this code executes once)\n    self$dense1 <- layer_dense(units = 32, activation = \"relu\")\n    self$dense2 <- layer_dense(units = num_classes, activation = \"softmax\")\n    if (use_dp)\n      self$dp <- layer_dropout(rate = 0.5)\n    if (use_bn)\n      self$bn <- layer_batch_normalization(axis = -1)\n    \n    # implement call (this code executes during training & inference)\n    function(inputs, mask = NULL) {\n      x <- self$dense1(inputs)\n      if (use_dp)\n        x <- self$dp(x)\n      if (use_bn)\n        x <- self$bn(x)\n      self$dense2(x)\n    }\n  })\n}\n\nmodel <- keras_model_simple_mlp(num_classes = 10)\n\nFirst of all, a subclassed model that has never been used cannot be saved.\nThat’s because a subclassed model needs to be called on some data in order to create its weights.\nUntil the model has been called, it does not know the shape and dtype of the input data it should be expecting, and thus cannot create its weight variables. You may remember that in the Functional model from the first section, the shape and dtype of the inputs was specified in advance (via layer_input) – that’s why Functional models have a state as soon as they’re instantiated.\nLet’s train the model, so as to give it a state:\n\nmodel %>% compile(loss = \"sparse_categorical_crossentropy\",\n                  optimizer = optimizer_rmsprop())\n\nhistory <- model %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nThe recommended way to save a subclassed model is to use save_model_weights_tf to create a TensorFlow SavedModel checkpoint, which will contain the value of all variables associated with the model: - The layers’ weights - The optimizer’s state - Any variables associated with stateful model metrics (if any).\n\nsave_model_weights_tf(model, \"my_weights\")\n\n\n# Save predictions for future checks\npredictions <- predict(model, x_test)\n# Also save the loss on the first batch\n# to later assert that the optimizer state was preserved\nfirst_batch_loss <- train_on_batch(model, x_train[1:64,], y_train[1:64])\n\nTo restore your model, you will need access to the code that created the model object.\nNote that in order to restore the optimizer state and the state of any stateful metric, you should compile the model (with the exact same arguments as before) and call it on some data before calling load_weights:\n\nnew_model <- keras_model_simple_mlp(num_classes = 10)\nnew_model %>% compile(loss = \"sparse_categorical_crossentropy\",\n                  optimizer = optimizer_rmsprop())\n\n# This initializes the variables used by the optimizers,\n# as well as any stateful metric variables\ntrain_on_batch(new_model, x_train[1:5,], y_train[1:5])\n\n# Load the state of the old model\nload_model_weights_tf(new_model, \"my_weights\")\n\n# Check that the model state has been preserved\nnew_predictions <- predict(new_model, x_test)\nall.equal(predictions, new_predictions)\n\n# The optimizer state is preserved as well,\n# so you can resume training where you left off\nnew_first_batch_loss <- train_on_batch(new_model, x_train[1:64,], y_train[1:64])\nfirst_batch_loss == new_first_batch_loss\n\nYou’ve reached the end of this guide! This covers everything you need to know about saving and serializing models with Keras in TensorFlow 2.0."
  },
  {
    "objectID": "v1/guide/keras/sequential_model.html",
    "href": "v1/guide/keras/sequential_model.html",
    "title": "Guide to the Sequential Model",
    "section": "",
    "text": "The sequential model is a linear stack of layers.\nYou create a sequential model by calling the keras_model_sequential() function then a series of layer functions:\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 32, input_shape = c(784)) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax')\n\nNote that Keras objects are modified in place which is why it’s not necessary for model to be assigned back to after the layers are added.\nPrint a summary of the model’s structure using the summary() function:\n\nsummary(model)\n\nModel\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 256)                     200960      \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 128)                     32896       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nThe model needs to know what input shape it should expect. For this reason, the first layer in a sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape.\nAs illustrated in the example above, this is done by passing an input_shape argument to the first layer. This is a list of integers or NULL entries, where NULL indicates that any positive integer may be expected. In input_shape, the batch dimension is not included.\nIf you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=c(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8)."
  },
  {
    "objectID": "v1/guide/keras/sequential_model.html#compilation",
    "href": "v1/guide/keras/sequential_model.html#compilation",
    "title": "Guide to the Sequential Model",
    "section": "Compilation",
    "text": "Compilation\nBefore training a model, you need to configure the learning process, which is done via the compile() function. It receives three arguments:\n\nAn optimizer. This could be the string identifier of an existing optimizer (e.g. as “rmsprop” or “adagrad”) or a call to an optimizer function (e.g. optimizer_sgd()).\nA loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (e.g. “categorical_crossentropy” or “mse”) or a call to a loss function (e.g. loss_mean_squared_error()).\nA list of metrics. For any classification problem you will want to set this to metrics = c('accuracy'). A metric could be the string identifier of an existing metric or a call to metric function (e.g. metric_binary_crossentropy()).\n\nHere’s the definition of a model along with the compilation step (the compile() function has arguments appropriate for a multi-class classification problem):\n\n# For a multi-class classification problem\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 32, input_shape = c(784)) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax')\n\nmodel %>% compile(\n  optimizer = 'rmsprop',\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\nHere’s what compilation might look like for a mean squared error regression problem:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.002),\n  loss = 'mse'\n)\n\nHere’s compilation for a binary classification problem:\n\nmodel %>% compile( \n  optimizer = optimizer_rmsprop(),\n  loss = loss_binary_crossentropy,\n  metrics = metric_binary_accuracy\n)\n\nHere’s compilation with a custom metric:\n\n# create metric using backend tensor functions\nmetric_mean_pred <- custom_metric(\"mean_pred\", function(y_true, y_pred) {\n  k_mean(y_pred) \n})\n\nmodel %>% compile( \n  optimizer = optimizer_rmsprop(),\n  loss = loss_binary_crossentropy,\n  metrics = c('accuracy', metric_mean_pred)\n)"
  },
  {
    "objectID": "v1/guide/keras/sequential_model.html#training",
    "href": "v1/guide/keras/sequential_model.html#training",
    "title": "Guide to the Sequential Model",
    "section": "Training",
    "text": "Training\nKeras models are trained on R matrices or higher dimensional arrays of input data and labels. For training a model, you will typically use the fit() function.\nHere’s a single-input model with 2 classes (binary classification):\n\n# create model\nmodel <- keras_model_sequential()\n\n# add layers and compile the model\nmodel %>% \n  layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>% \n  layer_dense(units = 1, activation = 'sigmoid') %>% \n  compile(\n    optimizer = 'rmsprop',\n    loss = 'binary_crossentropy',\n    metrics = c('accuracy')\n  )\n\n# Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel %>% fit(data, labels, epochs=10, batch_size=32)\n\nHere’s a single-input model with 10 classes (categorical classification):\n\n# create model\nmodel <- keras_model_sequential()\n\n# define and compile the model\nmodel %>% \n  layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>% \n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    optimizer = 'rmsprop',\n    loss = 'categorical_crossentropy',\n    metrics = c('accuracy')\n  )\n\n# Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)\n\n# Convert labels to categorical one-hot encoding\none_hot_labels <- to_categorical(labels, num_classes = 10)\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel %>% fit(data, one_hot_labels, epochs=10, batch_size=32)"
  },
  {
    "objectID": "v1/guide/keras/sequential_model.html#examples",
    "href": "v1/guide/keras/sequential_model.html#examples",
    "title": "Guide to the Sequential Model",
    "section": "Examples",
    "text": "Examples\nHere are a few examples to get you started!\nOn the examples page you will also find example models for real datasets:\n\nCIFAR10 small images classification\nIMDB movie review sentiment classification\nReuters newswires topic classification\nMNIST handwritten digits classification\nCharacter-level text generation with LSTM\n\nSome additional examples are provided below.\n\nMultilayer Perceptron (MLP) for multi-class softmax classification\n\nlibrary(keras)\n\n# generate dummy data\nx_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)\n\ny_train <- runif(1000, min = 0, max = 9) %>% \n  round() %>%\n  matrix(nrow = 1000, ncol = 1) %>% \n  to_categorical(num_classes = 10)\n\nx_test  <- matrix(runif(100*20), nrow = 100, ncol = 20)\n\ny_test <- runif(100, min = 0, max = 9) %>% \n  round() %>%\n  matrix(nrow = 100, ncol = 1) %>% \n  to_categorical(num_classes = 10)\n\n# create model\nmodel <- keras_model_sequential()\n\n# define and compile the model\nmodel %>% \n  layer_dense(units = 64, activation = 'relu', input_shape = c(20)) %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_sgd(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = TRUE),\n    metrics = c('accuracy')     \n  )\n\n# train\nmodel %>% fit(x_train, y_train, epochs = 20, batch_size = 128)\n\n# evaluate\nscore <- model %>% evaluate(x_test, y_test, batch_size = 128)\n\n\n\nMLP for binary classification\n\nlibrary(keras)\n\n# generate dummy data\nx_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)\ny_train <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)\nx_test <- matrix(runif(100*20), nrow = 100, ncol = 20)\ny_test <- matrix(round(runif(100, min = 0, max = 1)), nrow = 100, ncol = 1)\n\n# create model\nmodel <- keras_model_sequential()\n\n# define and compile the model\nmodel %>% \n  layer_dense(units = 64, activation = 'relu', input_shape = c(20)) %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = 'sigmoid') %>% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\n# train \nmodel %>% fit(x_train, y_train, epochs = 20, batch_size = 128)\n\n# evaluate\nscore = model %>% evaluate(x_test, y_test, batch_size=128)\n\n\n\nVGG-like convnet\n\nlibrary(keras)\n\n# generate dummy data\nx_train <- array(runif(100 * 100 * 100 * 3), dim = c(100, 100, 100, 3))\n\ny_train <- runif(100, min = 0, max = 9) %>% \n  round() %>%\n  matrix(nrow = 100, ncol = 1) %>% \n  to_categorical(num_classes = 10)\n\nx_test <- array(runif(20 * 100 * 100 * 3), dim = c(20, 100, 100, 3))\n\ny_test <- runif(20, min = 0, max = 9) %>% \n  round() %>%\n  matrix(nrow = 20, ncol = 1) %>% \n  to_categorical(num_classes = 10)\n\n# create model\nmodel <- keras_model_sequential()\n\n# define and compile model\n# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel %>% \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', \n                input_shape = c(100,100,3)) %>% \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = 'relu') %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy', \n    optimizer = optimizer_sgd(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = TRUE)\n  )\n  \n# train\nmodel %>% fit(x_train, y_train, batch_size = 32, epochs = 10)\n\n# evaluate\nscore <- model %>% evaluate(x_test, y_test, batch_size = 32)\n\n\n\nSequence classification with LSTM\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_embedding(input_dim = max_features, output_dim - 256) %>% \n  layer_lstm(units = 128) %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = 'sigmoid') %>% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\nmodel %>% fit(x_train, y_train, batch_size = 16, epochs = 10)\nscore <- model %>% evaluate(x_test, y_test, batch_size = 16)\n\n\n\nSequence classification with 1D convolutions:\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_1d(filters = 64, kernel_size = 3, activation = 'relu',\n                input_shape = c(seq_length, 100)) %>% \n  layer_conv_1d(filters = 64, kernel_size = 3, activation = 'relu') %>% \n  layer_max_pooling_1d(pool_size = 3) %>% \n  layer_conv_1d(filters = 128, kernel_size = 3, activation = 'relu') %>% \n  layer_conv_1d(filters = 128, kernel_size = 3, activation = 'relu') %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = 'sigmoid') %>% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\nmodel %>% fit(x_train, y_train, batch_size = 16, epochs = 10)\nscore <- model %>% evaluate(x_test, y_test, batch_size = 16)\n\n\n\nStacked LSTM for sequence classification\nIn this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\nThe first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n\n\nlibrary(keras)\n\n# constants\ndata_dim <- 16\ntimesteps <- 8\nnum_classes <- 10\n\n# define and compile model\n# expected input data shape: (batch_size, timesteps, data_dim)\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_lstm(units = 32, return_sequences = TRUE, input_shape = c(timesteps, data_dim)) %>% \n  layer_lstm(units = 32, return_sequences = TRUE) %>% \n  layer_lstm(units = 32) %>% # return a single vector dimension 32\n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n  \n# generate dummy training data\nx_train <- array(runif(1000 * timesteps * data_dim), dim = c(1000, timesteps, data_dim))\ny_train <- matrix(runif(1000 * num_classes), nrow = 1000, ncol = num_classes)\n\n# generate dummy validation data\nx_val <- array(runif(100 * timesteps * data_dim), dim = c(100, timesteps, data_dim))\ny_val <- matrix(runif(100 * num_classes), nrow = 100, ncol = num_classes)\n\n# train\nmodel %>% fit( \n  x_train, y_train, batch_size = 64, epochs = 5, validation_data = list(x_val, y_val)\n)\n\n\n\nSame stacked LSTM model, rendered “stateful”\nA stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable.\nYou can read more about stateful RNNs in the FAQ.\n\nlibrary(keras)\n\n# constants\ndata_dim <- 16\ntimesteps <- 8\nnum_classes <- 10\nbatch_size <- 32\n\n# define and compile model\n# Expected input batch shape: (batch_size, timesteps, data_dim)\n# Note that we have to provide the full batch_input_shape since the network is stateful.\n# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_lstm(units = 32, return_sequences = TRUE, stateful = TRUE,\n             batch_input_shape = c(batch_size, timesteps, data_dim)) %>% \n  layer_lstm(units = 32, return_sequences = TRUE, stateful = TRUE) %>% \n  layer_lstm(units = 32, stateful = TRUE) %>% \n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n  \n# generate dummy training data\nx_train <- array(runif( (batch_size * 10) * timesteps * data_dim), \n                 dim = c(batch_size * 10, timesteps, data_dim))\ny_train <- matrix(runif( (batch_size * 10) * num_classes), \n                  nrow = batch_size * 10, ncol = num_classes)\n\n# generate dummy validation data\nx_val <- array(runif( (batch_size * 3) * timesteps * data_dim), \n               dim = c(batch_size * 3, timesteps, data_dim))\ny_val <- matrix(runif( (batch_size * 3) * num_classes), \n                nrow = batch_size * 3, ncol = num_classes)\n\n# train\nmodel %>% fit( \n  x_train, \n  y_train, \n  batch_size = batch_size, \n  epochs = 5, \n  shuffle = FALSE,\n  validation_data = list(x_val, y_val)\n)"
  },
  {
    "objectID": "v1/guide/keras/training_callbacks.html",
    "href": "v1/guide/keras/training_callbacks.html",
    "title": "Training Callbacks",
    "section": "",
    "text": "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the fit() function. The relevant methods of the callbacks will then be called at each stage of the training.\nFor example:\n\nlibrary(keras)\n\n# generate dummy training data\ndata <- matrix(rexp(1000*784), nrow = 1000, ncol = 784)\nlabels <- matrix(round(runif(1000*10, min = 0, max = 9)), nrow = 1000, ncol = 10)\n\n# create model\nmodel <- keras_model_sequential() \n\n# add layers and compile\nmodel %>%\n  layer_dense(32, input_shape = c(784)) %>%\n  layer_activation('relu') %>%\n  layer_dense(10) %>%\n  layer_activation('softmax') %>% \n  compile(\n    loss='binary_crossentropy',\n    optimizer = optimizer_sgd(),\n    metrics='accuracy'\n  )\n  \n# fit with callbacks\nmodel %>% fit(data, labels, callbacks = list(\n  callback_model_checkpoint(\"checkpoints.h5\"),\n  callback_reduce_lr_on_plateau(monitor = \"val_loss\", factor = 0.1)\n))"
  },
  {
    "objectID": "v1/guide/keras/training_callbacks.html#built-in-callbacks",
    "href": "v1/guide/keras/training_callbacks.html#built-in-callbacks",
    "title": "Training Callbacks",
    "section": "Built in Callbacks",
    "text": "Built in Callbacks\nThe following built-in callbacks are available as part of Keras:\n\n\n\n\n\n\ncallback_progbar_logger()\n\n\n\nCallback that prints metrics to stdout.\n\n\n\n\n\n\ncallback_model_checkpoint()\n\n\n\nSave the model after every epoch.\n\n\n\n\n\n\ncallback_early_stopping()\n\n\n\nStop training when a monitored quantity has stopped improving.\n\n\n\n\n\n\ncallback_remote_monitor()\n\n\n\nCallback used to stream events to a server.\n\n\n\n\n\n\ncallback_learning_rate_scheduler()\n\n\n\nLearning rate scheduler.\n\n\n\n\n\n\ncallback_tensorboard()\n\n\n\nTensorBoard basic visualizations\n\n\n\n\n\n\ncallback_reduce_lr_on_plateau()\n\n\n\nReduce learning rate when a metric has stopped improving.\n\n\n\n\n\n\ncallback_csv_logger()\n\n\n\nCallback that streams epoch results to a csv file\n\n\n\n\n\n\ncallback_lambda()\n\n\n\nCreate a custom callback"
  },
  {
    "objectID": "v1/guide/keras/training_callbacks.html#custom-callbacks",
    "href": "v1/guide/keras/training_callbacks.html#custom-callbacks",
    "title": "Training Callbacks",
    "section": "Custom Callbacks",
    "text": "Custom Callbacks\nYou can create a custom callback by creating a new R6 class that inherits from the KerasCallback class.\nHere’s a simple example saving a list of losses over each batch during training:\n\nlibrary(keras)\n\n# define custom callback class\nLossHistory <- R6::R6Class(\"LossHistory\",\n  inherit = KerasCallback,\n  \n  public = list(\n    \n    losses = NULL,\n     \n    on_batch_end = function(batch, logs = list()) {\n      self$losses <- c(self$losses, logs[[\"loss\"]])\n    }\n))\n\n# define model\nmodel <- keras_model_sequential() \n\n# add layers and compile\nmodel %>% \n  layer_dense(units = 10, input_shape = c(784)) %>% \n  layer_activation(activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy', \n    optimizer = 'rmsprop'\n  )\n\n# create history callback object and use it during training\nhistory <- LossHistory$new()\nmodel %>% fit(\n  X_train, Y_train,\n  batch_size=128, epochs=20, verbose=0,\n  callbacks= list(history)\n)\n\n# print the accumulated losses\nhistory$losses\n\n[1] 0.6604760 0.3547246 0.2595316 0.2590170 ...\n\nFields\nCustom callback objects have access to the current model and it’s training parameters via the following fields:\n\nself$params\n\nNamed list with training parameters (eg. verbosity, batch size, number of epochs…).\n\nself$model\n\nReference to the Keras model being trained.\n\n\n\n\nMethods\nCustom callback objects can implement one or more of the following methods:\n\non_epoch_begin(epoch, logs)\n\nCalled at the beginning of each epoch.\n\non_epoch_end(epoch, logs)\n\nCalled at the end of each epoch.\n\non_batch_begin(batch, logs)\n\nCalled at the beginning of each batch.\n\non_batch_end(batch, logs)\n\nCalled at the end of each batch.\n\non_train_begin(logs)\n\nCalled at the beginning of training.\n\non_train_end(logs)\n\nCalled at the end of training.\n\non_train_batch_begin\n\nCalled at the beginning of every batch.\n\non_train_batch_end\n\nCalled at the end of every batch.`\n\non_predict_batch_begin\n\nCalled at the beginning of a batch in predict methods.\n\non_predict_batch_end\n\nCalled at the end of a batch in predict methods.\n\non_predict_begin\n\nCalled at the beginning of prediction.\n\non_predict_end\n\nCalled at the end of prediction.\n\non_test_batch_begin\n\nCalled at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided.\n\non_test_batch_end\n\nCalled at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided.\n\non_test_begin\n\nCalled at the beginning of evaluation or validation.\n\non_test_end\n\nCalled at the end of evaluation or validation."
  },
  {
    "objectID": "v1/guide/keras/training_visualization.html",
    "href": "v1/guide/keras/training_visualization.html",
    "title": "Training Visualization",
    "section": "",
    "text": "There are a number of tools available for visualizing the training of Keras models, including:\n\nA plot method for the Keras training history returned from fit().\nReal time visualization of training metrics within the RStudio IDE.\nIntegration with the TensorBoard visualization tool included with TensorFlow. Beyond just training metrics, TensorBoard has a wide variety of other visualizations available including the underlying TensorFlow graph, gradient histograms, model weights, and more. TensorBoard also enables you to compare metrics across multiple training runs.\n\nEach of these tools is described in more detail below."
  },
  {
    "objectID": "v1/guide/keras/training_visualization.html#plotting-history",
    "href": "v1/guide/keras/training_visualization.html#plotting-history",
    "title": "Training Visualization",
    "section": "Plotting History",
    "text": "Plotting History\nThe Keras fit() method returns an R object containing the training history, including the value of metrics at the end of each epoch . You can plot the training metrics by epoch using the plot() method.\nFor example, here we compile and fit a model with the “accuracy” metric:\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nWe can then plot the training history as follows:\n\nplot(history)\n\n\nThe history will be plotted using ggplot2 if available (if not then base graphics will be used), include all specified metrics as well as the loss, and draw a smoothing line if there are 10 or more epochs. You can customize all of this behavior via various options of the plot method.\nIf you want to create a custom visualization you can call the as.data.frame() method on the history to obtain a data frame with factors for each metric as well as training vs. validation:\n\nhistory_df <- as.data.frame(history)\nstr(history_df)\n\n'data.frame':   120 obs. of  4 variables:\n $ epoch : int  1 2 3 4 5 6 7 8 9 10 ...\n $ value : num  0.87 0.941 0.954 0.962 0.965 ...\n $ metric: Factor w/ 2 levels \"acc\",\"loss\": 1 1 1 1 1 1 1 1 1 1 ...\n $ data  : Factor w/ 2 levels \"training\",\"validation\": 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "v1/guide/keras/training_visualization.html#rstudio-ide",
    "href": "v1/guide/keras/training_visualization.html#rstudio-ide",
    "title": "Training Visualization",
    "section": "RStudio IDE",
    "text": "RStudio IDE\nIf you are training your model within the RStudio IDE then real time metrics are available within the Viewer pane:\n\n\n\n\n\nThe view_metrics argument of the fit() method controls whether real time metrics are displayed. By default metrics are automatically displayed if one or more metrics are specified in the call to compile() and there is more than one training epoch.\nYou can explicitly control whether metrics are displayed by specifying the view_metrics argument. You can also set a global session default using the keras.view_metrics option:\n\n# don't show metrics during this run\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  view_metrics = FALSE,\n  validation_split = 0.2\n)\n\n# set global default to never show metrics\noptions(keras.view_metrics = FALSE)\n\nNote that when view_metrics is TRUE metrics will be displayed even when not running within RStudio (in that case metrics will be displayed in an external web browser)."
  },
  {
    "objectID": "v1/guide/keras/training_visualization.html#tensorboard",
    "href": "v1/guide/keras/training_visualization.html#tensorboard",
    "title": "Training Visualization",
    "section": "TensorBoard",
    "text": "TensorBoard\nTensorBoard is a visualization tool included with TensorFlow that enables you to visualize dynamic graphs of your Keras training and test metrics, as well as activation histograms for the different layers in your model.\nFor example, here’s a TensorBoard display for Keras accuracy and loss metrics:\n\n\nRecording Data\nTo record data that can be visualized with TensorBoard, you add a TensorBoard callback to the fit() function. For example:\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  callbacks = callback_tensorboard(\"logs/run_a\"),\n  validation_split = 0.2\n)\n\nSee the documentation on the callback_tensorboard() function for the various available options. The most important option is the log_dir, which determines which directory logs are written to for a given training run.\nYou should either use a distinct log directory for each training run or remove the log directory between runs.\n\n\nViewing Data\nTo view TensorBoard data for a given set of runs you use the tensorboard() function, pointing it to the previously specified log_dir:\n\ntensorboard(\"logs/run_a\")\n\nIt’s often useful to run TensorBoard while you are training a model. To do this, simply launch tensorboard within the training directory right before you begin training:\n\n# launch TensorBoard (data won't show up until after the first epoch)\ntensorboard(\"logs/run_a\")\n\n# fit the model with the TensorBoard callback\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  callbacks = callback_tensorboard(\"logs/run_a\"),\n  validation_split = 0.2\n)\n\nKeras writes TensorBoard data at the end of each epoch so you won’t see any data in TensorBoard until 10-20 seconds after the end of the first epoch (TensorBoard automatically refreshes it’s display every 30 seconds during training).\n\n\nComparing Runs\nTensorBoard will automatically include all runs logged within the sub-directories of the specified log_dir, for example, if you logged another run using:\n\ncallback_tensorboard(log_dir = \"logs/run_b\")\n\nThen called tensorboard as follows:\n\ntensorboard(\"logs\")\n\nThe TensorBoard visualization would look like this:\n\nYou can also pass multiple log directories. For example:\n\ntensorboard(c(\"logs/run_a\", \"logs/run_b\"))\n\n\n\nCustomization\n\nMetrics\nIn the above examples TensorBoard metrics are logged for loss and accuracy. The TensorBoard callback will log data for any metrics which are specified in the metrics parameter of the compile() function. For example, in the following code:\nmodel %>% compile(\n  loss = 'mean_squared_error',\n  optimizer = 'sgd',\n  metrics= c('mae', 'acc')\n)\nTensorBoard data series will be created for the loss (mean squared error) as well as for the mean absolute error and accuracy metrics.\n\n\nOptions\nThe callback_tensorboard() function includes a number of other options that control logging during training:\ncallback_tensorboard(log_dir = \"logs\", histogram_freq = 0,\n  write_graph = TRUE, write_images = FALSE, embeddings_freq = 0,\n  embeddings_layer_names = NULL, embeddings_metadata = NULL)\n\n\n\nName\nDescription\n\n\n\n\nlog_dir\nPath of the directory to save the log files to be parsed by Tensorboard.\n\n\nhistogram_freq\nFrequency (in epochs) at which to compute activation histograms for the layers of the model. If set to 0 (the default), histograms won’t be computed.\n\n\nwrite_graph\nWhether to visualize the graph in Tensorboard. The log file can become quite large when write_graph is set to TRUE\n\n\nwrite_images\nWhether to write model weights to visualize as image in Tensorboard.\n\n\nembeddings_freq\nFrequency (in epochs) at which selected embedding layers will be saved.\n\n\nembeddings_layer_names\nA list of names of layers to keep eye on. If NULL or empty list all the embedding layers will be watched.\n\n\nembeddings_metadata\nA named list which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about the metadata file format. In case if the same metadata file is used for all embedding layers, string can be passed."
  },
  {
    "objectID": "v1/guide/saving/checkpoints.html",
    "href": "v1/guide/saving/checkpoints.html",
    "title": "Checkpoints",
    "section": "",
    "text": "The phrase “Saving a TensorFlow model” typically means one of two things:\nCheckpoints capture the exact value of all parameters (tf$Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\nThe SavedModel format on the other hand includes a serialized description of the computation defined by the model in addition to the parameter values (checkpoint). Models in this format are independent of the source code that created the model. They are thus suitable for deployment via TensorFlow Serving, TensorFlow Lite, TensorFlow.js, or programs in other programming languages (the C, C++, Java, Go, Rust, C# etc. TensorFlow APIs).\nThis guide covers APIs for writing and reading checkpoints."
  },
  {
    "objectID": "v1/guide/saving/checkpoints.html#setup",
    "href": "v1/guide/saving/checkpoints.html#setup",
    "title": "Checkpoints",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)\nlibrary(tensorflow)\n\n# A simple linear model\ncreate_model <- function(size) {\n  keras_model_custom(name = \"linear_model\", function(self) {\n    \n    self$l1 <- layer_dense(units = size)\n    \n    function(inputs, mask = NULL) {\n      self$l1(inputs)\n    }\n  })\n}\n\nmodel <- create_model(5)"
  },
  {
    "objectID": "v1/guide/saving/checkpoints.html#saving-models-through-the-keras-api",
    "href": "v1/guide/saving/checkpoints.html#saving-models-through-the-keras-api",
    "title": "Checkpoints",
    "section": "Saving models through the Keras API",
    "text": "Saving models through the Keras API\nThe respective Keras guide explains how to use the Keras API to save and restore complete models as well as model weights.\nCalling save_weights effectively results in saving a TensorFlow checkpoint:\n\nmodel %>% save_model_weights_tf(\"weights_only\")\n\nMore customization is available through lower-level TensorFlow methods."
  },
  {
    "objectID": "v1/guide/saving/checkpoints.html#writing-checkpoints",
    "href": "v1/guide/saving/checkpoints.html#writing-checkpoints",
    "title": "Checkpoints",
    "section": "Writing checkpoints",
    "text": "Writing checkpoints\nThe persistent state of a TensorFlow model is stored in tf$Variable objects. These can be constructed directly, but are often created through high-level APIs like Keras layers or models.\nThe easiest way to manage variables is by attaching them to Python objects, then referencing those objects.\nSubclasses of tf$train$Checkpoint, tf$keras$layers$Layer, and tf$keras$Model automatically track variables assigned to their attributes. The following example constructs a simple linear model, then writes checkpoints which contain values for all of the model’s variables.\n\nManual checkpointing\n\nSetup\nTo help demonstrate all the features of tf$train$Checkpoint, define a toy dataset and an optimization step:\n\nlibrary(tfdatasets)\ntoy_dataset <- function() {\n  inputs <- matrix(0:9, ncol = 1) %>% tf$cast(tf$float32)\n  labels <- matrix(0:49, ncol = 5, byrow = TRUE) %>% tf$cast(tf$float32)\n  tensor_slices_dataset(list(x = inputs, y = labels)) %>%\n    dataset_repeat(10) %>%\n    dataset_batch(2)\n}\n\ntrain_step <- function(model, example, optimizer) {\n  with(tf$GradientTape() %as% tape, {\n    output <- model(example$x)\n    loss <- tf$reduce_mean(tf$abs(output - example$y))\n  })\n  variables <- model$trainable_variables\n  gradients <- tape$gradient(loss, variables)\n  optimizer$apply_gradients(purrr::transpose(list(gradients, variables)))\n  loss\n}\n\n\n\nCreate the checkpoint objects\nTo manually make a checkpoint you will need a tf$train$Checkpoint object. Where the objects you want to checkpoint are set as attributes on the object.\nA tf$train$CheckpointManager can also be helpful for managing multiple checkpoints.\n\nopt <- optimizer_adam(0.1)\nckpt <- tf$train$Checkpoint(step = tf$Variable(1), optimizer = opt, net = model)\nmanager <- tf$train$CheckpointManager(ckpt, './tf_ckpts', max_to_keep = 3)\n\n\n\nTrain and checkpoint the model\nThe following training loop creates an instance of the model and of an optimizer, then gathers them into a tf$train$Checkpoint object. It calls the training step in a loop on each batch of data, and periodically writes checkpoints to disk.\n\nlibrary(glue)\nlibrary(tfautograph)\n\ntrain_and_checkpoint <- autograph(function(model, manager, dataset) {\n  \n  ckpt$restore(manager$latest_checkpoint)\n  if (!is.null(manager$latest_checkpoint)) {\n    tf$print(glue(\"Restored from {manager$latest_checkpoint}\\n\"))\n  } else {\n    tf$print(\"Initializing from scratch.\\n\")\n  }\n\n  for (example in dataset()) {\n    loss <- train_step(model, example, opt)\n    ckpt$step$assign_add(1)\n    if (ckpt$step %% 10 == 0) {\n      save_path <- manager$save()\n      tf$print(glue(\"Saved checkpoint for step {as.numeric(ckpt$step)}: {as.character(save_path)}\"))\n      tf$print(glue(\"loss: {round(as.numeric(loss), 2)}\"))\n    }\n  }\n})\n\ntrain_and_checkpoint(model, manager, toy_dataset)\n\n\n\nRestore and continue training\nAfter the first you can pass a new model and manager, but pickup training exactly where you left off:\n\nopt <- optimizer_adam(0.1)\nnet <- create_model(5)\nckpt <- tf$train$Checkpoint(step = tf$Variable(1), optimizer = opt, net = net)\nmanager <- tf$train$CheckpointManager(ckpt, './tf_ckpts', max_to_keep = 3)\n\ntrain_and_checkpoint(net, manager, toy_dataset)\n\nThe tf$train$CheckpointManager object deletes old checkpoints. Above it’s configured to keep only the three most recent checkpoints.\n\nmanager$checkpoints  # List the three remaining checkpoints\n\nThese paths, e.g. ‘./tf_ckpts/ckpt-10’, are not files on disk. Instead they are prefixes for an index file and one or more data files which contain the variable values. These prefixes are grouped together in a single checkpoint file (‘./tf_ckpts/checkpoint’) where the CheckpointManager saves its state.\n\nlist.files(\"./tf_ckpts/\")\n\nTensorFlow matches variables to checkpointed values by traversing a directed graph with named edges, starting from the object being loaded. Edge names typically come from attribute names in objects, for example the “l1” in self$l1 <- layer_dense(units = size). tf$train$Checkpoint uses its keyword argument names, as in the “step” in tf$train$CheckpointManager(step = ...).\nThe dependency graph from the example above looks like this:\n\nWith the optimizer in red, regular variables in blue, and optimizer slot variables in orange. The other nodes, for example representing the tf$train$Checkpoint, are black.\nSlot variables are part of the optimizer’s state, but are created for a specific variable. For example the m edges above correspond to momentum, which the Adam optimizer tracks for each variable. Slot variables are only saved in a checkpoint if the variable and the optimizer would both be saved, thus the dashed edges.\nCalling restore() on a tf$train$Checkpoint object queues the requested restorations, restoring variable values as soon as there’s a matching path from the Checkpoint object. For example we can load just the kernel from the model we defined above by reconstructing one path to it through the network and the layer.\n\nto_restore <- tf$Variable(tf$zeros(list(5L)))\nas.numeric(to_restore)  # All zeros\nfake_layer <- tf$train$Checkpoint(bias = to_restore)\nfake_net <- tf$train$Checkpoint(l1 = fake_layer)\nnew_root <- tf$train$Checkpoint(net = fake_net)\nstatus <- new_root$restore(tf$train$latest_checkpoint('./tf_ckpts/'))\nas.numeric(to_restore)  # We get the restored value now\n\nThe dependency graph for these new objects is a much smaller subgraph of the larger checkpoint we wrote above. It includes only the bias and a save counter that tf$train$Checkpoint uses to number checkpoints.\n\nrestore() returns a status object, which has optional assertions. All of the objects we’ve created in our new Checkpoint have been restored, so status.assert_existing_objects_matched() passes.\n\nstatus$assert_existing_objects_matched()\n\nThere are many objects in the checkpoint which haven’t matched, including the layer’s kernel and the optimizer’s variables. status$assert_consumed() only passes if the checkpoint and the program match exactly, and would throw an exception here.\n\n\nDelayed restorations\nLayer objects in TensorFlow may delay the creation of variables to their first call, when input shapes are available. For example the shape of a Dense layer’s kernel depends on both the layer’s input and output shapes, and so the output shape required as a constructor argument is not enough information to create the variable on its own. Since calling a Layer also reads the variable’s value, a restore must happen between the variable’s creation and its first use.\nTo support this idiom, tf$train$Checkpoint queues restores which don’t yet have a matching variable.\n\ndelayed_restore <- tf$Variable(tf$zeros(list(1L, 5L)))\nas.numeric(delayed_restore)  # Not restored; still zeros\nfake_layer$kernel <- delayed_restore\nas.numeric(delayed_restore)  # Restored\n\n\n\nManually inspecting checkpoints\ntf$train$list_variables lists the checkpoint keys and shapes of variables in a checkpoint. Checkpoint keys are paths in the graph displayed above.\n\ntf$train$list_variables(tf$train$latest_checkpoint('./tf_ckpts/'))\n\n\n\nList and dictionary tracking\nAs with direct attribute assignments like self$l1 <- layer_dense(units = size), assigning lists and dictionaries to attributes will track their contents.\n\nsave <- tf$train$Checkpoint()\nsave$listed <- list(tf$Variable(1))\nsave$listed$append(tf$Variable(2))\nsave$mapped <- dict(one = save$listed[0])\nreticulate::py_set_item(save$mapped, \"two\", save$listed[1])\n#save$mapped['two'] <- save$listed[1]\nsave_path <- save$save('./tf_list_example')\n\nrestore <- tf$train$Checkpoint()\nv2 <- tf$Variable(0)\n0 == as.numeric(v2)  # Not restored yet\nrestore$mapped <- dict(two = v2)\nrestore$restore(save_path)\n2 == as.numeric(v2)\n\nYou may notice wrapper objects for lists and dictionaries. These wrappers are checkpointable versions of the underlying data-structures. Just like the attribute based loading, these wrappers restore a variable’s value as soon as it’s added to the container.\n\nrestore$listed <- list()\nprint(restore$listed)  # ListWrapper([])\nv1 <- tf$Variable(0.)\nrestore$listed$append(v1)  # Restores v1, from restore() in the previous cell\n1. == as.numeric(v1)\n\nThe same tracking is automatically applied to subclasses of tf.keras.Model, and may be used for example to track lists of layers."
  },
  {
    "objectID": "v1/guide/saving/saved_model.html",
    "href": "v1/guide/saving/saved_model.html",
    "title": "Saved Model",
    "section": "",
    "text": "A SavedModel contains a complete TensorFlow program, including weights and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying (with TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub).\nIf you have code for a model in R and merely want to load weights into it, see the guide to training checkpoints."
  },
  {
    "objectID": "v1/guide/saving/saved_model.html#creating-a-savedmodel-from-keras",
    "href": "v1/guide/saving/saved_model.html#creating-a-savedmodel-from-keras",
    "title": "Saved Model",
    "section": "Creating a SavedModel from Keras",
    "text": "Creating a SavedModel from Keras\nFor a quick introduction, this section exports a pre-trained Keras model and serves image classification requests with it. The rest of the guide will fill in details and discuss other ways to create SavedModels.\n\nlibrary(keras)\nlibrary(tensorflow)\n\nphysical_devices <- tf$config$experimental$list_physical_devices('GPU')\nif (length(physical_devices) != 0)\n  tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)\n\nWe’ll use an image of Grace Hopper as a running example, and a Keras pre-trained image classification model since it’s easy to use. Custom models work too, and are covered in detail later.\n\nfile <- get_file(\n    \"grace_hopper.jpg\",\n    \"https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\")\nlibrary(magick)\nimg <- image_read(file)\nimg <- image_draw(img)\nimg\n\n\nx <- image_load(file, target_size = list(224, 224)) %>%\n  image_to_array() %>% \n  mobilenet_preprocess_input\ndim(x) <- c(1, dim(x))\n\n\npretrained_model <- tf$keras$applications$MobileNet()\nresult_before_save <- pretrained_model(x)\npreds <- imagenet_decode_predictions(as.matrix(result_before_save))\n\n\ntf$saved_model$save(pretrained_model, \"/tmp/mobilenet/1/\")\n\nThe save-path follows a convention used by TensorFlow Serving where the last path component (1/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.\nSavedModels have named functions called signatures. Keras models export their forward pass under the serving_default signature key. The SavedModel command line interface (see below) is useful for inspecting SavedModels on disk:\n\nsaved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve --signature_def serving_default\n\nWe can load the SavedModel back into Python with tf.saved_model.load and see how Admiral Hopper’s image is classified.\n\nloaded <- tf$saved_model$load(\"/tmp/mobilenet/1/\")\nloaded$signatures$keys()  # [\"serving_default\"]\n\nImported signatures always return dictionaries.\n\ninfer <- loaded$signatures[\"serving_default\"]\ninfer$structured_outputs\n\nRunning inference from the SavedModel gives the same result as the original model.\n\nlabeling <- infer(tf$constant(x, dtype = tf$float32))[pretrained_model$output_names[1]]\nimagenet_decode_predictions(as.matrix(labeling[[1]]))"
  },
  {
    "objectID": "v1/guide/saving/saved_model.html#the-savedmodel-format-on-disk",
    "href": "v1/guide/saving/saved_model.html#the-savedmodel-format-on-disk",
    "title": "Saved Model",
    "section": "The SavedModel format on disk",
    "text": "The SavedModel format on disk\nA SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n\nls /tmp/mobilenet/1\n\nThe saved_model.pb file stores the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs.\nSavedModels may contain multiple variants of the model (multiple v1$MetaGraphDefs, identified with the –tag_set flag to saved_model_cli), but this is rare.\n\nsaved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve\n\nThe variables directory contains a standard training checkpoint (see the guide to training checkpoints).\n\nls /tmp/mobilenet/1/variables\n\nThe assets directory contains files used by the TensorFlow graph, for example text files used to initialize vocabulary tables. It is unused in this example.\nSavedModels may have an assets$extra directory for any files not used by the TensorFlow graph, for example information for consumers about what to do with the SavedModel. TensorFlow itself does not use this directory."
  },
  {
    "objectID": "v1/guide/saving/saved_model.html#load-a-savedmodel-in-c",
    "href": "v1/guide/saving/saved_model.html#load-a-savedmodel-in-c",
    "title": "Saved Model",
    "section": "Load a SavedModel in C++",
    "text": "Load a SavedModel in C++\nThe C++ version of the SavedModel loader provides an API to load a SavedModel from a path, while allowing SessionOptions and RunOptions. You have to specify the tags associated with the graph to be loaded. The loaded version of SavedModel is referred to as SavedModelBundle and contains the MetaGraphDef and the session within which it is loaded.\n\nconst string export_dir = ...\nSavedModelBundle bundle;\n...\nLoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},\n               &bundle);"
  },
  {
    "objectID": "v1/guide/saving/saved_model.html#details-of-the-savedmodel-command-line-interface",
    "href": "v1/guide/saving/saved_model.html#details-of-the-savedmodel-command-line-interface",
    "title": "Saved Model",
    "section": "Details of the SavedModel command line interface",
    "text": "Details of the SavedModel command line interface\nYou can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. For example, you can use the CLI to inspect the model’s SignatureDefs. The CLI enables you to quickly confirm that the input Tensor dtype and shape match the model. Moreover, if you want to test your model, you can use the CLI to do a sanity check by passing in sample inputs in various formats (for example, Python expressions) and then fetching the output.\n\nInstall the SavedModel CLI\nBroadly speaking, you can install TensorFlow in either of the following two ways:\n\nBy installing a pre-built TensorFlow binary.\nBy building TensorFlow from source code.\n\nIf you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system at pathname bin/saved_model_cli.\nIf you built TensorFlow from source code, you must run the following additional command to build saved_model_cli:\n\n$ bazel build tensorflow/python/tools:saved_model_cli\n\n\n\nOverview of commands\nThe SavedModel CLI supports the following two commands on a SavedModel:\n\nshow, which shows the computations available from a SavedModel.\nrun, which runs a computation from a SavedModel.\n\n\nshow command\nA SavedModel contains one or more model variants (technically, v1.MetaGraphDefs), identified by their tag-sets. To serve a model, you might wonder what kind of SignatureDefs are in each model variant, and what are their inputs and outputs. The show command let you examine the contents of the SavedModel in hierarchical order. Here’s the syntax:\n\nusage: saved_model_cli show [-h] --dir DIR [--all]\n[--tag_set TAG_SET] [--signature_def SIGNATURE_DEF_KEY]\n\nFor example, the following command shows all available tag-sets in the SavedModel:\n\nsaved_model_cli show --dir /tmp/saved_model_dir\n\nThe following command shows all available SignatureDef keys for a tag set:\n\nsaved_model_cli show --dir /tmp/saved_model_dir --tag_set serve\n\nIf there are multiple tags in the tag-set, you must specify all tags, each tag separated by a comma. For example:\n\n$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve,gpu\n\nTo show all inputs and outputs TensorInfo for a specific SignatureDef, pass in the SignatureDef key to signature_def option. This is very useful when you want to know the tensor key value, dtype and shape of the input tensors for executing the computation graph later. For example:\n\nsaved_model_cli show --dir \\\n/tmp/saved_model_dir --tag_set serve --signature_def serving_default\n\nTo show all available information in the SavedModel, use the –all option. For example:\n\nsaved_model_cli show --dir /tmp/saved_model_dir --all\n\n\n\nrun command\nInvoke the run command to run a graph computation, passing inputs and then displaying (and optionally saving) the outputs. Here’s the syntax:\n\nusage: saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n                           [--input_exprs INPUT_EXPRS]\n                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n                           [--overwrite] [--tf_debug]\n\nThe run command provides the following three ways to pass inputs to the model:\n\n--inputs option enables you to pass numpy ndarray in files.\n--input_exprs option enables you to pass Python expressions.\n--input_examples option enables you to pass tf$train$Example.\n\n\n--inputs\nTo pass input data in files, specify the --inputs option, which takes the following general format:\n--inputs <INPUTS>\nwhere INPUTS is either of the following formats:\n\n<input_key>=<filename>\n<input_key>=<filename>[<variable_name>]\n\nYou may pass multiple INPUTS. If you do pass multiple inputs, use a semicolon to separate each of the INPUTS.\nsaved_model_cli uses numpy.load to load the filename. The filename may be in any of the following formats:\n\n.npy\n.npz\npickle format\n\nA .npy file always contains a numpy ndarray. Therefore, when loading from a .npy file, the content will be directly assigned to the specified input tensor. If you specify a variable_name with that .npy file, the variable_name will be ignored and a warning will be issued.\nWhen loading from a .npz (zip) file, you may optionally specify a variable_name to identify the variable within the zip file to load for the input tensor key. If you don’t specify a variable_name, the SavedModel CLI will check that only one file is included in the zip file and load it for the specified input tensor key.\nWhen loading from a pickle file, if no variable_name is specified in the square brackets, whatever that is inside the pickle file will be passed to the specified input tensor key. Otherwise, the SavedModel CLI will assume a dictionary is stored in the pickle file and the value corresponding to the variable_name will be used.\n\n\n--input_exprs\nTo pass inputs through Python expressions, specify the --input_exprs option. This can be useful for when you don’t have data files lying around, but still want to sanity check the model with some simple inputs that match the dtype and shape of the model’s SignatureDefs. For example:\n<input_key>=[[1],[2],[3]]\nIn addition to Python expressions, you may also pass numpy functions. For example:\n<input_key>=np.ones((32,32,3))\n(Note that the numpy module is already available to you as np.)\n\n\n--input_examples\nTo pass tf$train$Example as inputs, specify the --input_examples option. For each input key, it takes a list of dictionary, where each dictionary is an instance of tf$train$Example. The dictionary keys are the features and the values are the value lists for each feature. For example:\n<input_key>=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]\n\n\n\nSave output\nBy default, the SavedModel CLI writes output to stdout. If a directory is passed to --outdir option, the outputs will be saved as .npy files named after output tensor keys under the given directory.\nUse --overwrite to overwrite existing output files."
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html",
    "href": "v1/guide/tensorflow/eager_execution.html",
    "title": "Eager execution",
    "section": "",
    "text": "TensorFlow’s eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well. To follow along with this guide, run the code samples below in an interactive R interpreter.\nEager execution is a flexible machine learning platform for research and experimentation, providing:\nEager execution supports most TensorFlow operations and GPU acceleration.\nNote: Some models may experience increased overhead with eager execution enabled. Performance improvements are ongoing, but please file a bug if you find a problem and share your benchmarks."
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#setup-and-basic-usage",
    "href": "v1/guide/tensorflow/eager_execution.html#setup-and-basic-usage",
    "title": "Eager execution",
    "section": "Setup and basic usage",
    "text": "Setup and basic usage\n\nlibrary(tensorflow)\nlibrary(tfautograph)\nlibrary(keras)\nlibrary(tfdatasets)\n\nIn Tensorflow 2.0, eager execution is enabled by default.\n\ntf$executing_eagerly()\n\nNow you can run TensorFlow operations and the results will return immediately:\n\nx <- matrix(2, ncol = 1, nrow = 1)\nm <- tf$matmul(x, x)\nm\n\nEnabling eager execution changes how TensorFlow operations behave—now they immediately evaluate and return their values to R tf$Tensor objects reference concrete values instead of symbolic handles to nodes in a computational graph. Since there isn’t a computational graph to build and run later in a session, it’s easy to inspect results using print() or a debugger. Evaluating, printing, and checking tensor values does not break the flow for computing gradients.\nEager execution works nicely with R. TensorFlow math operations convert R objects and R arrays to tf$Tensor objects. The as.array method returns the object’s value as an R array.\n\na <- tf$constant(matrix(c(1,2,3,4), ncol = 2))\na\n\n\n# Broadcasting support\nb <- tf$add(a, 1)\nb\n\n\n# Operator overloading is supported\na * b\n\n\n# Obtain an R value from a Tensor\nas.array(a)"
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#dynamic-control-flow",
    "href": "v1/guide/tensorflow/eager_execution.html#dynamic-control-flow",
    "title": "Eager execution",
    "section": "Dynamic control flow",
    "text": "Dynamic control flow\nA major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write fizzbuzz:\n\nfizzbuzz <- autograph(function(max_num) {\n  counter <- tf$constant(0)\n  max_num <- tf$convert_to_tensor(max_num)\n  for (num in (tf$range(max_num) + 1)) {\n    if ((num %% 3 == 0) & (num %% 5 == 0)) {\n      tf$print(\"FizzBuzz\")\n    } else if (num %% 3 == 0) {\n      tf$print(\"Fizz\")\n    } else if (num %% 5 == 0) {\n      tf$print(\"Buzz\")\n    } else {\n      tf$print(num)\n    }\n    counter <- counter + 1\n  }\n})\nfizzbuzz(15)\n\nThis has conditionals that depend on tensor values and it prints these values at runtime."
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#eager-training",
    "href": "v1/guide/tensorflow/eager_execution.html#eager-training",
    "title": "Eager execution",
    "section": "Eager training",
    "text": "Eager training\n\nComputing gradients\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks. During eager execution, use tf$GradientTape to trace operations for computing gradients later.\nYou can use tf$GradientTape to train and/or compute gradients in eager. It is especially useful for complicated training loops.\nSince different operations can occur during each call, all forward-pass operations get recorded to a “tape”. To compute the gradient, play the tape backwards and then discard. A particular tf$GradientTape can only compute one gradient; subsequent calls throw a runtime error.\n\nw <- tf$Variable(1)\nwith(tf$GradientTape() %as% tape, {\n  loss <- w * w\n})\ngrad <- tape$gradient(loss, w)\ngrad\n\n\n\nTrain a model\nThe following example creates a multi-layer model that classifies the standard MNIST handwritten digits. It demonstrates the optimizer and layer APIs to build trainable graphs in an eager execution environment.\n\n# Fetch and format the mnist data\nmnist <- dataset_mnist()\ndataset <- tensor_slices_dataset(mnist$train) %>% \n  dataset_map(function(x) {\n    x$x <- tf$cast(x$x, tf$float32)/255\n    x$x <- tf$expand_dims(x$x, axis = -1L)\n    unname(x)\n  }) %>% \n  dataset_shuffle(1000) %>% \n  dataset_batch(32)\n\ndataset\n\n\nmnist_model <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation= \"relu\",\n                input_shape = shape(NULL, NULL, 1)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_global_average_pooling_2d() %>% \n  layer_dense(units = 10)\n\nEven without training, call the model and inspect the output in eager execution:\n\nel <- dataset %>% \n  dataset_take(1) %>% \n  dataset_collect()\nmnist_model(el[[1]])\n\nWhile keras models have a builtin training loop (using the fit method), sometimes you need more customization. Here’s an example, of a training loop implemented with eager:\n\noptimizer <- optimizer_adam()\nloss_object <- tf$keras$losses$SparseCategoricalCrossentropy(from_logits = TRUE)\n\nloss_history <- list()\n\nNote: Use the assert functions in tf$debugging to check if a condition holds up. This works in eager and graph execution.\n\ntrain_step <- function(images, labels) {\n  with(tf$GradientTape() %as% tape, {\n    logits <- mnist_model(images, training = TRUE)\n    tf$debugging$assert_equal(logits$shape, shape(32, 10))\n    loss_value <- loss_object(labels, logits)\n  })\n  loss_history <<- append(loss_history, loss_value)\n  grads <- tape$gradient(loss_value, mnist_model$trainable_variables)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, mnist_model$trainable_variables))\n  )\n}\n\ntrain <- autograph(function() {\n  for (epoch in seq_len(3)) {\n    for (batch in dataset) {\n      train_step(batch[[1]], batch[[2]])\n    }\n    tf$print(\"Epoch\", epoch, \"finished.\")\n  }\n})\n\ntrain()\n\n\nhistory <- loss_history %>% \n  purrr::map(as.numeric) %>% \n  purrr::flatten_dbl()\nggplot2::qplot(x = seq_along(history), y = history, geom = \"line\")\n\n\n\nVariables and optimizers\ntf$Variable objects store mutable tf$Tensor-like values accessed during training to make automatic differentiation easier.\nThe collections of variables can be encapsulated into layers or models, along with methods that operate on them. See Custom Keras layers and models for details. The main difference between layers and models is that models add methods like fit, evaluate, and save.\nFor example, the automatic differentiation example above can be rewritten:\n\nLinear <- function() {\n  keras_model_custom(model_fn = function(self) {\n    self$w <- tf$Variable(5, name = \"weight\")\n    self$b <- tf$Variable(10, name = \"bias\")\n    function(inputs, mask = NULL, training = TRUE) {\n      inputs*self$w + self$b\n    }\n  }) \n}\n\n\n# A toy dataset of points around 3 * x + 2\nNUM_EXAMPLES <- 2000\ntraining_inputs <- tf$random$normal(shape = shape(NUM_EXAMPLES))\nnoise <- tf$random$normal(shape = shape(NUM_EXAMPLES))\ntraining_outputs <- training_inputs * 3 + 2 + noise\n\n# The loss function to be optimized\nloss <- function(model, inputs, targets) {\n  error <- model(inputs) - targets\n  tf$reduce_mean(tf$square(error))\n}\n\ngrad <- function(model, inputs, targets) {\n  with(tf$GradientTape() %as% tape, {\n    loss_value <- loss(model, inputs, targets)\n  })\n  tape$gradient(loss_value, list(model$w, model$b))\n}\n\nNext:\n\nCreate the model.\nThe Derivatives of a loss function with respect to model parameters.\nA strategy for updating the variables based on the derivatives.\n\n\nmodel <- Linear()\noptimizer <- optimizer_sgd(lr = 0.01)\n\ncat(\"Initial loss: \", as.numeric(loss(model, training_inputs, training_outputs), \"\\n\"))\n\nfor (i in seq_len(300)) {\n  grads <- grad(model, training_inputs, training_outputs)\n  optimizer$apply_gradients(purrr::transpose(\n    list(grads, list(model$w, model$b))\n  ))\n  if (i %% 20 == 0)\n    cat(\"Loss at step \", i, \": \", as.numeric(loss(model, training_inputs, training_outputs)), \"\\n\")\n}\n\n\nmodel$w\nmodel$b\n\nNote: Variables persist until the last reference to the object is removed, and is the variable is deleted.\n\n\nObject-based saving\nA Keras model includes a convinient save_weights method allowing you to easily create a checkpoint:\n\nsave_model_weights_tf(model, \"weights\")\nload_model_weights_tf(model, filepath = \"weights\")\n\nUsing tf$train$Checkpoint you can take full control over this process.\nThis section is an abbreviated version of the guide to training checkpoints.\n\nx <- tf$Variable(10)\ncheckpoint <- tf$train$Checkpoint(x = x)\n\n\nx$assign(2) # Assign a new value to the variables and save.\ncheckpoint_path <- \"ckpt/\"\ncheckpoint$save(checkpoint_path)\n\n\nx$assign(11) # Change the variable after saving.\ncheckpoint$restore(tf$train$latest_checkpoint(checkpoint_path))\nx\n\nTo save and load models, tf$train$Checkpoint stores the internal state of objects, without requiring hidden variables. To record the state of a model, an optimizer, and a global step, pass them to a tf$train$Checkpoint:\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_global_average_pooling_2d() %>% \n  layer_dense(units = 10)\n\noptimizer <- optimizer_adam(lr = 0.001)\n\ncheckpoint_dir <- 'path/to/model_dir'\nif (!dir.exists(checkpoint_dir))\n  dir.create(checkpoint_dir, recursive = TRUE)\n\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\n\nroot <- tf$train$Checkpoint(optimizer = optimizer, model = model)\n\nroot$save(checkpoint_prefix)\nroot$restore(tf$train$latest_checkpoint(checkpoint_dir))\n\nNote: In many training loops, variables are created after tf\\(train\\)Checkpoint.restore is called. These variables will be restored as soon as they are created, and assertions are available to ensure that a checkpoint has been fully loaded. See the guide to training checkpoints for details.\n\n\nObject-oriented metrics\ntf$keras$metrics are stored as objects. Update a metric by passing the new data to the callable, and retrieve the result using the tf$keras$metrics$result method, for example:\n\nm <- tf$keras$metrics$Mean(\"loss\")\nm(0)\nm(5)\nm$result()\nm(c(8, 9))\nm$result()\n\n\n\nSummaries and TensorBoard\nTensorBoard is a visualization tool for understanding, debugging and optimizing the model training process. It uses summary events that are written while executing the program.\nYou can use tf$summary to record summaries of variable in eager execution. For example, to record summaries of loss once every 100 training steps:\n\nlogdir <- \"./tb/\"\nwriter = tf$summary$create_file_writer(logdir)\ntensorboard(log_dir = logdir) # This will open a browser window pointing to Tensorboard\n\nwith(writer$as_default(), {\n  for (step in seq_len(1000)) {\n    # Calculate loss with your real train function.\n    loss = 1 - 0.001 * step\n    if (step %% 100 == 0)\n      tf$summary$scalar('loss', loss, step=step)\n  }\n})"
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#advanced-automatic-differentiation-topics",
    "href": "v1/guide/tensorflow/eager_execution.html#advanced-automatic-differentiation-topics",
    "title": "Eager execution",
    "section": "Advanced automatic differentiation topics",
    "text": "Advanced automatic differentiation topics\n\nDynamic models\ntf$GradientTape can also be used in dynamic models. This example for a backtracking line search algorithm looks like normal R code, except there are gradients and is differentiable, despite the complex control flow:\n\nline_search_step <- tf$custom_gradient(autograph(function(fn, init_x, rate = 1) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(init_x)\n    value <- fn(init_x)\n  })\n  grad <- tape$gradient(value, init_x)\n  grad_norm <- tf$reduce_sum(grad * grad)\n  init_value <- value\n  while(value > (init_value - rate * grad_norm)) {\n    x <- init_x - rate * grad\n    value <- fn(x)\n    rate = rate/2\n  }\n  list(x, value)\n}))\n\n\n\nCustom gradients\nCustom gradients are an easy way to override gradients. Within the forward function, define the gradient with respect to the inputs, outputs, or intermediate results. For example, here’s an easy way to clip the norm of the gradients in the backward pass:\n\nclip_gradient_by_norm <- function(x, norm) {\n  y <- tf$identity(x)\n  grad_fn <- function(dresult) {\n    list(tf$clip_by_norm(dresult, norm), NULL)\n  }\n  list(y, grad_fn)\n}\n\nCustom gradients are commonly used to provide a numerically stable gradient for a sequence of operations:\n\nlog1pexp <- function(x) {\n  tf$math$log(1 + tf$exp(x))\n}\n\ngrad_log1pexp <- function(x) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(x)\n    value <- log1pexp(x)\n  })\n  tape$gradient(value, x)\n}\n\n\n# The gradient computation works fine at x = 0.\ngrad_log1pexp(tf$constant(0))\n\n\n# However, x = 100 fails because of numerical instability.\ngrad_log1pexp(tf$constant(100))\n\nHere, the log1pexp function can be analytically simplified with a custom gradient. The implementation below reuses the value for tf$exp(x) that is computed during the forward pass—making it more efficient by eliminating redundant calculations:\n\nlog1pexp <- tf$custom_gradient(f = function(x) {\n  e <- tf$exp(x)\n  grad_fn <- function(dy) {\n    dy * (1 - 1/(e + e))\n  }\n  list(tf$math$log(1 + e), grad_fn)\n})\n\ngrad_log1pexp <- function(x) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(x)\n    value <- log1pexp(x)\n  })\n  tape$gradient(value, x)\n}\n\n\n# As before, the gradient computation works fine at x = 0.\ngrad_log1pexp(tf$constant(0))\n\n\n# And the gradient computation also works at x = 100.\ngrad_log1pexp(tf$constant(100))"
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#performance",
    "href": "v1/guide/tensorflow/eager_execution.html#performance",
    "title": "Eager execution",
    "section": "Performance",
    "text": "Performance\nComputation is automatically offloaded to GPUs during eager execution. If you want control over where a computation runs you can enclose it in a tf$device('/gpu:0') block (or the CPU equivalent):\n\nfun <- function(device, steps = 200) {\n  with(tf$device(device), {\n    x <- tf$random$normal(shape = shape)\n    for (i in seq_len(steps)) {\n      tf$matmul(x, x)  \n    }  \n  })\n}\nmicrobenchmark::microbenchmark(\n  fun(\"/cpu:0\"),\n  fun(\"/gpu:0\")\n)\n\n\n# Unit: milliseconds\n#           expr      min        lq      mean    median        uq       max neval\n#  fun(\"/cpu:0\") 1117.596 1135.5450 1165.6269 1157.2208 1195.1529 1300.2236   100\n#  fun(\"/gpu:0\")  112.888  121.7164  127.8525  126.6708  132.0415  228.1009   100\n\nA tf$Tensor object can be copied to a different device to execute its operations:\n\nx <- tf$random$normal(shape = shape(10,10))\n\nx_gpu0 <- x$gpu()\nx_cpu <- x$cpu()\n\ntf$matmul(x_cpu, x_cpu)    # Runs on CPU\ntf$matmul(x_gpu0, x_gpu0)  # Runs on GPU:0\n\n\nBenchmarks\nFor compute-heavy models, such as ResNet50 training on a GPU, eager execution performance is comparable to tf_function execution. But this gap grows larger for models with less computation and there is work to be done for optimizing hot code paths for models with lots of small operations."
  },
  {
    "objectID": "v1/guide/tensorflow/eager_execution.html#work-with-functions",
    "href": "v1/guide/tensorflow/eager_execution.html#work-with-functions",
    "title": "Eager execution",
    "section": "Work with functions",
    "text": "Work with functions\nWhile eager execution makes development and debugging more interactive, TensorFlow 1.x style graph execution has advantages for distributed training, performance optimizations, and production deployment. To bridge this gap, TensorFlow 2.0 introduces functions via the tf_function API. For more information, see the tf_function guide."
  },
  {
    "objectID": "v1/guide/tensorflow/ragged_tensors.html",
    "href": "v1/guide/tensorflow/ragged_tensors.html",
    "title": "Ragged tensors",
    "section": "",
    "text": "Your data comes in many shapes; your tensors should too. Ragged tensors are the TensorFlow equivalent of nested variable-length lists. They make it easy to store and process data with non-uniform shapes, including:\n\nVariable-length features, such as the set of actors in a movie.\nBatches of variable-length sequential inputs, such as sentences or video clips.\nHierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words.\nIndividual fields in structured inputs, such as protocol buffers.\n\n\n\nRagged tensors are supported by more than a hundred TensorFlow operations, including math operations (such as tf$add and tf$reduce_mean), array operations (such as tf$concat and tf$tile), string manipulation ops (such as tf$substr), and many others:\n\nlibrary(tensorflow)\ndigits <- tf$ragged$constant(\n  list(list(3, 1, 4, 1), list(), list(5, 9, 2), list(6), list())\n)\nwords = tf$ragged$constant(\n  list(list(\"So\", \"long\"), list(\"thanks\", \"for\", \"all\", \"the\", \"fish\"))\n)\ntf$add(digits, 3)\ntf$reduce_mean(digits, axis=1L)\ntf$concat(list(digits, list(list(5, 3))), axis=0L)\ntf$tile(digits, list(1L, 2L))\ntf$strings$substr(words, 0L, 2L)\n\nThere are also a number of methods and operations that are specific to ragged tensors, including factory methods, conversion methods, and value-mapping operations.\nAs with normal tensors, you can use R-style indexing to access specific slices of a ragged tensor. For more information, see the section on Indexing below.\n\ndigits[1,] # First row\n\n\ndigits[, 1:2]   # First two values in each row.\n\n\ndigits[, `-2:`]  # Last two values in each row.\n\nAnd just like normal tensors, you can use Python arithmetic and comparison operators to perform elementwise operations. For more information, see the section on Overloaded Operators below.\n\ndigits + 3\n\n\ndigits + tf$ragged$constant(list(list(1, 2, 3, 4), list(), list(5, 6, 7), list(8), list()))\n\nIf you need to perform an elementwise transformation to the values of a RaggedTensor, you can use tf$ragged$map_flat_values, which takes a function plus one or more arguments, and applies the function to transform the RaggedTensor’s values.\n\ntimes_two_plus_one <- function(x) {\n  2*x + 1\n}\ntf$ragged$map_flat_values(times_two_plus_one, digits)\n\n\n\n\nThe simplest way to construct a ragged tensor is using tf$ragged$constant, which builds the RaggedTensor corresponding to a given nested list:\n\nsentences <- tf$ragged$constant(list(\n    list(\"Let's\", \"build\", \"some\", \"ragged\", \"tensors\", \"!\"),\n    list(\"We\", \"can\", \"use\", \"tf.ragged.constant\", \".\")))\n\n\nparagraphs <- tf$ragged$constant(list(\n    list(list('I', 'have', 'a', 'cat'), list('His', 'name', 'is', 'Mat')),\n    list(list('Do', 'you', 'want', 'to', 'come', 'visit'), list(\"I'm\", 'free', 'tomorrow'))\n))\nparagraphs\n\nRagged tensors can also be constructed by pairing flat values tensors with row-partitioning tensors indicating how those values should be divided into rows, using factory classmethods such as tf$RaggedTensor$from_value_rowids, tf$RaggedTensor$from_row_lengths, and tf$RaggedTensor$from_row_splits.\n\n\nIf you know which row each value belongs in, then you can build a RaggedTensor using a value_rowids row-partitioning tensor:\n\n\ntf$RaggedTensor$from_value_rowids(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    value_rowids=as.integer(c(0, 0, 0, 0, 2, 2, 2, 3)))\n\n\n\n\nIf you know how long each row is, then you can use a row_lengths row-partitioning tensor:\n\n\ntf$RaggedTensor$from_row_lengths(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    row_lengths=as.integer(c(4, 0, 3, 1)))\n\n\n\n\nIf you know the index where each row starts and ends, then you can use a row_splits row-partitioning tensor:\n\n\n\nrow_splits\n\n\n\ntf$RaggedTensor$from_row_splits(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    row_splits=as.integer(c(0, 4, 4, 7, 8)))\n\nSee the tf.RaggedTensor class documentation for a full list of factory methods.\n\n\n\n\nAs with normal Tensors, the values in a RaggedTensor must all have the same type; and the values must all be at the same nesting depth (the rank of the tensor):\n\ntf$ragged$constant(list(list(\"Hi\"), list(\"How\", \"are\", \"you\"))) # ok: type=string, rank=2\n\n\ntf$ragged$constant(list(list(\"one\", \"two\"), list(3, 4))) # bad: multiple types\n\n\ntf$ragged$constant(list(\"A\", list(\"B\", \"C\"))) # bad: multiple nesting depths\n\nThis is a small introduction to Ragged Tensors in TensorFlow. See the complete tutorial (in Python) here."
  },
  {
    "objectID": "v1/guide/tensorflow/tensors.html",
    "href": "v1/guide/tensorflow/tensors.html",
    "title": "TensorFlow tensors",
    "section": "",
    "text": "TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\nWhen writing a TensorFlow program, the main object you manipulate and pass around is the tf$Tensor. A tf$Tensor object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of tf$Tensor objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\nA tf$Tensor has the following properties:\nEach element in the Tensor has the same data type, and the data type is always known. The shape (that is, the number of dimensions it has and the size of each dimension) might be only partially known. Most operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it’s only possible to find the shape of a tensor at graph execution time.\nSome types of tensors are special, and these will be covered in other units of the TensorFlow guide. The main ones are:\nWith the exception of tf.Variable, the value of a tensor is immutable, which means that in the context of a single execution tensors only have a single value. However, evaluating the same tensor twice can return different values; for example that tensor can be the result of reading data from disk, or generating a random number."
  },
  {
    "objectID": "v1/guide/tensorflow/tensors.html#rank",
    "href": "v1/guide/tensorflow/tensors.html#rank",
    "title": "TensorFlow tensors",
    "section": "Rank",
    "text": "Rank\nThe rank of a tf$Tensor object is its number of dimensions. Synonyms for rank include order or degree or n-dimension. Note that rank in TensorFlow is not the same as matrix rank in mathematics. As the following table shows, each rank in TensorFlow corresponds to a different mathematical entity:\n\n\n\nRank\nMath entity\n\n\n\n\n0\nScalar (magnitude only)\n\n\n1\nVector (magnitude and direction)\n\n\n2\nMatrix (table of numbers)\n\n\n3\n3-Tensor (cube of numbers)\n\n\nn\nn-Tensor (you get the idea)\n\n\n\n\nRank 0\nThe following snippet demonstrates creating a few rank 0 variables:\n\nlibrary(tensorflow)\nmammal <- tf$Variable(\"Elephant\", tf$string)\nignition <- tf$Variable(451, tf$int16)\nfloating <- tf$Variable(3.14159265359, tf$float64)\nits_complicated <- tf$Variable(12.3 - 4.85i, tf$complex64)\n\nNote: A string is treated as a single object in TensorFlow, not as a sequence of characters. It is possible to have scalar strings, vectors of strings, etc.\n\n\nRank 1\nTo create a rank 1 tf.Tensor object, you can pass a list of items as the initial value. For example:\n\nmystr <- tf$Variable(c(\"Hello\"), tf$string)\ncool_numbers <- tf$Variable(c(3.14159, 2.71828), tf$float32)\nfirst_primes <- tf$Variable(c(2, 3, 5, 7, 11), tf$int32)\nits_very_complicated <- tf$Variable(c(12.3 - 4.85i, 7.5 - 6.23i), tf$complex64)\n\n\n\nHigher ranks\nA rank 2 tf$Tensor object consists of at least one row and at least one column:\n\nmymat <- tf$Variable(list(c(7, 2),c(11, 3)), tf$int16)\n\nHigher-rank Tensors, similarly, consist of an n-dimensional array. For example, during image processing, many tensors of rank 4 are used, with dimensions corresponding to example-in-batch, image height, image width, and color channel.\n\nmy_image <- tf$zeros(shape = shape(10, 299, 299, 3)) # batch x height x width x color\n\n\n\nGetting a tf$Tensor object’s rank\nTo determine the rank of a tf$Tensor object, call the tf$rank method. For example, the following method programmatically determines the rank of the tf$Tensor defined in the previous section:\n\nr <- tf$rank(my_image)\nr\n\n\n\nReferring to tf$Tensor slices\nTensor elements can be extracted either by using functions like tf$gather() and tf$slice(), or by using [ syntax.\nExtracting tensor elements with [ in R is similar to extracting elements from standard R arrays, albeit with some minor differences in capabilities. In contrast to most tf$ functions, [ defaults to R style 1-based rather than 0-based indexes. Currently, only numeric indexes in [ are supported (no logical or character indexes)\nExtracting works identically to R arrays if the slicing index is missing, supplied as a scalar, or as a sequence (e.g, created by : or seq_len())\nx[,1] )      # all rows, first column\nx[1:2,] )    # first two rows, all columns\nx[,1, drop = FALSE] ) # all rows, first column, but preserving the tensor rank\n[ also supports slices with a strided step, which can be specified in traditional R style with seq() or with a python-style second colon. If you are unfamiliar with python-style strided step syntax, see here for a quick primer\nx[, seq(1, 5, by = 2)]  # R style\nx[, 1:5:2]              # Equivalent python-style strided step \nMissing arguments for python syntax are valid, but they must be supplied as NULL or whole expression must by backticked.\nx[, `::2`] \nx[, NULL:NULL:2] \nx[, `2:`] \n[ also accepts tf$newaxis and all_dims() as arguments\nx[,, tf$newaxis]\nx[all_dims(), 1] # all_dims expands to the shape of the tensor\nAn important difference between extracting R arrays and tensorflow tensors with [ is how negative numbers are interpreted. For tensorflow tensors, negative numbers are interpreted as selecting elements by counting from tail (e.g, they are interpreted python-style).\nx[-1,]  # the last row\nTensors are accepted by [ as well, but note that tensors suplied to [ are not translated from R to python. Meaning that tensors are interpreted as 0-based, and if slicing a range with :, then the returned arrays is exclusive of the upper bound.\nx[, tf$constant(1L)] # second column\nx[, tf$constant(0L):tf$constant(2L)] # first two columns\nIf you are translating existing python code to R, note that you can set an option to have all [ arguments be interpreted pure-python style by setting options(tensorflow.extract.style = \"python\"). See ?`[.tensorflow.tensor` for additional options and details."
  },
  {
    "objectID": "v1/guide/tensorflow/tensors.html#shape",
    "href": "v1/guide/tensorflow/tensors.html#shape",
    "title": "TensorFlow tensors",
    "section": "Shape",
    "text": "Shape\nThe shape of a tensor is the number of elements in each dimension. TensorFlow automatically infers shapes during graph construction. These inferred shapes might have known or unknown rank. If the rank is known, the sizes of each dimension might be known or unknown.\nThe TensorFlow documentation uses three notational conventions to describe tensor dimensionality: rank, shape, and dimension number. The following table shows how these relate to one another:\n\n\n\nRank\nShape\nDimension number\nExample\n\n\n\n\n0\n[]\n0-D\nA 0-D tensor. A scalar.\n\n\n1\n[D0]\n1-D\nA 1-D tensor with shape [5].\n\n\n2\n[D0, D1]\n2-D\nA 2-D tensor with shape [3, 4].\n\n\n3\n[D0, D1, D2]\n3-D\nA 3-D tensor with shape [1, 4, 3].\n\n\nn\n[D0, D1, … Dn-1]\nn-D\nA tensor with shape [D0, D1, … Dn-1].\n\n\n\nShapes can be represented via lists of ints, or with the tf$TensorShape.\n\nGetting a tf$Tensor object’s shape\nThere are two ways of accessing the shape of a tf$Tensor. While building the graph, it is often useful to ask what is already known about a tensor’s shape. This can be done by reading the shape property of a tf$Tensor object. This method returns a TensorShape object, which is a convenient way of representing partially-specified shapes (since, when building the graph, not all shapes will be fully known).\nIt is also possible to get a tf$Tensor that will represent the fully-defined shape of another tf$Tensor at runtime. This is done by calling the tf$shape operation. This way, you can build a graph that manipulates the shapes of tensors by building other tensors that depend on the dynamic shape of the input tf$Tensor.\nFor example, here is how to make a vector of zeros with the same size as the number of columns in a given matrix:\n\nzeros <- tf$zeros(my_image$shape[2])\n\n\n\nChanging the shape of a tf$Tensor\nThe number of elements of a tensor is the product of the sizes of all its shapes. The number of elements of a scalar is always 1. Since there are often many different shapes that have the same number of elements, it’s often convenient to be able to change the shape of a tf.Tensor, keeping its elements fixed. This can be done with tf.reshape.\nThe following examples demonstrate how to reshape tensors:\n\nrank_three_tensor <- tf$ones(shape(3, 4, 5))\nmatrix <- tf$reshape(rank_three_tensor, shape(6, 10))  # Reshape existing content into\n                                                 # a 6x10 matrix\nmatrixB <- tf$reshape(matrix, shape(3, -1))  #  Reshape existing content into a 3x20\n                                       # matrix. -1 tells reshape to calculate\n                                       # the size of this dimension.\nmatrixAlt <- tf$reshape(matrixB, shape(4, 3, -1))  # Reshape existing content into a\n                                             #4x3x5 tensor\n\n# Note that the number of elements of the reshaped Tensors has to match the\n# original number of elements. Therefore, the following example generates an\n# error because no possible value for the last dimension will match the number\n# of elements."
  },
  {
    "objectID": "v1/guide/tensorflow/tensors.html#data-types",
    "href": "v1/guide/tensorflow/tensors.html#data-types",
    "title": "TensorFlow tensors",
    "section": "Data types",
    "text": "Data types\nIn addition to dimensionality, Tensors have a data type. Refer to the tf$DType page for a complete list of the data types.\nIt is not possible to have a tf$Tensor with more than one data type. It is possible, however, to serialize arbitrary data structures as strings and store those in tf$Tensors.\nIt is possible to cast tf$Tensors from one datatype to another using tf$cast:\n\n# Cast a constant integer tensor into floating point.\nfloat_tensor <- tf$cast(tf$constant(c(1L, 2L, 3L)), dtype=tf$float32)\n\nTo inspect a tf$Tensor’s data type use the Tensor.dtype property.\nWhen creating a tf$Tensor from an R object you may optionally specify the datatype. If you don’t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts R integers to tf$int32 and R floating point numbers to tf$float32. Otherwise TensorFlow uses the same rules numpy uses when converting to arrays."
  },
  {
    "objectID": "v1/guide/tensorflow/variable.html",
    "href": "v1/guide/tensorflow/variable.html",
    "title": "TensorFlow variables",
    "section": "",
    "text": "A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\nVariables are manipulated via the tf$Variable class. A tf$Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like Keras use tf$Variable to store model parameters. This guide covers how to create, update, and manage tf$Variables in TensorFlow."
  },
  {
    "objectID": "v1/guide/tensorflow/variable.html#create-a-variable",
    "href": "v1/guide/tensorflow/variable.html#create-a-variable",
    "title": "TensorFlow variables",
    "section": "Create a variable",
    "text": "Create a variable\nTo create a variable, simply provide the initial value\n\nlibrary(tensorflow)\nmy_variable <- tf$Variable(tf$zeros(shape(1,2,3)))\n\nThis creates a variable which is a three-dimensional tensor with shape [1, 2, 3] filled with zeros. This variable will, by default, have the dtype tf$float32. The dtype is, if not specified, inferred from the initial value.\nIf there’s a tf$device scope active, the variable will be placed on that device; otherwise the variable will be placed on the “fastest” device compatible with its dtype (this means most variables are automatically placed on a GPU if one is available). For example, the following snippet creates a variable named v and places it on the second GPU device:\n\nwith(tf$device(\"/device:GPU:1\"), {\n  v <- tf$Variable(tf$zeros(c(10, 10)))\n})\n\nIdeally though you should use the tf$distribute API, as that allows you to write your code once and have it work under many different distributed setups."
  },
  {
    "objectID": "v1/guide/tensorflow/variable.html#use-a-variable",
    "href": "v1/guide/tensorflow/variable.html#use-a-variable",
    "title": "TensorFlow variables",
    "section": "Use a variable",
    "text": "Use a variable\nTo use the value of a tf$Variable in a TensorFlow graph, simply treat it like a normal tf$Tensor:\n\nv <- tf$Variable(0)\nw <- v + 1 # w is a tf.Tensor which is computed based on the value of v.\n# Any time a variable is used in an expression it gets automatically\n# converted to a tf$Tensor representing its value.\n\nTo assign a value to a variable, use the methods assign, assign_add, and friends in the tf$Variable class. For example, here is how you can call these methods:\n\nv <- tf$Variable(0)\nv$assign_add(1)\n\nMost TensorFlow optimizers have specialized ops that efficiently update the values of variables according to some gradient descent-like algorithm. See tf$keras$optimizers$Optimizer for an explanation of how to use optimizers.\nYou can also explicitly read the current value of a variable, using read_value:\n\nv <- tf$Variable(0)\nv$assign_add(1)\nv$read_value()\n\nWhen the last reference to a tf$Variable goes out of scope its memory is freed."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html",
    "href": "v1/guide/tfdatasets/feature_columns.html",
    "title": "Feature columns",
    "section": "",
    "text": "This document is an adaptation of the official TensorFlow Feature Columns guide.\nThis document details feature columns and how they can be used as inputs to neural networks using TensorFlow. Feature columns are very rich, enabling you to transform a diverse range of raw data into formats that neural networks can use, allowing easy experimentation.\nWhat kind of data can a deep neural network operate on? The answer is, of course, numbers (for example, tf$float32). After all, every neuron in a neural network performs multiplication and addition operations on weights and input data. Real-life input data, however, often contains non-numerical (categorical) data. For example, consider a product_class feature that can contain the following three non-numerical values:\nML models generally represent categorical values as simple vectors in which a 1 represents the presence of a value and a 0 represents the absence of a value. For example, when product_class is set to sports, an ML model would usually represent product_class as [0, 0, 1], meaning:\nSo, although raw data can be numerical or categorical, an ML model represents all features as numbers.\nThis document explains nine of the feature columns available in tfdatasets. As the following figure shows, all nine functions return either a categorical_column or a dense_column object, except bucketized_column, which inherits from both classes:\nLet’s look at these functions in more detail."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#feature-spec-interface",
    "href": "v1/guide/tfdatasets/feature_columns.html#feature-spec-interface",
    "title": "Feature columns",
    "section": "Feature spec interface",
    "text": "Feature spec interface\nWe are going to use the feature_spec interface in the examples. The feature_spec interface is an abstraction that makes it easier to define feature columns in R.\nYou can initialize a feature_spec with:\n\nlibrary(tfdatasets)\nhearts_dataset <- tensor_slices_dataset(hearts)\nspec <- feature_spec(hearts_dataset, target ~ .)\n\nWe then add steps in order to define feature_columns. Read the feature_spec vignette for more information."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#numeric-column",
    "href": "v1/guide/tfdatasets/feature_columns.html#numeric-column",
    "title": "Feature columns",
    "section": "Numeric column",
    "text": "Numeric column\nWe use step_numeric_column to add numeric columns to our spec.\n\nspec %>% \n  step_numeric_column(age)\n\nBy default, a numeric column creates a single value (scalar). Use the shape argument to specify another shape. For example:\n\n# Represent a 10-element vector in which each cell contains a tf$float32.\nspec %>% \n  step_numeric_column(bowling, shape = 10)\n\n# Represent a 10x5 matrix in which each cell contains a tf$float32.\nspec %>% \n  step_numeric_column(my_matrix, shape = c(10, 5))\n\nA nice feature of step_numeric_columnis that you can also specify normalizer functions. When using a scaler, the scaling constants will be learned from data when fitting the feature_spec.\n\n# use a function that defines tensorflow ops.\nspec %>% \n  step_numeric_column(age, normalizer_fn = function(x) (x-10)/5)\n\n# use a scaler\nspec %>% \n  step_numeric_column(age, normalizer_fn = scaler_standard())"
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#bucketized-column",
    "href": "v1/guide/tfdatasets/feature_columns.html#bucketized-column",
    "title": "Feature columns",
    "section": "Bucketized column",
    "text": "Bucketized column\nOften, you don’t want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. To do so, create a bucketized_column. For example, consider raw data that represents the year a house was built. Instead of representing that year as a scalar numeric column, we could split the year into the following four buckets:\n\n\n\nDividing year data into four buckets.\n\n\nWhy would you want to split a number — a perfectly valid input to your model — into a categorical value? Well, notice that the categorization splits a single input number into a four-element vector. Therefore, the model now can learn four individual weights rather than just one; four weights creates a richer model than one weight. More importantly, bucketizing enables the model to clearly distinguish between different year categories since only one of the elements is set (1) and the other three elements are cleared (0). For example, when we just use a single number (a year) as input, a linear model can only learn a linear relationship. So, bucketing provides the model with additional flexibility that the model can use to learn.\nThe following code demonstrates how to create a bucketized feature:\n\n# First, convert the raw input to a numeric column.\nspec <- spec %>% \n  step_numeric_column(age)\n\n# Then, bucketize the numeric column.\nspec <-  spec %>% \n  step_bucketized_column(age, boundaries = c(30, 50, 70))\n\nNote that specifying a three-element boundaries vector creates a four-element bucketized vector."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#categorical-vocabulary-column",
    "href": "v1/guide/tfdatasets/feature_columns.html#categorical-vocabulary-column",
    "title": "Feature columns",
    "section": "Categorical vocabulary column",
    "text": "Categorical vocabulary column\nWe cannot input strings directly to a model. Instead, we must first map strings to numeric or categorical values. Categorical vocabulary columns provide a good way to represent strings as a one-hot vector. For example:\n\n\n\nMapping string values to vocabulary columns.\n\n\nAs you can see, categorical vocabulary columns are kind of an enum version of categorical identity columns. tfdatasets provides two different functions to create categorical vocabulary columns:\n\nstep_categorical_column_with_vocabulary_list\nstep_categorical_column_with_vocabulary_file\n\ncategorical_column_with_vocabulary_list maps each string to an integer based on an explicit vocabulary list. For example:\n\nspec <- spec %>% \n  step_categorical_column_with_vocabulary_list(\n    thal, \n    vocabulary_list = c(\"fixed\", \"normal\", \"reversible\")\n  )\n\nNote that the vocabulary_list argument is optional in R and the vocabulary will be discovered when fitting the featture_spec which saves us a lot of typing.\nYou can also place the vocabulary in a separate file that should contain one line for each vocabulary element. You can then use:\n\nspec <- spec %>% \n  step_categorical_column_with_vocabulary_file(thal, vocabulary_file = \"thal.txt\")"
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#hashed-column",
    "href": "v1/guide/tfdatasets/feature_columns.html#hashed-column",
    "title": "Feature columns",
    "section": "Hashed Column",
    "text": "Hashed Column\nSo far, we’ve worked with a naively small number of categories. For example, our product_class example has only 3 categories. Often though, the number of categories can be so big that it’s not possible to have individual categories for each vocabulary word or integer because that would consume too much memory. For these cases, we can instead turn the question around and ask, “How many categories am I willing to have for my input?” In fact, the step_categorical_column_with_hash_bucket function enables you to specify the number of categories. For this type of feature column the model calculates a hash value of the input, then puts it into one of the hash_bucket_size categories using the modulo operator, as in the following pseudocode:\n# pseudocode\nfeature_id = hash(raw_feature) % hash_bucket_size\nThe code to add the feature_column to the feature_spec might look something like this:\n\nspec <- spec %>% \n  step_categorical_column_with_hash_bucket(thal, hash_bucket_size = 100)\n\nAt this point, you might rightfully think: “This is crazy!” After all, we are forcing the different input values to a smaller set of categories. This means that two probably unrelated inputs will be mapped to the same category, and consequently mean the same thing to the neural network. The following figure illustrates this dilemma, showing that kitchenware and sports both get assigned to category (hash bucket) 12:\n\n\n\nRepresenting data with hash buckets.\n\n\nAs with many counterintuitive phenomena in machine learning, it turns out that hashing often works well in practice. That’s because hash categories provide the model with some separation. The model can use additional features to further separate kitchenware from sports."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#crossed-column",
    "href": "v1/guide/tfdatasets/feature_columns.html#crossed-column",
    "title": "Feature columns",
    "section": "Crossed column",
    "text": "Crossed column\nCombining features into a single feature, better known as feature crosses, enables the model to learn separate weights for each combination of features.\nMore concretely, suppose we want our model to calculate real estate prices in Atlanta, GA. Real-estate prices within this city vary greatly depending on location. Representing latitude and longitude as separate features isn’t very useful in identifying real-estate location dependencies; however, crossing latitude and longitude into a single feature can pinpoint locations. Suppose we represent Atlanta as a grid of 100x100 rectangular sections, identifying each of the 10,000 sections by a feature cross of latitude and longitude. This feature cross enables the model to train on pricing conditions related to each individual section, which is a much stronger signal than latitude and longitude alone.\nThe following figure shows our plan, with the latitude & longitude values for the corners of the city in red text:\n\n\n\nMap of Atlanta. Imagine this map divided into 10,000 sections of equal size.\n\n\nFor the solution, we used a combination of the bucketized_column we looked at earlier, with the step_crossed_column function.\n\nspec <- feature_spec(dataset, target ~ latitute + longitude) %>% \n  step_numeric_column(latitude, longitude) %>% \n  step_bucketized_column(latitude, boundaries = c(latitude_edges)) %>% \n  step_bucketized_column(longitude, boundaries = c(longitude_edges)) %>% \n  step_crossed_column(latitude_longitude = c(latitude, longitude), hash_bucket_size = 100)\n\nYou may create a feature cross from either of the following:\n\nAny categorical column, except categorical_column_with_hash_bucket (since crossed_column hashes the input).\n\nWhen the feature columns latitude_bucket_fc and longitude_bucket_fc are crossed, TensorFlow will create (latitude_fc, longitude_fc) pairs for each example. This would produce a full grid of possibilities as follows:\n(0,0),  (0,1)...  (0,99)\n (1,0),  (1,1)...  (1,99)\n   ...     ...       ...\n(99,0), (99,1)...(99, 99)\nExcept that a full grid would only be tractable for inputs with limited vocabularies. Instead of building this, potentially huge, table of inputs, the crossed_column only builds the number requested by the hash_bucket_size argument. The feature column assigns an example to a index by running a hash function on the tuple of inputs, followed by a modulo operation with hash_bucket_size.\nAs discussed earlier, performing the hash and modulo function limits the number of categories, but can cause category collisions; that is, multiple (latitude, longitude) feature crosses will end up in the same hash bucket. In practice though, performing feature crosses still adds significant value to the learning capability of your models.\nSomewhat counterintuitively, when creating feature crosses, you typically still should include the original (uncrossed) features in your model (as in the preceding code snippet). The independent latitude and longitude features help the model distinguish between examples where a hash collision has occurred in the crossed feature."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_columns.html#indicator-and-embedding-columns",
    "href": "v1/guide/tfdatasets/feature_columns.html#indicator-and-embedding-columns",
    "title": "Feature columns",
    "section": "Indicator and embedding columns",
    "text": "Indicator and embedding columns\nIndicator columns and embedding columns never work on features directly, but instead take categorical columns as input.\nWhen using an indicator column, we’re telling TensorFlow to do exactly what we’ve seen in our categorical product_class example. That is, an indicator column treats each category as an element in a one-hot vector, where the matching category has value 1 and the rest have 0s:\n\n\n\nRepresenting data in indicator columns.\n\n\nHere’s how you create an indicator column:\n\nspec <- feature_spec(dataset, target ~ .) %>% \n  step_categorical_column_with_vocabulary_list(product_class) %>% \n  step_indicator_column(product_class)\n\nNow, suppose instead of having just three possible classes, we have a million. Or maybe a billion. For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using indicator columns.\nWe can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an embedding column represents that data as a lower-dimensional, ordinary vector in which each cell can contain any number, not just 0 or 1. By permitting a richer palette of numbers for every cell, an embedding column contains far fewer cells than an indicator column.\nLet’s look at an example comparing indicator and embedding columns. Suppose our input examples consist of different words from a limited palette of only 81 words. Further suppose that the data set provides the following input words in 4 separate examples:\n\n“dog”\n“spoon”\n“scissors”\n“guitar”\n\nIn that case, the following figure illustrates the processing path for embedding columns or indicator columns.\n\n\n\nAn embedding column stores categorical data in a lower-dimensional vector than an indicator column. (We just placed random numbers into the embedding vectors; training determines the actual numbers.)\n\n\nWhen an example is processed, one of the categorical_column_with_[…] functions maps the example string to a numerical categorical value. For example, a function maps “spoon” to [32]. (The 32 comes from our imagination — the actual values depend on the mapping function.) You may then represent these numerical categorical values in either of the following two ways:\n\nAs an indicator column. A function converts each numeric categorical value into an 81-element vector (because our palette consists of 81 words), placing a 1 in the index of the categorical value (0, 32, 79, 80) and a 0 in all the other positions.\nAs an embedding column. A function uses the numerical categorical values (0, 32, 79, 80) as indices to a lookup table. Each slot in that lookup table contains a 3-element vector.\n\nHow do the values in the embeddings vectors magically get assigned? Actually, the assignments happen during training. That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem. Embedding columns increase your model’s capabilities, since an embeddings vector learns new relationships between categories from the training data.\nWhy is the embedding vector size 3 in our example? Well, the following “formula” provides a general rule of thumb about the number of embedding dimensions:\nembedding_dimensions =  number_of_categories**0.25\nThat is, the embedding vector dimension should be the 4th root of the number of categories. Since our vocabulary size in this example is 81, the recommended number of dimensions is 3:\n3 =  81**0.25\n\nNote: This is just a general guideline; you can set the number of embedding dimensions as you please.\n\nCall step_embedding_column to create an embedding_column as suggested by the following snippet:\n\nspec <- feature_spec(dataset, target ~ .) %>% \n  step_categorical_column_with_vocabulary_list(product_class) %>% \n  step_embedding_column(product_class, dimension = 3)\n\nEmbeddings is a significant topic within machine learning. This information was just to get you started using them as feature columns."
  },
  {
    "objectID": "v1/guide/tfdatasets/feature_spec.html",
    "href": "v1/guide/tfdatasets/feature_spec.html",
    "title": "Feature Spec interface",
    "section": "",
    "text": "In this document we will demonstrate the basic usage of the feature_spec interface in tfdatasets.\nThe feature_spec interface is a user friendly interface to feature_columns. It allows us to specify column transformations and representations when working with structured data.\nWe will use the hearts dataset and it can be loaded with data(hearts).\n\nlibrary(tfdatasets)\nlibrary(dplyr)\ndata(hearts)\n\n\nhead(hearts)\n\nWe want to train a model to predict the target variable using Keras but, before that we need to prepare the data. We need to transform the categorical variables into some form of dense variable, we usually want to normalize all numeric columns too.\nThe feature spec interface works with data.frames or TensorFlow datasets objects.\n\nids_train <- sample.int(nrow(hearts), size = 0.75*nrow(hearts))\nhearts_train <- hearts[ids_train,]\nhearts_test <- hearts[-ids_train,]\n\nNow let’s start creating our feature specification:\n\nspec <- feature_spec(hearts_train, target ~ .)\n\nThe first thing we need to do after creating the feature_spec is decide on the variables’ types.\nWe can do this by adding steps to the spec object.\n\nspec <- spec %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal)\n\nThe following steps can be used to define the variable type:\n\nstep_numeric_column to define numeric variables\nstep_categorical_with_vocabulary_list for categorical variables with a fixed vocabulary\nstep_categorical_column_with_hash_bucket for categorical variables using the hash trick\nstep_categorical_column_with_identity to store categorical variables as integers\nstep_categorical_column_with_vocabulary_file when you have the possible vocabulary in a file\n\nWhen using step_categorical_column_with_vocabulary_list you can also provide a vocabulary argument with the fixed vocabulary. The recipe will find all the unique values in the dataset and use it as the vocabulary.\nYou can also specify a normalizer_fn to the step_numeric_column. In this case the variable will be transformed by the feature column. Note that the transformation will occur in the TensorFlow Graph, so it must use only TensorFlow ops. Like in the example we offer pre-made normalizers - and they will compute the normalizing function during the recipe preparation.\nYou can also use selectors like:\n\nstarts_with(), ends_with(), matches() etc. (from tidyselect)\nall_numeric() to select all numeric variables\nall_nominal() to select all strings\nhas_type(\"float32\") to select based on TensorFlow variable type.\n\nNow we can print the recipe:\n\nspec\n\nAfter specifying the types of the columns you can add transformation steps. For example you may want to bucketize a numeric column:\n\nspec <- spec %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))\n\nYou can also specify the kind of numeric representation that you want to use for your categorical variables.\n\nspec <- spec %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2)\n\nAnother common transformation is to add interactions between variables using crossed columns.\n\nspec <- spec %>% \n  step_crossed_column(thal_and_age = c(thal, bucketized_age), hash_bucket_size = 1000) %>% \n  step_indicator_column(thal_and_age)\n\nNote that the crossed_column is a categorical column, so we need to also specify what kind of numeric tranformation we want to use. Also note that we can name the transformed variables - each step uses a default naming for columns, eg. bucketized_age is the default name when you use step_bucketized_column with column called age.\nWith the above code we have created our recipe. Note we can also define the recipe by chaining a sequence of methods:\n\nspec <- feature_spec(hearts_train, target ~ .) %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2) %>% \n  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %>%\n  step_indicator_column(crossed_thal_bucketized_age)\n\nAfter defining the recipe we need to fit it. It’s when fitting that we compute the vocabulary list for categorical variables or find the mean and standard deviation for the normalizing functions. Fitting involves evaluating the full dataset, so if you have provided the vocabulary list and your columns are already normalized you can skip the fitting step (TODO).\nIn our case, we will fit the feature spec, since we didn’t specify the vocabulary list for the categorical variables.\n\nspec_prep <- fit(spec)\n\nAfter preparing we can see the list of dense features that were defined:\n\nstr(spec_prep$dense_features())\n\nNow we are ready to define our model in Keras. We will use a specialized layer_dense_features that knows what to do with the feature columns specification.\nWe also use a new layer_input_from_dataset that is useful to create a Keras input object copying the structure from a data.frame or TensorFlow dataset.\n\nlibrary(keras)\n\ninput <- layer_input_from_dataset(hearts_train %>% select(-target))\n\noutput <- input %>% \n  layer_dense_features(dense_features(spec_prep)) %>% \n  layer_dense(units = 32, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy, \n  optimizer = \"adam\", \n  metrics = \"binary_accuracy\"\n)\n\nWe can finally train the model on the dataset:\n\nhistory <- model %>% \n  fit(\n    x = hearts_train %>% select(-target),\n    y = hearts_train$target, \n    epochs = 15, \n    validation_split = 0.2\n  )\n\nplot(history)\n\nFinally we can make predictions in the test set and calculate performance metrics like the AUC of the ROC curve:\n\nhearts_test$pred <- predict(model, hearts_test)\nMetrics::auc(hearts_test$target, hearts_test$pred)"
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html",
    "href": "v1/guide/tfdatasets/introduction.html",
    "title": "R interface to TensorFlow Dataset API",
    "section": "",
    "text": "The TensorFlow Dataset API provides various facilities for creating scalable input pipelines for TensorFlow models, including:\n\nReading data from a variety of formats including CSV files and TFRecords files (the standard binary format for TensorFlow training data).\nTransforming datasets in a variety of ways including mapping arbitrary functions against them.\nShuffling, batching, and repeating datasets over a number of epochs.\nStreaming interface to data for reading arbitrarily large datasets.\nReading and transforming data are TensorFlow graph operations, so are executed in C++ and in parallel with model training.\n\nThe R interface to TensorFlow datasets provides access to the Dataset API, including high-level convenience functions for easy integration with the keras R package."
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#installation",
    "href": "v1/guide/tfdatasets/introduction.html#installation",
    "title": "R interface to TensorFlow Dataset API",
    "section": "Installation",
    "text": "Installation\nTo use tfdatasets you need to install both the R package as well as TensorFlow itself.\nFirst, install the tfdatasets R package from GitHub as follows:\n\ndevtools::install_github(\"rstudio/tfdatasets\")\n\nThen, use the install_tensorflow() function to install TensorFlow:\n\nlibrary(tfdtasets)\ninstall_tensorflow()"
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#creating-a-dataset",
    "href": "v1/guide/tfdatasets/introduction.html#creating-a-dataset",
    "title": "R interface to TensorFlow Dataset API",
    "section": "Creating a Dataset",
    "text": "Creating a Dataset\nTo create a dataset, use one of the dataset creation functions. Dataset can be created from delimted text files, TFRecords files, as well as from in-memory data.\n\nText Files\nFor example, to create a dataset from a text file, first create a specification for how records will be decoded from the file, then call text_line_dataset() with the file to be read and the specification:\n\nlibrary(tfdatasets)\n\n# create specification for parsing records from an example file\niris_spec <- csv_record_spec(\"iris.csv\")\n\n# read the datset\ndataset <- text_line_dataset(\"iris.csv\", record_spec = iris_spec) \n\n# take a glimpse at the dataset\nstr(dataset)\n\n<MapDataset shapes: {Sepal.Length: (), Sepal.Width: (), Petal.Length: (), Petal.Width: (),\nSpecies: ()}, types: {Sepal.Length: tf.float32, Sepal.Width: tf.float32, Petal.Length:\ntf.float32, Petal.Width: tf.float32, Species: tf.int32}>\nIn the example above, the csv_record_spec() function is passed an example file which is used to automatically detect column names and types (done by reading up to the first 1,000 lines of the file). You can also provide explicit column names and/or data types using the names and types parameters (note that in this case we don’t pass an example file):\n\n# provide colum names and types explicitly\niris_spec <- csv_record_spec(\n  names = c(\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"),\n  types = c(\"double\", \"double\", \"double\", \"double\", \"integer\"), \n  skip = 1\n)\n\n# read the datset\ndataset <- text_line_dataset(\"iris.csv\", record_spec = iris_spec)\n\nNote that we’ve also specified skip = 1 to indicate that the first row of the CSV that contains column names should be skipped.\nSupported column types are integer, double, and character. You can also provide types in a more compact form using single-letter abbreviations (e.g. types = \"dddi\"). For example:\n\nmtcars_spec <- csv_record_spec(\"mtcars.csv\", types = \"dididddiiii\")\n\n\nParallel Decoding\nDecoding lines of text into a record can be computationally expensive. You can parallelize these computations using the parallel_records parameter. For example:\n\ndataset <- text_line_dataset(\"iris.csv\", record_spec = iris_spec, parallel_records = 4)\n\nYou can also parallelize the reading of data from storage by requesting that a buffer of records be prefected. You do this with the dataset_prefetch() function. For example:\n\ndataset <- text_line_dataset(\"iris.csv\", record_spec = iris_spec, parallel_records = 4) %>% \n  dataset_batch(128) %>% \n  dataset_prefetch(1)\n\nThis code will result in the prefetching of a single batch of data on a background thread (i.e. in parallel with training operations).\nIf you have multiple input files, you can also parallelize reading of these files both across multiple machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). See the section on Reading Multiple Files below for additional details.\n\n\n\nTFRecords Files\nYou can read datasets from TFRecords files using the tfrecord_dataset() function.\nIn many cases you’ll want to map the records in the dataset into a set of named columns. You can do this using the dataset_map() function along with the tf$parse_single_example() function. for example:\n\n# Creates a dataset that reads all of the examples from two files, and extracts\n# the image and label features.\nfilenames <- c(\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\")\ndataset <- tfrecord_dataset(filenames) %>%\n  dataset_map(function(example_proto) {\n    features <- list(\n      image = tf$FixedLenFeature(shape(), tf$string),\n      label = tf$FixedLenFeature(shape(), tf$int32)\n    )\n    tf$parse_single_example(example_proto, features)\n  })\n\nYou can parallelize reading of TFRecord files using the num_parallel_reads option, for example:\n\nfilenames <- c(\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\")\ndataset <- tfrecord_dataset(filenames, num_parallel_reads = 4)"
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#sqlite-databases",
    "href": "v1/guide/tfdatasets/introduction.html#sqlite-databases",
    "title": "R interface to TensorFlow Dataset API",
    "section": "SQLite Databases",
    "text": "SQLite Databases\nYou can read datasets from SQLite databases using the sqlite_dataset() function. To use sqlite_dataset() you provide the filename of the database, a SQL query to execute, and sql_record_spec() that describes the names and TensorFlow types of columns within the query. For example:\n\nlibrary(tfdatasets)\n\nrecord_spec <- sql_record_spec(\n  names = c(\"disp\", \"drat\", \"vs\", \"gear\", \"mpg\", \"qsec\", \"hp\", \"am\", \"wt\",  \"carb\", \"cyl\"),\n  types = c(tf$float64, tf$int32, tf$float64, tf$int32, tf$float64, tf$float64,\n            tf$float64, tf$int32, tf$int32, tf$int32, tf$int32)\n)\n\ndataset <- sqlite_dataset(\n  \"data/mtcars.sqlite3\",\n  \"select * from mtcars\",\n  record_spec\n)\n\ndataset\n\n<MapDataset shapes: {disp: (), drat: (), vs: (), gear: (), mpg: (), qsec: (), hp: (), am: (),\nwt: (), carb: (), cyl: ()}, types: {disp: tf.float64, drat: tf.int32, vs: tf.float64, gear:\ntf.int32, mpg: tf.float64, qsec: tf.float64, hp: tf.float64, am: tf.int32, wt: tf.int32, carb:\ntf.int32, cyl: tf.int32}>\nNote that for floating point data you must use tf$float64 (reading tf$float32 is not supported for SQLite databases)."
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#transformations",
    "href": "v1/guide/tfdatasets/introduction.html#transformations",
    "title": "R interface to TensorFlow Dataset API",
    "section": "Transformations",
    "text": "Transformations\n\nMapping\nYou can map arbitrary transformation functions onto dataset records using the dataset_map() function. For example, to transform the “Species” column into a one-hot encoded vector you would do this:\n\ndataset <- dataset %>% \n  dataset_map(function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  })\n\nNote that while dataset_map() is defined using an R function, there are some special constraints on this function which allow it to execute not within R but rather within the TensorFlow graph.\nFor a dataset created with the csv_dataset() function, the passed record will be named list of tensors (one for each column of the dataset). The return value should be another set of tensors which were created from TensorFlow functions (e.g. tf$one_hot as illustrated above). This function will be converted to a TensorFlow graph operation that performs the transformation within native code.\n\nParallel Mapping\nIf these transformations are computationally expensive they can be executed on multiple threads using the num_parallel_calls parameter. For example:\n\ndataset <- dataset %>% \n  dataset_map(num_parallel_calls = 4, function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  })\n\nYou can control the maximum number of processed elements that will be buffered when processing in parallel using the dataset_prefetch() transformation. For example:\n\ndataset <- dataset %>% \n  dataset_map(num_parallel_calls = 4, function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  }) %>% \n  datset_prefetch(1)\n\nIf you are batching your data for training, you can optimize performance using the dataset_map_and_batch() function (which fuses together the map and batch operations). For example:\n\ndataset <- dataset %>% \n  dataset_map_and_batch(batch_size = 128, function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  }) %>% \n  datset_prefetch(1)\n\n\n\n\nFiltering\nYou can filter the elements of a dataset using the dataset_filter() function, which takes a predicate function that returns a boolean tensor for records that should be included. For example:\n\ndataset <- csv_dataset(\"mtcars.csv\") %>%\n  dataset_filter(function(record) {\n    record$mpg >= 20\n})\n\ndataset <- csv_dataset(\"mtcars.csv\") %>%\n  dataset_filter(function(record) {\n    record$mpg >= 20 & record$cyl >= 6L\n  })\n\nNote that the functions used inside the predicate must be tensor operations (e.g. tf$not_equal, tf$less, etc.). R generic methods for relational operators (e.g. <, >, <=, etc.) and logical operators (e.g. !, &, |, etc.) are provided so you can use shorthand syntax for most common comparisons (as illustrated above).\n\n\nFeatures and Response\nA common transformation is taking a column oriented dataset (e.g. one created by csv_dataset() or tfrecord_dataset()) and transforming it into a two-element list with features (“x”) and response (“y”). You can use the dataset_prepare() function to do this type of transformation. For example:\n\nmtcars_dataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl)\n\niris_dataset <- text_line_dataset(\"iris.csv\", record_spec = iris_spec) %>% \n  dataset_prepare(x = -Species, y = Species)\n\nThe dataset_prepare() function also accepts standard R formula syntax for defining features and response:\n\nmtcars_dataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(cyl ~ mpg + disp)\n\nIf you are batching your data for training you add a batch_size parameter to fuse together the dataset_prepare() and dataset_batch() steps (which generally results in faster training). For example:\n\nmtcars_dataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(cyl ~ mpg + disp, batch_size = 16)\n\n\n\nShuffling and Batching\nThere are several functions which control how batches are drawn from the dataset. For example, the following specifies that data will be drawn in batches of 128 from a shuffled window of 1000 records, and that the dataset will be repeated for 10 epochs:\n\ndataset <- dataset %>% \n  dataset_shuffle(1000) %>%\n  dataset_repeat(10) %>% \n  dataset_batch(128) %>% \n\nNote that you can optimize performance by fusing the shuffle and repeat operations into a single step using the dataset_shuffle_and_repeat() function. For example:\n\ndataset <- dataset %>% \n  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10) %>%\n  dataset_batch(128)\n\n\n\nPrefetching\nEarlier we alluded to the dataset_prefetch() function, which enables you to ensure that a given number of records (or batches of records) are prefetched in parallel so they are ready to go when the next batch is processed. For example:\n\ndataset <- dataset %>% \n  dataset_map_and_batch(batch_size = 128, function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  }) %>% \n  dataset_prefetch(1)\n\nIf you are using a GPU for training, you can also use the dataset_prefetch_to_device() function to specify that the parallel prefetch operation stage the data directly into GPU memory. For example:\n\ndataset <- dataset %>% \n  dataset_map_and_batch(batch_size = 128, function(record) {\n    record$Species <- tf$one_hot(record$Species, 3L)\n    record\n  }) %>% \n  dataset_prefetch_to_device(\"/gpu:0\")\n\nIn this case the buffer size for prefetches is determined automatically (you can manually speicfy it using the buffer_size parameter).\n\n\nComplete Example\nHere’s a complete example of using the various dataset transformation functions together. We’ll read the mtcars dataset from a CSV, filter it on some threshold values, map it into x and y components for modeling, and specify desired shuffling and batch iteration behavior:\n\ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>%\n  dataset_filter(function(record) {\n    record$mpg >= 20 & record$cyl >= 6L\n  }) %>% \n  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10) %>% \n  dataset_prepare(cyl ~ mpg + disp, batch_size = 128) %>% \n  dataset_prefetch(1)"
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#reading-datasets",
    "href": "v1/guide/tfdatasets/introduction.html#reading-datasets",
    "title": "R interface to TensorFlow Dataset API",
    "section": "Reading Datasets",
    "text": "Reading Datasets\nThe method for reading data from a TensorFlow Dataset varies depending upon which API you are using to build your models. If you are using the keras, then TensorFlow Datasets can be used much like in-memory R matrices and arrays. If you are using the lower-level tensorflow core API then you’ll use explicit dataset iteration functions.\nThe sections below provide additional details and examples for each of the supported APIs.\n\nkeras package\nIMPORTANT NOTE: Using TensorFlow Datasets with Keras requires that you are running the very latest versions of Keras (v2.2) and TensorFlow (v1.9). You can ensure that you have the latest versions of the core Keras and TensorFlow libraries with:\nlibrary(keras)\ninstall_keras()\nKeras models are often trained by passing in-memory arrays directly to the fit function. For example:\n\nmodel %>% fit(\n  x_train, y_train, \n  epochs = 30, \n  batch_size = 128\n)\n\nHowever, this requires loading data into an R data frame or matrix before calling fit. You can use the train_on_batch() function to stream data one batch at a time, however the reading and processing of the input data is still being done serially and outside of native code.\nAlternatively, Keras enables you to pass a dataset directly as the x argument to fit() and evaluate(). Here’s a complete example that uses datasets to read from TFRecord files containing MNIST digits:\n\nlibrary(keras)\nlibrary(tfdatasets)\n\nbatch_size = 128\nsteps_per_epoch = 500\n\n# function to read and preprocess mnist dataset\nmnist_dataset <- function(filename) {\n  dataset <- tfrecord_dataset(filename) %>%\n    dataset_map(function(example_proto) {\n\n      # parse record\n      features <- tf$parse_single_example(\n        example_proto,\n        features = list(\n          image_raw = tf$FixedLenFeature(shape(), tf$string),\n          label = tf$FixedLenFeature(shape(), tf$int64)\n        )\n      )\n\n      # preprocess image\n      image <- tf$decode_raw(features$image_raw, tf$uint8)\n      image <- tf$cast(image, tf$float32) / 255\n\n      # convert label to one-hot\n      label <- tf$one_hot(tf$cast(features$label, tf$int32), 10L)\n\n      # return\n      list(image, label)\n    }) %>%\n    dataset_repeat() %>%\n    dataset_shuffle(1000) %>%\n    dataset_batch(batch_size, drop_remainder = TRUE) %>%\n    dataset_prefetch(1)\n}\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%\n  layer_dropout(rate = 0.4) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  mnist_dataset(\"mnist/train.tfrecords\"),\n  steps_per_epoch = steps_per_epoch,\n  epochs = 20,\n  validation_data = mnist_dataset(\"mnist/validation.tfrecords\"),\n  validation_steps = steps_per_epoch\n)\n\nscore <- model %>% evaluate(\n  mnist_dataset(\"mnist/test.tfrecords\"),\n  steps = steps_per_epoch\n)\n\nprint(score)\n\nNote that all data preprocessing (e.g. one-hot encoding of the response variable) is done within the dataset_map() operation.\nAlso note that we pass drop_remainder = TRUE to the dataset_batch() function (this is to make sure that all batches are of equal size, a requirement for Keras tensor inputs).\n\n\ntensorflow package\nYou read batches of data from a dataset by using tensors that yield the next batch. You can obtain this tensor from a dataset via the make_iterator_one_shot() and iterator_get_next() functions. For example:\n\ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(cyl ~ mpg + disp) %>% \n  dataset_shuffle(20) %>% \n  dataset_batch(5)\n\niter <- make_iterator_one_shot(dataset)\nnext_batch <- iterator_get_next(iter)\nnext_batch\n\n$x\nTensor(\"IteratorGetNext_13:0\", shape=(?, 2), dtype=float32)\n\n$y\nTensor(\"IteratorGetNext_13:1\", shape=(?,), dtype=int32)\nAs you can see next_batch isn’t the data itself but rather a tensor that will yield the next batch of data when it is evaluated:\n\nsess <- tf$Session()\nsess$run(next_batch)\n\n$x\n     [,1] [,2]\n[1,] 21.0  160\n[2,] 21.0  160\n[3,] 22.8  108\n[4,] 21.4  258\n[5,] 18.7  360\n\n$y\n[1] 6 6 4 6 8\nIf you are iterating over a dataset using these functions, you will need to determine at what point to stop iteration. One approach to this is to use the dataset_repeat() function to create an dataset that yields values infinitely. For example:\n\nlibrary(tfdatasets)\n\nsess <- tf$Session()\n\nmtcars_spec <- csv_record_spec(\"mtcars.csv\")\ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_shuffle(5000) %>% \n  dataset_repeat() # repeat infinitely\n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n\niter <- make_iterator_one_shot(dataset)\nnext_batch <- iterator_get_next(iter)\n\nsteps <- 200\nfor (i in 1:steps) {\n  \n  # use next_batch for training, etc. \n  \n  # (note that you need to actually use the next_batch e.g. by passing it to a\n  # function that consumes a tensor or by running it explicitly) in order to \n  # advance to the next batch)\n}\n\nIn this case the steps variable is used to determine when to stop drawing new batches of training data (we could have equally included code to detect a learning plateau or any other custom method of determining when to stop training).\nAnother approach is to detect when all batches have been yielded from the dataset. When a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by using out_of_range_handler as the error argument to tryCatch(). For example:\n\nlibrary(tfdatasets)\n\nsess <- tf$Session()\n\nmtcars_spec <- csv_record_spec(\"mtcars.csv\")\ndataset <- text_line_dataset(\"mtcars.csv\", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10)\n  \niter <- make_iterator_one_shot(dataset)\nnext_batch <- iterator_get_next(iter)\n\ntryCatch({\n  while(TRUE) {\n    batch <- sess$run(next_batch)\n    str(batch)\n  }\n}, error = out_of_range_handler)\n\nYou can write this iteration more elegantly using the until_out_of_range() function, which automatically handles the error and provides the while(TRUE) around an expression:\n\nuntil_out_of_range({\n  batch <- sess$run(next_batch)\n  str(batch)\n})\n\nWhen running under eager execution, you organize the code a bit differently (since you don’t need to explicitly run() tensors):\n\niter <- make_iterator_one_shot(dataset)\n\nuntil_out_of_range({\n  batch <- iterator_get_next(iter)\n  str(batch)\n})"
  },
  {
    "objectID": "v1/guide/tfdatasets/introduction.html#reading-multiple-files",
    "href": "v1/guide/tfdatasets/introduction.html#reading-multiple-files",
    "title": "R interface to TensorFlow Dataset API",
    "section": "Reading Multiple Files",
    "text": "Reading Multiple Files\nIf you have multiple input files you can process them in parallel both across machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). The read_files() function provides a high-level interface to parallel file reading.\nThe read_files() function takes a set of files and a read function along with various options to orchestrate parallel reading. For example, the following function reads all CSV files in a directory using the text_line_dataset() function:\n\ndataset <- read_files(\"data/*.csv\", text_line_dataset, record_spec = mtcars_spec,\n                      parallel_files = 4, parallel_interleave = 16) %>% \n  dataset_prefetch(5000) %>% \n  dataset_shuffle_and_repeat(buffer_size = 1000, count = 3) %>% \n  dataset_batch(128)\n\nThe parallel_files argument requests that 4 files be processed in parallel and the parallel_interleave argument requests that blocks of 16 consecutive records from each file be interleaved in the resulting dataset.\nNote that because we are processing files in parallel we do not pass the parallel_records argument to text_line_dataset(), since we are already parallelizing at the file level.\n\nMultiple Machines\nIf you are training on multiple machines and the training supervisor passes a shard index to your training script, you can also parallelizing reading by sharding the file list. For example:\n\n# command line flags for training script (shard info is passed by training \n# supervisor that executes the script)\nFLAGS <- flags(\n  flag_integer(\"num_shards\", 1),\n  flag_integer(\"shard_index\", 1)\n)\n\n# forward shard info to read_files\ndataset <- read_files(\"data/*.csv\", text_line_dataset, record_spec = mtcars_spec,\n                      parallel_files = 4, parallel_interleave = 16,\n                      num_shards = FLAGS$num_shards, shard_index = FLAGS$shard_index) %>% \n  dataset_shuffle_and_repeat(buffer_size = 1000, count = 3) %>% \n  dataset_batch(128) %>% \n  dataset_prefetch(1)"
  },
  {
    "objectID": "v1/guide/tfestimators/creating_estimators.html",
    "href": "v1/guide/tfestimators/creating_estimators.html",
    "title": "Custom Estimators",
    "section": "",
    "text": "The tfestimators framework makes it easy to construct and build machine learning models via its high-level Estimator API. Estimator offers classes you can instantiate to quickly configure common model types such as regressors and classifiers.\nBut what if none of the predefined model types meets your needs? Perhaps you need more granular control over model configuration, such as the ability to customize the loss function used for optimization, or specify different activation functions for each neural network layer. Or maybe you’re implementing a ranking or recommendation system, and neither a classifier nor a regressor is appropriate for generating predictions. The figure on the right illustrates the basic components of an estimator. Users can implement custom behaviors and or architecture inside the model_fn of the estimator.\nThis tutorial covers how to create your own Estimator using the building blocks provided in tfestimators package, which will predict the ages of abalones based on their physical measurements. You’ll learn how to do the following:\nThe complete code for this tutorial can be found here."
  },
  {
    "objectID": "v1/guide/tfestimators/creating_estimators.html#configuring-a-neural-network-with-feature_column-and-layers",
    "href": "v1/guide/tfestimators/creating_estimators.html#configuring-a-neural-network-with-feature_column-and-layers",
    "title": "Custom Estimators",
    "section": "Configuring a neural network with feature_column and layers",
    "text": "Configuring a neural network with feature_column and layers\nConstructing a neural network entails creating and connecting the input layer, the hidden layers, and the output layer.\nThe input layer is a series of nodes (one for each feature in the model) that will accept the feature data that is passed to the model_fn in the features argument. If features contains an n-dimensional Tensor with all your feature data, then it can serve as the input layer. If features contains a dict of feature columns passed to the model via an input function, you can convert it to an input-layer Tensor with the input_layer function:\ninput_layer <- input_layer(\n    features = features, feature_columns = c(age, height, weight))\nAs shown above, input_layer() takes two required arguments:\n\nfeatures. A mapping from string keys to the Tensors containing the corresponding feature data. This is exactly what is passed to the model_fn in the features argument.\nfeature_columns. A list of all the FeatureColumns in the model — age, height, and weight in the above example.\n\nThe input layer of the neural network then must be connected to one or more hidden layers via an activation function that performs a nonlinear transformation on the data from the previous layer. The last hidden layer is then connected to the output layer, the final layer in the model. tf$layers provides the tf$layers$dense function for constructing fully connected layers. The activation is controlled by the activation argument. Some options to pass to the activation argument are:\n\ntf$nn$relu. The following code creates a layer of units nodes fully connected to the previous layer input_layer with a ReLU activation function:\n\nhidden_layer <- tf$layers$dense(inputs = input_layer, units = 10L, activation = tf$nn$relu)\n\ntf$nn$relu6. The following code creates a layer of units nodes fully connected to the previous layer hidden_layer with a ReLU 6 activation function:\n\nsecond_hidden_layer <- tf$layers$dense(\ninputs = hidden_layer, units = 20L, activation = tf$nn$relu)\n\nNULL. The following code creates a layer of units nodes fully connected to the previous layer second_hidden_layer with no activation function, just a linear transformation:\n\noutput_layer <- tf$layers$dense(inputs = second_hidden_layer,\nunits = 3L, activation = NULL)\nOther activation functions are possible, e.g.:\noutput_layer <- tf$layers$dense(inputs = second_hidden_layer,\nunits = 10L, activation_fn = tf$sigmoid)\nThe above code creates the neural network layer output_layer, which is fully connected to second_hidden_layer with a sigmoid activation function tf$sigmoid.\nThe network contains two hidden layers, each with 10 nodes and a ReLU activation function. The output layer contains no activation function, and is tf$reshape to a one-dimensional tensor to capture the model’s predictions, which are stored in predictions_dict."
  },
  {
    "objectID": "v1/guide/tfestimators/creating_estimators.html#defining-loss",
    "href": "v1/guide/tfestimators/creating_estimators.html#defining-loss",
    "title": "Custom Estimators",
    "section": "Defining loss for the model",
    "text": "Defining loss for the model\nThe estimator_spec returned by the model_fn must contain loss: a Tensor representing the loss value, which quantifies how well the model’s predictions reflect the label values during training and evaluation runs. The tf$losses module provides convenience functions for calculating loss using a variety of metrics, including:\n\nabsolute_difference(labels, predictions). Calculates loss using the absolute-difference formula (also known as L1 loss).\nlog_loss(labels, predictions). Calculates loss using the logistic loss forumula (typically used in logistic regression).\nmean_squared_error(labels, predictions). Calculates loss using the mean squared error (MSE; also known as L2 loss).\n\nThe following example adds a definition for loss to the abalone model_fn using mean_squared_error():\nloss <- tf$losses$mean_squared_error(labels, predictions)\nSupplementary metrics for evaluation can be added to an eval_metric_ops dict. The following code defines an rmse metric, which calculates the root mean squared error for the model predictions. Note that the labels tensor is cast to a float64 type to match the data type of the predictions tensor, which will contain real values:\neval_metric_ops <- list(rmse = tf$metrics$root_mean_squared_error(\ntf$cast(labels, tf$float64), predictions\n))"
  },
  {
    "objectID": "v1/guide/tfestimators/creating_estimators.html#defining-the-training-op-for-the-model",
    "href": "v1/guide/tfestimators/creating_estimators.html#defining-the-training-op-for-the-model",
    "title": "Custom Estimators",
    "section": "Defining the training op for the model",
    "text": "Defining the training op for the model\nThe training op defines the optimization algorithm TensorFlow will use when fitting the model to the training data. Typically when training, the goal is to minimize loss. A simple way to create the training op is to instantiate a tf$train$Optimizer subclass and call the minimize method.\nThe following code defines a training op for the abalone model_fn using the loss value calculated in Defining Loss for the Model, the learning rate passed to the function in params, and the gradient descent optimizer. For global_step, the convenience function tf$train$get_global_step takes care of generating an integer variable:\noptimizer <- tf$train$GradientDescentOptimizer(learning_rate = params$learning_rate)\ntrain_op <- optimizer$minimize(loss = loss, global_step = tf$train$get_global_step())"
  },
  {
    "objectID": "v1/guide/tfestimators/creating_estimators.html#the-complete-abalone-model_fn",
    "href": "v1/guide/tfestimators/creating_estimators.html#the-complete-abalone-model_fn",
    "title": "Custom Estimators",
    "section": "The complete abalone model_fn",
    "text": "The complete abalone model_fn\nHere’s the final, complete model_fn for the abalone age predictor. The following code configures the neural network; defines loss and the training op; and returns a estimator_spec object containing mode, predictions_dict, loss, and train_op:\nmodel_fn <- function(features, labels, mode, params, config) {\n  # Connect the first hidden layer to input layer\n  first_hidden_layer <- tf$layers$dense(features, 10L, activation = tf$nn$relu)\n  \n  # Connect the second hidden layer to first hidden layer with relu\n  second_hidden_layer <- tf$layers$dense(first_hidden_layer, 10L, activation = tf$nn$relu)\n  \n  # Connect the output layer to second hidden layer (no activation fn)\n  output_layer <- tf$layers$dense(second_hidden_layer, 1L)\n  \n  # Reshape output layer to 1-dim Tensor to return predictions\n  predictions <- tf$reshape(output_layer, list(-1L))\n  predictions_list <- list(ages = predictions)\n  \n  # Calculate loss using mean squared error\n  loss <- tf$losses$mean_squared_error(labels, predictions)\n  \n  eval_metric_ops <- list(rmse = tf$metrics$root_mean_squared_error(\n    tf$cast(labels, tf$float64), predictions\n  ))\n  \n  optimizer <- tf$train$GradientDescentOptimizer(learning_rate = params$learning_rate)\n  train_op <- optimizer$minimize(loss = loss, global_step = tf$train$get_global_step())\n  \n  return(estimator_spec(\n    mode = mode,\n    predictions = predictions_list,\n    loss = loss,\n    train_op = train_op,\n    eval_metric_ops = eval_metric_ops\n  ))\n}\n\nmodel_params <- list(learning_rate = 0.001)\nmodel <- estimator(model_fn, params = model_params)"
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html",
    "href": "v1/guide/tfestimators/dataset_api.html",
    "title": "Dataset API",
    "section": "",
    "text": "We can access the TensorFlow Dataset API via the tfdatasets package, which enables us to create scalable input pipelines that can be used with tfestimators. In this vignette, we demonstrate the capability to stream datasets stored on disk for training by building a classifier on the iris dataset."
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html#dataset-preparation",
    "href": "v1/guide/tfestimators/dataset_api.html#dataset-preparation",
    "title": "Dataset API",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\nLet’s assume we’re given a dataset (which could be arbitrarily large) split into training and validation, and a small sample of the dataset. To simulate this scenario, we’ll create a few CSV files as follows:\n\nset.seed(123)\ntrain_idx <- sample(nrow(iris), nrow(iris) * 2/3)\n\niris_train <- iris[train_idx,]\niris_validation <- iris[-train_idx,]\niris_sample <- iris_train %>%\n  head(10)\n\nwrite.csv(iris_train, \"iris_train.csv\", row.names = FALSE)\nwrite.csv(iris_validation, \"iris_validation.csv\", row.names = FALSE)\nwrite.csv(iris_sample, \"iris_sample.csv\", row.names = FALSE)"
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html#estimator-construction",
    "href": "v1/guide/tfestimators/dataset_api.html#estimator-construction",
    "title": "Dataset API",
    "section": "Estimator Construction",
    "text": "Estimator Construction\nWe construct the classifier as usual – see Estimator Basics for details on feature columns and creating estimators.\n\nlibrary(tfestimators)\nresponse <- \"Species\"\nfeatures <- setdiff(names(iris), response)\nfeature_columns <- feature_columns(\n  column_numeric(features)\n)\n\nclassifier <- dnn_classifier(\n  feature_columns = feature_columns,\n  hidden_units = c(16, 32, 16),\n  n_classes = 3,\n  label_vocabulary = c(\"setosa\", \"virginica\", \"versicolor\")\n)"
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html#input-function",
    "href": "v1/guide/tfestimators/dataset_api.html#input-function",
    "title": "Dataset API",
    "section": "Input Function",
    "text": "Input Function\nThe creation of the input function is similar to the in-memory case. However, instead of passing data frames or matrices to iris_input_fn(), we pass TensorFlow dataset objects which are internally iterators of the dataset files.\n\niris_input_fn <- function(data) {\n  input_fn(data, features = features, response = response)\n}\n\niris_spec <- csv_record_spec(\"iris_sample.csv\")\niris_train <- text_line_dataset(\n  \"iris_train.csv\", record_spec = iris_spec) %>%\n  dataset_batch(10) %>% \n  dataset_repeat(10)\niris_validation <- text_line_dataset(\n  \"iris_validation.csv\", record_spec = iris_spec) %>%\n  dataset_batch(10) %>%\n  dataset_repeat(1)\n\nThe csv_record_spec() function is a helper function that creates a specification from a sample file; the returned specification is required by the text_line_dataset() function to parse the files. There are many transformations available for dataset objects, but here we just demonstrate dataset_batch() and dataset_repeat() which control the batch size and how many times we iterate through the dataset files, respectively."
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html#training-and-evaluation",
    "href": "v1/guide/tfestimators/dataset_api.html#training-and-evaluation",
    "title": "Dataset API",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\nOnce the input functions and datasets are defined, the training and evaluation interface is exactly the same as in the in-memory case.\n\nhistory <- train(classifier, input_fn = iris_input_fn(iris_train))\nplot(history)\npredictions <- predict(classifier, input_fn = iris_input_fn(iris_validation))\npredictions\nevaluation <- evaluate(classifier, input_fn = iris_input_fn(iris_validation))\nevaluation"
  },
  {
    "objectID": "v1/guide/tfestimators/dataset_api.html#learning-more",
    "href": "v1/guide/tfestimators/dataset_api.html#learning-more",
    "title": "Dataset API",
    "section": "Learning More",
    "text": "Learning More\nSee the documetnation for the tfdatasets package for additional details on using TensorFlow datasets."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html",
    "href": "v1/guide/tfestimators/estimator_basics.html",
    "title": "Estimator Basics",
    "section": "",
    "text": "TensorFlow Estimators has not received updates by the TensorFlow team in a long time, and it’s no longer recommended. You may find bugs when using tfestimators with recent versions of TensorFlow (>= 1.10)."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#overview",
    "href": "v1/guide/tfestimators/estimator_basics.html#overview",
    "title": "Estimator Basics",
    "section": "Overview",
    "text": "Overview\nThe basic components of the TensorFlow Estimators API include:\n\nCanned estimators (pre-built implementations of various models).\nCustom estimators (custom model implementations).\nEstimator methods (core methods like train(), predict(), evaluate(), etc. which work the same for all canned and custom estimators).\nFeature columns (definitions of how features should be transformed during modeling).\nInput functions (sources of data for training, evaluation, and prediction).\n\nIn addition, there are APIs that cover more advanced usages:\n\nExperiments (wrappers around estimators that handle concerns like distributed training, hyperparameter tuning, etc.)\nRun hooks (callbacks for recording context and interacting with each modelling processes)\nSavedModel export utilities (exports the trained model to be deployed in places like CloudML)\n\nPlease read our white paper if you are interested in the detailed design of the above components.\nBelow we enumerate some of the core functions in each of these components to provide a high level overview of what’s available. See the linked articles for more details on using all of the components together."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#canned-estimators",
    "href": "v1/guide/tfestimators/estimator_basics.html#canned-estimators",
    "title": "Estimator Basics",
    "section": "Canned Estimators",
    "text": "Canned Estimators\nThe tfestimators package includes a wide variety of canned estimators for common machine learning tasks. Currently, the following canned estimators are available:\n\n\n\n\n\n\n\nEstimator\nDescription\n\n\n\n\nlinear_regressor()\nLinear regressor model.\n\n\nlinear_classifier()\nLinear classifier model.\n\n\ndnn_regressor()\nDNN Regression.\n\n\ndnn_classifier()\nDNN Classification.\n\n\ndnn_linear_combined_regressor()\nDNN Linear Combined Regression.\n\n\ndnn_linear_combined_classifier()\nDNN Linear Combined Classification.\n\n\n\nBefore you can use an estimator, you need to provide an input function and define a set of feature columns. The following two sections cover how to do this."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#input-functions",
    "href": "v1/guide/tfestimators/estimator_basics.html#input-functions",
    "title": "Estimator Basics",
    "section": "Input Functions",
    "text": "Input Functions\nInput functions are used to provide data to estimators during training, evaluation and prediction. The R interface provides several high-level input function implementations for various common R data sources, including:\n\nFormulas\nMatrices\nData Frames\nLists of vectors\n\nFor example, here’s how we might construct an input function that uses the mtcars data frame as a data source, treating the drat, mpg, and am variables as feature columns, and vs as a response.\n\ninput <- input_fn(mtcars, \n                  features = c(\"drat\", \"mpg\", \"am\"),\n                  response = \"vs\",\n                  batch_size = 128,\n                  epochs = 3)\n\nThe formula interface is a bit more succinct, in this case, and should feel familiar to R users who have experience fitting models using the R stats package.\n\ninput <- input_fn(vs ~ drat + mpg + am, data = mtcars,\n                  batch_size = 128,\n                  epochs = 3)\n\nYou can also write fully custom input functions that draw data from arbitrary data sources. See the input functions article for additional details."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#feature-columns",
    "href": "v1/guide/tfestimators/estimator_basics.html#feature-columns",
    "title": "Estimator Basics",
    "section": "Feature Columns",
    "text": "Feature Columns\nIn TensorFlow, feature columns are used to specify the ‘shapes’, or ‘types’, of inputs that can be expected by a particular model. For example, in the following code, we define two simple feature columns: a numeric column called \"drat\", and a indicator column called \"am\" with one-hot representation.\n\ncols <- feature_columns(\n  column_numeric(\"drat\"),\n  column_indicator(\"am\")\n)\n\nThere are a wide variety of feature column functions available:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ncolumn_indicator()\nRepresents multi-hot representation of given categorical column.\n\n\ncolumn_numeric()\nRepresents real valued or numerical features.\n\n\ncolumn_embedding()\nCreates an dense column that converts from sparse, categorical input.\n\n\ncolumn_bucketized()\nRepresents discretized dense input.\n\n\ncolumn_categorical_weighted()\nApplies weight values to a categorical column.\n\n\ncolumn_categorical_with_vocabulary_list()\nCreates a categorical column with in-memory vocabulary.\n\n\ncolumn_categorical_with_vocabulary_file()\nCreates a categorical column with a vocabulary file.\n\n\ncolumn_categorical_with_identity()\nCreates a categorical column that returns identity values.\n\n\ncolumn_categorical_with_hash_bucket()\nRepresents sparse feature where ids are set by hashing.\n\n\n\nSee the article on feature columns for additional details."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#creating-an-estimator",
    "href": "v1/guide/tfestimators/estimator_basics.html#creating-an-estimator",
    "title": "Estimator Basics",
    "section": "Creating an Estimator",
    "text": "Creating an Estimator\nHere’s an example of creating a DNN Linear Combined canned Estimator. In creating the estimator we pass the feature columns and other parameters that specifies the layers and architecture of the model. Note that this particular estimator takes two sets of feature columns – one used for constructing the linear layer, and the other used for the fully connected deep layer.\n\n# construct feature columns\nlinear_feature_columns <- feature_columns(column_numeric(\"mpg\"))\ndnn_feature_columns <- feature_columns(column_numeric(\"drat\"))\n\n# generate classifier\nclassifier <-\n    dnn_linear_combined_classifier(\n      linear_feature_columns = linear_feature_columns,\n      dnn_feature_columns = dnn_feature_columns,\n      dnn_hidden_units = c(3, 3),\n      dnn_optimizer = \"Adagrad\"\n    )"
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#training-and-prediction",
    "href": "v1/guide/tfestimators/estimator_basics.html#training-and-prediction",
    "title": "Estimator Basics",
    "section": "Training and Prediction",
    "text": "Training and Prediction\nUsers can then call train() to train the initialized Estimator for a number of steps:\n\nclassifier %>%\n  train(input_fn = input, steps = 2)\n\nOnce a model is trained, users can use predict() that makes predictions on a given input function that represents the inference data source.\n\npredictions <- predict(classifier, input_fn = input)\n\nUsers can also pass a key to predict_keys argument in predict() that generates different types of predictions, such as probabilities using \"probabilities\":\n\npredictions <- predict(\n  classifier,\n  input_fn = input,\n  predict_keys = \"probabilities\")\n\nor logistic:\n\npredictions <- predict(\n  classifier,\n  input_fn = input,\n  predict_keys = \"logistic\")\n\nYou can find all the available keys by printing prediction_keys(). However, not all keys can be used by different types of estimators. For example, regressors cannot use \"probabilities\" as one of the keys since probability output only makes sense for classification models."
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#model-persistence",
    "href": "v1/guide/tfestimators/estimator_basics.html#model-persistence",
    "title": "Estimator Basics",
    "section": "Model Persistence",
    "text": "Model Persistence\nModels created via tfestimators are persisted on disk. To obtain the location of where the model artifacts are stored, we can call model_dir():\n\nsaved_model_dir <- model_dir(classifier)\n\nAnd subsequently load the saved model (in a new session) by passing the directory to the model_dir argument of the model constructor and use it for prediction or continue training:\n\nlibrary(tfestimators)\nlinear_feature_columns <- feature_columns(column_numeric(\"mpg\"))\ndnn_feature_columns <- feature_columns(column_numeric(\"drat\"))\n\nloaded_model <-\n    dnn_linear_combined_classifier(\n      linear_feature_columns = linear_feature_columns,\n      dnn_feature_columns = dnn_feature_columns,\n      dnn_hidden_units = c(3, 3),\n      dnn_optimizer = \"Adagrad\",\n      model_dir = saved_model_dir\n    )\nloaded_model"
  },
  {
    "objectID": "v1/guide/tfestimators/estimator_basics.html#generic-methods",
    "href": "v1/guide/tfestimators/estimator_basics.html#generic-methods",
    "title": "Estimator Basics",
    "section": "Generic methods",
    "text": "Generic methods\nThere are a number of estimator methods which can be used generically with any canned or custom estimator:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ntrain()\nTrains a model given training data input_fn.\n\n\npredict()\nReturns predictions for given features.\n\n\nevaluate()\nEvaluates the model given evaluation data input_fn.\n\n\ntrain_and_evaluate()\nTrains and evaluates a model for both local and distributed configurations.\n\n\nexport_savedmodel()\nExports inference graph as a SavedModel into a given directory."
  },
  {
    "objectID": "v1/guide/tfestimators/examples/custom_estimator.html",
    "href": "v1/guide/tfestimators/examples/custom_estimator.html",
    "title": "custom_estimator",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/custom_estimator.R"
  },
  {
    "objectID": "v1/guide/tfestimators/examples/index.html",
    "href": "v1/guide/tfestimators/examples/index.html",
    "title": "TensorFlow Estimator API Examples",
    "section": "",
    "text": "This collection of examples will show you how you can use tfestimators to easily construct powerful models using TensorFlow."
  },
  {
    "objectID": "v1/guide/tfestimators/examples/iris_custom_decay_dnn.html",
    "href": "v1/guide/tfestimators/examples/iris_custom_decay_dnn.html",
    "title": "iris_custom_decay_dnn",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/iris_custom_decay_dnn.R"
  },
  {
    "objectID": "v1/guide/tfestimators/examples/iris_dnn_classifier.html",
    "href": "v1/guide/tfestimators/examples/iris_dnn_classifier.html",
    "title": "iris_dnn_classifier",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/iris_dnn_classifier.R"
  },
  {
    "objectID": "v1/guide/tfestimators/examples/mnist.html",
    "href": "v1/guide/tfestimators/examples/mnist.html",
    "title": "mnist",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/mnist.R"
  },
  {
    "objectID": "v1/guide/tfestimators/examples/tensorflow_layers.html",
    "href": "v1/guide/tfestimators/examples/tensorflow_layers.html",
    "title": "tensorflow_layers",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/tensorflow_layers.R"
  },
  {
    "objectID": "v1/guide/tfestimators/examples/wide_and_deep.html",
    "href": "v1/guide/tfestimators/examples/wide_and_deep.html",
    "title": "wide_and_deep",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/wide_and_deep.R"
  },
  {
    "objectID": "v1/guide/tfestimators/feature_columns.html",
    "href": "v1/guide/tfestimators/feature_columns.html",
    "title": "Feature Columns",
    "section": "",
    "text": "Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model. A feature column can be a plain mapping to some input column (e.g. column_numeric() for a column of numerical data), or a transformation of other feature columns (e.g. column_crossed() to define a new column as the cross of two other feature columns).\nThe following feature columns are available:\n\n\n\n\n\n\n\nFeature Column\nDescription\n\n\n\n\ncolumn_categorical_with_vocabulary_list()\nConstruct a Categorical Column with In-Memory Vocabulary.\n\n\ncolumn_categorical_with_vocabulary_file()\nConstruct a Categorical Column with a Vocabulary File.\n\n\ncolumn_categorical_with_identity()\nConstruct a Categorical Column that Returns Identity Values.\n\n\ncolumn_categorical_with_hash_bucket()\nRepresents Sparse Feature where IDs are set by Hashing.\n\n\ncolumn_categorical_weighted()\nConstruct a Weighted Categorical Column.\n\n\ncolumn_indicator()\nRepresents Multi-Hot Representation of Given Categorical Column.\n\n\ncolumn_numeric()\nConstruct a Real-Valued Column.\n\n\ncolumn_embedding()\nConstruct a Dense Column.\n\n\ncolumn_crossed()\nConstruct a Crossed Column.\n\n\ncolumn_bucketized()\nConstruct a Bucketized Column.\n\n\n\nSome typical mappings of R data types to feature column are:\n\n\n\nData Type\nFeature Column\n\n\n\n\nNumeric\ncolumn_numeric()\n\n\nFactor\ncolumn_categorical_with_identity()\n\n\nCharacter\ncolumn_categorical_with_hash_bucket()\n\n\n\nWe’ll use the flights dataset from the nycflights13 package to explore how feature columns can be constructed. The flights dataset records airline on-time data for all flights departing NYC in 2013.\n\nlibrary(nycflights13)\nprint(flights)\n\n> print(flights)\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin  dest air_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>     <dbl>   <chr>  <int>   <chr>  <chr> <chr>    <dbl>\n 1  2013     1     1      517            515         2      830            819        11      UA   1545  N14228    EWR   IAH      227\n 2  2013     1     1      533            529         4      850            830        20      UA   1714  N24211    LGA   IAH      227\n 3  2013     1     1      542            540         2      923            850        33      AA   1141  N619AA    JFK   MIA      160\n# ... with 336,766 more rows, and 4 more variables: distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\nFor example, we can define numeric columns based on the dep_time and dep_delay variables:\n\ncols <- feature_columns(\n  column_numeric(\"dep_time\"),\n  column_numeric(\"dep_delay\")\n)\n\nYou can also define multiple feature columns at once.\n\ncols <- feature_columns(\n  column_numeric(\"dep_time\", \"dep_delay\")\n)"
  },
  {
    "objectID": "v1/guide/tfestimators/feature_columns.html#pattern-matching",
    "href": "v1/guide/tfestimators/feature_columns.html#pattern-matching",
    "title": "Feature Columns",
    "section": "Pattern Matching",
    "text": "Pattern Matching\nOften, you will find that you want to generate a number of feature column definitions based on some pattern existing in the names of your data set. tfestimators uses the tidyselect package to make it easy to define feature columns, similar to what you might be familiar with in the dplyr package. You can use the names = argument of feature_columns() function to define a context from which variable names will be selected.\nFor example, we can use the ends_with() helper to assert that all columns ending with \"time\" are numeric columns as follows:\n\nlibrary(nycflights13)\n\ncols <- feature_columns(names = flights,\n  column_numeric(ends_with(\"time\"))\n)\n\nThe names parameter can either be a character vector with the names as-is, or any named R object.\nIf the code you are using to compose columns is more complicated, or if you need to save references to columns for use in column embeddings you can also establish a scope for given set of column names using the with_columns() function:\n\ncols <- with_columns(flights, {\n  feature_columns(\n    column_numeric(ends_with(\"time\"))\n  )\n})\n\nYou can also use an alternate syntax of the form (pattern) ~ (column), which can add clarity when longer pattern rules are used, as it separates the matching rule from the column definition:\n\ncols <- with_columns(flights, {\n  feature_columns(\n    ends_with(\"time\") ~ column_numeric(),\n  )\n})\n\nAvailable pattern matching operators include:\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nstarts_with()\nStarts with a prefix\n\n\nends_with()\nEnds with a suffix\n\n\ncontains()\nContains a literal string\n\n\nmatches()\nMatches a regular expression\n\n\none_of()\nIncluded in character vector\n\n\neverything()\nAll columns\n\n\n\nSee help(\"select_helpers\", package = \"tidyselect\") for full information on the set of helpers made available by the tidyselect package."
  },
  {
    "objectID": "v1/guide/tfestimators/input_functions.html",
    "href": "v1/guide/tfestimators/input_functions.html",
    "title": "Input Functions",
    "section": "",
    "text": "TensorFlow estimators receive data through input functions. Input functions take an arbitrary data source (in-memory data sets, streaming data, custom data format, and so on) and generate Tensors that can be supplied to TensorFlow models.\nMore concretely, input functions are used to:\n\nTurn raw data sources into Tensors, and\nConfigure how data is drawn during training (shuffling, batch size, epochs, etc.)\n\nYou can also perform feature engineering within an input function; however, it’s better to use feature columns for this purpose whenever possible, as in that case the tranformations are made part of the TensorFlow graph and so can be executed without an R runtime (e.g. when the model is deployed onto a device or server).\nThe tfestimators package includes an input_fn() function that can create TensorFlow input functions from common R data sources (e.g. data frames and matrices). It’s also possible to write a fully custom input function. Both methods of creating input functions are covered below."
  },
  {
    "objectID": "v1/guide/tfestimators/input_functions.html#data-frame-input",
    "href": "v1/guide/tfestimators/input_functions.html#data-frame-input",
    "title": "Input Functions",
    "section": "Data Frame Input",
    "text": "Data Frame Input\nYou can create an input function from an R data frame using the input_fn() method. You can specify feature and response variables either explicitly or using the R formula interface.\nFor example, to create an input function for the mtcars dataset with features “drat” and “cyl” and response “mpg” you could use this code:\n\nmodel %>% train(\n  input_fn(mtcars, \n           features = c(drat, cyl), \n           response = mpg,\n           batch_size = 128,\n           epochs = 3)\n)\n\nOr alternatively use the R formula interface like this:\n\nmodel %>% train(\n  input_fn(mpg ~ drat + cyl, \n           data = mtcars,\n           batch_size = 128,\n           epochs = 3)\n)\n\nNote that input_fn functions provide several parameters for controlling how data is drawn from the input source. These include batch_size (defaults to 128), shuffle (default to \"auto\"), and epochs (defaults to 1). Note that, by default, shuffling is disabled during prediction.\n\nTraining vs. Evaluation\nIt’s often the case that you’ll want to use the same basic input function for training and evaluation, but need to provide a distinct dataset for each step. In that case you can create a wrapper function that returns the same input function with varying input data.\nFor example, imagine we have already split the mtcars dataset into training and test subsets. We could have an input function generator like this:\n\nmtcars_input_fn <- function(data, ...) {\n  input_fn(data,\n           features = c(\"drat\", \"cyl\"),\n           response = \"mpg\",\n           ...)\n}\n\nThe ... parameter is used to forward additional options to input_fn().\nThis helper function could then be used during training and evaluation as follows:\n\n# train the model\nmodel %>% train(mtcars_input_fn(train_data))\n\n# evaluate the model\nmodel %>% evaluate(mtcars_input_fn(test_data))"
  },
  {
    "objectID": "v1/guide/tfestimators/input_functions.html#matrix-input",
    "href": "v1/guide/tfestimators/input_functions.html#matrix-input",
    "title": "Input Functions",
    "section": "Matrix Input",
    "text": "Matrix Input\nAs with data frames, you can also pass an R matrix to input_fn() to automatically create an input function for the matrix. Note however that in order to specify the features and response parameters you will need to ensure that your matrix columns are named. For example:\n\nm <- matrix(c(1:12), nrow = 4, ncol = 3)\ncolnames(m) <- c(\"x1\", \"x2\", \"y\")\ninput_fn(m, features = c(\"x1\", \"x2\"), response = \"y\")"
  },
  {
    "objectID": "v1/guide/tfestimators/input_functions.html#list-input",
    "href": "v1/guide/tfestimators/input_functions.html#list-input",
    "title": "Input Functions",
    "section": "List Input",
    "text": "List Input\nThere’s also a built-in input_fn() that works on nested lists, for example:\n\ninput_fn(\n  object = list(\n    inputs = list(\n      list(list(1), list(2), list(3)),\n      list(list(4), list(5), list(6))),\n    output = list(\n      list(1, 2, 3), list(4, 5, 6))),\n  features = \"inputs\",\n  response = \"output\"\n)\n\nIn the above example, the data is a list of two named lists where each named list can be seen as different columns in a dataset. In this case, a column named features is being used as features to the model and a column named response is being used as the response variable."
  },
  {
    "objectID": "v1/guide/tfestimators/parsing_spec.html",
    "href": "v1/guide/tfestimators/parsing_spec.html",
    "title": "Parsing Utilities",
    "section": "",
    "text": "Parsing utilities are a set of functions that helps generate parsing spec for tf$parse_example to be used with estimators. If users keep data in tf$Example format, they need to call tf$parse_example with a proper feature spec. There are two main things that these utility functions help:\n\nUsers need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf$Example instance. The utility functions combine these specs.\nIt is difficult to map expected label by a estimator such as dnn_classifier to corresponding tf$parse_example spec. The utility functions encode it by getting related information from users (key, dtype)."
  },
  {
    "objectID": "v1/guide/tfestimators/parsing_spec.html#example-output-of-parsing-spec",
    "href": "v1/guide/tfestimators/parsing_spec.html#example-output-of-parsing-spec",
    "title": "Parsing Utilities",
    "section": "Example output of parsing spec",
    "text": "Example output of parsing spec\n\nparsing_spec <- classifier_parse_example_spec(\n  feature_columns = column_numeric('a'),\n  label_key = 'b',\n  weight_column = 'c'\n)\n\nFor the above example, classifier_parse_example_spec would return the following:\n\nexpected_spec <- list(\n  a = tf$python$ops$parsing_ops$FixedLenFeature(reticulate::tuple(1L), dtype = tf$float32),\n  c = tf$python$ops$parsing_ops$FixedLenFeature(reticulate::tuple(1L), dtype = tf$float32),\n  b = tf$python$ops$parsing_ops$FixedLenFeature(reticulate::tuple(1L), dtype = tf$int64)\n)\n\n# This should be the same as the one we constructed using `classifier_parse_example_spec`\ntestthat::expect_equal(parsing_spec, expected_spec)"
  },
  {
    "objectID": "v1/guide/tfestimators/parsing_spec.html#example-usage-with-a-classifier",
    "href": "v1/guide/tfestimators/parsing_spec.html#example-usage-with-a-classifier",
    "title": "Parsing Utilities",
    "section": "Example usage with a classifier",
    "text": "Example usage with a classifier\nFirstly, define features transformations and initiailize your classifier similar to the following:\n\nfcs <- feature_columns(...)\n\nmodel <- dnn_classifier(\n  n_classes = 1000,\n  feature_columns = fcs,\n  weight_column = 'example-weight',\n  label_vocabulary= c('photos', 'keep', ...),\n  hidden_units = c(256, 64, 16)\n)\n\nNext, create the parsing configuration for tf$parse_example using classifier_parse_example_spec and the feature columns fcs we have just defined:\n\nparsing_spec <- classifier_parse_example_spec(\n  feature_columns = fcs,\n  label_key = 'my-label',\n  label_dtype = tf$string,\n  weight_column = 'example-weight'\n)\n\nThis label configuration tells the classifier the following:\n\nweights are retrieved with key ‘example-weight’\nlabel is string and can be one of the following c('photos', 'keep', ...)\ninteger id for label ‘photos’ is 0, ‘keep’ is 1, etc\n\nThen define your input function with the help of read_batch_features that reads the batches of features from files in tf$Example format with the parsing configuration parsing_spec we just defined:\n\ninput_fn_train <- function() {\n  features <- tf$contrib$learn$read_batch_features(\n    file_pattern = train_files,\n    batch_size = batch_size,\n    features = parsing_spec,\n    reader = tf$RecordIOReader)\n  labels <- features[[\"my-label\"]]\n  return(list(features, labels))\n}\n\nFinally we can train the model using the training input function parsed by classifier_parse_example_spec:\n\ntrain(model, input_fn = input_fn_train)"
  },
  {
    "objectID": "v1/guide/tfestimators/run_hooks.html",
    "href": "v1/guide/tfestimators/run_hooks.html",
    "title": "Run Hooks",
    "section": "",
    "text": "SessionRunHooks are useful to track training, report progress, request early stopping and more. Users can attach an arbitrary number of hooks to an estimator. SessionRunHooks use the observer pattern and notify at the following points:\n\nwhen a session starts being used\nbefore a call to the session.run()\nafter a call to the session.run()\nwhen the session closed\n\nA SessionRunHook encapsulates a piece of reusable/composable computation that can piggyback a call to MonitoredSession.run(). A hook can add any ops-or-tensor/feeds to the run call, and when the run call finishes with success gets the outputs it requested. Hooks are allowed to add ops to the graph in hook.begin(). The graph is finalized after the begin() method is called."
  },
  {
    "objectID": "v1/guide/tfestimators/run_hooks.html#built-in-run-hooks",
    "href": "v1/guide/tfestimators/run_hooks.html#built-in-run-hooks",
    "title": "Run Hooks",
    "section": "Built-in Run Hooks",
    "text": "Built-in Run Hooks\nThere are a few pre-defined SessionRunHooks available, for example: Run hooks are useful for tracking training, reporting progress, requesting early stopping, and more. Users can attach an arbitrary number of hooks to an estimator. Some built-in run hooks include:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nhook_checkpoint_saver()\nSaves checkpoints every N steps or seconds.\n\n\nhook_global_step_waiter()\nDelay execution until global step reaches to wait_until_step.\n\n\nhook_history_saver()\nSaves Metrics History.\n\n\nhook_logging_tensor()\nPrints the given tensors once every N local steps or once every N seconds.\n\n\nhook_nan_tensor()\nNaN Loss monitor.\n\n\nhook_progress_bar()\nCreates and updates progress bar.\n\n\nhook_step_counter()\nSteps per second monitor.\n\n\nhook_stop_at_step()\nMonitor to request stop at a specified step.\n\n\nhook_summary_saver()\nSaves summaries every N steps.\n\n\n\nFor example, we can use hook_progress_bar() to attach a hook to create and update a progress bar during the model training process.\n\nfcs <- feature_columns(column_numeric(\"drat\"))\ninput <- input_fn(mtcars, response = \"mpg\", features = c(\"drat\", \"cyl\"), batch_size = 8L)\nlr <- linear_regressor(\n  feature_columns = fcs\n) %>% train(\n  input_fn = input,\n  steps = 2,\n  hooks = list(\n    hook_progress_bar()\n  ))\n\nTraining 2/2 [======================================] - ETA:  0s - loss: 3136.10\nAnother example is to use hook_history_saver() to save the training history every 2 training steps like the following:\n\nlr <- linear_regressor(feature_columns = fcs) \ntraining_history <- train(\n  lr,\n  input_fn = input,\n  steps = 4,\n  hooks = list(\n    hook_history_saver(every_n_step = 2)\n  ))\n\ntrain() returns the saved training metrics history:\n> training_history\n  mean_losses total_losses steps\n1    343.9690     2751.752     2\n2    419.7618     3358.094     4"
  },
  {
    "objectID": "v1/guide/tfestimators/run_hooks.html#custom-run-hooks",
    "href": "v1/guide/tfestimators/run_hooks.html#custom-run-hooks",
    "title": "Run Hooks",
    "section": "Custom Run Hooks",
    "text": "Custom Run Hooks\nUsers can also create custom run hooks by defining the behaviors of the hook in different phases of a session.\nWe can implement a custom run hook by defining a list of call back functions as part of session_run_hook() initialization. It has the following optional parameters that can be overriden by a custom defined function:\n\nbegin: An function with signature function(), to be called once before using the session.\nafter_create_session: An function with signature function(session, coord), to be called once the new TensorFlow session has been created.\nbefore_run: An function with signature function(run_context)to be called before a run.\nafter_run: An function with signature function(run_context, run_values) to be called after a run.\nend: An function with signature function(session) to be called at the end of the session.\n\nFor example, let’s try to implement the hook_history_saver() that we showed in previous section. We first initialize a iter_count variable to save the current count of the steps being run. We increment the count as part of after_run() after each session.run calls. Inside before_run(), we use the context to access the current losses and save it to a tensor named “losses” so that later we can access it inside after_run() via values$results$losses that contains the evaluated value of tensor “losses”. Finally, we calculate the mean of the raw losses and append it to a global state varibale named “mean_losses_history” with the list of mean losses.\n\nmean_losses_history <<- NULL\nhook_history_saver_custom <- function(every_n_step) {\n\n    iter_count <<- 0\n\n    session_run_hook(\n\n      before_run = function(context) {\n        session_run_args(\n          losses = context$session$graph$get_collection(\"losses\")\n        )\n      },\n      \n      after_run = function(context, values) {\n        iter_count <<- iter_count + 1\n        print(paste0(\"Running step: \", iter_count))\n        if (iter_count %% every_n_step == 0) {\n          raw_losses <- values$results$losses[[1]]\n          mean_losses_history <<- c(mean_losses_history, mean(raw_losses))\n        }\n      }\n    )\n}\n\nNext, we can then attach this hook to our estimator:\n\nlr <- linear_regressor(\n  feature_columns = fcs\n) %>% train(\n  input_fn = input,\n  steps = 4,\n  hooks = list(\n    hook_history_saver_custom(every_n_step = 1)\n  ))\n\n[1] \"Running step: 1\"\n[1] \"Running step: 2\"\n[1] \"Running step: 3\"\n[1] \"Running step: 4\"\nWe saved the losses history at every step. Let’s check the list of losses:\n> mean_losses_history\n[1] 415.8088 452.2128 376.7346 331.6045"
  },
  {
    "objectID": "v1/guide/tfestimators/tensorboard.html",
    "href": "v1/guide/tfestimators/tensorboard.html",
    "title": "TensorBoard Visualization",
    "section": "",
    "text": "TensorBoard is a visualization tool included with TensorFlow that enables you to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\nModels built using tfestimators automatically contain most of the necessary information to visualize the TensorFlow graph, variables, etc., for you so you can easily launch the TensorBoard without additional manual specifications."
  },
  {
    "objectID": "v1/guide/tfestimators/tensorboard.html#examples",
    "href": "v1/guide/tfestimators/tensorboard.html#examples",
    "title": "TensorBoard Visualization",
    "section": "Examples",
    "text": "Examples\nTo start the TensorBoard, you trained a model in a similar fashion as follows:\n\nestimator(\n    model_fn = model_fn,\n    model_dir = \"/tmp/test\"\n) %>% train(input_fn = input, steps = 100L)\n\nand then you can call\ntensorboard(log_dir = \"/tmp/test\", launch_browser = TRUE)\nwith log_dir being the directory you used to save the model checkpoints, to launch the TensorBoard in browser.\nFor example, here’s a TensorBoard display for a model’s training loss:\n\nHere’s another TensorBoard display for this model’s TensorFlow graph that demonstrates the architecture of the model:\n\nYou can also click to see each subgraph’s details:"
  },
  {
    "objectID": "v1/guide/tfestimators/tensorflow_layers.html",
    "href": "v1/guide/tfestimators/tensorflow_layers.html",
    "title": "TensorFlow Layers",
    "section": "",
    "text": "The TensorFlow tf$layers module provides a high-level API that makes it easy to construct a neural network. It provides methods that facilitate the creation of dense (fully connected) layers and convolutional layers, adding activation functions, and applying dropout regularization. In this tutorial, you’ll learn how to use layers to build a convolutional neural network model to recognize the handwritten digits in the MNIST data set. The complete code for this tutorial can be found here.\nThe MNIST dataset comprises 60,000 training examples and 10,000 test examples of the handwritten digits 0–9, formatted as 28x28-pixel monochrome images."
  },
  {
    "objectID": "v1/guide/tfestimators/tensorflow_layers.html#getting-started",
    "href": "v1/guide/tfestimators/tensorflow_layers.html#getting-started",
    "title": "TensorFlow Layers",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s set up the skeleton for our TensorFlow program by adding the following code to import the necessary libraries and change the logging verbosity:\nlibrary(tensorflow)\nlibrary(tfestimators)\ntf$logging$set_verbosity(tf$logging$INFO)\nAs you work through the tutorial, you’ll add code to construct, train, and evaluate the convolutional neural network."
  },
  {
    "objectID": "v1/guide/tfestimators/tensorflow_layers.html#intro-to-convolutional-neural-networks",
    "href": "v1/guide/tfestimators/tensorflow_layers.html#intro-to-convolutional-neural-networks",
    "title": "TensorFlow Layers",
    "section": "Intro to Convolutional Neural Networks",
    "text": "Intro to Convolutional Neural Networks\nConvolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n\nConvolutional layers, which apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\nPooling layers, which downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\nDense (fully connected) layers, which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n\nTypically, a CNN is composed of a stack of convolutional modules that perform feature extraction. Each module consists of a convolutional layer followed by a pooling layer. The last convolutional module is followed by one or more dense layers that perform classification. The final dense layer in a CNN contains a single node for each target class in the model (all the possible classes the model may predict), with a softmax activation function to generate a value between 0–1 for each node (the sum of all these softmax values is equal to 1). We can interpret the softmax values for a given image as relative measurements of how likely it is that the image falls into each target class.\n\nNote: For a more comprehensive walkthrough of CNN architecture, see Stanford University’s  Convolutional Neural Networks for Visual Recognition course materials."
  },
  {
    "objectID": "v1/guide/tfestimators/tensorflow_layers.html#building_the_cnn_mnist_classifier",
    "href": "v1/guide/tfestimators/tensorflow_layers.html#building_the_cnn_mnist_classifier",
    "title": "TensorFlow Layers",
    "section": "Building the CNN MNIST Classifier",
    "text": "Building the CNN MNIST Classifier\nLet’s build a model to classify the images in the MNIST dataset using the following CNN architecture:\n\nConvolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\nPooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\nConvolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\nPooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\nDense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\nDense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0–9).\n\nThe tf$layers module contains methods to create each of the three layer types above:\n\nconv2d(). Constructs a two-dimensional convolutional layer. Takes number of filters, filter kernel size, padding, and activation function as arguments.\nmax_pooling2d(). Constructs a two-dimensional pooling layer using the max-pooling algorithm. Takes pooling filter size and stride as arguments.\ndense(). Constructs a dense layer. Takes number of neurons and activation function as arguments.\n\nEach of these methods accepts a tensor as input and returns a transformed tensor as output. This makes it easy to connect one layer to another: just take the output from one layer-creation method and supply it as input to another.\nThe following cnn_model_fn function conforms to the interface expected by TensorFlow’s Estimator API (more on this later in Create the Estimator). This example takes MNIST feature data, labels, and mode_keys() (e.g. \"train\", \"eval\", \"infer\") as arguments; configures the CNN; and returns predictions, loss, and a training operation:\ncnn_model_fn <- function(features, labels, mode, params, config) {\n  \n  # Input Layer\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n  # MNIST images are 28x28 pixels, and have one color channel\n  input_layer <- tf$reshape(features$x, c(-1L, 28L, 28L, 1L))\n  \n  # Convolutional Layer #1\n  # Computes 32 features using a 5x5 filter with ReLU activation.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\n  conv1 <- tf$layers$conv2d(\n    inputs = input_layer,\n    filters = 32L,\n    kernel_size = c(5L, 5L),\n    padding = \"same\",\n    activation = tf$nn$relu)\n  \n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n  pool1 <- tf$layers$max_pooling2d(inputs = conv1, pool_size = c(2L, 2L), strides = 2L)\n  \n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n  conv2 <- tf$layers$conv2d(\n    inputs = pool1,\n    filters = 64L,\n    kernel_size = c(5L, 5L),\n    padding = \"same\",\n    activation = tf$nn$relu)\n  \n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n  pool2 <- tf$layers$max_pooling2d(inputs = conv2, pool_size = c(2L, 2L), strides = 2L)\n  \n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n  pool2_flat <- tf$reshape(pool2, c(-1L, 7L * 7L * 64L))\n  \n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n  dense <- tf$layers$dense(inputs = pool2_flat, units = 1024L, activation = tf$nn$relu)\n  \n  # Add dropout operation; 0.6 probability that element will be kept\n  dropout <- tf$layers$dropout(\n    inputs = dense, rate = 0.4, training = (mode == \"train\"))\n  \n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n  logits <- tf$layers$dense(inputs = dropout, units = 10L)\n  \n  # Generate Predictions (for prediction mode)\n  predicted_classes <- tf$argmax(input = logits, axis = 1L, name = \"predicted_classes\")\n  if (mode == \"infer\") {\n    predictions <- list(\n      classes = predicted_classes,\n      probabilities = tf$nn$softmax(logits, name = \"softmax_tensor\")\n    )\n    return(estimator_spec(mode = mode, predictions = predictions))\n  }\n  \n  # Calculate Loss (for both \"train\" and \"eval\" modes)\n  onehot_labels <- tf$one_hot(indices = tf$cast(labels, tf$int32), depth = 10L)\n  loss <- tf$losses$softmax_cross_entropy(\n    onehot_labels = onehot_labels, logits = logits)\n  \n  # Configure the Training Op (for \"train\" mode)\n  if (mode == \"train\") {\n    optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.001)\n    train_op <- optimizer$minimize(\n      loss = loss,\n      global_step = tf$train$get_global_step())\n    return(estimator_spec(mode = mode, loss = loss, train_op = train_op))\n  }\n    \n  # Add evaluation metrics (for EVAL mode)\n  eval_metric_ops <- list(accuracy = tf$metrics$accuracy(\n    labels = labels, predictions = predicted_classes))\n\n  return(estimator_spec(\n    mode = mode, loss = loss, eval_metric_ops = eval_metric_ops))\n}\nThe following sections (with headings corresponding to each code block above) dive deeper into the tf$layers code used to create each layer, as well as how to calculate loss, configure the training op, and generate predictions. If you’re already experienced with CNNs and creatings estimators in tfestimators, and find the above code intuitive, you may want to skim these sections or just skip ahead to “Training and Evaluating the CNN MNIST Classifier”.\n\nInput Layer\nThe methods in the layers module for creating convolutional and pooling layers for two-dimensional image data expect input tensors to have a shape of [batch_size, image_width, image_height, channels], defined as follows:\n\nbatch_size. Size of the subset of examples to use when performing gradient descent during training.\nimage_width. Width of the example images.\nimage_height. Height of the example images.\nchannels. Number of color channels in the example images. For color images, the number of channels is 3 (red, green, blue). For monochrome images, there is just 1 channel (black).\n\nHere, our MNIST dataset is composed of monochrome 28x28 pixel images, so the desired shape for our input layer is [batch_size, 28, 28, 1].\nTo convert our input feature map (features) to this shape, we can perform the following reshape operation:\ninput_layer <- tf$reshape(features$x, c(-1L, 28L, 28L, 1L))\nNote that we’ve indicated -1 for batch size, which specifies that this dimension should be dynamically computed based on the number of input values in features$x, holding the size of all other dimensions constant. This allows us to treat batch_size as a hyperparameter that we can tune. For example, if we feed examples into our model in batches of 5, features$x will contain 3,920 values (one value for each pixel in each image), and input_layer will have a shape of [5, 28, 28, 1]. Similarly, if we feed examples in batches of 100, features$x will contain 78,400 values, and input_layer will have a shape of [100, 28, 28, 1].\n\n\nConvolutional Layer #1\nIn our first convolutional layer, we want to apply 32 5x5 filters to the input layer, with a ReLU activation function. We can use the conv2d() method in the layers module to create this layer as follows:\nconv1 <- tf$layers$conv2d(\n    inputs = input_layer,\n    filters = 32L,\n    kernel_size = c(5L, 5L),\n    padding = \"same\",\n    activation = tf$nn$relu)\nThe inputs argument specifies our input tensor, which must have the shape [batch_size, image_width, image_height, channels]. Here, we’re connecting our first convolutional layer to input_layer, which has the shape [batch_size, 28, 28, 1].\n\nNote: conv2d() will instead accept a shape of [channels, batch_size, image_width, image_height] when passed the argument data_format=channels_first.\n\nThe filters argument specifies the number of filters to apply (here, 32), and kernel_size specifies the dimensions of the filters as [width, height] (here, [5, 5]).\n\nTIP: If filter width and height have the same value, you can instead specify a single integer for kernel_size—e.g., kernel_size=5.\n\nThe padding argument specifies one of two enumerated values (case-insensitive): valid (default value) or same. To specify that the output tensor should have the same width and height values as the input tensor, we set padding=same here, which instructs TensorFlow to add 0 values to the edges of the output tensor to preserve width and height of 28. (Without padding, a 5x5 convolution over a 28x28 tensor will produce a 24x24 tensor, as there are 24x24 locations to extract a 5x5 tile from a 28x28 grid.)\nThe activation argument specifies the activation function to apply to the output of the convolution. Here, we specify ReLU activation with @tf.nn.relu.\nOur output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 32]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.\n\n\nPooling Layer #1\nNext, we connect our first pooling layer to the convolutional layer we just created. We can use the max_pooling2d() method in layers to construct a layer that performs max pooling with a 2x2 filter and stride of 2:\npool1 <- tf$layers$max_pooling2d(inputs = conv1, pool_size = c(2L, 2L), strides = 2L)\nAgain, inputs specifies the input tensor, with a shape of [batch_size, image_width, image_height, channels]. Here, our input tensor is conv1, the output from the first convolutional layer, which has a shape of [batch_size, 28, 28, 32].\n\nNote: As with conv2d(), max_pooling2d() will instead accept a shape of [channels, batch_size, image_width, image_height] when passed the argument data_format=channels_first.\n\nThe pool_size argument specifies the size of the max pooling filter as [width, height] (here, [2, 2]). If both dimensions have the same value, you can instead specify a single integer (e.g., pool_size = 2).\nThe strides argument specifies the size of the stride. Here, we set a stride of 2, which indicates that the subregions extracted by the filter should be separated by 2 pixels in both the width and height dimensions (for a 2x2 filter, this means that none of the regions extracted will overlap). If you want to set different stride values for width and height, you can instead specify a tuple or list (e.g., stride = c(3, 6)).\nOur output tensor produced by max_pooling2d() (pool1) has a shape of [batch_size, 14, 14, 32]: the 2x2 filter reduces width and height by 50% each.\n\n\nConvolutional Layer #2 and Pooling Layer #2\nWe can connect a second convolutional and pooling layer to our CNN using conv2d() and max_pooling2d() as before. For convolutional layer #2, we configure 64 5x5 filters with ReLU activation, and for pooling layer #2, we use the same specs as pooling layer #1 (a 2x2 max pooling filter with stride of 2):\nconv2 <- tf$layers$conv2d(\n    inputs = pool1,\n    filters = 64L,\n    kernel_size = c(5L, 5L),\n    padding = \"same\",\n    activation = tf$nn$relu)\n\npool2 <- tf$layers$max_pooling2d(inputs = conv2, pool_size = c(2L, 2L), strides = 2L)\nNote that convolutional layer #2 takes the output tensor of our first pooling layer (pool1) as input, and produces the tensor conv2 as output. conv2 has a shape of [batch_size, 14, 14, 64], the same width and height as pool1 (due to padding=\"same\"), and 64 channels for the 64 filters applied.\nPooling layer #2 takes conv2 as input, producing pool2 as output. pool2 has shape [batch_size, 7, 7, 64] (50% reduction of width and height from conv2).\n\n\nDense Layer\nNext, we want to add a dense layer (with 1,024 neurons and ReLU activation) to our CNN to perform classification on the features extracted by the convolution/pooling layers. Before we connect the layer, however, we’ll flatten our feature map (pool2) to shape [batch_size, features], so that our tensor has only two dimensions:\npool2_flat <- tf$reshape(pool2, c(-1L, 7L * 7L * 64L))\nIn the reshape() operation above, the -1 signifies that the batch_size dimension will be dynamically calculated based on the number of examples in our input data. Each example has 7 (pool2 width) * 7 (pool2 height) * 64 (pool2 channels) features, so we want the features dimension to have a value of 7 * 7 * 64 (3136 in total). The output tensor, pool2_flat, has shape [batch_size, 3136].\nNow, we can use the dense() method in layers to connect our dense layer as follows:\ndense <- tf$layers$dense(inputs = pool2_flat, units = 1024L, activation = tf$nn$relu)\nThe inputs argument specifies the input tensor: our flattened feature map, pool2_flat. The units argument specifies the number of neurons in the dense layer (1,024). The activation argument takes the activation function; again, we’ll use tf.nn.relu to add ReLU activation.\nTo help improve the results of our model, we also apply dropout regularization to our dense layer, using the dropout method in layers:\ndropout <- tf$layers$dropout(\n    inputs = dense, rate = 0.4, training = (mode == \"train\"))\nAgain, inputs specifies the input tensor, which is the output tensor from our dense layer (dense).\nThe rate argument specifies the dropout rate; here, we use 0.4, which means 40% of the elements will be randomly dropped out during training.\nThe training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True. Here, we check if the mode passed to our model function cnn_model_fn is \"train mode.\nOur output tensor dropout has shape [batch_size, 1024].\n\n\nLogits Layer\nThe final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default):\nlogits <- tf$layers$dense(inputs = dropout, units = 10L)\nOur final output tensor of the CNN, logits, has shape [batch_size, 10].\n\n\nGenerate Predictions\nThe logits layer of our model returns our predictions as raw values in a [batch_size, 10]-dimensional tensor. Let’s convert these raw values into two different formats that our model function can return:\n\nThe predicted class for each example: a digit from 0–9.\nThe probabilities for each possible target class for each example: the probability that the example is a 0, is a 1, is a 2, etc.\n\nFor a given example, our predicted class is the element in the corresponding row of the logits tensor with the highest raw value. We can find the index of this element using the @tf.argmax function:\ntf$argmax(input = logits, axis = 1L)\nThe input argument specifies the tensor from which to extract maximum values—here logits. The axis argument specifies the axis of the input tensor along which to find the greatest value. Here, we want to find the largest value along the dimension with index of 1, which corresponds to our predictions (recall that our logits tensor has shape [batch_size, 10]).\nWe can derive probabilities from our logits layer by applying softmax activation using @tf.nn.softmax:\ntf$nn$softmax(logits, name = \"softmax_tensor\")\n\nNote: We use the name argument to explicitly name this operation softmax_tensor, so we can reference it later.\n\nWe compile our predictions in a dict, and return an estimator_spec object:\npredicted_classes <- tf$argmax(input = logits, axis = 1L)\nif (mode == \"infer\") {\n  predictions <- list(\n    classes = predicted_classes,\n    probabilities = tf$nn$softmax(logits, name = \"softmax_tensor\")\n  )\n  return(estimator_spec(mode = mode, predictions = predictions))\n}\n\n\nCalculate Loss\nFor both training and evaluation, we need to define a loss function that measures how closely the model’s predictions match the target classes. For multiclass classification problems like MNIST, cross entropy is typically used as the loss metric. The following code calculates cross entropy when the model runs in either TRAIN or EVAL mode:\nonehot_labels <- tf$one_hot(indices = tf$cast(labels, tf$int32), depth = 10L)\nloss <- tf$losses$softmax_cross_entropy(\n  onehot_labels = onehot_labels, logits = logits)\nLet’s take a closer look at what’s happening above.\nOur labels tensor contains a list of predictions for our examples, e.g. [1, 9, ...]. In order to calculate cross-entropy, first we need to convert labels to the corresponding one-hot encoding:\n[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n ...]\nWe use the tf$one_hot function to perform this conversion. tf$one_hot has two required arguments:\n\nindices. The locations in the one-hot tensor that will have “on values”—i.e., the locations of 1 values in the tensor shown above.\ndepth. The depth of the one-hot tensor—i.e., the number of target classes. Here, the depth is 10.\n\nThe following code creates the one-hot tensor for our labels, onehot_labels:\nonehot_labels <- tf$one_hot(indices = tf$cast(labels, tf$int32), depth = 10L)\nBecause labels contains a series of values from 0–9, indices is just our labels tensor, with values cast to integers. The depth is 10 because we have 10 possible target classes, one for each digit.\nNext, we compute cross-entropy of onehot_labels and the softmax of the predictions from our logits layer. tf$losses$softmax_cross_entropy() takes onehot_labels and logits as arguments, performs softmax activation on logits, calculates cross-entropy, and returns our loss as a scalar Tensor:\nloss <- tf$losses$softmax_cross_entropy(\n    onehot_labels = onehot_labels, logits = logits)\n\n\nConfigure the Training Op\nIn the previous section, we defined loss for our CNN as the softmax cross-entropy of the logits layer and our labels. Let’s configure our model to optimize this loss value during training. We’ll use a learning rate of 0.001 and stochastic gradient descent as the optimization algorithm:\nif (mode == \"train\") {\n  optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.001)\n  train_op <- optimizer$minimize(\n    loss = loss,\n    global_step = tf$train$get_global_step())\n  return(estimator_spec(mode = mode, loss = loss, train_op = train_op))\n}\n\n\nAdd evaluation metrics\nTo add accuracy metric in our model, we define eval_metric_ops dict in EVAL mode as follows:\neval_metric_ops <- list(accuracy = tf$metrics$accuracy(\n  labels = labels, predictions = predicted_classes))\n\nreturn(estimator_spec(\n  mode = mode, loss = loss, eval_metric_ops = eval_metric_ops))"
  },
  {
    "objectID": "v1/guide/tfestimators/tensorflow_layers.html#training_and_evaluating_the_cnn_mnist_classifier",
    "href": "v1/guide/tfestimators/tensorflow_layers.html#training_and_evaluating_the_cnn_mnist_classifier",
    "title": "TensorFlow Layers",
    "section": "Training and Evaluating the CNN MNIST Classifier",
    "text": "Training and Evaluating the CNN MNIST Classifier\nWe’ve coded our MNIST CNN model function; now we’re ready to train and evaluate it.\n\nLoad Training and Test Data\nFirst, let’s load our training and test data:\nnp <- import(\"numpy\", convert = FALSE)\n# Load training and eval data\nmnist <- tf$contrib$learn$datasets$load_dataset(\"mnist\")\ntrain_data <- np$asmatrix(mnist$train$images, dtype = np$float32)\ntrain_labels <- np$asarray(mnist$train$labels, dtype = np$int32)\neval_data <- np$asmatrix(mnist$test$images, dtype = np$float32)\neval_labels <- np$asarray(mnist$test$labels, dtype = np$int32)\nWe store the training feature data (the raw pixel values for 55,000 images of hand-drawn digits) and training labels (the corresponding value from 0–9 for each image) as numpy arrays in train_data and train_labels, respectively. Similarly, we store the evaluation feature data (10,000 images) and evaluation labels in eval_data and eval_labels, respectively.\n\n\nCreate the Estimator\nNext, let’s create an estimator (a TensorFlow class for performing high-level model training, evaluation, and inference) for our model.\n# Create the Estimator\nmnist_classifier <- estimator(\n  model_fn = cnn_model_fn, model_dir = \"/tmp/mnist_convnet_model\")\nThe model_fn argument specifies the model function to use for training, evaluation, and prediction; we pass it the cnn_model_fn we created in “Building the CNN MNIST Classifier.” The model_dir argument specifies the directory where model data (checkpoints) will be saved (here, we specify the temp directory /tmp/mnist_convnet_model, but feel free to change to another directory of your choice).\n\nNote: For an in-depth walkthrough of the TensorFlow Estimator API, see the tutorial for custom estimator.\n\n\n\nSet Up a Logging Hook\nSince CNNs can take a while to train, let’s set up some logging so we can track progress during training. We can use TensorFlow’s SessionRunHook to create a hook_logging_tensor that will log the predicted classes from the argmax operation.\n# Set up logging for predicted classes\ntensors_to_log <- list(predicted_classes = \"predicted_classes\")\nlogging_hook <- hook_logging_tensor(\n  tensors = tensors_to_log, every_n_iter = 50)\nWe store a dict of the tensors we want to log in tensors_to_log. Each key is a label of our choice that will be printed in the log output, and the corresponding label is the name of a Tensor in the TensorFlow graph. Here, our predicted classes can be found in predicted_classes, the name we gave our argmax operation earlier when we generated the predicted classes in cnn_model_fn.\nNext, we create the hook_logging_tensor, passing tensors_to_log to the tensors argument. We set every_n_iter = 50, which specifies that probabilities should be logged after every 50 steps of training.\n\n\nTrain the Model\nNow we’re ready to train our model, which we can do by creating train_input_fn ans calling train() on mnist_classifier.\n# Train the model\ntrain_input_fn <- function(features_as_named_list) {\n  tf$estimator$inputs$numpy_input_fn(\n    x = list(x = train_data),\n    y = train_labels,\n    batch_size = 100L,\n    num_epochs = NULL,\n    shuffle = TRUE)\n}\ntrain(\n  mnist_classifier,\n  input_fn = train_input_fn,\n  steps = 20,\n  hooks = logging_hook)\nIn the numpy_input_fn call, we pass the training feature data and labels to x (as a dict) and y, respectively. We set a batch_size of 100 (which means that the model will train on minibatches of 100 examples at each step). num_epochs = NULL means that the model will train until the specified number of steps is reached. We also set shuffle = TRUE to shuffle the training data. In the train call, we set steps = 2 (which means the model will train for 10 steps total).\n\n\nEvaluate the Model\nOnce training is complete, we want to evaluate our model to determine its accuracy on the MNIST test set. We call the evaluate method, which evaluates the metrics we specified in eval_metric_ops argument in the model_fn.\n# Evaluate the model and print results\neval_input_fn <- function(features_as_named_list) {\n  tf$estimator$inputs$numpy_input_fn(\n    x = list(x = eval_data),\n    y = eval_labels,\n    batch_size = 100L,\n    num_epochs = NULL,\n    shuffle = TRUE)\n}\nevaluate(\n  mnist_classifier,\n  input_fn = eval_input_fn,\n  steps = 10,\n  hooks = logging_hook)\nTo create eval_input_fn, we set num_epochs = 1, so that the model evaluates the metrics over one epoch of data and returns the result. We also set shuffle = FALSE to iterate through the data sequentially.\nWe pass our logging_hook to the hooks argument, so that it will be triggered during evaluation.\n\n\nRun the Model\nWe’ve coded the CNN model function, Estimator, and the training/evaluation logic; now let’s see the results.\nAs the model trains, you’ll see log output like the following:\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-5\nINFO:tensorflow:Saving checkpoints for 6 into /tmp/mnist_convnet_model/model.ckpt.\nINFO:tensorflow:loss = 2.29727, step = 6\nINFO:tensorflow:Saving checkpoints for 7 into /tmp/mnist_convnet_model/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.30916.\nYou’ll see log output like the following during model evaluation with the predicted_classes that we included in the logging_hook:\nINFO:tensorflow:Starting evaluation at 2017-07-04-17:05:28\nINFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-19\nINFO:tensorflow:predicted_classes = [6 9 1 9 9 1 9 1 9 6 1 9 9 9 9 1 9 9 9 3 1 1 1 9 6 1 9 9 9 9 9 9 1 1 9 9 9\n 9 9 9 9 1 9 9 1 4 4 1 9 1 9 9 1 9 9 9 9 9 9 1 6 9 9 1 9 6 9 9 9 9 9 9 1 9\n 9 3 9 9 9 9 1 9 9 9 3 9 1 9 9 9 9 9 9 9 9 9 1 9 9 9]\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:predicted_classes = [9 9 1 9 1 9 9 9 9 9 9 9 9 9 9 9 9 9 1 4 1 9 9 9 1 9 9 9 9 9 1 9 4 9 1 1 9\n 9 1 1 9 9 1 9 9 9 9 1 9 9 9 9 4 9 9 9 9 4 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n 9 1 9 9 9 9 9 9 9 9 1 1 9 9 9 9 1 9 9 9 9 9 9 9 9 9] (0.204 sec)\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [4/10]\n..."
  },
  {
    "objectID": "v1/guide/tfhub/examples/biggan_image_generation.html",
    "href": "v1/guide/tfhub/examples/biggan_image_generation.html",
    "title": "biggan_image_generation",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/biggan_image_generation.R"
  },
  {
    "objectID": "v1/guide/tfhub/examples/feature_column.html",
    "href": "v1/guide/tfhub/examples/feature_column.html",
    "title": "feature_column",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/feature_column.R"
  },
  {
    "objectID": "v1/guide/tfhub/examples/image_classification.html",
    "href": "v1/guide/tfhub/examples/image_classification.html",
    "title": "image_classification",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/image_classification.R"
  },
  {
    "objectID": "v1/guide/tfhub/examples/index.html",
    "href": "v1/guide/tfhub/examples/index.html",
    "title": "TensorFlow Hub Examples",
    "section": "",
    "text": "Example\nDescription\n\n\n\n\ntext_classification\nIn this example we use tfhub to obtain pre-trained word-embeddings and we use the word vectors to identify and classify toxic comments.\n\n\nrecipes\nIn this example we use tfhub and recipes to obtain pre-trained sentence embeddings. We then firt a logistic regression model.\n\n\nfeature_column\nIn this example we will use the PetFinder dataset to demonstrate the feature_spec functionality with TensorFlow Hub.\n\n\nbiggan_image_generation\nThis example is a demo of BigGAN image generators available on TF Hub."
  },
  {
    "objectID": "v1/guide/tfhub/examples/recipes.html",
    "href": "v1/guide/tfhub/examples/recipes.html",
    "title": "recipes",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/recipes.R"
  },
  {
    "objectID": "v1/guide/tfhub/examples/text_classification.html",
    "href": "v1/guide/tfhub/examples/text_classification.html",
    "title": "text_classification",
    "section": "",
    "text": "Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/text_classification.R"
  },
  {
    "objectID": "v1/guide/tfhub/hub-with-keras.html",
    "href": "v1/guide/tfhub/hub-with-keras.html",
    "title": "TensorFlow Hub with Keras",
    "section": "",
    "text": "TensorFlow Hub is a way to share pretrained model components. See the TensorFlow Module Hub for a searchable listing of pre-trained models. This tutorial demonstrates:"
  },
  {
    "objectID": "v1/guide/tfhub/hub-with-keras.html#setup",
    "href": "v1/guide/tfhub/hub-with-keras.html#setup",
    "title": "TensorFlow Hub with Keras",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)\nlibrary(tfhub)\nlibrary(magick)"
  },
  {
    "objectID": "v1/guide/tfhub/hub-with-keras.html#an-imagenet-classifier",
    "href": "v1/guide/tfhub/hub-with-keras.html#an-imagenet-classifier",
    "title": "TensorFlow Hub with Keras",
    "section": "An ImageNet classifier",
    "text": "An ImageNet classifier\n\nDownload the classifier\nUse layer_hub to load a mobilenet and transform it into a Keras layer. Any TensorFlow 2 compatible image classifier URL from tfhub.dev will work here.\n\nclassifier_url <- \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\" \nmobilenet_layer <- layer_hub(handle = classifier_url)\n\nWe can then create our Keras model:\n\ninput <- layer_input(shape = c(224, 224, 3))\noutput <- input %>% \n  mobilenet_layer()\n\nmodel <- keras_model(input, output)\n\n\n\nRun it on a single image\nDownload a single image to try the model on.\n\nimg <- image_read('https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg') %>%\n  image_resize(geometry = \"224x224x3!\") %>% \n  image_data() %>% \n  as.numeric() %>% \n  abind::abind(along = 0) # expand to batch dimension\n\n\n\n\n\nresult <- predict(model, img)\nmobilenet_decode_predictions(result[,-1, drop = FALSE])"
  },
  {
    "objectID": "v1/guide/tfhub/hub-with-keras.html#simple-transfer-learning",
    "href": "v1/guide/tfhub/hub-with-keras.html#simple-transfer-learning",
    "title": "TensorFlow Hub with Keras",
    "section": "Simple transfer learning",
    "text": "Simple transfer learning\nUsing TF Hub it is simple to retrain the top layer of the model to recognize the classes in our dataset.\n\nDataset\nFor this example you will use the TensorFlow flowers dataset:\n\ndata_root <- pins::pin(\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\", \"flower_photos\")\ndata_root <- fs::path_dir(fs::path_dir(data_root[100])) # go down 2 levels\n\nThe simplest way to load this data into our model is using image_data_generator\nAll of TensorFlow Hub’s image modules expect float inputs in the [0, 1] range. Use the image_data_generator’s rescale parameter to achieve this.\n\nimage_generator <- image_data_generator(rescale = 1/255, validation_split = 0.2)\ntraining_data <- flow_images_from_directory(\n  directory = data_root, \n  generator = image_generator,\n  target_size = c(224, 224), \n  subset = \"training\"\n)\n\nvalidation_data <- flow_images_from_directory(\n  directory = data_root, \n  generator = image_generator,\n  target_size = c(224, 224), \n  subset = \"validation\"\n)\n\nThe resulting object is an iterator that returns image_batch, label_batch pairs.\n\n\nDownload the headless model\nTensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\nAny Tensorflow 2 compatible image feature vector URL from tfhub.dev will work here.\n\nfeature_extractor_url <- \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\nfeature_extractor_layer <- layer_hub(handle = feature_extractor_url)\n\n\n\nAttach a classification head\nNow we can create our classification model by attaching a classification head into the feature extractor layer. We define the following model:\n\ninput <- layer_input(shape = c(224, 224, 3))\noutput <- input %>% \n  feature_extractor_layer() %>% \n  layer_dense(units = training_data$num_classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\nsummary(model)\n\n\n\nTrain the model\nWe can now train our model in the same way we would train any other Keras model. We first use compile to configure the training process:\n\nmodel %>% \n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"acc\"\n  )\n\nWe can then use the fit function to fit our model.\n\nmodel %>% \n  fit_generator(\n    training_data, \n    steps_per_epoch = training_data$n/training_data$batch_size,\n    validation_data = validation_data\n  )\n\nYou can then export your model with:\n\nsave_model_tf(model, \"model\")\n\nYou can also reload the model_from_saved_model function. Note that you need to pass the custom_object with the definition of the KerasLayer since it/s not a default Keras layer.\n\nreloaded_model <- load_model_tf(\"model\")\n\nWe can verify that the predictions of both the trained model and the reloaded model are equal:\n\nsteps <- as.integer(validation_data$n/validation_data$batch_size)\nall.equal(\n  predict_generator(model, validation_data, steps = steps),\n  predict_generator(reloaded_model, validation_data, steps = steps),\n)\n\nThe saved model can also be loaded for inference later or be converted to TFLite or TFjs."
  },
  {
    "objectID": "v1/guide/tfhub/intro.html",
    "href": "v1/guide/tfhub/intro.html",
    "title": "Overview",
    "section": "",
    "text": "The tfhub package provides R wrappers to TensorFlow Hub.\nTensorFlow Hub is a library for reusable machine learning modules.\nTensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. Transfer learning can:"
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#installation",
    "href": "v1/guide/tfhub/intro.html#installation",
    "title": "Overview",
    "section": "Installation",
    "text": "Installation\nYou can install the released version of tfhub from CRAN with:\ninstall.packages(\"tfhub\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"rstudio/tfhub\")\nAfter installing the tfhub package you need to install the TensorFlow Hub python module:\nlibrary(tfhub)\ninstall_tfhub()"
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#loading-modules",
    "href": "v1/guide/tfhub/intro.html#loading-modules",
    "title": "Overview",
    "section": "Loading modules",
    "text": "Loading modules\nModules can be loaded from URL’s and local paths using hub_load()\nmodule <- hub_load(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\")\nModule’s behave like functions and can be called with Tensors eg:\ninput <- tf$random$uniform(shape = shape(1,224,224,3), minval = 0, maxval = 1)\noutput <- module(input)"
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#using-with-keras",
    "href": "v1/guide/tfhub/intro.html#using-with-keras",
    "title": "Overview",
    "section": "Using with Keras",
    "text": "Using with Keras\nThe easiest way to get started with tfhub is using layer_hub. A Keras layer that loads a TensorFlow Hub module and prepares it for using with your model.\nlibrary(tfhub)\nlibrary(keras)\n\ninput <- layer_input(shape = c(32, 32, 3))\n\noutput <- input %>%\n  # we are using a pre-trained MobileNet model!\n  layer_hub(handle = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\nmodel %>%\n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\nWe can then fit our model in the CIFAR10 dataset:\ncifar <- dataset_cifar10()\ncifar$train$x <- tf$image$resize(cifar$train$x/255, size = shape(224,224))\n\nmodel %>%\n  fit(\n    x = cifar$train$x,\n    y = cifar$train$y,\n    validation_split = 0.2,\n    batch_size = 128\n  )"
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#using-with-tfdatasets",
    "href": "v1/guide/tfhub/intro.html#using-with-tfdatasets",
    "title": "Overview",
    "section": "Using with tfdatasets",
    "text": "Using with tfdatasets\ntfhub can also be used with tfdatasets since it provides implementations of feature_columns:\n\nhub_text_embedding_column()\nhub_sparse_text_embedding_column()\nhub_image_embedding_column()\n\nYou can find a working example here."
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#using-with-recipes",
    "href": "v1/guide/tfhub/intro.html#using-with-recipes",
    "title": "Overview",
    "section": "Using with recipes",
    "text": "Using with recipes\ntfhub adds a step_pretrained_text_embedding that can be used with the recipes package.\nAn example can be found here."
  },
  {
    "objectID": "v1/guide/tfhub/intro.html#tfhub.dev",
    "href": "v1/guide/tfhub/intro.html#tfhub.dev",
    "title": "Overview",
    "section": "tfhub.dev",
    "text": "tfhub.dev\ntfhub.dev is a gallery of pre-trained model ready to be used with TensorFlow Hub."
  },
  {
    "objectID": "v1/guide/tfhub/key-concepts.html",
    "href": "v1/guide/tfhub/key-concepts.html",
    "title": "Key concepts",
    "section": "",
    "text": "A TensorFlow Hub module is imported into a TensorFlow program by creating a Module object from a string with its URL or filesystem path, such as:\n\nlibrary(tfhub)\nm <- hub_load(\"path/to/a/module_dir\")\n\nThis adds the module’s variables to the current TensorFlow graph.\n\n\n\nWhen creating a module from a URL, the module content is downloaded and cached in the local system temporary directory. The location where modules are cached can be overridden using TFHUB_CACHE_DIR environment variable.\nFor example, setting TFHUB_CACHE_DIR to /my_module_cache:\n\nSys.setenv(TFHUB_CACHE_DIR = \"/my_module_cache\")\n\nand then creating a module from a URL:\n\nm <- hub_load(\"https://tfhub.dev/google/progan-128/1\")\n\nresults in downloading and unpacking the module into /my_module_cache.\n\n\n\nOnce instantiated, a module m can be called zero or more times like a Python function from tensor inputs to tensor outputs:\n\ny <- m(x)\n\nEach such call adds operations to the current TensorFlow graph to compute y from x. If this involves variables with trained weights, these are shared between all applications.\nModules can define multiple named signatures in order to allow being applied in more than one way (similar to how Python objects have methods). A module’s documentation should describe the available signatures. The call above applies the signature named “default”. Any signature can be selected by passing its name to the optional signature= argument.\nIf a signature has multiple inputs, they must be passed as a dict, with the keys defined by the signature. Likewise, if a signature has multiple outputs, these can be retrieved as a dict by passing as_dict=True, under the keys defined by the signature. (The key \"default\" is for the single output returned if as_dict=FALSE) So the most general form of applying a Module looks like:\n\noutputs <- m(list(apples=x1, oranges=x2), signature=\"fruit_to_pet\", as_dict=TRUE)\ny1 = outputs$cats\ny2 = outputs$dogs\n\nA caller must supply all inputs defined by a signature, but there is no requirement to use all of a module’s outputs. Module consumers should handle additional outputs gracefully."
  },
  {
    "objectID": "v1/guide/tfhub/key-concepts.html#creating-a-new-module",
    "href": "v1/guide/tfhub/key-concepts.html#creating-a-new-module",
    "title": "Key concepts",
    "section": "Creating a new Module",
    "text": "Creating a new Module\n\nGeneral approach\nA Hub Module is simply a TensorFlow graph in the SavedModel format. In order to create a Module you can run the export_savedmodel function with any TensorFlow object.\nFor example:\n\nlibrary(keras)\n\nmnist <- dataset_mnist()\n\ninput <- layer_input(shape(28,28), dtype = \"int32\")\n\noutput <- input %>% \n  layer_flatten() %>% \n  layer_lambda(tensorflow::tf_function(function(x) tf$cast(x, tf$float32)/255)) %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"acc\"\n  )\n\nmodel %>% \n  fit(x = mnist$train$x, y = mnist$train$y, validation_split = 0.2, epochs =1 )\n\nsave_model_tf(model, \"my_module/\", include_optimizer = FALSE)\n\nAfter exporting the model to the SavedModel format you can load it using hub_load, and use it for predictions for example:\n\nmodule <- hub_load(\"my_module/\")\n\npredictions <- module(mnist$test$x) %>% \n  tf$argmax(axis = 1L) \n\nmean(as.integer(predictions) == mnist$test$y)\n\nExporting a module serializes its definition together with the current state of its variables in session into the passed path. This can be used when exporting a module for the first time, as well as when exporting a fine tuned module.\nModule publishers should implement a common signature when possible, so that consumers can easily exchange modules and find the best one for their problem."
  },
  {
    "objectID": "v1/guide/tfhub/key-concepts.html#fine-tuning",
    "href": "v1/guide/tfhub/key-concepts.html#fine-tuning",
    "title": "Key concepts",
    "section": "Fine-Tuning",
    "text": "Fine-Tuning\nTraining the variables of an imported module together with those of the model around it is called fine-tuning. Fine-tuning can result in better quality, but adds new complications. We advise consumers to look into fine-tuning only after exploring simpler quality tweaks.\n\nFor consumers\nTo enable fine-tuning, instantiate the module with hub_module(..., trainable = TRUE) to make its variables trainable and import TensorFlow’s REGULARIZATION_LOSSES. If the module has multiple graph variants, make sure to pick the one appropriate for training. Usually, that’s the one with tags {\"train\"}.\nChoose a training regime that does not ruin the pre-trained weights, for example, a lower learning rate than for training from scratch.\n\n\nFor publishers\nTo make fine-tuning easier for consumers, please be mindful of the following:\n\nFine-tuning needs regularization. Your module is exported with the REGULARIZATION_LOSSES collection, which is what puts your choice of layer_dense(..., kernel_regularizer=...) etc. into what the consumer gets from tf$losses$get_regularization_losses(). Prefer this way of defining L1/L2 regularization losses.\nIn the publisher model, avoid defining L1/L2 regularization via the l1_ and l2_regularization_strength parameters of tf$train$FtrlOptimizer, tf$train$ProximalGradientDescentOptimizer, and other proximal optimizers. These are not exported alongside the module, and setting regularization strengths globally may not be appropriate for the consumer. Except for L1 regularization in wide (i.e. sparse linear) or wide & deep models, it should be possible to use individual regularization losses instead.\nIf you use dropout, batch normalization, or similar training techniques, set dropout rate and other hyperparameters to values that make sense across many expected uses."
  },
  {
    "objectID": "v1/guide/tfhub/key-concepts.html#hosting-a-module",
    "href": "v1/guide/tfhub/key-concepts.html#hosting-a-module",
    "title": "Key concepts",
    "section": "Hosting a Module",
    "text": "Hosting a Module\nTensorFlow Hub supports HTTP based distribution of modules. In particular the protocol allows to use the URL identifying the module both as the documentation of the module and the endpoint to fetch the module.\n\nProtocol\nWhen a URL such as https://example.com/module is used to identify a module to load or instantiate, the module resolver will attempt to download a compressed tar ball from the URL after appending a query parameter ?tf-hub-format=compressed.\nThe query param is to be interpreted as a comma separated list of the module formats that the client is interested in. For now only the \"compressed\" format is defined.\nThe compressed format indicates that the client expects a tar.gz archive with the module contents. The root of the archive is the root of the module directory and should contain a module e.g.:\n# Create a compressed module from an exported module directory.\n$ tar -cz -f module.tar.gz --owner=0 --group=0 -C /tmp/export-module/ .\n\n# Inspect files inside a compressed module\n$ tar -tf module.tar.gz\n./\n./tfhub_module.pb\n./variables/\n./variables/variables.data-00000-of-00001\n./variables/variables.index\n./assets/\n./saved_model.pb"
  },
  {
    "objectID": "v1/index.html",
    "href": "v1/index.html",
    "title": "R Interface to Tensorflow",
    "section": "",
    "text": "path\n\n\n\n\n\n\n/v1/tutorials/advanced/customization/autodiff.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_basic_classification.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_basic_regression.qmd\n\n\n\n\n/v1/guide/saving/checkpoints.qmd\n\n\n\n\n/v1/tutorials/advanced/structured/classify.qmd\n\n\n\n\n/v1/installation/gpu/cloud_desktop_gpu/index.qmd\n\n\n\n\n/v1/installation/gpu/cloud_server_gpu/index.qmd\n\n\n\n\n/v1/tutorials/advanced/images/cnn.qmd\n\n\n\n\n/v1/guide/tfestimators/creating_estimators.qmd\n\n\n\n\n/v1/installation/custom.qmd\n\n\n\n\n/v1/tutorials/advanced/customization/custom-layers.qmd\n\n\n\n\n/v1/tutorials/advanced/customization/custom-training.qmd\n\n\n\n\n/v1/guide/tfestimators/dataset_api.qmd\n\n\n\n\n/v1/tools/cloudml/deployment.qmd\n\n\n\n\n/v1/deploy/shiny.qmd\n\n\n\n\n/v1/deploy/plumber.qmd\n\n\n\n\n/v1/deploy/rsconnect.qmd\n\n\n\n\n/v1/deploy/docker.qmd\n\n\n\n\n/v1/tutorials/advanced/distributed/distributed_training_with_keras.qmd\n\n\n\n\n/v1/guide/tensorflow/eager_execution.qmd\n\n\n\n\n/v1/guide/tfestimators/estimator_basics.qmd\n\n\n\n\n/v1/guide/tfestimators/feature_columns.qmd\n\n\n\n\n/v1/guide/tfdatasets/feature_spec.qmd\n\n\n\n\n/v1/guide/tfdatasets/feature_columns.qmd\n\n\n\n\n/v1/guide/keras/faq.qmd\n\n\n\n\n/v1/guide/keras/index.qmd\n\n\n\n\n/v1/tools/cloudml/storage.qmd\n\n\n\n\n/v1/guide/keras/guide_keras.qmd\n\n\n\n\n/v1/guide/keras/functional_api.qmd\n\n\n\n\n/v1/guide/keras/sequential_model.qmd\n\n\n\n\n/v1/tools/cloudml/tuning.qmd\n\n\n\n\n/v1/tools/tfruns/tuning.qmd\n\n\n\n\n/v1/tools/tensorboard/hparams.qmd\n\n\n\n\n/v1/guide/tfestimators/input_functions.qmd\n\n\n\n\n/v1/guide/keras/examples/index.qmd\n\n\n\n\n/v1/guide/tfhub/key-concepts.qmd\n\n\n\n\n/v1/learn/resources.qmd\n\n\n\n\n/v1/tutorials/beginners/load/load_csv.qmd\n\n\n\n\n/v1/tutorials/beginners/load/load_image.qmd\n\n\n\n\n/v1/installation/gpu/local_gpu/index.qmd\n\n\n\n\n/v1/tools/tfruns/managing.qmd\n\n\n\n\n/v1/deploy/index.qmd\n\n\n\n\n/v1/guide/index.qmd\n\n\n\n\n/v1/guide/tfhub/intro.qmd\n\n\n\n\n/v1/installation/gpu/index.qmd\n\n\n\n\n/v1/tools/index.qmd\n\n\n\n\n/v1/tutorials/advanced/index.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/index.qmd\n\n\n\n\n/v1/tutorials/beginners/index.qmd\n\n\n\n\n/v1/tutorials/index.qmd\n\n\n\n\n/v1/guide/tfestimators/parsing_spec.qmd\n\n\n\n\n/v1/installation/index.qmd\n\n\n\n\n/v1/tools/cloudml/getting_started.qmd\n\n\n\n\n/v1/guide/tfdatasets/introduction.qmd\n\n\n\n\n/v1/guide/tensorflow/ragged_tensors.qmd\n\n\n\n\n/v1/guide/tfestimators/run_hooks.qmd\n\n\n\n\n/v1/guide/saving/saved_model.qmd\n\n\n\n\n/v1/guide/keras/saving_serializing.qmd\n\n\n\n\n/v1/search.qmd\n\n\n\n\n/v1/tools/tensorboard/tensorboard.qmd\n\n\n\n\n/v1/guide/tfestimators/tensorboard.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/index.qmd\n\n\n\n\n/v1/guide/tfhub/examples/index.qmd\n\n\n\n\n/v1/guide/tfhub/hub-with-keras.qmd\n\n\n\n\n/v1/guide/tfestimators/tensorflow_layers.qmd\n\n\n\n\n/v1/guide/tensorflow/tensors.qmd\n\n\n\n\n/v1/guide/tensorflow/variable.qmd\n\n\n\n\n/v1/tutorials/advanced/customization/tensors-operations.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.qmd\n\n\n\n\n/v1/guide/keras/training_callbacks.qmd\n\n\n\n\n/v1/guide/keras/training_visualization.qmd\n\n\n\n\n/v1/tools/cloudml/training.qmd\n\n\n\n\n/v1/tutorials/advanced/images/transfer-learning-hub.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.qmd\n\n\n\n\n/v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.qmd\n\n\n\n\n/v1/guide/keras/applications.qmd\n\n\n\n\n/v1/guide/keras/custom_layers.qmd\n\n\n\n\n/v1/guide/keras/custom_models.qmd\n\n\n\n\n/v1/guide/keras/examples/addition_rnn.qmd\n\n\n\n\n/v1/guide/keras/examples/babi_memnn.qmd\n\n\n\n\n/v1/guide/keras/examples/babi_rnn.qmd\n\n\n\n\n/v1/guide/tfhub/examples/biggan_image_generation.qmd\n\n\n\n\n/v1/guide/keras/examples/cifar10_cnn.qmd\n\n\n\n\n/v1/guide/keras/examples/cifar10_densenet.qmd\n\n\n\n\n/v1/guide/keras/examples/cifar10_resnet.qmd\n\n\n\n\n/v1/guide/keras/examples/conv_filter_visualization.qmd\n\n\n\n\n/v1/guide/keras/examples/conv_lstm.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/custom_estimator.qmd\n\n\n\n\n/v1/guide/keras/examples/deep_dream.qmd\n\n\n\n\n/v1/guide/keras/examples/eager_cvae.qmd\n\n\n\n\n/v1/guide/keras/examples/eager_dcgan.qmd\n\n\n\n\n/v1/guide/keras/examples/eager_image_captioning.qmd\n\n\n\n\n/v1/guide/keras/examples/eager_pix2pix.qmd\n\n\n\n\n/v1/guide/keras/examples/eager_styletransfer.qmd\n\n\n\n\n/v1/guide/tfhub/examples/feature_column.qmd\n\n\n\n\n/v1/guide/keras/examples/fine_tuning.qmd\n\n\n\n\n/v1/guide/tfhub/examples/image_classification.qmd\n\n\n\n\n/v1/guide/keras/examples/image_ocr.qmd\n\n\n\n\n/v1/guide/keras/examples/imdb_bidirectional_lstm.qmd\n\n\n\n\n/v1/guide/keras/examples/imdb_cnn.qmd\n\n\n\n\n/v1/guide/keras/examples/imdb_cnn_lstm.qmd\n\n\n\n\n/v1/guide/keras/examples/imdb_fasttext.qmd\n\n\n\n\n/v1/guide/keras/examples/imdb_lstm.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/iris_custom_decay_dnn.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/iris_dnn_classifier.qmd\n\n\n\n\n/v1/guide/keras/examples/lstm_benchmark.qmd\n\n\n\n\n/v1/guide/keras/examples/lstm_seq2seq.qmd\n\n\n\n\n/v1/guide/keras/examples/lstm_stateful.qmd\n\n\n\n\n/v1/guide/keras/examples/lstm_text_generation.qmd\n\n\n\n\n/v1/guide/keras/examples/mmd_cvae.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/mnist.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_acgan.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_antirectifier.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_cnn.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_cnn_embeddings.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_dataset_api.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_hierarchical_rnn.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_irnn.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_mlp.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_net2net.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_siamese_graph.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_swwae.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_tfrecord.qmd\n\n\n\n\n/v1/guide/keras/examples/mnist_transfer_cnn.qmd\n\n\n\n\n/v1/guide/keras/examples/neural_style_transfer.qmd\n\n\n\n\n/v1/guide/keras/examples/nmt_attention.qmd\n\n\n\n\n/v1/guide/keras/examples/nueral_doodle.qmd\n\n\n\n\n/v1/guide/keras/examples/pretrained_word_embeddings.qmd\n\n\n\n\n/v1/guide/keras/examples/quora_siamese_lstm.qmd\n\n\n\n\n/v1/guide/tfhub/examples/recipes.qmd\n\n\n\n\n/v1/guide/keras/examples/reuters_mlp.qmd\n\n\n\n\n/v1/guide/keras/examples/reuters_mlp_relu_vs_selu.qmd\n\n\n\n\n/v1/guide/keras/examples/stateful_lstm.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/tensorflow_layers.qmd\n\n\n\n\n/v1/guide/tfhub/examples/text_classification.qmd\n\n\n\n\n/v1/guide/keras/examples/text_explanation_lime.qmd\n\n\n\n\n/v1/guide/keras/examples/tfprob_vae.qmd\n\n\n\n\n/v1/tools/tfruns/overview.qmd\n\n\n\n\n/v1/guide/keras/examples/unet.qmd\n\n\n\n\n/v1/guide/keras/examples/unet_linux.qmd\n\n\n\n\n/v1/guide/keras/examples/variational_autoencoder.qmd\n\n\n\n\n/v1/guide/keras/examples/variational_autoencoder_deconv.qmd\n\n\n\n\n/v1/guide/keras/examples/vq_vae.qmd\n\n\n\n\n/v1/guide/tfestimators/examples/wide_and_deep.qmd\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "v1/installation/custom.html",
    "href": "v1/installation/custom.html",
    "title": "Custom Installation",
    "section": "",
    "text": "The install_tensorflow() function is provided as a convenient way to get started, but is not required. If you have an existing installation of TensorFlow or just prefer your own custom installation that’s fine too.\nThe full instructions for installing TensorFlow on various platforms are here: https://www.tensorflow.org/install/. After installing, please refer to the sections below on locating TensorFlow and meeting additional dependencies to ensure that the tensorflow for R package functions correctly with your installation."
  },
  {
    "objectID": "v1/installation/custom.html#supported-platforms",
    "href": "v1/installation/custom.html#supported-platforms",
    "title": "Custom Installation",
    "section": "Supported Platforms",
    "text": "Supported Platforms\nNote that binary installations of TensorFlow are provided for Windows, OS X, and Ubuntu 16.04 or higher. It’s possible that binary installations will work on other Linux variants but Ubuntu is the only platform tested and supported.\nIn particular, if you are running on RedHat or CentOS you will need to install from source then follow the instructions in the [Custom Installation] section to ensure that your installation of TensorFlow can be used with the tensorflow R package."
  },
  {
    "objectID": "v1/installation/gpu/cloud_desktop_gpu/index.html",
    "href": "v1/installation/gpu/cloud_desktop_gpu/index.html",
    "title": "Cloud Desktop GPUs",
    "section": "",
    "text": "Cloud desktops with various GPU configurations are available from Paperspace. With Paperspace, you can access a full Linux desktop running Ubuntu 16.04 all from within a web browser. An SSH interface is also available, as is a browser based RStudio Server interface (via SSH tunnel).\nPaperspace offers an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0) pre-installed, along with the GPU version of TensorFlow v1.4 and the R keras, tfestimators, and tensorflow packages. Follow the instructions below to get started with using RStudio on Paperspace."
  },
  {
    "objectID": "v1/installation/gpu/cloud_desktop_gpu/index.html#getting-started",
    "href": "v1/installation/gpu/cloud_desktop_gpu/index.html#getting-started",
    "title": "Cloud Desktop GPUs",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, sign up for a Paperspace account here: https://www.paperspace.com/account/signup (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\n\nAfter you’ve signed up and verified your account email, you will be taken to a Create Machine page. Here you’ll select various options including your compute region and machine template. You should select the RStudio template:\n\nBe sure to select one of the GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\nAfter your machine is provisioned (this can take a few minutes) you are ready to access it via a web browser. Hover over the machine in the Paperspace Console and click the “Launch” link:\n\nAfter the machine is launched you’ll see your Linux desktop within the browser you launched it from. You may need to use the Scaling Settings to adjust the desktop to a comfortable resolution:\n\nYou should also change your default password using the passwd utility (your default password should have been sent to you in an email titled “Your new Paperspace Linux machine is ready”):\n\nYou now have a Linux desktop equipped ready to use with TensorFlow for R! Go ahead and run RStudio from the application bar:\n\nNVIDIA GPU libraries (CUDA 9 and cuDNN 7) are pre-installed, along with the GPU version of TensorFlow v1.7. The R keras, tfestimators, and tensorflow packages are also pre-installed, as are all of the packages from the [tidyverse[(https://www.tidyverse.org/)] (dplyr, ggplot2, etc.).\nAn important note about the pre-installed dependencies: Since the NVIDIA CUDA libraries, TensorFlow, and Keras are all pre-installed on the Paperspace instances, you should not use the install_tensorflow() or install_keras() functions, but rather rely on the existing, pre-configured versions of these libraries. Installing or updating other versions of these libraries will likely not work at all!"
  },
  {
    "objectID": "v1/installation/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "href": "v1/installation/gpu/cloud_desktop_gpu/index.html#automatic-shutdown",
    "title": "Cloud Desktop GPUs",
    "section": "Automatic Shutdown",
    "text": "Automatic Shutdown\nYou can set Paperspace machines to automatically shutdown when they have not been used for a set period of time (this is especially important since machine time is billed by the hour). You can access this setting from the Paperspace console for your machine:\n\nHere the auto-shutdown time is set to 1 day, however you can also choose shorter or longer intervals."
  },
  {
    "objectID": "v1/installation/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "href": "v1/installation/gpu/cloud_desktop_gpu/index.html#terminal-access",
    "title": "Cloud Desktop GPUs",
    "section": "Terminal Access",
    "text": "Terminal Access\n\nWeb Terminal\nYou can use the Open Terminal command on the Paperspace console for your machine to open a web based terminal to your machine:\n\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\n\nSSH Login\nYou can also login to your Paperspace instance using a standard SSH client. This requires that you first Assign a public IP address to your machine (note that public IP addresses cost an additional $3/month).\nOnce you have your public IP address, you can SSH into your machine as follows:\n$ ssh paperspace@<public IP>\nYou’ll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created."
  },
  {
    "objectID": "v1/installation/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "href": "v1/installation/gpu/cloud_desktop_gpu/index.html#rstudio-server",
    "title": "Cloud Desktop GPUs",
    "section": "RStudio Server",
    "text": "RStudio Server\nYou may prefer using the RStudio Server browser-based interface to the virtual Linux desktop provided by Paperspace (especially when on slower internet connections). This section describes how to access your Paperspace machine using an SSH tunnel.\nTo start with, follow the instructions for SSH Login immediately above and ensure that you can login to your machine remotely via SSH.\nOnce you’ve verified this, you should also be able to setup an SSH tunnel to RStudio Server as follows:\n$ ssh -N -L 8787:127.0.0.1:8787 paperspace@<public-ip>\nYou can access RStudio Server by navigating to port 8787 on your local machine and logging in using the paperspace account and either the default password emailed to you when you created the machine or to the new password which you subsequently created.\nhttp://localhost:8787"
  },
  {
    "objectID": "v1/installation/gpu/cloud_server_gpu/index.html",
    "href": "v1/installation/gpu/cloud_server_gpu/index.html",
    "title": "Cloud Server GPUs",
    "section": "",
    "text": "Cloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally."
  },
  {
    "objectID": "v1/installation/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "href": "v1/installation/gpu/cloud_server_gpu/index.html#amazon-ec2",
    "title": "Cloud Server GPUs",
    "section": "Amazon EC2",
    "text": "Amazon EC2\nRStudio has AWS Marketplace offerings that are designed to provide stable, secure, and high performance execution environments for deep learning applications running on Amazon EC2. The tensorflow, tfestimators, and keras R packages (along with their pre-requisites, including the GPU version of TensorFlow) are installed as part of the image.\n\nLaunching the Server\nThere are AMIs on the Amazon Cloud Marketplace for both the open-source and Professional versions of RStudio Server. You can find them here:\n\nOpen Source: https://aws.amazon.com/marketplace/pp/B0785SXYB2\nProfessional: https://aws.amazon.com/marketplace/pp/B07B8G3FZP\n\nYou should launch these AMIs on the p2.xlarge instance type. This type includes a single GPU whereas other GPU-based images include up to 16 GPUs (however they are commensurately much more expensive). Note that you may need to select a different region than your default to be able to launch p2.xlarge instances (for example, selecting “US East (Ohio)” rather than “US East (N Virginia)”).\n\n\n\nAccessing the Server\nAfter you’ve launched the server you can access an instance of RStudio Server running on port 8787. For example:\nhttp://ec2-18-217-204-43.us-east-2.compute.amazonaws.com:8787\nNote that the above server address needs to be substituted for the public IP of the server you launched, which you can find in the EC2 Dashboard.\nThe first time you access the server you will be presented with a login screen:\n\nLogin with user id “rstudio-user” and password the instance ID of your AWS server (for example “i-0a8ea329c18892dfa”, your specific ID is available via the EC2 dashboard).\nThen, use the RStudio Terminal to change the default password using the passwd utility:\n\nYour EC2 deep learning instance is now ready to use (the tensorflow, tfestimators, and keras R packages along with their pre-requisites, including the GPU version of TensorFlow, are installed as part of the image).\nSee the sections below for discussion of various ways in which you can make your EC2 instance more secure.\n\n\nLimiting Inbound Traffic\nThe EC2 instance is by default configured to allow access to SSH and HTTP traffic from all IP addresses on the internet, whereas it would be more desirable to restrict this to IP addresses that you know you will access the server from (this can however be challenging if you plan on accessing the server from a variety of public networks).\nYou can see these settings in the Security Group of your EC2 instance:\n\nEdit the Source for the SSH and HTTP protocols to limit access to specific blocks of IP addresses.\n\n\nUsing HTTPS\nBy default the EC2 instance which you launched is accessed over HTTP, a non-encrypted channel. This means that data transmitted to the instance (including your username and password) can potentially be compromised during transmission.\nThere are many ways to add HTTPS support to a server including AWS Elastic Load Balancing, CloudFlare SSL, and setting up reverse proxy from an Nginx or Apache web server configured with SSL support.\nThe details of adding HTTPS support to your server are beyond the scope of this article (see the links above to learn more). An alternative to this is to prohibit external HTTP connections entirely and access the server over an SSH Tunnel, this option is covered in the next section.\n\n\nSSH Tunnel\nUsing an SSH Tunnel to access your EC2 instance provides a number of benefits, including:\n\nUse of the SSH authentication protocol to identify and authorize remote users\nEncrypting traffic that would otherwise be sent in the clear\n\nNote that SSH tunnel access as described below works only for Linux and OS X clients.\n\nSecurity Group\nTo use an SSH Tunnel with your EC2 instance, first configure the Security Group of your instance to only accept SSH traffic (removing any HTTP entry that existed previously):\n\nNote that you may also want to restrict the Source of SSH traffic to the specific block of IP addresses you plan to access the server from.\n\n\nServer Configuration\nNext, connect to your instance over SSH (click the Connect button in the EC2 console for instructions specific to your server):\nssh -i \"my-security-key.pem\" ubuntu@my-ec2-server-address\nNote that if you copied and pasted the command from the EC2 console you may see this error message:\nPlease login as the user \"ubuntu\" rather than the user \"root\".\nIn that case be sure that you use ubuntu@my-ec2-server-address rather than root@my-ec2-server-address.\nExecute the following commands to configure RStudio Server to only accept local connections:\n# Configure RStudio to only allow local connections \nsudo /bin/bash -c \"echo 'www-address=127.0.0.1' >> /etc/rstudio/rserver.conf\"\n\n# Restart RStudio with new settings\nsudo rstudio-server restart\n\n\nConnecting to the Server\nYou should now be able to connect to the server via SSH tunnel as follows:\nssh -N -L 8787:localhost:8787 -i my-security-key.pem ubuntu@my-ec2-server-address\n(where my-security-key.pem and my-ec2-server-address are specific to your server configuration).\nOnce the SSH connection is established, RStudio Server will be available at http://localhost:8787/"
  },
  {
    "objectID": "v1/installation/gpu/index.html",
    "href": "v1/installation/gpu/index.html",
    "title": "Overview",
    "section": "",
    "text": "If your local workstation doesn’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple, low-cost way for you to get started without having to buy any additional hardware. See the documentation below for details on using both local and cloud GPUs.\n\n\n\n\n\n\n\nLocal GPU\nFor systems that have a recent, high-end NVIDIA® GPU, TensorFlow is available in a GPU version that takes advantage of the CUDA and cuDNN libraries to accelerate training performance. Note that the GPU version of TensorFlow is currently only supported on Windows and Linux (there is no GPU version available for Mac OS X since NVIDIA GPUs are not commonly available on that platform).\n\n\nCloudML\nGoogle CloudML is a managed service that provides on-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA. CloudML also provides hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\n\n\nCloud Server\nCloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally.\n\n\nCloud Desktop\nVirtual cloud desktops with GPUs are available from Paperspace. This provides an Ubuntu 16.04 desktop environment that you can access entirely within a web browser (note that this requires a reasonbly fast internet connection to be usable)."
  },
  {
    "objectID": "v1/installation/gpu/local_gpu/index.html",
    "href": "v1/installation/gpu/local_gpu/index.html",
    "title": "Local GPU",
    "section": "",
    "text": "TensorFlow can be configured to run on either CPUs or GPUs. The CPU version is much easier to install and configure so is the best starting place especially when you are first learning how to use TensorFlow. Here’s the guidance on CPU vs. GPU versions from the TensorFlow website:\n\nTensorFlow with CPU support only. If your system does not have a NVIDIA® GPU, you must install this version. Note that this version of TensorFlow is typically much easier to install (typically, in 5 or 10 minutes), so even if you have an NVIDIA GPU, we recommend installing this version first.\nTensorFlow with GPU support. TensorFlow programs typically run significantly faster on a GPU than on a CPU. Therefore, if your system has a NVIDIA® GPU meeting the prerequisites shown below and you need to run performance-critical applications, you should ultimately install this version.\n\nSo if you are just getting started with TensorFlow you may want to stick with the CPU version to start out, then install the GPU version once your training becomes more computationally demanding.\nThe prerequisites for the GPU version of TensorFlow on each platform are covered below. Once you’ve met the prerequisites installing the GPU version in a single-user / desktop environment is as simple as:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf you are using Keras you can install both Keras and the GPU version of TensorFlow with:\n\nlibrary(keras)\ninstall_keras(tensorflow = \"gpu\")\n\nNote that on all platforms you must be running an NVIDIA® GPU with CUDA® Compute Capability 3.5 or higher in order to run the GPU version of TensorFlow. See the list of CUDA-enabled GPU cards."
  },
  {
    "objectID": "v1/installation/gpu/local_gpu/index.html#prerequisties",
    "href": "v1/installation/gpu/local_gpu/index.html#prerequisties",
    "title": "Local GPU",
    "section": "Prerequisties",
    "text": "Prerequisties\n\nWindows\nThis article describes how to detect whether your graphics card uses an NVIDIA® GPU:\nhttp://nvidia.custhelp.com/app/answers/detail/a_id/2040/~/identifying-the-graphics-card-model-and-device-id-in-a-pc\nOnce you’ve confirmed that you have an NVIDIA® GPU, the following article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/gpu#hardware_requirements\nNote that the documentation on installation of the last component (cuDNN v7.4.1) is a bit sparse. Once you join the NVIDIA® developer program and download the zip file containing cuDNN you need to extract the zip file and add the location where you extracted it to your system PATH.\n\n\nUbuntu\nThis article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN >= v7.4.1:\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support\nThe specifics of installing required software differ by Linux version so please review the NVIDIA® documentation carefully to ensure you install everything correctly.\nThe following section provides as example of the installation commands you might use on Ubuntu 16.04.\n\nUbuntu 16.04 Example\nFirst, install the NVIDIA drivers:\n# Add NVIDIA package repositories\n# Add HTTPS support for apt-key\nsudo apt-get install gnupg-curl\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\nsudo apt-get update\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\nsudo apt-get update\n\n# Install NVIDIA driver\n# Issue with driver install requires creating /usr/lib/nvidia\nsudo mkdir /usr/lib/nvidia\nsudo apt-get install --no-install-recommends nvidia-410\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\nNext install CUDA Toolkit v10.0 and cuDNN v7.4.1 with:\n# Install development and runtime libraries (~4GB)\nsudo apt-get install --no-install-recommends \\\n    cuda-10-0 \\\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\nNote that it’s important to download CUDA 10.0 (rather than CUDA 10.1, which may be the choice initially presented) as v10.0 is what TensorFlow is built against.\nYou can see more for the installation here.\n\n\nEnvironment Variables\nOn Linux, part of the setup for CUDA libraries is adding the path to the CUDA binaries to your PATH and LD_LIBRARY_PATH as well as setting the CUDA_HOME environment variable. You will set these variables in distinct ways depending on whether you are installing TensorFlow on a single-user workstation or on a multi-user server. If you are running RStudio Server there is some additional setup required which is also covered below.\nIn all cases these are the environment variables that need to be set/modified in order for TensorFlow to find the required CUDA libraries. For example (paths will change depending on your specific installation of CUDA):\nexport CUDA_HOME=/usr/local/cuda\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${CUDA_HOME}/lib64 \nPATH=${CUDA_HOME}/bin:${PATH} \nexport PATH\n\n\nSingle-User Installation\nIn a single-user environment (e.g. a desktop system) you should define the environment variables within your ~/.profile file. It’s necessary to use ~/.profile rather than ~/.bashrc, because ~/.profile is read by desktop applications (e.g. RStudio) as well as terminal sessions whereas ~/.bashrc applies only to terminal sessions.\nNote that you need to restart your system after editing the ~/.profile file for the changes to take effect. Note also that the ~/.profile file will not be read by bash if you have either a ~/.bash_profile or ~/.bash_login file.\nTo summarize the recommendations above:\n\nDefine CUDA related environment variables in ~/.profile rather than ~/.bashrc;\nEnsure that you don’t have either a ~/.bash_profile or ~/.bash_login file (as these will prevent bash from seeing the variables you’ve added into ~/.profile);\nRestart your system after editing ~/.profile so that the changes take effect.\n\n\n\nMulti-User Installation\nIn a multi-user installation (e.g. a server) you should define the environment variables within the system-wide bash startup file (/etc/profile) so all users have access to them.\nIf you are running RStudio Server you need to also provide these variable definitions in an R / RStudio specific fashion (as RStudio Server doesn’t execute system profile scripts for R sessions).\nTo modify the LD_LIBRARY_PATH you use the rsession-ld-library-path in the /etc/rstudio/rserver.conf configuration file\n/etc/rstudio/rserver.conf\nrsession-ld-library-path=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nYou should set the CUDA_HOME and PATH variables in the /usr/lib/R/etc/Rprofile.site configuration file:\n/usr/lib/R/etc/Rprofile.site\nSys.setenv(CUDA_HOME=\"/usr/local/cuda\")\nSys.setenv(PATH=paste(Sys.getenv(\"PATH\"), \"/usr/local/cuda/bin\", sep = \":\"))\nIn a server environment you might also find it more convenient to install TensorFlow into a system-wide location where all users of the server can share access to it. Details on doing this are covered in the multi-user installation section below.\n\n\n\nMac OS X\nAs of version 1.2 of TensorFlow, GPU support is no longer available on Mac OS X. If you want to use a GPU on Mac OS X you will need to install TensorFlow v1.1 as follows:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"1.1-gpu\")\n\nHowever, before you install you should ensure that you have an NVIDIA® GPU and that you have the required CUDA libraries on your system.\nWhile some older Macs include NVIDIA® GPU’s, most Macs (especially newer ones) do not, so you should check the type of graphics card you have in your Mac before proceeding.\nHere is a list of Mac systems which include built in NVIDIA GPU’s:\nhttps://support.apple.com/en-us/HT204349\nYou can check which graphics card your Mac has via the System Report button found within the About This Mac dialog:\n\nThe MacBook Pro system displayed above does not have an NVIDIA® GPU installed (rather it has an Intel Iris Pro).\nIf you do have an NVIDIA® GPU, the following article describes how to install the base CUDA libraries:\nhttp://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\nYou also need to intall the cuDNN library 5.1 library for OS X from here:\nhttps://developer.nvidia.com/cudnn\nAfter installing these components, you need to ensure that both CUDA and cuDNN are available to your R session via the DYLD_LIBRARY_PATH. This typically involves setting environment variables in your .bash_profile as described in the NVIDIA documentation for CUDA and cuDNN.\nNote that environment variables set in .bash_profile will not be available by default to OS X desktop applications like R GUI and RStudio. To use CUDA within those environments you should start the application from a system terminal as follows:\nopen -a R         # R GUI\nopen -a RStudio   # RStudio"
  },
  {
    "objectID": "v1/installation/gpu/local_gpu/index.html#installation",
    "href": "v1/installation/gpu/local_gpu/index.html#installation",
    "title": "Local GPU",
    "section": "Installation",
    "text": "Installation\n\nSingle User\nIn a single-user desktop environment you can install TensorFlow with GPU support via:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf this version doesn’t load successfully you should review the prerequisites above and ensure that you’ve provided definitions of CUDA environment variables as recommended above.\nSee the main installation article for details on other available options (e.g. virtualenv vs. conda installation, installing development versions, etc.).\n\n\nMultiple Users\nIn a multi-user server environment you may want to install a system-wide version of TensorFlow with GPU support so all users can share the same configuration. To do this, start by following the directions for native pip installation of the GPU version of TensorFlow here:\nhttps://www.tensorflow.org/install/install_linux#InstallingNativePip\nThere are some components of TensorFlow (e.g. the Keras library) which have dependencies on additional Python packages.\nYou can install Keras and it’s optional dependencies with the following command (ensuring you have the correct privilege to write to system library locations as required via sudo, etc.):\npip install keras h5py pyyaml requests Pillow scipy\nIf you have any trouble with locating the system-wide version of TensorFlow from within R please see the section on locating TensorFlow."
  },
  {
    "objectID": "v1/installation/index.html",
    "href": "v1/installation/index.html",
    "title": "Quick start",
    "section": "",
    "text": "Prior to using the tensorflow R package you need to install a version of TensorFlow on your system. Below we describe how to install TensorFlow as well the various options available for customizing your installation.\nNote that this article principally covers the use of the R install_tensorflow() function, which provides an easy to use wrapper for the various steps required to install TensorFlow.\nYou can also choose to install TensorFlow manually (as described at https://www.tensorflow.org/install/). In that case the Custom Installation section covers how to arrange for the tensorflow R package to use the version you installed.\nTensorFlow is tested and supported on the following 64-bit systems:"
  },
  {
    "objectID": "v1/installation/index.html#installation",
    "href": "v1/installation/index.html#installation",
    "title": "Quick start",
    "section": "Installation",
    "text": "Installation\nFirst, install the tensorflow R package from GitHub as follows:\n\ninstall.packages(\"tensorflow\")\n\nThen, use the install_tensorflow() function to install TensorFlow. Note that on Windows you need a working installation of Anaconda.\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\nYou can confirm that the installation succeeded with:\n\nlibrary(tensorflow)\ntf$constant(\"Hellow Tensorflow\")\n\nThis will provide you with a default installation of TensorFlow suitable for use with the tensorflow R package. Read on if you want to learn about additional installation options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed."
  },
  {
    "objectID": "v1/installation/index.html#installation-methods",
    "href": "v1/installation/index.html#installation-methods",
    "title": "Quick start",
    "section": "Installation methods",
    "text": "Installation methods\nTensorFlow is distributed as a Python package and so needs to be installed within a Python environment on your system. By default, the install_tensorflow() function attempts to install TensorFlow within an isolated Python environment (“r-reticulate”).\nThese are the available methods and their behavior:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nauto\nAutomatically choose an appropriate default for the current platform.\n\n\nvirtualenv\nInstall into a Python virtual environment at ~/.virtualenvs/r-reticulate\n\n\nconda\nInstall into an Anaconda Python environment named r-reticulate\n\n\nsystem\nInstall into the system Python environment\n\n\n\nThe “virtualenv” and “conda” methods are available on Linux and OS X and only the “conda” method is available on Windows.\ninstall_tensorflow is a wraper around reticulate::py_install. Please refer to ‘Installing Python Packages’ for more information."
  },
  {
    "objectID": "v1/installation/index.html#alternate-versions",
    "href": "v1/installation/index.html#alternate-versions",
    "title": "Quick start",
    "section": "Alternate Versions",
    "text": "Alternate Versions\nBy default, install_tensorflow() install the latest release version of TensorFlow. You can override this behavior by specifying the version parameter. For example:\n\ninstall_tensorflow(version = \"2.0.0\")\n\nNote that this should be a full major.minor.patch version specification (rather than just major and minor versions).\nYou can install the nightly build of TensorFlow (CPU or GPU version) with:\n\ninstall_tensorflow(version = \"nightly\")      # cpu version\ninstall_tensorflow(version = \"nightly-gpu\")  # gpu version\n\nYou can install any other build of TensorFlow by specifying a URL to a TensorFlow binary. For example:\n\ninstall_tensorflow(version = \"https://files.pythonhosted.org/packages/c2/c1/a035e377cf5a5b90eff27f096448fa5c5a90cbcf13b7eb0673df888f2c2d/tf_nightly-1.12.0.dev20180918-cp36-cp36m-manylinux1_x86_64.whl\")"
  },
  {
    "objectID": "v1/learn/resources.html",
    "href": "v1/learn/resources.html",
    "title": "Learning Resources",
    "section": "",
    "text": "Deep Learning with R Deep Learning with R is meant for statisticians, analysts, engineers, and students with a reasonable amount of R experience but no significant knowledge of machine learning and deep learning. You’ll learn from more than 30 code examples that include detailed commentary and practical recommendations. You don’t need previous experience with machine learning or deep learning: this book covers from scratch all the necessary basics. You don’t need an advanced mathematics background, either—high school level mathematics should suffice in order to follow along.\n\n\n\nDeep Learning An introduction to a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology.\n\n\n\nDeep Learning with Keras Cheatsheet A quick reference guide to the concepts and available functions in the R interface to Keras. Covers the various types of Keras layers, data preprocessing, training workflow, and pre-trained models.\n\n\n\nGallery In-depth examples of using TensorFlow with R, including detailed explanatory narrative as well as coverage of ancillary tasks like data preprocessing and visualization. A great resource for taking the next step after you’ve learned the basics.\n\n\n\nTutorials  Introductory examples of using TensorFlow with R. These examples cover the basics of training models with the keras and tensorflow packages."
  },
  {
    "objectID": "v1/tools/cloudml/deployment.html",
    "href": "v1/tools/cloudml/deployment.html",
    "title": "Deploying Models",
    "section": "",
    "text": "You can host your trained machine learning models in the cloud and use the Cloud ML prediction service to infer target values for new data. This page discusses model hosting and prediction and introduces considerations you should keep in mind for your projects."
  },
  {
    "objectID": "v1/tools/cloudml/deployment.html#model-deployment",
    "href": "v1/tools/cloudml/deployment.html#model-deployment",
    "title": "Deploying Models",
    "section": "Model Deployment",
    "text": "Model Deployment\nCloud ML Engine can host your models so that you can get predictions from them in the cloud. The process of hosting a saved model is called deployment. The prediction service manages the infrastructure needed to run your model at scale, and makes it available for online and batch prediction requests. This section describes model deployment.\n\nExporting a SavedModel\nThe Cloud ML prediction service makes use of models exported through the export_savedmodel() function which is available for models created using the tensorflow, keras and tfestimators packages or any other tool that support the tf.train.Saver interface.\nFor instance, we can use examples/keras/train.R included in this package to define and train an MNIST keras model by running:\n\nlibrary(keras)\n\nFLAGS <- flags(\n  flag_numeric(\"dropout_rate\", 0.4)\n)\n\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\nx_train <- x_train / 255\nx_test <- x_test / 255\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%\n  layer_dropout(rate = FLAGS$dropout_rate) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nmodel %>% fit(\n  x_train, y_train,\n  epochs = 20, batch_size = 128,\n  validation_split = 0.2\n)\n\nexport_savedmodel(model, \"savedmodel\")\n\n\n\nDeploying the Model\nDeployment is performed through cloudml_deploy() which uses the same gcloud and cloudml configuration concepts used while training. We can train any exported model by running:\n\ncloudml_deploy(\"savedmodel\", name = \"keras_mnist\")\n\nCopying file://savedmodel/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\nCopying file://savedmodel/saved_model.pb [Content-Type=application/octet-stream]...\nCopying file://savedmodel/variables/variables.index [Content-Type=application/octet-stream]...\n/ [3/3 files][  1.9 MiB/  1.9 MiB] 100% Done                                    \nOperation completed over 3 objects/1.9 MiB.\n\nModel created and available in https://console.cloud.google.com/mlengine/models/keras_mnist\nNotice that models make use of unique names and versions which can be specified using the name and version parameters in cloudml_deploy()."
  },
  {
    "objectID": "v1/tools/cloudml/deployment.html#prediction",
    "href": "v1/tools/cloudml/deployment.html#prediction",
    "title": "Deploying Models",
    "section": "Prediction",
    "text": "Prediction\nOnce a model is deployed, predictions can be performed by providing a list of inputs into cloudml_predict():\n\nmnist_image <- keras::dataset_mnist()$train$x[1,,]\ngrid::grid.raster(mnist_image / 255)\n\n \n\ncloudml_predict(\n  list(\n    as.vector(t(mnist_image))\n  ),\n  name = \"keras_mnist\",\n)\n\n$predictions\n                       dense_3\n1 0, 0, 0, 0, 0, 1, 0, 0, 0, 0\nFor additional information visit Google Cloud Platform - Prediction Basics"
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html",
    "href": "v1/tools/cloudml/getting_started.html",
    "title": "R Interface to Google CloudML",
    "section": "",
    "text": "The cloudml package provides an R interface to Google Cloud Machine Learning Engine, a managed service that enables:\n\nScalable training of models built with the keras, tfestimators, and tensorflow R packages.\nOn-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA®.\nHyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\nDeployment of trained models to the Google global prediction platform that can support thousands of users and TBs of data.\n\nCloudML is a managed service where you pay only for the hardware resources that you use. Prices vary depending on configuration (e.g. CPU vs. GPU vs. multiple GPUs). See https://cloud.google.com/ml-engine/pricing for additional details."
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html#google-cloud-account",
    "href": "v1/tools/cloudml/getting_started.html#google-cloud-account",
    "title": "R Interface to Google CloudML",
    "section": "Google Cloud Account",
    "text": "Google Cloud Account\nBefore you can begin training models with CloudML you need to have a Google Cloud Account. If you don’t already have an account you can create one at https://console.cloud.google.com.\nIf you are a new customer of Google Cloud you will receive a 12-month, $300 credit that can be applied to your use of CloudML. In addition, Google is providing a $200 credit for users of the R interface to CloudML (this credit applies to both new and existing customers). Use this link to apply for the $200 credit.\nThe account creation process will lead you through creating a new project. To enable the Machine Learning API for this project navigate to the “ML Engine” menu on the left. Doing this for the first time will enable the ML API and allow you to submit ML jobs."
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html#installation",
    "href": "v1/tools/cloudml/getting_started.html#installation",
    "title": "R Interface to Google CloudML",
    "section": "Installation",
    "text": "Installation\nStart by installing the cloudml R package from CRAN as follows:\ninstall.packages(\"cloudml\")\nThen, install the Google Cloud SDK, a set of utilties that enable you to interact with your Google Cloud account from within R. You can install the SDK using the gcloud_install() function.\n\nlibrary(cloudml)\ngcloud_install()\n\nNote that in order to ensure that the cloudml package can find your installation of the SDK you should accept the default installation location (~/) suggested within the installer.\nAs part of the installation you are asked to specify a default account, project, and compute region for Google Cloud. These settings are then used automatically for all CloudML jobs. To change the default account, project, or region you can use the gcloud_init() function:\n\ngcloud_init()\n\nNote that you don’t need to execute gcloud_init() now as this was done automatically as part of gcloud_install().\nOnce you’ve completed these steps you are ready to train models with CloudML!"
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html#training-on-cloudml",
    "href": "v1/tools/cloudml/getting_started.html#training-on-cloudml",
    "title": "R Interface to Google CloudML",
    "section": "Training on CloudML",
    "text": "Training on CloudML\nTo train a model on CloudML, first work the training script locally (perhaps with a smaller sample of your dataset). The script can contain arbitrary R code which trains and/or evaluates a model. Once you’ve confirmed that things work as expected, you can submit a CloudML job to perform training in the cloud.\n\nSubmitting a Job\nTo submit a job, call the cloudml_train() function, specifying the R script to execute for training:\n\nlibrary(cloudml)\ncloudml_train(\"train.R\")\n\nAll of the files within the current working directory will be bundled up and sent along with the script to CloudML.\n\nNote that the very first time you submit a job to CloudML the various packages required to run your script will be compiled from source. This will make the execution time of the job considerably longer that you might expect. It’s only the first job that incurs this overhead though (since the package installations are cached), and subsequent jobs will run more quickly.\n\nIf you are using RStudio v1.1 or higher, then the CloudML training job is monitored (and it’s results collected) using a background terminal:\n\n\n\nCollecting Results\nWhen the job is complete, training results can be collected back to your local system (this is done automatically when monitoring the job using a background terminal in RStudio). A run report is displayed after the job is collected:\n\nYou can list all previous runs as a data frame using the ls_runs() function:\n\nls_runs()\n\nData frame: 6 x 37 \n                            run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss metric_val_acc\n6 runs/cloudml_2018_01_26_135812740    0.1049   0.9789      0.0852     0.9760          0.1093         0.9770\n2 runs/cloudml_2018_01_26_140015601    0.1402   0.9664      0.1708     0.9517          0.1379         0.9687\n5 runs/cloudml_2018_01_26_135848817    0.1159   0.9793      0.0378     0.9887          0.1130         0.9792\n3 runs/cloudml_2018_01_26_135936130    0.0963   0.9780      0.0701     0.9792          0.0969         0.9790\n1 runs/cloudml_2018_01_26_140045584    0.1486   0.9682      0.1860     0.9504          0.1453         0.9693\n4 runs/cloudml_2018_01_26_135912819    0.1141   0.9759      0.1272     0.9655          0.1087         0.9762\n# ... with 30 more columns:\n#   flag_dense_units1, flag_dropout1, flag_dense_units2, flag_dropout2, samples, validation_samples,\n#   batch_size, epochs, epochs_completed, metrics, model, loss_function, optimizer, learning_rate,\n#   script, start, end, completed, output, source_code, context, type, cloudml_console_url,\n#   cloudml_created, cloudml_end, cloudml_job, cloudml_log_url, cloudml_ml_units, cloudml_start,\n#   cloudml_state\nYou can view run reports using the view_run() function:\n\n# view the latest run\nview_run()\n\n# view a specific run\nview_run(\"runs/cloudml_2017_12_15_182614794\")\n\nThere are many tools available to list, filter, and compare training runs. For additional information see the documentation for the tfruns package."
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html#training-with-a-gpu",
    "href": "v1/tools/cloudml/getting_started.html#training-with-a-gpu",
    "title": "R Interface to Google CloudML",
    "section": "Training with a GPU",
    "text": "Training with a GPU\nBy default, CloudML utilizes “standard” CPU-based instances suitable for training simple models with small to moderate datasets. You can request the use of other machine types, including ones with GPUs, using the master_type parameter of cloudml_train().\nFor example, the following would train the same model as above but with a Tesla K80 GPU:\n\ncloudml_train(\"train.R\", master_type = \"standard_gpu\")\n\nTo train using a Tesla P100 GPU you would specify \"standard_p100\":\n\ncloudml_train(\"train.R\", master_type = \"standard_p100\")\n\nTo train on a machine with 4 Tesla P100 GPU’s you would specify \"complex_model_m_p100\":\n\ncloudml_train(\"train.R\", master_type = \"complex_model_m_p100\")\n\nSee the CloudML website for documentation on available machine types. Also note that GPU instances can be considerably more expensive that CPU ones! See the documentation on CloudML Pricing for details."
  },
  {
    "objectID": "v1/tools/cloudml/getting_started.html#learning-more",
    "href": "v1/tools/cloudml/getting_started.html#learning-more",
    "title": "R Interface to Google CloudML",
    "section": "Learning More",
    "text": "Learning More\nTo learn more about using CloudML with R, see the following articles:\n\nTraining with CloudML goes into additional depth on managing training jobs and their output.\nHyperparameter Tuning explores how you can improve the performance of your models by running many trials with distinct hyperparameters (e.g. number and size of layers) to determine their optimal values.\nGoogle Cloud Storage provides information on copying data between your local machine and Google Storage and also describes how to use data within Google Storage during training.\nDeploying Models describes how to deploy trained models and generate predictions from them."
  },
  {
    "objectID": "v1/tools/cloudml/storage.html",
    "href": "v1/tools/cloudml/storage.html",
    "title": "Google Cloud Storage",
    "section": "",
    "text": "Google Cloud Storage is often used along with CloudML to manage and serve training data. This article provides details on:\n\nCopying and synchronizing files between your local workstation and Google Cloud.\nReading data from Google Cloud Storage buckets from within a training script.\nVarying data source configuration between local script development and CloudML training."
  },
  {
    "objectID": "v1/tools/cloudml/storage.html#copying-data",
    "href": "v1/tools/cloudml/storage.html#copying-data",
    "title": "Google Cloud Storage",
    "section": "Copying Data",
    "text": "Copying Data\nGoogle Cloud Storage is organized around storage units named “buckets”, which are roughly analogous to filesystem directories. You can copy data between your local system and cloud storage using the gs_copy() function. For example:\n\nlibrary(cloudml)\n\n# copy from a local directory to a bucket\ngs_copy(\"training-data\", \"gs://quarter-deck-529/training-data\")\n\n# copy from a bucket to a local directory \ngs_copy(\"gs://quarter-deck-529/training-data\", \"training-data\")\n\nYou can also use the gs_rsync() function to syncrhonize a local directory and a bucket in Google Storage (this is much more efficient than copying the data each time):\n\n# synchronize a bucket and a local directory\ngs_rsync(\"gs://quarter-deck-529/training-data\", \"training-data\")\n\nNote that to use these functions you need to import the cloudml package with library(cloudml) as illustrated above."
  },
  {
    "objectID": "v1/tools/cloudml/storage.html#reading-data",
    "href": "v1/tools/cloudml/storage.html#reading-data",
    "title": "Google Cloud Storage",
    "section": "Reading Data",
    "text": "Reading Data\nThere are two distinct ways to read data from Google Storage. Which you use will depend on whether the TensorFlow API you are using supports direct references to gs:// bucket URLs.\nIf you are using the TensorFlow Datasets API, then you can use gs:// bucket URLs directly. In this case you’ll want to use the gs:// URL when running on CloudML, and a synchonized copy of the bucket when running locally. You can use the gs_data_dir() function to accomplish this. For example:\n\nlibrary(tfdatasets)\nlibrary(cloudml)\n\ndata_dir <- gs_data_dir(\"gs://mtcars-data\")\nmtcars_csv <- file.path(data_dir, \"mtcars.csv\")\n\nmtcars_dataset <- csv_dataset(mtcars_csv) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl)\n\nWhile some TensorFlow APIs can take gs:// URLs directly, in many cases a local filesystem path will be required. If you want to store data in Google Storage but still use it with APIs that require local paths you can use the gs_data_dir_local() function to provide the local path.\nFor example, this code reads CSV files from Google Storage:\n\nlibrary(cloudml)\nlibrary(readr)\ndata_dir <- gs_data_dir_local(\"gs://quarter-deck-529/training-data\")\ntrain_data <- read_csv(file.path(data_dir, \"train.csv\"))\ntest_data <- read_csv(file.path(data_dir, \"test.csv\"))\n\nUnder the hood this function will rsync data from Google Storage as required to provide the local filesystem interface to it.\nHere’s another example which creates a Keras image data generator from a bucket:\n\ntrain_generator <- flow_images_from_directory(\n  gs_data_dir_local(\"gs://quarter-deck-529/../images/train\"),\n  image_data_generator(rescale = 1/255),\n  target_size = c(150, 150),\n  batch_size = 32,\n  class_mode = \"binary\"\n)\n\nNote that if the path passed to gs_data_dir_local() is from the local filesystem it will be returned unmodified."
  },
  {
    "objectID": "v1/tools/cloudml/storage.html#data-source-configuration",
    "href": "v1/tools/cloudml/storage.html#data-source-configuration",
    "title": "Google Cloud Storage",
    "section": "Data Source Configuration",
    "text": "Data Source Configuration\nIt’s often useful to do training script development with a local subsample of data that you’ve extracted from the complete set of training data. In this configuration, you’ll want your training script to dynamically use the local subsample during development then use the complete dataset stored in Google Cloud Storage when running on CloudML. You can accomplish this with a combination of training flags and the gs_local_dir() function described above.\nHere’s a complete example. We start with a training script that declares a flag for the location of the training data:\n\nlibrary(keras)\nlibrary(cloudml)\n\n# define a flag for the location of the data directory\nFLAGS <- flags(\n  flag_string(\"data_dir\", \"data\")\n)\n\n# determine the location of the directory (during local development this will\n# be the default \"data\" subdirectory specified in the FLAGS declaration above)\ndata_dir <- gs_data_dir_local(FLAGS$data_dir)\n\n# read the data\ntrain_data <- read_csv(file.path(FLAGS$data_dir, \"train.csv\"))\n\nNote that the data_dir R variable is computed by passing FLAGS$data_dir to the gs_data_dir_local() function. This enables it to take on a dynamic value depending upon the training environment.\nThe way to vary this value when running on CloudML is by adding a flags.yml configuration file to your project directory. For example:\nflags.yml\ncloudml:\n  data_dir: \"gs://quarter-deck-529/training-data\"\nWith the addition of this config file, your script will resolve the data_dir flag to specified the Google Storage bucket, but only when it is running on CloudML."
  },
  {
    "objectID": "v1/tools/cloudml/storage.html#managing-storage",
    "href": "v1/tools/cloudml/storage.html#managing-storage",
    "title": "Google Cloud Storage",
    "section": "Managing Storage",
    "text": "Managing Storage\nYou can view and manage data within Google Cloud Storage buckets using either a web based user-interface or via command line utilities included with the Google Cloud SDK.\n\nGoogle Storage Browser\nTo access the web-bqsed UI, navigate to https://console.cloud.google.com/storage/browser.\nHere’s what the storage browser looks like for a sample project:\n\n\n\nGoogle Cloud SDK\nThe Google Cloud SDK includes the gsutil utility program for managing cloud storage buckets. Documentation for gsutil can be found here: https://cloud.google.com/storage/docs/gsutil.\nYou use gsutil from within a terminal. If you are running within RStudio v1.1 or higher you can activate a terminal with the gcloud_terminal() function:\n\ngcloud_terminal()\n\nHere is an example of using the gsutil ls command to list the contents of a bucket within a terminal:"
  },
  {
    "objectID": "v1/tools/cloudml/training.html",
    "href": "v1/tools/cloudml/training.html",
    "title": "Training with CloudML",
    "section": "",
    "text": "Training models with CloudML uses the following workflow:\n\nDevelop and test an R training script locally\nSubmit a job to CloudML to execute your script in the cloud\nMonitor and collect the results of the job\nTune your model based on the results and repeat training as necessary\n\nCloudML is a managed service where you pay only for the hardware resources that you use. Prices vary depending on configuration (e.g. CPU vs. GPU vs. multiple GPUs). See https://cloud.google.com/ml-engine/pricing for additional details."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#local-development",
    "href": "v1/tools/cloudml/training.html#local-development",
    "title": "Training with CloudML",
    "section": "Local Development",
    "text": "Local Development\nWorking on a CloudML project always begins with developing a training script that runs on your local machine. This will typically involve using one of these packages:\n\nkeras — A high-level interface for neural networks, with a focus on enabling fast experimentation.\ntfestimators — High-level implementations of common model types such as regressors and classifiers.\ntensorflow — Lower-level interface that provides full access to the TensorFlow computational graph.\n\nThere are no special requirements for your training script, however there are a couple of things to keep in mind:\n\nWhen you train a model on CloudML all of the files in the current working directory are uploaded. Therefore, your training script should be within the current working directory and references to other scripts, data files, etc. should be relative to the current working directory. The most straightforward way to organize your work on a CloudML application is to use an RStudio Project.\nYour training data may be contained within the working directory, or it may be located within Google Cloud Storage. If your training data is large and/or located in cloud storage, the most straightforward workflow for development is to use a local subsample of your data. See the article on Google Cloud Storage for a detailed example of using distinct data for local and CloudML execution contexts, as well as reading data from Google Cloud Storage buckets.\n\nOnce your script is working the way you expect you are ready to submit it as a job to CloudML."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#submitting-jobs",
    "href": "v1/tools/cloudml/training.html#submitting-jobs",
    "title": "Training with CloudML",
    "section": "Submitting Jobs",
    "text": "Submitting Jobs\nThe core unit of work in CloudML is a job. A job consists of a training script and related files (e.g. other scripts, data files, etc. within the working directory). To submit a job to CloudML you use the cloudml_train() function, passing it the name of the training script to run. For example:\n\nlibrary(cloudml)\njob <- cloudml_train(\"mnist_mlp.R\")\n\n\nNote that the very first time you submit a job to CloudML the various packages required to run your script will be compiled from source. This will make the execution time of the job considerably longer that you might expect. It’s only the first job that incurs this overhead though (since the package installations are cached), and subsequent jobs will run more quickly.\n\nThe cloudml_train() function returns a job object. This is a reference to the training job which you can use later to check it’s status, collect it’s output, etc. For example:\n\njob_status(job)\n\n $ createTime    : chr \"2017-12-18T20:35:21Z\"\n $ etag          : chr \"2KRqIbAhzvM=\"\n $ jobId         : chr \"cloudml_2017_12_18_203510175\"\n $ startTime     : chr \"2017-12-18T20:35:52Z\"\n $ state         : chr \"RUNNING\"\n $ trainingInput :List of 3\n  ..$ jobDir        : chr \"gs://cedar-card-791/r-cloudml/staging\"\n  ..$ region        : chr \"us-central1\"\n  ..$ runtimeVersion: chr \"1.4\"\n $ trainingOutput:List of 1\n  ..$ consumedMLUnits: num 0.04\n\nView job in the Cloud Console at:\nhttps://console.cloud.google.com/ml/jobs/cloudml_2017_12_18_203510175?project=cedar-card-791\n\nView logs at:\nhttps://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Fcloudml_2017_12_18_203510175&project=cedar-card-791\nTo interact with jobs you don’t need the job object returned from cloudml_train(). If you call job_status() or with no arguments it will act on the most recently submitted job:\n\njob_status()   # get status of last job"
  },
  {
    "objectID": "v1/tools/cloudml/training.html#collecting-job-results",
    "href": "v1/tools/cloudml/training.html#collecting-job-results",
    "title": "Training with CloudML",
    "section": "Collecting Job Results",
    "text": "Collecting Job Results\nYou can call job_collect() at any time to download a job:\n\njob_collect()     # collect last job\njob_collect(job)  # collect specific job\n\nNote also that if you are using RStudio v1.1 or higher you’ll be given the to monitor and collect submitted jobs in the background using an RStudio terminal:\n\nIn this case you don’t need to call job_collect() explicitly as this will be done from within the background terminal after the job completes.\nOnce the job is complete it’s results will be downloaded and a report will be automatically displayed:\n\n\nTraining Runs\nEach training job will produce one or more training runs (it’s typically only a single run, however when doing hyperparmeter turning there will be multiple runs). When you collect a job from CloudML it is automatically downloaded into the runs sub-directory of the current working directory.\nYou can list all of the runs as a data frame using the ls_runs() function:\n\nls_runs()\n\nData frame: 6 x 37 \n                            run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss metric_val_acc\n6 runs/cloudml_2018_01_26_135812740    0.1049   0.9789      0.0852     0.9760          0.1093         0.9770\n2 runs/cloudml_2018_01_26_140015601    0.1402   0.9664      0.1708     0.9517          0.1379         0.9687\n5 runs/cloudml_2018_01_26_135848817    0.1159   0.9793      0.0378     0.9887          0.1130         0.9792\n3 runs/cloudml_2018_01_26_135936130    0.0963   0.9780      0.0701     0.9792          0.0969         0.9790\n1 runs/cloudml_2018_01_26_140045584    0.1486   0.9682      0.1860     0.9504          0.1453         0.9693\n4 runs/cloudml_2018_01_26_135912819    0.1141   0.9759      0.1272     0.9655          0.1087         0.9762\n# ... with 30 more columns:\n#   flag_dense_units1, flag_dropout1, flag_dense_units2, flag_dropout2, samples, validation_samples,\n#   batch_size, epochs, epochs_completed, metrics, model, loss_function, optimizer, learning_rate,\n#   script, start, end, completed, output, source_code, context, type, cloudml_console_url,\n#   cloudml_created, cloudml_end, cloudml_job, cloudml_log_url, cloudml_ml_units, cloudml_start,\n#   cloudml_state\nYou can view run reports using the view_run() function:\n\n# view the latest run\nview_run()\n\n# view a specific run\nview_run(\"runs/cloudml_2017_12_15_182614794\")\n\nThere are many tools available to list, filter, and compare training runs. For additional information see the documentation for the tfruns package."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#managing-jobs",
    "href": "v1/tools/cloudml/training.html#managing-jobs",
    "title": "Training with CloudML",
    "section": "Managing Jobs",
    "text": "Managing Jobs\nYou can enumerate previously submitted jobs using the job_list() function:\n\njob_list()\n\n                        JOB_ID    STATUS             CREATED\n1 cloudml_2017_12_18_203510175 SUCCEEDED 2017-12-18 15:35:21\n2 cloudml_2017_12_18_202228264    FAILED 2017-12-18 15:22:39\n3 cloudml_2017_12_18_201607948 SUCCEEDED 2017-12-18 15:16:18\n4 cloudml_2017_12_18_132620918 SUCCEEDED 2017-12-18 08:26:30\n5 cloudml_2017_12_15_182614794 SUCCEEDED 2017-12-15 13:26:29\n6 cloudml_2017_12_14_183247626 SUCCEEDED 2017-12-14 13:33:04\nYou can use the JOB_ID field to interact with any of these jobs:\n\njob_status(\"cloudml_2017_12_18_203510175\")\n\nThe job_stream_logs() function can be used to view the live log of a running job:\n\njob_stream_logs(\"cloudml_2017_12_18_203510175\")\n\nThe job_cancel() function can be used to cancel a running job:\n\njob_cancel(\"cloudml_2017_12_18_203510175\")"
  },
  {
    "objectID": "v1/tools/cloudml/training.html#tuning-your-application",
    "href": "v1/tools/cloudml/training.html#tuning-your-application",
    "title": "Training with CloudML",
    "section": "Tuning Your Application",
    "text": "Tuning Your Application\nTuning your application typically requires choosing and then optimizing a set of hyperparameters that influence your model’s performance. This could include the number and type of layers, units within layers, drop rates, regularization, etc.\nYou can experiment with hyperparameters on an ad-hoc basis, but in general it’s better to explore them more systematnically. The key to doing this with CloudML is by defining training flags within your script and the parameterizing runs using those flags.\nFor example, you might define the following training flags:\n\nlibrary(keras)\n\nFLAGS <- flags(\n  flag_integer(\"dense_units1\", 128),\n  flag_numeric(\"dropout1\", 0.4),\n  flag_integer(\"dense_units2\", 128),\n  flag_numeric(\"dropout2\", 0.3),\n)\n\nThen use the flags in a script as follows:\n\ninput <- layer_input(shape = c(784))\npredictions <- input %>% \n  layer_dense(units = FLAGS$dense_units1, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout1) %>%\n  layer_dense(units = FLAGS$dense_units2, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel <- keras_model(input, predictions) %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = 128,\n  epochs = 30,\n  verbose = 1,\n  validation_split = 0.2\n)\n\nNote that instead of literal values for the various hyperparameters we want to vary we now reference members of the FLAGS list returned from the flags() function.\nYou can try out different flags by passing a named list of flags to the cloudml_train() function. For example:\n\ncloudml_train(\"minst_mlp.R\", flags = list(dropout1 = 0.3, dropout2 = 0.2))\n\nThese flags are passed to your script and are also retained as part of the results recorded for the training run.\nYou can also more systematically try combinations of flags using CloudML hyperparameter tuning."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#training-with-a-gpu",
    "href": "v1/tools/cloudml/training.html#training-with-a-gpu",
    "title": "Training with CloudML",
    "section": "Training with a GPU",
    "text": "Training with a GPU\nBy default, CloudML utilizes “standard” CPU-based instances suitable for training simple models with small to moderate datasets. You can request the use of other machine types, including ones with GPUs, using the master_type parameter of cloudml_train().\nFor example, the following would train the same model as above but with a Tesla K80 GPU:\n\ncloudml_train(\"train.R\", master_type = \"standard_gpu\")\n\nTo train using a Tesla P100 GPU you would specify \"standard_p100\":\n\ncloudml_train(\"train.R\", master_type = \"standard_p100\")\n\nTo train on a machine with 4 Tesla P100 GPU’s you would specify \"complex_model_m_p100\":\n\ncloudml_train(\"train.R\", master_type = \"complex_model_m_p100\")\n\nSee the CloudML website for documentation on available machine types. Also note that GPU instances can be considerably more expensive that CPU ones! See the documentation on CloudML Pricing for details."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#training-configuration",
    "href": "v1/tools/cloudml/training.html#training-configuration",
    "title": "Training with CloudML",
    "section": "Training Configuration",
    "text": "Training Configuration\nYou can provide custom configuration for training by creating a cloudml.yml file within the working directory from which you submit your training job. This file can be used to customize various aspects of training behavior including the virtual machines used as well as the runtime version of CloudML used in the job.\nFor example, the following config file specifies a custom scale tier with a master type of “large_model”. It also specifies that the CloudML runtime version should be 1.2.\ncloudml.yml\ntrainingInput:\n  scaleTier: CUSTOM\n  masterType: large_model\n  runtimeVersion: 1.4\nYou can also pass a named configuration file (i.e. one for a hyperparameter tuning job) via the config parmater of cloudml_train(). For example:\n\ncloudml_train(\"mnist_mlp.R\", config = \"tuning.yml\")\n\nNote that trainingInput is used as the top level key in the config file (this is required). Additional documentation on available fields in the configuration file is available here https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput."
  },
  {
    "objectID": "v1/tools/cloudml/training.html#learning-more",
    "href": "v1/tools/cloudml/training.html#learning-more",
    "title": "Training with CloudML",
    "section": "Learning More",
    "text": "Learning More\nThe following articles provide additional documentation on training and deploying models with CloudML:\n\nHyperparameter Tuning explores how you can improve the performance of your models by running many trials with distinct hyperparameters (e.g. number and size of layers) to determine their optimal values.\nGoogle Cloud Storage provides information on copying data between your local machine and Google Storage and also describes how to use data within Google Storage during training.\nDeploying Models describes how to deploy trained models and generate predictions from them."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html",
    "href": "v1/tools/cloudml/tuning.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "This article describes hyperparameter tuning, which is the automated model enhancer provided by Cloud Machine Learning Engine. Hyperparameter tuning takes advantage of the processing infrastructure of Google Cloud Platform to test different hyperparameter configurations when training your model. It can give you optimized values for hyperparameters, which maximizes your model’s predictive accuracy."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#whats-a-hyperparameter",
    "href": "v1/tools/cloudml/tuning.html#whats-a-hyperparameter",
    "title": "Hyperparameter Tuning",
    "section": "What’s a hyperparameter?",
    "text": "What’s a hyperparameter?\nIf you’re new to machine learning, you may have never encountered the term hyperparameters before. Your trainer handles three categories of data as it trains your model:\n\nYour input data (also called training data) is a collection of individual records (instances) containing the features important to your machine learning problem. This data is used during training to configure your model to accurately make predictions about new instances of similar data. However, the actual values in your input data never directly become part of your model.\nYour model’s parameters are the variables that your chosen machine learning technique uses to adjust to your data. For example, a deep neural network (DNN) is composed of processing nodes (neurons), each with an operation performed on data as it travels through the network. When your DNN is trained, each node has a weight value that tells your model how much impact it has on the final prediction. Those weights are an example of your model’s parameters. In many ways, your model’s parameters are the model—they are what distinguishes your particular model from other models of the same type working on similar data.\nIf model parameters are variables that get adjusted by training with existing data, your hyperparameters are the variables about the training process itself. For example, part of setting up a deep neural network is deciding how many “hidden” layers of nodes to use between the input layer and the output layer, as well as how many nodes each layer should use. These variables are not directly related to the training data at all. They are configuration variables. Another difference is that parameters change during a training job, while the hyperparameters are usually constant during a job.\n\nYour model parameters are optimized (you could say “tuned”) by the training process: you run data through the operations of the model, compare the resulting prediction with the actual value for each data instance, evaluate the accuracy, and adjust until you find the best values. Hyperparameters are similarly tuned by running your whole training job, looking at the aggregate accuracy, and adjusting. In both cases you are modifying the composition of your model in an effort to find the best combination to handle your problem.\nWithout an automated technology like Cloud ML Engine hyperparameter tuning, you need to make manual adjustments to the hyperparameters over the course of many training runs to arrive at the optimal values. Hyperparameter tuning makes the process of determining the best hyperparameter settings easier and less tedious."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#how-it-works",
    "href": "v1/tools/cloudml/tuning.html#how-it-works",
    "title": "Hyperparameter Tuning",
    "section": "How it works",
    "text": "How it works\nHyperparameter tuning works by running multiple trials in a single training job. Each trial is a complete execution of your training application with values for your chosen hyperparameters set within limits you specify. The Cloud ML Engine training service keeps track of the results of each trial and makes adjustments for subsequent trials. When the job is finished, you can get a summary of all the trials along with the most effective configuration of values according to the criteria you specify.\nHyperparameter tuning requires more explicit communication between the Cloud ML Engine training service and your training application. You define all the information that your model needs in your training application. The best way to think about this interaction is that you define the hyperparameters (variables) that you want to adjust and you define a target value.\nTo learn more about how Bayesian optimization is used for hyperparameter tuning in Cloud ML Engine, read the August 2017 Google Cloud Big Data and Machine Learning Blog post named Hyperparameter Tuning in Cloud Machine Learning Engine using Bayesian Optimization."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#what-it-optimizes",
    "href": "v1/tools/cloudml/tuning.html#what-it-optimizes",
    "title": "Hyperparameter Tuning",
    "section": "What it optimizes",
    "text": "What it optimizes\nHyperparameter tuning optimizes a single target variable (also called the hyperparameter metric) that you specify. The accuracy of the model, as calculated from an evaluation pass, is a common metric. The metric must be a numeric value, and you can specify whether you want to tune your model to maximize or minimize your metric.\nWhen you start a job with hyperparameter tuning, you establish the name of your hyperparameter metric. The appropriate name will depend on whether you are using keras, tfestimators, or the core TensorFlow API. This will be covered below in the section on [Hyperparameter tuning configuration].\n\nHow Cloud ML Engine gets your metric\nYou may notice that there are no instructions in this documentation for passing your hyperparameter metric to the Cloud ML Engine training service. That’s because the service automatically monitors TensorFlow summary events generated by your trainer and retrieves the metric.\n\n\nThe flow of hyperparameter values\nWithout hyperparameter tuning, you can set your hyperparameters by whatever means you like in your trainer. You might configure them according to command-line arguments to your main application module, or feed them to your application in a configuration file, for example. When you use hyperparameter tuning, you must set the values of the hyperparameters that you’re using for tuning with a specific procedure:\n\nDefine a training flag within your training script for each tuned hyperparameter.\nUse the value passed for those arguments to set the corresponding hyperparameter in your training code.\n\nWhen you configure a training job with hyperparameter tuning, you define each hyperparameter to tune, its type, and the range of values to try. You identify each hyperparameter using exactly the same name as the corresponding argument you defined in your main module. The training service includes command-line arguments using these names when it runs your trainer, which are in turn propagated to the FLAGS within your script."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#selecting-hyperparameters",
    "href": "v1/tools/cloudml/tuning.html#selecting-hyperparameters",
    "title": "Hyperparameter Tuning",
    "section": "Selecting hyperparameters",
    "text": "Selecting hyperparameters\nThere is very little universal advice to give about how to choose which hyperparameters you should tune. If you have experience with the machine learning technique that you’re using, you may have insight into how its hyperparameters behave. You may also be able to find advice from machine learning communities.\nHowever you choose them, it’s important to understand the implications. Every hyperparameter that you choose to tune has the potential to exponentially increase the number of trials required for a successful tuning job. When you train on Cloud ML Engine you are charged for the duration of the job, so careless assignment of hyperparameters to tune can greatly increase the cost of training your model."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#preparing-your-script",
    "href": "v1/tools/cloudml/tuning.html#preparing-your-script",
    "title": "Hyperparameter Tuning",
    "section": "Preparing your script",
    "text": "Preparing your script\nTo prepare your training script for tuning, you should define a training flag within your script for each tuned hyperparameter. For example:\n\nlibrary(keras)\n\nFLAGS <- flags(\n  flag_integer(\"dense_units1\", 128),\n  flag_numeric(\"dropout1\", 0.4),\n  flag_integer(\"dense_units2\", 128),\n  flag_numeric(\"dropout2\", 0.3)\n)\n\nThese flags would then used within a script as follows:\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = FLAGS$dense_units1, activation = 'relu', \n              input_shape = c(784)) %>%\n  layer_dropout(rate = FLAGS$dropout1) %>%\n  layer_dense(units = FLAGS$dense_units2, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nNote that instead of literal values for the various parameters we want to vary we now reference members of the FLAGS list returned from the flags() function."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#tuning-configuration",
    "href": "v1/tools/cloudml/tuning.html#tuning-configuration",
    "title": "Hyperparameter Tuning",
    "section": "Tuning configuration",
    "text": "Tuning configuration\nBefore you submit you training script you need to create a configuration file that determines both the name of the metric to optimize as well as the training flags and corresponding values to use for optimization. The exact semantics of specifying a metric differ depending on what interface you are using, here we’ll use a Keras example (see the section on Optimization metrics for details on other interfaces).\nWith Keras, any named metric (as defined by the metrics argument passed to the compile() function) can be used as the target for optimization. For example, if this was the call to compile():\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nThen you could use the following as your CloudML training configuration file for a scenario where you wanted to explore the impact of different dropout ratios:\ntuning.yml\ntrainingInput:\n  scaleTier: CUSTOM\n  masterType: standard_gpu\n  hyperparameters:\n    goal: MAXIMIZE\n    hyperparameterMetricTag: acc\n    maxTrials: 10\n    maxParallelTrials: 2\n    params:\n      - parameterName: dropout1\n        type: DOUBLE\n        minValue: 0.2\n        maxValue: 0.6\n        scaleType: UNIT_LINEAR_SCALE\n      - parameterName: dropout2\n        type: DOUBLE\n        minValue: 0.1\n        maxValue: 0.5\n        scaleType: UNIT_LINEAR_SCALE\nWe specified hyperparameterMetricTag: acc as the metric to optimize for. Note that whenever attempting to optimize accuracy with Keras specify acc rather than accuracy as that is the standard abbreviation used by Keras for this metric.\nThe type field can be one of:\n\nINTEGER\nDOUBLE\nCATEGORICAL\nDISCRETE\n\nThe scaleType field for numerical types can be one of:\n\nUNIT_LINEAR_SCALE\nUNIT_LOG_SCALE\nUNIT_REVERSE_LOG_SCALE\n\nIf you are using CATEGORICAL or DISCRETE types you will need to pass the possible values to categoricalValues or discreteValues parameter. For example, you could have an hyperparameter defined like this:\n- parameterName: activation\n  type: CATEGORICAL\n  categoricalValues: [relu, tanh, sigmoid]\nNote also that configuration for the compute resources to use for the job can also be provided in the config file (e.g. the masterType field).\nComplete details on available options can be found in the HyperparameterSpec documentation."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#submitting-a-tuning-job",
    "href": "v1/tools/cloudml/tuning.html#submitting-a-tuning-job",
    "title": "Hyperparameter Tuning",
    "section": "Submitting a tuning job",
    "text": "Submitting a tuning job\nTo submit a hyperparmaeter tuning job, pass the name of the CloudML configuration file containing your hyperparmeters to cloudml_train():\n\ncloudml_train(\"mnist_mlp.R\", config = \"tuning.yml\")\n\nThe job will proceed as normal, and you can monitor it’s results within an RStudio terminal or via the job_status() and job_stream_logs() functions."
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#collecting-trials",
    "href": "v1/tools/cloudml/tuning.html#collecting-trials",
    "title": "Hyperparameter Tuning",
    "section": "Collecting trials",
    "text": "Collecting trials\nOnce the job is completed you can inspect all of the job trails using the job_trials() function. For example:\n\njob_trials(\"cloudml_2018_01_08_142717956\")\n\nfinalMetric.objectiveValue finalMetric.trainingStep hyperparameters.dropout1 hyperparameters.dropout2 trialId\n1                    0.973854                       19       0.2011326172916916      0.32774705750441724      10\n2                    0.973458                       19      0.20090378506439671      0.10079321757280404       3\n3                    0.973354                       19       0.5476299090261757      0.49998941144858033       6\n4                    0.972875                       19        0.597820322273044       0.4074512354566201       7\n5                    0.972729                       19      0.25969787952729828      0.42851076497180118       1\n6                    0.972417                       19      0.20045494784980847      0.15927383711937335       4\n7                    0.972188                       19      0.33367593781223304      0.10077055587860367       5\n8                    0.972188                       19      0.59880072314674071      0.10476853415572558       9\n9                    0.972021                       19         0.40078175292512      0.49982245025905447       8\n10                   0.971792                       19      0.46984175786143262      0.25901078861553267       2\nYou can collect jobs executed as part of a hyperparameter tunning run using the ’job_collect()` function:\n\njob_collect(\"cloudml_2018_01_08_142717956\")\n\nBy default this will only collect the job trial with the best metric (trials = \"best\"). You can pass trials = \"all\" to download all trials. For example:\n\njob_collect(\"cloudml_2018_01_08_142717956\", trials = \"all\")\n\nYou can also pass vector of trial IDs to download specific trials. For example, this code would download the top 5 performing trials:\n\ntrials <- job_trials(\"cloudml_2018_01_08_142717956\")\njob_collect(\"cloudml_2018_01_08_142717956\", trials = trials$trialId[1:5])"
  },
  {
    "objectID": "v1/tools/cloudml/tuning.html#optimization-metrics",
    "href": "v1/tools/cloudml/tuning.html#optimization-metrics",
    "title": "Hyperparameter Tuning",
    "section": "Optimization metrics",
    "text": "Optimization metrics\nThe hyperparameterMetricTag is the TensorFlow summary tag name used for optimizing trials. For current versions of TensorFlow, this tag name should exactly match what is shown in TensorBoard, including all scopes.\nYou can open Tensorboard by running tensorboard() over a completed run and inspecting the available metrics.\nTags vary across models but some common ones follow:\n\n\n\npackage\ntag\n\n\n\n\nkeras\nacc\n\n\nkeras\nloss\n\n\nkeras\nval_acc\n\n\nkeras\nval_loss\n\n\ntfestimators\naverage_loss\n\n\ntfestimators\nglobal_step\n\n\ntfestimators\nloss\n\n\n\nWhen using the Core TensorFlow API summary tags can be added explicitly as follows:\n\nsummary <- tf$Summary()\nsummary$value$add(tag = \"accuracy\", simple_value = accuracy)\nsummary_writer$add_summary(summary, iteration_number)\n\nYou can see examples training scripts and corresponding tuning.yml files for the various TensorFlow APIs here:\n\nkeras\ntfestimators\ntensorflow"
  },
  {
    "objectID": "v1/tools/index.html",
    "href": "v1/tools/index.html",
    "title": "Overview",
    "section": "",
    "text": "Training Runs: The tfruns package provides a suite of tools for tracking and managing TensorFlow training runs and experiments from R. Track the hyperparameters, metrics, output, and source code of every training run, visualize the results of individual runs and comparisons between runs.\nTensorBoard: The computations you’ll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, a suite of visualization tools called TensorBoard is available. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it."
  },
  {
    "objectID": "v1/tools/tensorboard/hparams.html",
    "href": "v1/tools/tensorboard/hparams.html",
    "title": "Hyperparameter Tuning with the HParams Dashboard",
    "section": "",
    "text": "When building machine learning models, you need to choose various hyperparameters, such as the dropout rate in a layer or the learning rate. These decisions impact model metrics, such as accuracy. Therefore, an important step in the machine learning workflow is to identify the best hyperparameters for your problem, which often involves experimentation. This process is known as “Hyperparameter Optimization” or “Hyperparameter Tuning”.\nThe HParams dashboard in TensorBoard provides several tools to help with this process of identifying the best experiment or most promising sets of hyperparameters.\nThis tutorial will focus on the following steps:\nImport TensorFlow and the TensorBoard HParams plugin:\nDownload the FashionMNIST dataset and scale it:"
  },
  {
    "objectID": "v1/tools/tensorboard/hparams.html#experiment-setup-and-the-hparams-experiment-summary",
    "href": "v1/tools/tensorboard/hparams.html#experiment-setup-and-the-hparams-experiment-summary",
    "title": "Hyperparameter Tuning with the HParams Dashboard",
    "section": "Experiment setup and the HParams experiment summary",
    "text": "Experiment setup and the HParams experiment summary\nExperiment with three hyperparameters in the model:\n\nNumber of units in the first dense layer\nDropout rate in the dropout layer\nOptimizer\n\nList the values to try, and log an experiment configuration to TensorBoard. This step is optional: you can provide domain information to enable more precise filtering of hyperparameters in the UI, and you can specify which metrics should be displayed.\nIf you choose to skip this step, you can use a string literal wherever you would otherwise use an HParam value: e.g., hparams[\"dropout\"] or hparams$dropout instead of hparams[HP_DROPOUT].\n\nHP_NUM_UNITS = hp$HParam('num_units', hp$Discrete(list(16, 32)))\nHP_DROPOUT = hp$HParam('dropout', hp$RealInterval(0.1, 0.2))\nHP_OPTIMIZER = hp$HParam('optimizer', hp$Discrete(list('adam', 'sgd')))\n\nMETRIC_ACCURACY = 'accuracy'\n\nwith(tf$summary$create_file_writer(\"logs/hparam_tuning\")$as_default(), {\n  hp$hparams_config(\n    hparams = list(HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER),\n    metrics = list(hp$Metric(METRIC_ACCURACY, display_name = \"Accuracy\"))\n  )\n})"
  },
  {
    "objectID": "v1/tools/tensorboard/hparams.html#adapt-tensorflow-runs-to-log-hyperparameters-and-metrics",
    "href": "v1/tools/tensorboard/hparams.html#adapt-tensorflow-runs-to-log-hyperparameters-and-metrics",
    "title": "Hyperparameter Tuning with the HParams Dashboard",
    "section": "Adapt TensorFlow runs to log hyperparameters and metrics",
    "text": "Adapt TensorFlow runs to log hyperparameters and metrics\nThe model will be quite simple: two dense layers with a dropout layer between them. The training code will look familiar, although the hyperparameters are no longer hardcoded. Instead, the hyperparameters are provided in an hparams dictionary and used throughout the training function:\n\ntrain_test_model <- function(hparams) {\n  \n  model <- keras_model_sequential() %>% \n    layer_flatten() %>% \n    layer_dense(py_to_r(hparams[HP_NUM_UNITS]), activation = \"relu\") %>% \n    layer_dropout(py_to_r(hparams[HP_DROPOUT])) %>% \n    layer_dense(units = 10, activation = \"softmax\")\n  \n  model %>% \n    compile(\n      optimizer = py_to_r(hparams[HP_OPTIMIZER]),\n      loss = \"sparse_categorical_crossentropy\",\n      metrics = \"accuracy\"\n    )\n  \n  model %>% fit(\n    x = fashion_mnist$train$x, \n    y = fashion_mnist$train$y, \n    epochs = 1\n  ) # Run with 1 epoch to speed things up for demo purposes\n  \n  results <- model %>% evaluate(\n    x = fashion_mnist$test$x, \n    y = fashion_mnist$test$y,\n    verbose = 0\n  )\n  \n  results$accuracy\n}\n\nFor each run, log an hparams summary with the hyperparameters and final accuracy:\n\nrun <- function(run_dir, hparams) {\n  with(tf$summary$create_file_writer(run_dir)$as_default(), {\n    hp$hparams(hparams) # record the values used in this trial\n    accuracy <- train_test_model(hparams)\n    tf$summary$scalar(METRIC_ACCURACY, accuracy, step = as.integer(1))\n  })\n}\n\nWhen training Keras models, you can use callbacks instead of writing these directly:\n\nmodel %>%  fit(\n    ...,\n    callbacks=list(\n        callback_tensorboard(logdir),  # log metrics\n        hp$KerasCallback(logdir, hparams),  # log hparams\n    ),\n)"
  },
  {
    "objectID": "v1/tools/tensorboard/hparams.html#start-runs-and-log-them-all-under-one-parent-directory",
    "href": "v1/tools/tensorboard/hparams.html#start-runs-and-log-them-all-under-one-parent-directory",
    "title": "Hyperparameter Tuning with the HParams Dashboard",
    "section": "Start runs and log them all under one parent directory",
    "text": "Start runs and log them all under one parent directory\nYou can now try multiple experiments, training each one with a different set of hyperparameters.\nFor simplicity, use a grid search: try all combinations of the discrete parameters and just the lower and upper bounds of the real-valued parameter. For more complex scenarios, it might be more effective to choose each hyperparameter value randomly (this is called a random search). There are more advanced methods that can be used.\nRun a few experiments, which will take a few minutes:\n\nsession_num <- 0\n\nfor(num_units in HP_NUM_UNITS$domain$values) {\n  for (dropout_rate in c(HP_DROPOUT$domain$min_value, HP_DROPOUT$domain$max_value)) {\n    for (optimizer in HP_OPTIMIZER$domain$values) {\n      hparams <- dict(\n        HP_NUM_UNITS = num_units,\n        HP_DROPOUT = dropout_rate,\n        HP_OPTIMIZER = optimizer\n      )\n      \n      run_name <- sprintf(\"run-%04d\", session_num)\n      print(sprintf('--- Starting trial: %s',  run_name))\n      #purrr::iwalk(hparams, ~print(paste(.y, .x, sep = \": \")))\n      run(paste0('logs/hparam_tuning/', run_name), hparams)\n      session_num <- session_num + 1\n    }\n  }\n}"
  },
  {
    "objectID": "v1/tools/tensorboard/hparams.html#visualize-the-results-in-tensorboards-hparams-plugin",
    "href": "v1/tools/tensorboard/hparams.html#visualize-the-results-in-tensorboards-hparams-plugin",
    "title": "Hyperparameter Tuning with the HParams Dashboard",
    "section": "Visualize the results in TensorBoard’s HParams plugin",
    "text": "Visualize the results in TensorBoard’s HParams plugin\n\ntensorboard(\"logs/hparam_tuning/\")\n\n\nThe left pane of the dashboard provides filtering capabilities that are active across all the views in the HParams dashboard:\n\nFilter which hyperparameters/metrics are shown in the dashboard\nFilter which hyperparameter/metrics values are shown in the dashboard\nFilter on run status (running, success, …)\nSort by hyperparameter/metric in the table view\nNumber of session groups to show (useful for performance when there are many experiments)\n\nThe HParams dashboard has three different views, with various useful information:\n\nThe Table View lists the runs, their hyperparameters, and their metrics.\nThe Parallel Coordinates View shows each run as a line going through an axis for each hyperparemeter and metric. Click and drag the mouse on any axis to mark a region which will highlight only the runs that pass through it. This can be useful for identifying which groups of hyperparameters are most important. The axes themselves can be re-ordered by dragging them.\nThe Scatter Plot View shows plots comparing each hyperparameter/metric with each metric. This can help identify correlations. Click and drag to select a region in a specific plot and highlight those sessions across the other plots.\n\nA table row, a parallel coordinates line, and a scatter plot market can be clicked to see a plot of the metrics as a function of training steps for that session (although in this tutorial only one step is used for each run)."
  },
  {
    "objectID": "v1/tools/tensorboard/tensorboard.html",
    "href": "v1/tools/tensorboard/tensorboard.html",
    "title": "TensorBoard",
    "section": "",
    "text": "The computations you’ll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, a suite of visualization tools called TensorBoard is available. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\nFor example, here’s a TensorBoard display for Keras accuracy and loss metrics:"
  },
  {
    "objectID": "v1/tools/tensorboard/tensorboard.html#recording-data",
    "href": "v1/tools/tensorboard/tensorboard.html#recording-data",
    "title": "TensorBoard",
    "section": "Recording Data",
    "text": "Recording Data\nThe method for recording events for visualization by TensorBoard varies depending upon which TensorFlow interface you are working with:\n\n\n\nKeras\nWhen using Keras, include the callback_tensorboard() when invoking the fit() function to train a model. See the Keras documentation for additional details.\n\n\nEstimators\nWhen using TF Estimators, TensorBoard events are automatically written to the model_dir specified when creating the estimator. See the Estimators documentation for additional details.\n\n\nCore API\nWhen using the core API, you need to attach tf$summary$scalar operations to the graph for the metrics you want to record for viewing in TensorBoard. See the core documentation for additional details.\n\n\n\nNote that in all cases it’s important that you use a unique directory to record training events (otherwise events from multiple training runs will be aggregated together).\nYou can remove and recreate event log directories between runs, or alternatively use the tfruns package to do training, which will automatically create a new directory for each training run."
  },
  {
    "objectID": "v1/tools/tensorboard/tensorboard.html#viewing-data",
    "href": "v1/tools/tensorboard/tensorboard.html#viewing-data",
    "title": "TensorBoard",
    "section": "Viewing Data",
    "text": "Viewing Data\nTo view TensorBoard data for a given set of runs you use the tensorboard() function, pointing it to to a directory which contains TensorBoard logs:\n\ntensorboard(\"logs/run_a\")\n\nIt’s often useful to run TensorBoard while you are training a model. To do this, simply launch tensorboard within the training directory right before you begin training:\n\n# launch TensorBoard (data won't show up until after the first epoch)\ntensorboard(\"logs/run_a\")\n\n# fit the model with the TensorBoard callback\nhistory <- model %>% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  verbose = 1,\n  callbacks = callback_tensorboard(\"logs/run_a\"),\n  validation_split = 0.2\n)\n\nKeras writes TensorBoard data at the end of each epoch so you won’t see any data in TensorBoard until 10-20 seconds after the end of the first epoch (TensorBoard automatically refreshes it’s display every 30 seconds during training).\n\ntfruns\nIf you are using the tfruns package to track and manage training runs then there are some shortcuts available for the tensorboard() function:\n\ntensorboard()                                # views the latest run by default\ntensorboard(latest_run())                    # view the latest run\ntensorboard(ls_runs(order = eval_acc)[1,])   # view the run with the best eval_acc"
  },
  {
    "objectID": "v1/tools/tensorboard/tensorboard.html#comparing-runs",
    "href": "v1/tools/tensorboard/tensorboard.html#comparing-runs",
    "title": "TensorBoard",
    "section": "Comparing Runs",
    "text": "Comparing Runs\nTensorBoard will automatically include all runs logged within the sub-directories of the specified log_dir, for example, if you logged another run using:\n\ncallback_tensorboard(log_dir = \"logs/run_b\")\n\nThen called tensorboard as follows:\n\ntensorboard(\"logs\")\n\nThe TensorBoard visualization would look like this:\n\nYou can also pass multiple log directories. For example:\n\ntensorboard(c(\"logs/run_a\", \"logs/run_b\"))\n\n\ntfruns\nIf you are using the tfruns package to track and manage training runs then you easily pass multiple runs that match a criteria using the ls_runs() function. For example:\n\ntensorboard(ls_runs(latest_n = 2))         # last 2 runs\ntensorboard(ls_runs(eval_acc > 0.98))      # all runs with > 0.98 eval accuracy\ntensorboard(ls_runs(order = eval_acc))[5,] # top 5 runs w/r/t eval accuracy"
  },
  {
    "objectID": "v1/tools/tfruns/managing.html",
    "href": "v1/tools/tfruns/managing.html",
    "title": "Managing Runs",
    "section": "",
    "text": "Any graphical or console output as well as file artifacts created by a training run (e.g. saved models or saved model weights) can be viewed from the Output tab of the run view:\n\nYou can use the copy_run_files() function to export file artifacts from runs into another directory. For example:\n\ncopy_run_files(\"runs/2017-09-24T10-54-00Z\", to = \"saved-model\")\n\nYou can also use the copy_run() function to export a run directory in it’s entirety. For example, this code exports the specified run to a “best-run” directory:\n\ncopy_run(\"runs/2017-09-24T10-54-00Z\", to = \"best-run\")\n\nNote that copy_run() will accept any number of runs. For example, this code exports all run directories with an evaluation accuracy greater than 0.98 to a “best-runs” directory:\n\ncopy_run(ls_runs(eval_acc >= 0.98), to = \"best-runs\")"
  },
  {
    "objectID": "v1/tools/tfruns/managing.html#cleaning-runs",
    "href": "v1/tools/tfruns/managing.html#cleaning-runs",
    "title": "Managing Runs",
    "section": "Cleaning Runs",
    "text": "Cleaning Runs\nYou can use the clean_runs() function to archive a set of runs you no longer need the data from. For example, this code archives all runs with an eval accuracy less than 0.98:\n\nclean_runs(ls_runs(eval_acc < 0.98))\n\nIf you don’t specify a set of runs to clean then all runs will be archived:\n\nclean_runs() # archives all runs in the \"runs\" directory\n\nNote that you’ll always get a confirmation prompt before the runs are actually archived."
  },
  {
    "objectID": "v1/tools/tfruns/managing.html#purging-runs",
    "href": "v1/tools/tfruns/managing.html#purging-runs",
    "title": "Managing Runs",
    "section": "Purging Runs",
    "text": "Purging Runs\nWhen runs are archived they are moved to the “archive” subdirectory of the “runs” directory. If you want to permanently remove runs from the archive you call the purge_runs() function:\n\npurge_runs()"
  },
  {
    "objectID": "v1/tools/tfruns/overview.html",
    "href": "v1/tools/tfruns/overview.html",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "",
    "text": "The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R:"
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#installation",
    "href": "v1/tools/tfruns/overview.html#installation",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Installation",
    "text": "Installation\nYou can install the tfruns package from CRAN as follows:\n\ninstall.packages(\"tfruns\")\n\nThe package is intended to be used with the keras and/or the tfestimators packages, both of which provide higher level interfaces to TensorFlow from R. These packages can be installed with:\n\ninstall.packages(\"keras\")\ninstall.packages(\"tfestimators\")"
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#training",
    "href": "v1/tools/tfruns/overview.html#training",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Training",
    "text": "Training\nIn the following sections we’ll describe the various capabilities of tfruns. Our example training script (mnist_mlp.R) trains a Keras model to recognize MNIST digits.\nTo train a model with tfruns, just use the training_run() function in place of the source() function to execute your R script. For example:\n\nlibrary(tfruns)\ntraining_run(\"mnist_mlp.R\")\n\nWhen training is completed, a summary of the run will automatically be displayed if you are within an interactive R session:\n\nThe metrics and output of each run are automatically captured within a run directory which is unique for each run that you initiate. Note that for Keras and TF Estimator models this data is captured automatically (no changes to your source code are required).\nYou can call the latest_run() function to view the results of the last run (including the path to the run directory which stores all of the run’s output):\n\nlatest_run()\n\n$ run_dir           : chr \"runs/2017-10-02T14-23-38Z\"\n$ eval_loss         : num 0.0956\n$ eval_acc          : num 0.98\n$ metric_loss       : num 0.0624\n$ metric_acc        : num 0.984\n$ metric_val_loss   : num 0.0962\n$ metric_val_acc    : num 0.98\n$ flag_dropout1     : num 0.4\n$ flag_dropout2     : num 0.3\n$ samples           : int 48000\n$ validation_samples: int 12000\n$ batch_size        : int 128\n$ epochs            : int 20\n$ epochs_completed  : int 20\n$ metrics           : chr \"(metrics data frame)\"\n$ model             : chr \"(model summary)\"\n$ loss_function     : chr \"categorical_crossentropy\"\n$ optimizer         : chr \"RMSprop\"\n$ learning_rate     : num 0.001\n$ script            : chr \"mnist_mlp.R\"\n$ start             : POSIXct[1:1], format: \"2017-10-02 14:23:38\"\n$ end               : POSIXct[1:1], format: \"2017-10-02 14:24:24\"\n$ completed         : logi TRUE\n$ output            : chr \"(script ouptut)\"\n$ source_code       : chr \"(source archive)\"\n$ context           : chr \"local\"\n$ type              : chr \"training\"\nThe run directory used in the example above is “runs/2017-10-02T14-23-38Z”. Run directories are by default generated within the “runs” subdirectory of the current working directory, and use a timestamp as the name of the run directory. You can view the report for any given run using the view_run() function:\n\nview_run(\"runs/2017-10-02T14-23-38Z\")"
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#comparing-runs",
    "href": "v1/tools/tfruns/overview.html#comparing-runs",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Comparing Runs",
    "text": "Comparing Runs\nLet’s make a couple of changes to our training script to see if we can improve model performance. We’ll change the number of units in our first dense layer to 128, change the learning_rate from 0.001 to 0.003 and run 30 rather than 20 epochs. After making these changes to the source code we re-run the script using training_run() as before:\n\ntraining_run(\"mnist_mlp.R\")\n\nThis will also show us a report summarizing the results of the run, but what we are really interested in is a comparison between this run and the previous one. We can view a comparison via the compare_runs() function:\n\ncompare_runs()\n\n\nThe comparison report shows the model attributes and metrics side-by-side, as well as differences in the source code and output of the training script.\nNote that compare_runs() will by default compare the last two runs, however you can pass any two run directories you like to be compared."
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#analyzing-runs",
    "href": "v1/tools/tfruns/overview.html#analyzing-runs",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Analyzing Runs",
    "text": "Analyzing Runs\nWe’ve demonstrated visualizing and comparing one or two runs, however as you accumulate more runs you’ll generally want to analyze and compare runs many runs. You can use the ls_runs() function to yield a data frame with summary information on all of the runs you’ve conducted within a given directory:\n\nls_runs()\n\nData frame: 4 x 28 \n                    run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss metric_val_acc\n1 runs/2017-12-09T21-01-11Z    0.1485   0.9562      0.2577     0.9240          0.1482         0.9545\n2 runs/2017-12-09T21-00-11Z    0.1438   0.9573      0.2655     0.9208          0.1505         0.9559\n3 runs/2017-12-09T19-59-44Z    0.1407   0.9580      0.2597     0.9241          0.1402         0.9578\n4 runs/2017-12-09T19-56-48Z    0.1437   0.9555      0.2610     0.9227          0.1459         0.9551\n# ... with 21 more columns:\n#   flag_batch_size, flag_dropout1, flag_dropout2, samples, validation_samples, batch_size,\n#   epochs, epochs_completed, metrics, model, loss_function, optimizer, learning_rate, script,\n#   start, end, completed, output, source_code, context, type\nYou can also render a sortable, filterable version all of the columns within RStudio using the View() function:\n\nView(ls_runs())\n\n\nThe ls_runs() function also supports subset and order arguments. For example, the following will yield all runs with an eval accuracy better than 0.98:\n\nls_runs(eval_acc > 0.9570, order = eval_acc)\n\nData frame: 2 x 28 \n                    run_dir eval_acc eval_loss metric_loss metric_acc metric_val_loss metric_val_acc\n1 runs/2017-12-09T19-59-44Z   0.9580    0.1407      0.2597     0.9241          0.1402         0.9578\n2 runs/2017-12-09T21-00-11Z   0.9573    0.1438      0.2655     0.9208          0.1505         0.9559\n# ... with 21 more columns:\n#   flag_batch_size, flag_dropout1, flag_dropout2, samples, validation_samples, batch_size,\n#   epochs, epochs_completed, metrics, model, loss_function, optimizer, learning_rate, script,\n#   start, end, completed, output, source_code, context, type\nYou can pass the results of ls_runs() to compare runs (which will always compare the first two runs passed). For example, this will compare the two runs that performed best in terms of evaluation accuracy:\n\ncompare_runs(ls_runs(eval_acc > 0.9570, order = eval_acc))"
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#rstudio-ide",
    "href": "v1/tools/tfruns/overview.html#rstudio-ide",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "RStudio IDE",
    "text": "RStudio IDE\nIf you use RStudio with tfruns, it’s strongly recommended that you update to the current Preview Release of RStudio v1.1, as there are are a number of points of integration with the IDE that require this newer release.\n\nAddin\nThe tfruns package installs an RStudio IDE addin which provides quick access to frequently used functions from the Addins menu:\n\nNote that you can use Tools -> Modify Keyboard Shortcuts within RStudio to assign a keyboard shortcut to one or more of the addin commands.\n\n\nBackground Training\nRStudio v1.1 includes a Terminal pane alongside the Console pane. Since training runs can become quite lengthy, it’s often useful to run them in the background in order to keep the R console free for other work. You can do this from a Terminal as follows:\n\nIf you are not running within RStudio then you can of course use a system terminal window for background training.\n\n\nPublishing Reports\nTraining run views and comparisons are HTML documents which can be saved and shared with others. When viewing a report within RStudio v1.1 you can save a copy of the report or publish it to RPubs or RStudio Connect:\n\nIf you are not running within RStudio then you can use the save_run_view() and save_run_comparison() functions to create standalone HTML versions of run reports."
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#hyperparameter-tuning",
    "href": "v1/tools/tfruns/overview.html#hyperparameter-tuning",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nTuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally to systematically train over the combinations of those parameters to determine which combination yields the best model. See the Hyperparmeter Tuning article for details on how to accomplish this with tfruns."
  },
  {
    "objectID": "v1/tools/tfruns/overview.html#managing-runs",
    "href": "v1/tools/tfruns/overview.html#managing-runs",
    "title": "tfruns: Track and Visualize Training Runs",
    "section": "Managing Runs",
    "text": "Managing Runs\nThere are a variety of tools available for managing training run output, including:\n\nExporting run artifacts (e.g. saved models).\nCopying and purging run directories.\nUsing a custom run directory for an experiment or other set of related runs.\n\nThe Managing Runs article provides additional details on using these features."
  },
  {
    "objectID": "v1/tools/tfruns/tuning.html",
    "href": "v1/tools/tfruns/tuning.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "Tuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not by changing the source code of the training script as we did above, but instead by defining flags for key parameters then training over the combinations of those flags to determine which combination of flags yields the best model."
  },
  {
    "objectID": "v1/tools/tfruns/tuning.html#training-flags",
    "href": "v1/tools/tfruns/tuning.html#training-flags",
    "title": "Hyperparameter Tuning",
    "section": "Training Flags",
    "text": "Training Flags\nHere’s a declaration of 2 flags that control dropout rate within a model:\n\nFLAGS <- flags(\n  flag_numeric(\"dropout1\", 0.4),\n  flag_numeric(\"dropout2\", 0.3)\n)\n\nThese flags are then used in the definition of the model here:\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%\n  layer_dropout(rate = FLAGS$dropout1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nOnce we’ve defined flags, we can pass alternate flag values to training_run() as follows:\n\ntraining_run('mnist_mlp.R', flags = list(dropout1 = 0.2, dropout2 = 0.2))\n\nYou aren’t required to specify all of the flags (any flags excluded will simply use their default value).\nFlags make it very straightforward to systematically explore the impact of changes to hyperparameters on model performance, for example:\n\nfor (dropout1 in c(0.1, 0.2, 0.3))\n  training_run('mnist_mlp.R', flags = list(dropout1 = dropout1))\n\nFlag values are automatically included in run data with a “flag_” prefix (e.g. flag_dropout1, flag_dropout2).\nSee the article on training flags for additional documentation on using flags."
  },
  {
    "objectID": "v1/tools/tfruns/tuning.html#tuning-runs",
    "href": "v1/tools/tfruns/tuning.html#tuning-runs",
    "title": "Hyperparameter Tuning",
    "section": "Tuning Runs",
    "text": "Tuning Runs\nAbove we demonstrated writing a loop to call training_run() with various different flag values. A better way to accomplish this is the tuning_run() function, which allows you to specify multiple values for each flag, and executes training runs for all combinations of the specified flags. For example:\n\n# run various combinations of dropout1 and dropout2\nruns <- tuning_run(\"mnist_mlp.R\", flags = list(\n  dropout1 = c(0.2, 0.3, 0.4),\n  dropout2 = c(0.2, 0.3, 0.4)\n))\n\n# find the best evaluation accuracy\nruns[order(runs$eval_acc, decreasing = TRUE), ]\n\nData frame: 9 x 28 \n                    run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss metric_val_acc\n9 runs/2018-01-26T13-21-03Z    0.1002   0.9817      0.0346     0.9900          0.1086         0.9794\n6 runs/2018-01-26T13-23-26Z    0.1133   0.9799      0.0409     0.9880          0.1236         0.9778\n5 runs/2018-01-26T13-24-11Z    0.1056   0.9796      0.0613     0.9826          0.1119         0.9777\n4 runs/2018-01-26T13-24-57Z    0.1098   0.9788      0.0868     0.9770          0.1071         0.9771\n2 runs/2018-01-26T13-26-28Z    0.1185   0.9783      0.0688     0.9819          0.1150         0.9783\n3 runs/2018-01-26T13-25-43Z    0.1238   0.9782      0.0431     0.9883          0.1246         0.9779\n8 runs/2018-01-26T13-21-53Z    0.1064   0.9781      0.0539     0.9843          0.1086         0.9795\n7 runs/2018-01-26T13-22-40Z    0.1043   0.9778      0.0796     0.9772          0.1094         0.9777\n1 runs/2018-01-26T13-27-14Z    0.1330   0.9769      0.0957     0.9744          0.1304         0.9751\n# ... with 21 more columns:\n#   flag_batch_size, flag_dropout1, flag_dropout2, samples, validation_samples, batch_size,\n#   epochs, epochs_completed, metrics, model, loss_function, optimizer, learning_rate, script,\n#   start, end, completed, output, source_code, context, type\nNote that the tuning_run() function returns a data frame containing a summary of all of the executed training runs."
  },
  {
    "objectID": "v1/tools/tfruns/tuning.html#experiment-scopes",
    "href": "v1/tools/tfruns/tuning.html#experiment-scopes",
    "title": "Hyperparameter Tuning",
    "section": "Experiment Scopes",
    "text": "Experiment Scopes\nBy default all runs go into the “runs” sub-directory of the current working directory. For various types of ad-hoc experimentation this works well, but in some cases for a tuning run you may want to create a separate directory scope.\nYou can do this by specifying the runs_dir argument:\n\n# run various combinations of dropout1 and dropout2\ntuning_run(\"mnist_mlp.R\", runs_dir = \"dropout_tuning\", flags = list(\n  dropout1 = c(0.2, 0.3, 0.4),\n  dropout2 = c(0.2, 0.3, 0.4)\n))\n\n# list runs witin the specified runs_dir\nls_runs(order = eval_acc, runs_dir = \"dropout_tuning\")\n\nData frame: 9 x 28 \n                              run_dir eval_acc eval_loss metric_loss metric_acc metric_val_loss metric_val_acc\n9 dropout_tuning/2018-01-26T13-38-02Z   0.9803    0.0980      0.0324     0.9902          0.1096         0.9789\n6 dropout_tuning/2018-01-26T13-40-40Z   0.9795    0.1243      0.0396     0.9885          0.1341         0.9784\n2 dropout_tuning/2018-01-26T13-43-55Z   0.9791    0.1138      0.0725     0.9813          0.1205         0.9773\n7 dropout_tuning/2018-01-26T13-39-49Z   0.9786    0.1027      0.0796     0.9778          0.1053         0.9761\n3 dropout_tuning/2018-01-26T13-43-08Z   0.9784    0.1206      0.0479     0.9871          0.1246         0.9775\n4 dropout_tuning/2018-01-26T13-42-21Z   0.9784    0.1026      0.0869     0.9766          0.1108         0.9769\n5 dropout_tuning/2018-01-26T13-41-31Z   0.9783    0.1086      0.0589     0.9832          0.1216         0.9764\n8 dropout_tuning/2018-01-26T13-38-57Z   0.9780    0.1007      0.0511     0.9855          0.1100         0.9771\n1 dropout_tuning/2018-01-26T13-44-41Z   0.9770    0.1178      0.1017     0.9734          0.1244         0.9757\n# ... with 21 more columns:\n#   flag_batch_size, flag_dropout1, flag_dropout2, samples, validation_samples, batch_size, epochs,\n#   epochs_completed, metrics, model, loss_function, optimizer, learning_rate, script, start, end,\n#   completed, output, source_code, context, type"
  },
  {
    "objectID": "v1/tools/tfruns/tuning.html#sampling-flag-combinations",
    "href": "v1/tools/tfruns/tuning.html#sampling-flag-combinations",
    "title": "Hyperparameter Tuning",
    "section": "Sampling Flag Combinations",
    "text": "Sampling Flag Combinations\nIf the number of flag combinations is very large, you can also specify that only a random sample of combinations should be tried using the sample parmaeter. For example:\n\n# run random sample (0.3) of dropout1 and dropout2 combinations\nruns <- tuning_run(\"mnist_mlp.R\", sample = 0.3, flags = list(\n  dropout1 = c(0.2, 0.3, 0.4),\n  dropout2 = c(0.2, 0.3, 0.4)\n))"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/autodiff.html",
    "href": "v1/tutorials/advanced/customization/autodiff.html",
    "title": "Automatic differentiation and gradient tape",
    "section": "",
    "text": "In this tutorial we will cover automatic differentiation, a key technique for optimizing machine learning models."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/autodiff.html#setup",
    "href": "v1/tutorials/advanced/customization/autodiff.html#setup",
    "title": "Automatic differentiation and gradient tape",
    "section": "Setup",
    "text": "Setup\nWe will use the TensorFlow R package:\n\nlibrary(tensorflow)"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/autodiff.html#gradient-tapes",
    "href": "v1/tutorials/advanced/customization/autodiff.html#gradient-tapes",
    "title": "Automatic differentiation and gradient tape",
    "section": "Gradient Tapes",
    "text": "Gradient Tapes\nTensorFlow provides the tf$GradientTape API for automatic differentiation - computing the gradient of a computation with respect to its input variables.\nTensorflow “records” all operations executed inside the context of a tf$GradientTape onto a “tape”. Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a “recorded” computation using reverse mode differentiation.\nFor example:\n\nx <- tf$ones(shape(2, 2))\n\nwith(tf$GradientTape() %as% t, {\n  t$watch(x)\n  y <- tf$reduce_sum(x)\n  z <- tf$multiply(y, y)\n})\n\n# Derivative of z with respect to the original input tensor x\ndz_dx <- t$gradient(z, x)\ndz_dx\n\nYou can also request gradients of the output with respect to intermediate values computed during a “recorded” tf$GradientTape context.\n\nx <- tf$ones(shape(2, 2))\n\nwith(tf$GradientTape() %as% t, {\n  t$watch(x)\n  y <- tf$reduce_sum(x)\n  z <- tf$multiply(y, y)\n})\n\n# Use the tape to compute the derivative of z with respect to the\n# intermediate value y.\ndz_dy <- t$gradient(z, y)\ndz_dy\n\nBy default, the resources held by a GradientTape are released as soon as GradientTape$gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:\n\nx <- tf$constant(3)\n\nwith(tf$GradientTape(persistent = TRUE) %as% t, {\n  t$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the derivative of z with respect to the\n# intermediate value y.\ndz_dx <- t$gradient(z, x) # 108.0 (4*x^3 at x = 3)\ndz_dx\n\ndy_dx <- t$gradient(y, x) # 6.0\ndy_dx\n\nrm(t)  # Drop the reference to the tape\n\n\nRecording control flow\nBecause tapes record operations as they are executed, R control flow (using ifs and whiles for example) is naturally handled:\n\nf <- function(x, y) {\n  output <- 1\n  for (i in seq_len(y)) {\n    if (i > 2 & i <= 5)\n      output = tf$multiply(output, x)\n  }\n  output\n}\n\ngrad <- function(x, y) {\n  with(tf$GradientTape() %as% t, {\n    t$watch(x)\n    out <- f(x, y)\n  })\n  t$gradient(out, x)\n}\n\nx <- tf$constant(2)\ngrad(x, 6)\ngrad(x, 5)\ngrad(x, 4)\n\n\n\nHigher-order gradients\nOperations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:\n\nx <- tf$Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n\nwith(tf$GradientTape() %as% t, {\n  \n  with(tf$GradientTape() %as% t2, {\n    y <- x*x*x\n  })\n  \n  # Compute the gradient inside the 't' context manager\n  # which means the gradient computation is differentiable as well.\n  dy_dx <- t2$gradient(y, x)\n  \n})\n\nd2y_dx <- t$gradient(dy_dx, x)\n\ndy_dx\nd2y_dx"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-layers.html",
    "href": "v1/tutorials/advanced/customization/custom-layers.html",
    "title": "Custom layers",
    "section": "",
    "text": "We recommend using keras as a high-level API for building neural networks. That said, most TensorFlow APIs are usable with eager execution."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-layers.html#layers-common-sets-of-useful-operations",
    "href": "v1/tutorials/advanced/customization/custom-layers.html#layers-common-sets-of-useful-operations",
    "title": "Custom layers",
    "section": "Layers: common sets of useful operations",
    "text": "Layers: common sets of useful operations\nMost of the time when writing code for machine learning models you want to operate at a higher level of abstraction than individual operations and manipulation of individual variables.\nMany machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers.\nTensorFlow includes the full Keras API in the keras package, and the Keras layers are very useful when building your own models.\n\n# To construct a layer, simply construct the object. Most layers take as \n# a first argument the number of output dimensions / channels.\nlayer <- layer_dense(units = 100)\n\n# The number of input dimensions is often unnecessary, as it can be inferred\n# the first time the layer is used, but it can be provided if you want to\n# specify it manually, which is useful in some complex models.\nlayer <- layer_dense(units = 10, input_shape = shape(NULL, 5))\n\nThe full list of pre-existing layers can be seen in the documentation. It includes Dense (a fully-connected layer), Conv2D, LSTM, BatchNormalization, Dropout, and many others.\n\n# To use a layer, simply call it.\nlayer(tf$zeros(shape(10, 5)))\n\n\n# Layers have many useful methods. For example, you can inspect all variables\n# in a layer using `layer$variables` and trainable variables using\n# `layer$trainable_variables`. In this case a fully-connected layer\n# will have variables for weights and biases.\nlayer$variables\n\n\n# The variables are also accessible through nice accessors\nlayer$kernel\nlayer$bias"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-layers.html#implementing-custom-layers",
    "href": "v1/tutorials/advanced/customization/custom-layers.html#implementing-custom-layers",
    "title": "Custom layers",
    "section": "Implementing custom layers",
    "text": "Implementing custom layers\nThe best way to implement your own layer is extending the KerasLayer class and implementing:\n\ninitialize , where you can do all input-independent initialization\nbuild, where you know the shapes of the input tensors and can do the rest of the initialization\ncall, where you do the forward computation\n\nNote that you don’t have to wait until build is called to create your variables, you can also create them in initialize. However, the advantage of creating them in build is that it enables late variable creation based on the shape of the inputs the layer will operate on. On the other hand, creating variables in initialize would mean that shapes required to create the variables will need to be explicitly specified.\n\nMyDenseLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    num_outputs = NULL,\n    kernel = NULL,\n    \n    initialize = function(num_outputs) {\n      self$num_outputs <- num_outputs\n    },\n    \n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel', \n        shape = list(input_shape[[2]], self$num_outputs)\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      tf$matmul(x, self$kernel)\n    }\n  \n  )\n)\n\n\nLayer Wrapper Function\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\n# define layer wrapper function\nlayer_my_dense <- function(object, num_outputs, name = NULL, trainable = TRUE) {\n  create_layer(MyDenseLayer, object, list(\n    num_outputs = as.integer(num_outputs),\n    name = name,\n    trainable = trainable\n  ))\n}\n\nSome important things to note about the layer wrapper function:\n\nIt accepts object as its first parameter (the object will either be a Keras sequential model or another Keras layer). The object parameter enables the layer to be composed with other layers using the magrittr pipe (%>%) operator.\nIt converts it’s output_dim to integer using the as.integer() function. This is done as convenience to the user because Keras variables are strongly typed (you can’t pass a float if an integer is expected). This enables users of the function to write output_dim = 32 rather than output_dim = 32L.\nSome additional parameters not used by the layer (name and trainable) are in the function signature. Custom layer functions can include any of the core layer function arguments (input_shape, batch_input_shape, batch_size, dtype, name, trainable, and weights) and they will be automatically forwarded to the Layer base class.\n\nWe can use the defined layer, for example:\n\nlayer <- layer_my_dense(num_outputs = 10)\nlayer(tf$zeros(shape(10, 5)))\n\nOverall code is easier to read and maintain if it uses standard layers whenever possible, as other readers will be familiar with the behavior of standard layers. If you want to use a layer which is not present in tf.keras.layers, consider filing a github issue or, even better, sending us a pull request!"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-layers.html#models-composing-layers",
    "href": "v1/tutorials/advanced/customization/custom-layers.html#models-composing-layers",
    "title": "Custom layers",
    "section": "Models: Composing layers",
    "text": "Models: Composing layers\nMany interesting layer-like things in machine learning models are implemented by composing existing layers. For example, each residual block in a resnet is a composition of convolutions, batch normalizations, and a shortcut. Layers can be nested inside other layers.\nTypically you use keras_model_custom when you need the model methods like: fit,evaluate, and save (see Custom Keras layers and models for details).\nOne other feature provided by MOdel (instead of Layer) is that in addition to tracking variables, a Model also tracks its internal layers, making them easier to inspect.\nFor examplle here is a ResNet block:\n\nresnet_identity_block <- function(kernel_size, filters) {\n  keras_model_custom(function(self) {\n    \n    self$conv2a <- layer_conv_2d(filters = filters[1], kernel_size = c(1, 1))\n    self$bn2a <- layer_batch_normalization()\n\n    self$conv2b <- layer_conv_2d(\n      filters = filters[2], \n      kernel_size = kernel_size, \n      padding = 'same'\n    )\n    self$bn2b = layer_batch_normalization()\n\n    self$conv2c = layer_conv_2d(filters = filters[3], kernel_size = c(1, 1))\n    self$bn2c = layer_batch_normalization()\n    \n    function(inputs, mask = NULL, training = FALSE) {\n      \n      x <- inputs %>% \n        self$conv2a() %>% \n        self$bn2a(training = training) %>% \n        tf$nn$relu() %>% \n        self$conv2b() %>% \n        self$bn2b(training = training) %>% \n        tf$nn$relu() %>% \n        self$conv2c() %>% \n        self$bn2c(training = training)\n      \n      tf$nn$relu(x + inputs)\n    }\n  })\n}\n\nblock <- resnet_identity_block(kernel_size = 1, filters = c(1, 2, 3))\nblock(tf$zeros(shape(1, 2, 3, 3)))\n\n\nblock$layers\n\n\nlength(block$variables)\n\nMuch of the time, however, models which compose many layers simply call one layer after the other. This can be done in very little code using keras_model_sequential:\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 1, kernel_size = c(1, 1)) %>% \n  layer_batch_normalization() %>% \n  layer_conv_2d(\n    filters = 2, \n    kernel_size = c(1,1), \n    padding = 'same'\n  ) %>% \n  layer_batch_normalization() %>% \n  layer_conv_2d(filters = 3, kernel_size = c(1, 1)) %>% \n  layer_batch_normalization()\n\n# trigger model building\nmodel(tf$zeros(c(1, 2, 3, 3)))\n\n\nsummary(model)"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-training.html",
    "href": "v1/tutorials/advanced/customization/custom-training.html",
    "title": "Custom training: basics",
    "section": "",
    "text": "In the previous tutorial, you covered the TensorFlow APIs for automatic differentiation—a basic building block for machine learning. In this tutorial, you will use the TensorFlow primitives introduced in the prior tutorials to do some simple machine learning.\nTensorFlow also includes Keras —a high-level neural network API that provides useful abstractions to reduce boilerplate and makes TensorFlow easier to use without sacrificing flexibility and performance. We strongly recommend the Keras API for development. However, in this short tutorial you will learn how to train a neural network from first principles to establish a strong foundation."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-training.html#variables",
    "href": "v1/tutorials/advanced/customization/custom-training.html#variables",
    "title": "Custom training: basics",
    "section": "Variables",
    "text": "Variables\nTensors in TensorFlow are immutable stateless objects. Machine learning models, however, must have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!).\nTensorFlow has stateful operations built-in, and these are often easier than using low-level R representations for your state. Use tf$Variable to represent weights in a model.\nA tf$Variable object stores a value and implicitly reads from this stored value. There are operations (tf.assign_sub, tf.scatter_update, etc.) that manipulate the value stored in a TensorFlow variable.\n\nv <- tf$Variable(1)\n\n# Use Python's `assert` as a debugging statement to test the condition\nas.numeric(v) == 1\n\n# Reassign the value `v`\nv$assign(3)\nas.numeric(v) == 3\n\n# Use `v` in a TensorFlow `tf.square()` operation and reassign\nv$assign(tf$square(v))\nas.numeric(v) == 9\n\nComputations using tf$Variable are automatically traced when computing gradients. For variables that represent embeddings, TensorFlow will do sparse updates by default, which are more computation and memory efficient.\nA tf$Variable is also a way to show a reader of your code that a piece of state is mutable."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/custom-training.html#fit-a-linear-model",
    "href": "v1/tutorials/advanced/customization/custom-training.html#fit-a-linear-model",
    "title": "Custom training: basics",
    "section": "Fit a linear model",
    "text": "Fit a linear model\nLet’s use the concepts you have learned so far—Tensor, Variable, and GradientTape—to build and train a simple model. This typically involves a few steps:\n\nDefine the model.\nDefine a loss function.\nObtain training data.\nRun through the training data and use an “optimizer” to adjust the variables to fit the data.\n\nHere, you’ll create a simple linear model, f(x) = x * W + b, which has two variables: W (weights) and b (bias). You’ll synthesize data such that a well trained model would have W = 3.0 and b = 2.0.\n\nDefine the model\nYou may organize your TensorFlow code to build models the way you prefer, here we will suggest using an R6 class.\n\nModel <- R6::R6Class(\n  classname = \"Model\",\n  public = list(\n    W = NULL,\n    b = NULL,\n    \n    initialize = function() {\n      self$W <- tf$Variable(5)\n      self$b <- tf$Variable(0)\n    },\n    \n    call = function(x) {\n      self$W*x + self$b\n    }\n    \n  )\n)\n\nmodel <- Model$new()\nmodel$call(3)\n\n\n\nDefine the loss function\nA loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let’s use the standard L2 loss, also known as the least square errors:\n\nloss <- function(y_pred, y_true) {\n  tf$reduce_mean(tf$square(y_pred - y_true))\n}\n\n\n\nObtain training data\nFirst, synthesize the training data by adding random Gaussian (Normal) noise to the inputs:\n\nTRUE_W = 3.0\nTRUE_b = 2.0\nNUM_EXAMPLES = 1000\n\ninputs  <- tf$random$normal(shape=shape(NUM_EXAMPLES))\nnoise   <- tf$random$normal(shape=shape(NUM_EXAMPLES))\noutputs <- inputs * TRUE_W + TRUE_b + noise\n\nBefore training the model, visualize the loss value by plotting the model’s predictions in red and the training data in blue:\n\nlibrary(tidyverse)\ntibble(\n  inputs = as.numeric(inputs), \n  outputs = as.numeric(outputs),\n  predicted = as.numeric(model$call(inputs))\n) %>% \n  ggplot(aes(x = inputs)) +\n  geom_point(aes(y = outputs)) +\n  geom_line(aes(y = predicted), color = \"blue\")\n\n\n\nDefine a training loop\nWith the network and training data, train the model using gradient descent to update the weights variable (W) and the bias variable (b) to reduce the loss.\nThere are many variants of the gradient descent scheme that are captured in tf$train$Optimizer—our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of tf.GradientTape for automatic differentiation and tf.assign_sub for decrementing a value (which combines tf.assign and tf.sub):\n\ntrain <- function(model, inputs, outputs, learning_rate) {\n  with (tf$GradientTape() %as% t, {\n    current_loss = loss(model$call(inputs), outputs)\n  })\n  \n  d <- t$gradient(current_loss, list(model$W, model$b))\n  \n  model$W$assign_sub(learning_rate * d[[1]])\n  model$b$assign_sub(learning_rate * d[[2]])\n  current_loss\n}\n\nFinally, let’s repeatedly run through the training data and see how W and b evolve.\n\nmodel <- Model$new()\n\nWs <- bs <- c()\n\nfor (epoch in seq_len(20)) {\n  \n  Ws[epoch] <- as.numeric(model$W)\n  bs[epoch] <- as.numeric(model$b)\n  \n  current_loss <- train(model, inputs, outputs, learning_rate = 0.1)\n  cat(glue::glue(\"Epoch: {epoch}, Loss: {as.numeric(current_loss)}\"), \"\\n\")\n} \n\ntibble(\n  epoch = 1:20,\n  Ws = Ws,\n  bs = bs\n) %>% \n  pivot_longer(\n    c(Ws, bs),\n    names_to = \"parameter\", \n    values_to = \"estimate\"\n  ) %>% \n  ggplot(aes(x = epoch, y = estimate)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\")\n\nThis tutorial used tf$Variable to build and train a simple linear model.\nIn practice, the high-level APIs—such as Keras—are much more convenient to build neural networks. Keras provides higher level building blocks (called “layers”), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies, and more."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html",
    "title": "Tensors and operations",
    "section": "",
    "text": "This is an introductory TensorFlow tutorial shows how to:"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html#import-tensorflow",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html#import-tensorflow",
    "title": "Tensors and operations",
    "section": "Import TensorFlow",
    "text": "Import TensorFlow\nTo get started, import the tensorflow module. As of TensorFlow 2.0, eager execution is turned on by default. This enables a more interactive frontend to TensorFlow, the details of which we will discuss much later.\n\nlibrary(tensorflow)"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html#tensors",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html#tensors",
    "title": "Tensors and operations",
    "section": "Tensors",
    "text": "Tensors\nA Tensor is a multi-dimensional array. Similar to array objects in R, tf$Tensor objects have a data type and a shape. Additionally, tf$Tensors can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (tf$add, tf$matmul, tf$linalg$inv etc.) that consume and produce tf.Tensors. These operations automatically convert native R types, for example:\n\ntf$add(1, 2)\ntf$add(c(1, 2), c(3, 4))\ntf$square(5)\ntf$reduce_sum(c(1, 2, 3))\n\n# Operator overloading is also supported\ntf$square(2) + tf$square(3)\n\nEach tf$Tensor has a shape and a datatype:\n\nx = tf$matmul(matrix(1,ncol = 1), matrix(c(2, 3), nrow = 1))\nx\nx$shape\nx$dtype\n\nThe most obvious differences between arrays and tf$Tensors are:\n\nTensors can be backed by accelerator memory (like GPU, TPU).\nTensors are immutable."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html#r-arrays-compatibility",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html#r-arrays-compatibility",
    "title": "Tensors and operations",
    "section": "R arrays compatibility",
    "text": "R arrays compatibility\nConverting between a TensorFlow tf.Tensors and an array is easy:\n\nTensorFlow operations automatically convert R arrays to Tensors.\n\nTensors are explicitly converted to R arrays using the as.array, as.matrix or as.numeric methods. There’s always a memory copy when converting from a Tensor to an array in R.\n\n# TensorFlow operations convert arrays to Tensors automatically\n1 + tf$ones(shape = 1)\n\n# The as.array method explicitly converts a Tensor to an array\nas.array(tf$ones(shape = 1))"
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html#gpu-acceleration",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html#gpu-acceleration",
    "title": "Tensors and operations",
    "section": "GPU acceleration",
    "text": "GPU acceleration\nMany TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operation—copying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed, for example:\n\nx <- tf$random$uniform(shape(3, 3))\n\n# List devices\ntf$config$experimental$list_physical_devices()\n\n# What device is x placed\nx$device\n\n\nDevice Names\nThe Tensor$device property provides a fully qualified string name of the device hosting the contents of the tensor. This name encodes many details, such as an identifier of the network address of the host on which this program is executing and the device within that host. This is required for distributed execution of a TensorFlow program. The string ends with GPU:<N> if the tensor is placed on the N-th GPU on the host."
  },
  {
    "objectID": "v1/tutorials/advanced/customization/tensors-operations.html#explicit-device-placement",
    "href": "v1/tutorials/advanced/customization/tensors-operations.html#explicit-device-placement",
    "title": "Tensors and operations",
    "section": "Explicit Device Placement",
    "text": "Explicit Device Placement\nIn TensorFlow, placement refers to how individual operations are assigned (placed on) a device for execution. As mentioned, when there is no explicit guidance provided, TensorFlow automatically decides which device to execute an operation and copies tensors to that device, if needed. However, TensorFlow operations can be explicitly placed on specific devices using the tf$device context manager, for example:\n\nprint(\"On CPU:0:\")\nwith(tf$device(\"CPU:0\"), {\n  x <- tf$ones(shape(1000, 1000))\n  print(x$device)\n})\n\nprint(\"On GPU:0:\")\nwith(tf$device(\"GPU:0\"), {\n  x <- tf$ones(shape(1000, 1000))\n  print(x$device)\n})"
  },
  {
    "objectID": "v1/tutorials/advanced/distributed/distributed_training_with_keras.html",
    "href": "v1/tutorials/advanced/distributed/distributed_training_with_keras.html",
    "title": "Distributed training with Keras",
    "section": "",
    "text": "The tf$distribute$Strategy API provides an abstraction for distributing your training across multiple processing units. The goal is to allow users to enable distributed training using existing models and training code, with minimal changes.\nThis tutorial uses the tf$distribute$MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine. Essentially, it copies all of the model’s variables to each processor. Then, it uses all-reduce to combine the gradients from all processors and applies the combined value to all copies of the model.\nMirroredStategy is one of several distribution strategy available in TensorFlow core.\n\n\nThis example uses the keras API to build the model and training loop.\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n# used to load the MNIST dataset\nlibrary(tfds)\n\nlibrary(purrr)\nlibrary(glue)\n\n\n\n\nDownload the MNIST dataset and load it using tfds. This returns a dataset in tfdatasets format.\n\n# if you haven't done it yet:\n# tfds::install_tfds()\nmnist <- tfds_load(\"mnist\")\ninfo <- summary(mnist)\n\n\n\n\nCreate a MirroredStrategy object. This will handle distribution, and provides a context manager (tf$distribute$MirroredStrategy$scope) to build your model inside.\n\nstrategy <- tf$distribute$MirroredStrategy()\nstrategy$num_replicas_in_sync\n\n\n\n\nWhen training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly.\n\nnum_train_examples <- as.integer(info$splits[[2]]$statistics$numExamples)\nnum_test_examples <- as.integer(info$splits[[1]]$statistics$numExamples)\n\nBUFFER_SIZE <- 10000\n\nBATCH_SIZE_PER_REPLICA <- 64\nBATCH_SIZE <- BATCH_SIZE_PER_REPLICA * strategy$num_replicas_in_sync\n\nPixel values, which are 0-255, have to be normalized to the 0-1 range. Furthermore, we shuffle and batch the train and test datasets. Notice we are also keeping an in-memory cache of the training data to improve performance.\n\ntrain_dataset <- mnist$train %>% \n  dataset_map(function(record) {\n    record$image <- tf$cast(record$image, tf$float32) / 255\n    record}) %>%\n  dataset_cache() %>%\n  dataset_shuffle(BUFFER_SIZE) %>% \n  dataset_batch(BATCH_SIZE) %>% \n  dataset_map(unname)\n\ntest_dataset <- mnist$test %>% \n dataset_map(function(record) {\n    record$image <- tf$cast(record$image, tf$float32) / 255\n    record}) %>%\n  dataset_batch(BATCH_SIZE) %>% \n  dataset_map(unname)\n\n\n\n\nCreate and compile the Keras model in the context of strategy$scope.\n\nwith (strategy$scope(), {\n   model <- keras_model_sequential() %>%\n     layer_conv_2d(\n       filters = 32,\n       kernel_size = 3,\n       activation = 'relu',\n       input_shape = c(28, 28, 1)\n       ) %>%\n     layer_max_pooling_2d() %>%\n     layer_flatten() %>%\n     layer_dense(units = 64, activation = 'relu') %>%\n     layer_dense(units = 10, activation = 'softmax')\n   \n  model %>% compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = 'accuracy')\n})\n\n\n\n\nThe callbacks used here are:\n\nTensorBoard: This callback writes a log for TensorBoard which allows you to visualize the graphs.\nModel Checkpoint: This callback saves the model after every epoch.\nLearning Rate Scheduler: Using this callback, you can schedule the learning rate to change after every epoch/batch.\n\nFor illustrative purposes, add a print callback to display the learning rate.\n\n# Define the checkpoint directory to store the checkpoints\ncheckpoint_dir <- './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt_{epoch}\")\n\n\n# Function for decaying the learning rate.\n# You can define any decay function you need.\ndecay <- function(epoch, lr) {\n  if (epoch < 3) 1e-3\n    else if (epoch >= 3 && epoch < 7) 1e-4\n      else 1e-5\n}\n\n\n# Callback for printing the LR at the end of each epoch.\nPrintLR <- R6::R6Class(\"PrintLR\",\n  inherit = KerasCallback,\n  \n  public = list(\n    \n    losses = NULL,\n     \n    on_epoch_end = function(epoch, logs = list()) {\n      tf$print(glue('\\nLearning rate for epoch {epoch} is {as.numeric(model$optimizer$lr)}\\n'))\n    }\n))\n\nprint_lr <- PrintLR$new()\n\n\ncallbacks <- list(\n    callback_tensorboard(log_dir = '/tmp/logs'),\n    callback_model_checkpoint(filepath = checkpoint_prefix, save_weights_only = TRUE),\n    callback_learning_rate_scheduler(decay),\n    print_lr\n)\n\n\n\n\nNow, train the model in the usual way, calling fit on the model and passing in the dataset created at the beginning of the tutorial. This step is the same whether you are distributing the training or not.\n\nmodel %>% fit(train_dataset, epochs = 12, callbacks = callbacks)\n\nAs you can see below, the checkpoints are getting saved.\n\nlist.files(checkpoint_dir)\n\nTo see how the model performs, load the latest checkpoint and call evaluate on the test data.\n\nmodel %>% load_model_weights_tf(tf$train$latest_checkpoint(checkpoint_dir))\n\nmodel %>% evaluate(test_dataset)\n\n\ntensorboard(log_dir = \"/tmp/logs\")"
  },
  {
    "objectID": "v1/tutorials/advanced/distributed/distributed_training_with_keras.html#export-to-savedmodel",
    "href": "v1/tutorials/advanced/distributed/distributed_training_with_keras.html#export-to-savedmodel",
    "title": "Distributed training with Keras",
    "section": "Export to SavedModel",
    "text": "Export to SavedModel\nExport the graph and the variables to the platform-agnostic SavedModel format. After your model is saved, you can load it with or without the scope.\n\npath <- 'saved_model/'\nmodel %>% save_model_tf(path)\n\nLoad the model without strategy$scope.\n\nunreplicated_model <- load_model_tf(path)\n\nunreplicated_model %>% compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = 'accuracy')\n\nunreplicated_model %>% evaluate(test_dataset)\n\nLoad the model with strategy$scope.\n\nwith (strategy$scope(), {\n  replicated_model <- load_model_tf(path)\n\n  replicated_model %>% compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = 'accuracy')\n\n  replicated_model %>% evaluate(test_dataset)\n})"
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html",
    "href": "v1/tutorials/advanced/images/cnn.html",
    "title": "Convolutional Neural Network (CNN)",
    "section": "",
    "text": "This tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training our model will take just a few lines of code."
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#setup",
    "href": "v1/tutorials/advanced/images/cnn.html#setup",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)"
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#download-and-prepare-the-cifar10-dataset",
    "href": "v1/tutorials/advanced/images/cnn.html#download-and-prepare-the-cifar10-dataset",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Download and prepare the CIFAR10 dataset",
    "text": "Download and prepare the CIFAR10 dataset\nThe CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n\ncifar <- dataset_cifar10()"
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#verify-the-data",
    "href": "v1/tutorials/advanced/images/cnn.html#verify-the-data",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Verify the data",
    "text": "Verify the data\nTo verify that the dataset looks correct, let’s plot the first 25 images from the training set and display the class name below each image.\n\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck')\n\nindex <- 1:30\n\npar(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))\ncifar$train$x[index,,,] %>% \n  purrr::array_tree(1) %>%\n  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% \n  purrr::map(as.raster, max = 255) %>%\n  purrr::iwalk(~{plot(.x); title(.y)})"
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#create-the-convolutional-base",
    "href": "v1/tutorials/advanced/images/cnn.html#create-the-convolutional-base",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Create the convolutional base",
    "text": "Create the convolutional base\nThe 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\nAs input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument input_shape to our first layer.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = \"relu\", \n                input_shape = c(32,32,3)) %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\")\n\nLet’s display the architecture of our model so far.\n\nsummary(model)\n\nAbove, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer."
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#add-dense-layers-on-top",
    "href": "v1/tutorials/advanced/images/cnn.html#add-dense-layers-on-top",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Add Dense layers on top",
    "text": "Add Dense layers on top\nTo complete our model, you will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs and a softmax activation.\n\nmodel %>% \n  layer_flatten() %>% \n  layer_dense(units = 64, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nHere’s the complete architecture of our model.\n\nNote Keras models are mutable objects and you don’t need to re-assign model in the chubnk above.\n\n\nsummary(model)\n\nAs you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers."
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#compile-and-train-the-model",
    "href": "v1/tutorials/advanced/images/cnn.html#compile-and-train-the-model",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Compile and train the model",
    "text": "Compile and train the model\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% \n  fit(\n    x = cifar$train$x, y = cifar$train$y,\n    epochs = 10,\n    validation_data = unname(cifar$test),\n    verbose = 2\n  )"
  },
  {
    "objectID": "v1/tutorials/advanced/images/cnn.html#evaluate-the-model",
    "href": "v1/tutorials/advanced/images/cnn.html#evaluate-the-model",
    "title": "Convolutional Neural Network (CNN)",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nplot(history)\n\n\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\n\nOur simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code!"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "",
    "text": "TensorFlow Hub is a way to share pretrained model components. See the TensorFlow Module Hub for a searchable listing of pre-trained models. This tutorial demonstrates:"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html#setup",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html#setup",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)\nlibrary(tfhub)"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html#an-imagenet-classifier",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html#an-imagenet-classifier",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "An ImageNet classifier",
    "text": "An ImageNet classifier\n\nDownload the classifier\nUse layer_hub to load a mobilenet and wrap it up as a keras layer. Any TensorFlow 2 compatible image classifier URL from tfhub.dev will work here.\n\nclassifier_url =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\"\n\nimage_shape <- c(224L, 224L, 3L)\n\nclassifier <- layer_hub(handle = classifier_url, input_shape = image_shape)\n\n\n\nRun it on a single image\nDownload a single image to try the model on.\n\nimage_url <- \"https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\"\n\nimg <- pins::pin(image_url, name = \"grace_hopper\") %>%\n  tensorflow::tf$io$read_file() %>% \n  tensorflow::tf$image$decode_image(dtype = tf$float32) %>% \n  tensorflow::tf$image$resize(size = image_shape[-3])\n\n\nimg %>% \n  as.array() %>% \n  as.raster() %>% \n  plot()\n\nAdd a batch dimension, and pass the image to the model.\n\nresult <- img %>% \n  tf$expand_dims(0L) %>% \n  classifier()\n\nThe result is a 1001 element vector of logits, rating the probability of each class for the image.\nSo the top class ID can be found with argmax:\n\npredicted_class <- tf$argmax(result, axis = 1L) %>% as.integer()\npredicted_class\n\n\n\nDecode the predictions\nWe have the predicted class ID, Fetch the ImageNet labels, and decode the predictions:\n\nlabels_url <- \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n\nimagenet_labels <- pins::pin(labels_url, \"imagenet_labels\") %>% \n  readLines()\n\n\nimg %>% \n  as.array() %>% \n  as.raster() %>% \n  plot()\n# \ntitle(paste(\"Prediction:\" , imagenet_labels[predicted_class + 1]))"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html#simple-transfer-learning",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html#simple-transfer-learning",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "Simple transfer learning",
    "text": "Simple transfer learning\nUsing TF Hub it is simple to retrain the top layer of the model to recognize the classes in our dataset.\n\nflowers <- pins::pin(\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\", \"flower_photos\")\n\nThe simplest way to load this data into our model is using image_data_generator.\nAll of TensorFlow Hub’s image modules expect float inputs in the [0, 1] range. Use the image_data_generator rescale parameter to achieve this.\nThe image size will be handled later.\n\nimage_generator <- image_data_generator(rescale=1/255)\nimage_data <- flowers[1] %>% \n  dirname() %>% \n  dirname() %>% \n  flow_images_from_directory(image_generator, target_size = image_shape[-3])\n\nThe resulting object is an iterator that returns image_batch, label_batch pairs. We can iterate over it using the iter_next from reticulate:\n\nstr(reticulate::iter_next(image_data))\n\n\nRun the classifier on a batch of images\nNow run the classifier on the image batch.\n\nimage_batch <- reticulate::iter_next(image_data)\npredictions <- classifier(tf$constant(image_batch[[1]], tf$float32))\npredicted_classnames <- imagenet_labels[as.integer(tf$argmax(predictions, axis = 1L) + 1L)]\n\n\npar(mfcol = c(4,8), mar = rep(1, 4), oma = rep(0.2, 4))\nimage_batch[[1]] %>% \n  purrr::array_tree(1) %>%\n  purrr::set_names(predicted_classnames) %>% \n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x); title(.y)})\n\nSee the LICENSE.txt file for image attributions.\nThe results are far from perfect, but reasonable considering that these are not the classes the model was trained for (except “daisy”).\n\n\nDownload the headless model\nTensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\nAny Tensorflow 2 compatible image feature vector URL from tfhub.dev will work here.\n\nfeature_extractor_url <- \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n\nCreate the feature extractor.\n\nfeature_extractor_layer <- layer_hub(handle = feature_extractor_url, \n                                     input_shape = image_shape)\n\nIt returns a 1280-length vector for each image:\n\nfeature_batch <- feature_extractor_layer(tf$constant(image_batch[[1]], tf$float32))\nfeature_batch\n\nFreeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer.\n\nfreeze_weights(feature_extractor_layer)\n\n\n\nAttach a classification head\nNow let’s create a sequential model using the feature extraction layer and add a new classification layer.\n\nmodel <- keras_model_sequential(list(\n  feature_extractor_layer,\n  layer_dense(units = image_data$num_classes, activation='softmax')\n))\n\nsummary(model)"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html#train-the-model",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html#train-the-model",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "Train the model",
    "text": "Train the model\nUse compile to configure the training process:\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nNow use the fit method to train the model.\nTo keep this example short train just 2 epochs.\n\nhistory <- model %>% fit_generator(\n  image_data, epochs=2, \n  steps_per_epoch = image_data$n / image_data$batch_size,\n  verbose = 2\n)\n\nNow after, even just a few training iterations, we can already see that the model is making progress on the task.\nWe can then verify the predictions:\n\nimage_batch <- reticulate::iter_next(image_data)\npredictions <- predict_classes(model, image_batch[[1]])\n\npar(mfcol = c(4,8), mar = rep(1, 4), oma = rep(0.2, 4))\nimage_batch[[1]] %>% \n  purrr::array_tree(1) %>%\n  purrr::set_names(names(image_data$class_indices)[predictions + 1]) %>% \n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x); title(.y)})"
  },
  {
    "objectID": "v1/tutorials/advanced/images/transfer-learning-hub.html#export-your-model",
    "href": "v1/tutorials/advanced/images/transfer-learning-hub.html#export-your-model",
    "title": "Transfer learning with TensorFlow Hub",
    "section": "Export your model",
    "text": "Export your model\nNow that you’ve trained the model, export it as a saved model:\n\nsave_model_tf(model, \"mymodel/\", include_optimizer = FALSE)\n\nNow confirm that we can reload it, and it still gives the same results:\n\nmodel_ <- load_model_tf(\"mymodel/\")\n\n\nx <- tf$constant(image_batch[[1]], tf$float32)\nall.equal(\n  as.matrix(model(x)),\n  as.matrix(model_(x))\n)"
  },
  {
    "objectID": "v1/tutorials/advanced/index.html",
    "href": "v1/tutorials/advanced/index.html",
    "title": "Overview",
    "section": "",
    "text": "tfdatasets to manage input data.\nA custom model.\ntfautograph for building a custom training loop.\n\nBefore running the quickstart you need to have Keras installed. Please refer to the installation for installation instructions.\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\nLet’s start by loading and preparing the MNIST dataset. The values of the pixels are integers between 0 and 255, and we will convert them to floats between 0 and 1.\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\nNow let’s use tfdatasets to batch and shuffle the dataset.\n\ntrain_ds <- mnist$train %>% \n  tensor_slices_dataset() %>%\n  dataset_take(20000) %>% \n  dataset_map(~modify_at(.x, \"x\", tf$cast, dtype = tf$float32)) %>% \n  dataset_map(~modify_at(.x, \"y\", tf$cast, dtype = tf$int64)) %>% \n  dataset_shuffle(10000) %>% \n  dataset_batch(32)\n\ntest_ds <- mnist$test %>% \n  tensor_slices_dataset() %>% \n  dataset_take(2000) %>% \n  dataset_map(~modify_at(.x, \"x\", tf$cast, dtype = tf$float32)) %>%\n  dataset_map(~modify_at(.x, \"y\", tf$cast, dtype = tf$int64)) %>% \n  dataset_batch(32)\n\nWe will now define a Keras custom model.\n\nsimple_conv_nn <- function(filters, kernel_size) {\n  keras_model_custom(name = \"MyModel\", function(self) {\n    \n    self$conv1 <- layer_conv_2d(\n      filters = filters, \n      kernel_size = rep(kernel_size, 2),\n      activation = \"relu\"\n    )\n    \n    self$flatten <- layer_flatten()\n    \n    self$d1 <- layer_dense(units = 128, activation = \"relu\")\n    self$d2 <- layer_dense(units = 10, activation = \"softmax\")\n    \n    \n    function(inputs, mask = NULL) {\n      inputs %>% \n        self$conv1() %>% \n        self$flatten() %>% \n        self$d1() %>% \n        self$d2()\n    }\n  })\n}\n\nmodel <- simple_conv_nn(filters = 32, kernel_size = 3)\n\nWe can then choose an optimizer and loss function for training:\n\nloss <- loss_sparse_categorical_crossentropy\noptimizer <- optimizer_adam()\n\nSelect metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\ntrain_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss <- tf$keras$metrics$Mean(name='test_loss')\ntest_accuracy <- tf$keras$metrics$SparseCategoricalAccuracy(name='test_accuracy')\n\nWe then define a function that is able to make one training step:\n\ntrain_step <- function(images, labels) {\n  \n  with (tf$GradientTape() %as% tape, {\n    predictions <- model(images)\n    l <- loss(labels, predictions)\n  })\n  \n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n  \n  train_loss(l)\n  train_accuracy(labels, predictions)\n  \n}\n\nWe then provide a function that is able to test the model:\n\ntest_step <- function(images, labels) {\n  predictions <- model(images)\n  l <- loss(labels, predictions)\n  \n  test_loss(l)\n  test_accuracy(labels, predictions)\n}\n\nWe can then write our training loop function:\n\ntraining_loop <- tf_function(autograph(function(train_ds, test_ds) {\n  \n  for (b1 in train_ds) {\n    train_step(b1$x, b1$y)\n  }\n  \n  for (b2 in test_ds) {\n    test_step(b2$x, b2$y)\n  }\n  \n  tf$print(\"Acc\", train_accuracy$result(), \"Test Acc\", test_accuracy$result())\n  \n  train_loss$reset_states()\n  train_accuracy$reset_states()\n  test_loss$reset_states()\n  test_accuracy$reset_states()\n  \n}))\n\nFinally let’s run our training loop for 5 epochs:\n\nfor (epoch in 1:5) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop(train_ds, test_ds)  \n}\n\n## Epoch:  1  -----------\nAcc 0.93095 Test Acc 0.954\n## Epoch:  2  -----------\nAcc 0.956525 Test Acc 0.95825\n## Epoch:  3  -----------\nAcc 0.968066692 Test Acc 0.9575\n## Epoch:  4  -----------\nAcc 0.9752 Test Acc 0.960125\n## Epoch:  5  -----------\nAcc 0.9796 Test Acc 0.9617"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html",
    "href": "v1/tutorials/advanced/structured/classify.html",
    "title": "Classify structured data with feature columns",
    "section": "",
    "text": "This tutorial demonstrates how to classify structured data (e.g. tabular data in a CSV). We will use Keras to define the model, and feature columns as a bridge to map from columns in a CSV to features used to train the model. This tutorial contains complete code to:"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#the-dataset",
    "href": "v1/tutorials/advanced/structured/classify.html#the-dataset",
    "title": "Classify structured data with feature columns",
    "section": "The Dataset",
    "text": "The Dataset\nWe will use a small dataset provided by the Cleveland Clinic Foundation for Heart Disease. There are several hundred rows in the CSV. Each row describes a patient, and each column describes an attribute. We will use this information to predict whether a patient has heart disease, which in this dataset is a binary classification task.\nFollowing is a description of this dataset. Notice there are both numeric and categorical columns.\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nFeature Type\nData Type\n\n\n\n\nAge\nAge in years\nNumerical\ninteger\n\n\nSex\n(1 = male; 0 = female)\nCategorical\ninteger\n\n\nCP\nChest pain type (0, 1, 2, 3, 4)\nCategorical\ninteger\n\n\nTrestbpd\nResting blood pressure (in mm Hg on admission to the hospital)\nNumerical\ninteger\n\n\nChol\nSerum cholestoral in mg/dl\nNumerical\ninteger\n\n\nFBS\n(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\nCategorical\ninteger\n\n\nRestECG\nResting electrocardiographic results (0, 1, 2)\nCategorical\ninteger\n\n\nThalach\nMaximum heart rate achieved\nNumerical\ninteger\n\n\nExang\nExercise induced angina (1 = yes; 0 = no)\nCategorical\ninteger\n\n\nOldpeak\nST depression induced by exercise relative to rest\nNumerical\ninteger\n\n\nSlope\nThe slope of the peak exercise ST segment\nNumerical\nfloat\n\n\nCA\nNumber of major vessels (0-3) colored by flourosopy\nNumerical\ninteger\n\n\nThal\n3 = normal; 6 = fixed defect; 7 = reversable defect\nCategorical\nstring\n\n\nTarget\nDiagnosis of heart disease (1 = true; 0 = false)\nClassification\ninteger"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#setup",
    "href": "v1/tutorials/advanced/structured/classify.html#setup",
    "title": "Classify structured data with feature columns",
    "section": "Setup",
    "text": "Setup\nWe will use Keras and TensorFlow datasets.\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tidyverse)\nlibrary(rsample)"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#read-the-data",
    "href": "v1/tutorials/advanced/structured/classify.html#read-the-data",
    "title": "Classify structured data with feature columns",
    "section": "Read the data",
    "text": "Read the data\nWe will use read_csv in order to read the csv file to R.\n\nheart <- pins::pin(\"https://storage.googleapis.com/applied-dl/heart.csv\", \"heart\")\ndf <- read_csv(heart)\nglimpse(df)"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#split-the-dataframe-into-train-validation-and-test",
    "href": "v1/tutorials/advanced/structured/classify.html#split-the-dataframe-into-train-validation-and-test",
    "title": "Classify structured data with feature columns",
    "section": "Split the dataframe into train, validation, and test",
    "text": "Split the dataframe into train, validation, and test\nWe are going to use the rsample package to split the data into train, validation and test sets.\n\n# first we split between training and testing sets\nsplit <- initial_split(df, prop = 4/5)\ntrain <- training(split)\ntest <- testing(split)\n\n# the we split the training set into validation and training\nsplit <- initial_split(train, prop = 4/5)\ntrain <- training(split)\nval <- testing(split)\n\n\nnrow(train)\nnrow(val)\nnrow(test)"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#create-an-input-pipeline-using-tfdatasets",
    "href": "v1/tutorials/advanced/structured/classify.html#create-an-input-pipeline-using-tfdatasets",
    "title": "Classify structured data with feature columns",
    "section": "Create an input pipeline using tfdatasets",
    "text": "Create an input pipeline using tfdatasets\nNext, we will wrap the dataframes with tfdatasets. This will enable us to use feature columns as a bridge to map from the columns in the dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tfdatasets to read it from disk directly. That is not covered in this tutorial.\n\ndf_to_dataset <- function(df, shuffle = TRUE, batch_size = 32) {\n  ds <- df %>% \n    tensor_slices_dataset()\n  \n  if (shuffle)\n    ds <- ds %>% dataset_shuffle(buffer_size = nrow(df))\n  \n  ds %>% \n    dataset_batch(batch_size = batch_size)\n}\n\n\nbatch_size <- 5\ntrain_ds <- df_to_dataset(train, batch_size = batch_size)\nval_ds <- df_to_dataset(val, shuffle = FALSE, batch_size = batch_size)\ntest_ds <- df_to_dataset(test, shuffle = FALSE, batch_size = batch_size)"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#understand-the-input-pipeline",
    "href": "v1/tutorials/advanced/structured/classify.html#understand-the-input-pipeline",
    "title": "Classify structured data with feature columns",
    "section": "Understand the input pipeline",
    "text": "Understand the input pipeline\nNow that we have created the input pipeline, let’s call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n\ntrain_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n\nWe can see that the dataset returns a list of column names (from the dataframe) that map to column values from rows in the dataframe."
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#create-the-feature-spec",
    "href": "v1/tutorials/advanced/structured/classify.html#create-the-feature-spec",
    "title": "Classify structured data with feature columns",
    "section": "Create the feature spec",
    "text": "Create the feature spec\nWe want to train a model to predict the target variable using Keras but, before that we need to prepare the data. We need to transform the categorical variables into some form of dense variable, we usually want to normalize all numeric columns too.\nThe feature spec interface works with data.frames or TensorFlow datasets objects.\nLet’s start creating our feature specification:\n\nspec <- feature_spec(train_ds, target ~ .)\n\nThe first thing we need to do after creating the feature_spec is decide on the variables’ types.\nWe can do this by adding steps to the spec object.\n\nspec <- spec %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal)\n\nThe following steps can be used to define the variable type:\n\nstep_numeric_column to define numeric variables\nstep_categorical_with_vocabulary_list for categorical variables with a fixed vocabulary\nstep_categorical_column_with_hash_bucket for categorical variables using the hash trick\nstep_categorical_column_with_identity to store categorical variables as integers\nstep_categorical_column_with_vocabulary_file when you have the possible vocabulary in a file\n\nWhen using step_categorical_column_with_vocabulary_list you can also provide a vocabulary argument with the fixed vocabulary. The recipe will find all the unique values in the dataset and use it as the vocabulary.\nYou can also specify a normalizer_fn to the step_numeric_column. In this case the variable will be transformed by the feature column. Note that the transformation will occur in the TensorFlow Graph, so it must use only TensorFlow ops. Like in the example we offer pre-made normalizers - and they will compute the normalizing function during the recipe preparation.\nYou can also use selectors like:\n\nstarts_with(), ends_with(), matches() etc. (from tidyselect)\nall_numeric() to select all numeric variables\nall_nominal() to select all strings\nhas_type(\"float32\") to select based on TensorFlow variable type.\n\nNow we can print the recipe:\n\nspec\n\nAfter specifying the types of the columns you can add transformation steps. For example you may want to bucketize a numeric column:\n\nspec <- spec %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))\n\nYou can also specify the kind of numeric representation that you want to use for your categorical variables.\n\nspec <- spec %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2)\n\nAnother common transformation is to add interactions between variables using crossed columns.\n\nspec <- spec %>% \n  step_crossed_column(thal_and_age = c(thal, bucketized_age), hash_bucket_size = 1000) %>% \n  step_indicator_column(thal_and_age)\n\nNote that the crossed_column is a categorical column, so we need to also specify what kind of numeric tranformation we want to use. Also note that we can name the transformed variables - each step uses a default naming for columns, eg. bucketized_age is the default name when you use step_bucketized_column with column called age.\nWith the above code we have created our recipe. Note we can also define the recipe by chaining a sequence of methods:\n\nspec <- feature_spec(train_ds, target ~ .) %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2) %>% \n  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %>%\n  step_indicator_column(crossed_thal_bucketized_age)\n\nAfter defining the recipe we need to fit it. It’s when fitting that we compute the vocabulary list for categorical variables or find the mean and standard deviation for the normalizing functions. Fitting involves evaluating the full dataset, so if you have provided the vocabulary list and your columns are already normalized you can skip the fitting step (TODO).\nIn our case, we will fit the feature spec, since we didn’t specify the vocabulary list for the categorical variables.\n\nspec_prep <- fit(spec)\n\nAfter preparing we can see the list of dense features that were defined:\n\nstr(spec_prep$dense_features())"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#build-the-model",
    "href": "v1/tutorials/advanced/structured/classify.html#build-the-model",
    "title": "Classify structured data with feature columns",
    "section": "Build the model",
    "text": "Build the model\nNow we are ready to define our model in Keras. We will use a specialized layer_dense_features that knows what to do with the feature columns specification.\nWe also use a new layer_input_from_dataset that is useful to create a Keras input object copying the structure from a data.frame or TensorFlow dataset.\n\nmodel <- keras_model_sequential() %>% \n  layer_dense_features(dense_features(spec_prep)) %>% \n  layer_dense(units = 32, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy, \n  optimizer = \"adam\", \n  metrics = \"binary_accuracy\"\n)"
  },
  {
    "objectID": "v1/tutorials/advanced/structured/classify.html#train-the-model",
    "href": "v1/tutorials/advanced/structured/classify.html#train-the-model",
    "title": "Classify structured data with feature columns",
    "section": "Train the model",
    "text": "Train the model\nWe can finally train the model on the dataset:\n\nhistory <- model %>% \n  fit(\n    dataset_use_spec(train_ds, spec = spec_prep),\n    epochs = 15, \n    validation_data = dataset_use_spec(val_ds, spec_prep),\n    verbose = 2\n  )\nplot(history)\n\nFinally we can make predictions in the test set and calculate performance metrics like the AUC of the ROC curve:\n\npred <- predict(model, test)\nMetrics::auc(test$target, pred)"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/index.html",
    "href": "v1/tutorials/beginners/basic-ml/index.html",
    "title": "Overview",
    "section": "",
    "text": "Image Classification: image classification using the Fashing MNIST dataset.\nRegression: regression using the Boston Housing dataset.\nText Classification: text classification using the IMDB dataset.\nOverfitting and Underfitting: learn about these inportant concepts in ML.\nSave and Restore: learn how to save and restore TensorFlow models."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html",
    "title": "Basic Image Classification",
    "section": "",
    "text": "In this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts. It’s fine if you don’t understand all the details, this is a fast-paced overview of a complete Keras program with the details explained as we go."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#import-the-fashion-mnist-dataset",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#import-the-fashion-mnist-dataset",
    "title": "Basic Image Classification",
    "section": "Import the Fashion MNIST dataset",
    "text": "Import the Fashion MNIST dataset\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n\nFigure 1. Fashion-MNIST samples (by Zalando, MIT License).\n\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we’ll use here.\nThis guide uses Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashion_mnist <- dataset_fashion_mnist()\n\nc(train_images, train_labels) %<-% fashion_mnist$train\nc(test_images, test_labels) %<-% fashion_mnist$test\n\nAt this point we have four arrays: The train_images and train_labels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the test_images, and test_labels arrays.\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n\n\nDigit\nClass\n\n\n\n\n0\nT-shirt/top\n\n\n1\nTrouser\n\n\n2\nPullover\n\n\n3\nDress\n\n\n4\nCoat\n\n\n5\nSandal\n\n\n6\nShirt\n\n\n7\nSneaker\n\n\n8\nBag\n\n\n9\nAnkle boot\n\n\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we’ll store them in a vector to use later when plotting the images.\n\nclass_names = c('T-shirt/top',\n                'Trouser',\n                'Pullover',\n                'Dress',\n                'Coat', \n                'Sandal',\n                'Shirt',\n                'Sneaker',\n                'Bag',\n                'Ankle boot')"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#explore-the-data",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#explore-the-data",
    "title": "Basic Image Classification",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n[1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#preprocess-the-data",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#preprocess-the-data",
    "title": "Basic Image Classification",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nimage_1 <- as.data.frame(train_images[1, , ])\ncolnames(image_1) <- seq_len(ncol(image_1))\nimage_1$y <- seq_len(nrow(image_1))\nimage_1 <- gather(image_1, \"x\", \"value\", -y)\nimage_1$x <- as.integer(image_1$x)\n\nggplot(image_1, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\n  scale_y_reverse() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())   +\n  theme(aspect.ratio = 1) +\n  xlab(\"\") +\n  ylab(\"\")\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255.\nIt’s important that the training set and the testing set are preprocessed in the same way:\n\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n\nDisplay the first 25 images from the training set and display the class name below each image. Verify that the data is in the correct format and we’re ready to build and train the network.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- train_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste(class_names[train_labels[i] + 1]))\n}"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#build-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_classification.html#build-the-model",
    "title": "Basic Image Classification",
    "section": "Build the model",
    "text": "Build the model\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense, have parameters that are learned during training.\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_flatten(input_shape = c(28, 28)) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe first layer in this network, layer_flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two dense layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n\nCompile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function — This measures how accurate the model is during training. We want to minimize this function to “steer” the model in the right direction.\nOptimizer — This is how the model is updated based on the data it sees and its loss function.\nMetrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n\nmodel %>% compile(\n  optimizer = 'adam', \n  loss = 'sparse_categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\n\n\nTrain the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model — in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set — in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n\nTo start training, call the fit method — the model is “fit” to the training data:\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n\n\nEvaluate accuracy\nNext, compare how the model performs on the test dataset:\n\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\n\ncat('Test loss:', score$loss, \"\\n\")\ncat('Test accuracy:', score$acc, \"\\n\")\n\n10000/10000 [==============================] - 0s 19us/step\nTest loss: 0.3755946 \nTest accuracy: 0.8644 \nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n\n\nMake predictions\nWith the model trained, we can use it to make predictions about some images.\n\npredictions <- model %>% predict(test_images)\n\nHere, the model has predicted the label for each image in the testing set. Let’s take a look at the first prediction:\n\npredictions[1, ]\n\nA prediction is an array of 10 numbers. These describe the “confidence” of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\nAlternatively, we can also directly get the class prediction:\n\nclass_pred <- model %>% predict_classes(test_images)\nclass_pred[1:20]\n\nAs the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot. And we can check the test label to see this is correct:\n\ntest_labels[1]\n\nLet’s plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- test_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  # subtract 1 as labels go from 0 to 9\n  predicted_label <- which.max(predictions[i, ]) - 1\n  true_label <- test_labels[i]\n  if (predicted_label == true_label) {\n    color <- '#008800' \n  } else {\n    color <- '#bb0000'\n  }\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste0(class_names[predicted_label + 1], \" (\",\n                      class_names[true_label + 1], \")\"),\n        col.main = color)\n}\n\nFinally, use the trained model to make a prediction about a single image.\n\n# Grab an image from the test dataset\n# take care to keep the batch dimension, as this is expected by the model\nimg <- test_images[1, , , drop = FALSE]\ndim(img)\n\nNow predict the image:\n\npredictions <- model %>% predict(img)\npredictions\n\npredict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\n# subtract 1 as labels are 0-based\nprediction <- predictions[1, ] - 1\nwhich.max(prediction)\n\nOr, directly getting the class prediction again:\n\nclass_pred <- model %>% predict_classes(img)\nclass_pred\n\nAnd, as before, the model predicts a label of 9."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html",
    "title": "Basic Regression",
    "section": "",
    "text": "In a regression problem, we aim to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where we aim to predict a discrete label (for example, where a picture contains an apple or an orange).\nThis notebook builds a model to predict the median price of homes in a Boston suburb during the mid-1970s. To do this, we’ll provide the model with some data points about the suburb, such as the crime rate and the local property tax rate."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#the-boston-housing-prices-dataset",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#the-boston-housing-prices-dataset",
    "title": "Basic Regression",
    "section": "The Boston Housing Prices dataset",
    "text": "The Boston Housing Prices dataset\nThe Boston Housing Prices dataset is accessible directly from keras.\n\nboston_housing <- dataset_boston_housing()\n\nc(train_data, train_labels) %<-% boston_housing$train\nc(test_data, test_labels) %<-% boston_housing$test\n\n\nExamples and features\nThis dataset is much smaller than the others we’ve worked with so far: it has 506 total examples that are split between 404 training examples and 102 test examples:\n\npaste0(\"Training entries: \", length(train_data), \", labels: \", length(train_labels))\n\nThe dataset contains 13 different features:\n\nPer capita crime rate.\nThe proportion of residential land zoned for lots over 25,000 square feet.\nThe proportion of non-retail business acres per town.\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nNitric oxides concentration (parts per 10 million).\nThe average number of rooms per dwelling.\nThe proportion of owner-occupied units built before 1940.\nWeighted distances to five Boston employment centers.\nIndex of accessibility to radial highways.\nFull-value property-tax rate per $10,000.\nPupil-teacher ratio by town.\n1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\nPercentage lower status of the population.\n\nEach one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on.\n\ntrain_data[1, ] # Display sample features, notice the different scales\n\nLet’s add column names for better data inspection.\n\nlibrary(dplyr)\n\ncolumn_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')\n\ntrain_df <- train_data %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = train_labels)\n\ntest_df <- test_data %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = test_labels)\n\n\n\nLabels\nThe labels are the house prices in thousands of dollars. (You may notice the mid-1970s prices.)\n\ntrain_labels[1:10] # Display first 10 entries"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#normalize-features",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#normalize-features",
    "title": "Basic Regression",
    "section": "Normalize features",
    "text": "Normalize features\nIt’s recommended to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model more dependent on the choice of units used in the input.\nWe are going to use the feature_spec interface implemented in the tfdatasets package for normalization. The feature_columns interface allows for other common pre-processing operations on tabular data.\n\nspec <- feature_spec(train_df, label ~ . ) %>% \n  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% \n  fit()\n\nspec\n\nThe spec created with tfdatasets can be used together with layer_dense_features to perform pre-processing directly in the TensorFlow graph.\nWe can take a look at the output of a dense-features layer created by this spec:\n\nlayer <- layer_dense_features(\n  feature_columns = dense_features(spec), \n  dtype = tf$float32\n)\nlayer(train_df)\n\nNote that this returns a matrix (in the sense that it’s a 2-dimensional Tensor) with scaled values."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#create-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#create-the-model",
    "title": "Basic Regression",
    "section": "Create the model",
    "text": "Create the model\nLet’s build our model. Here we will use the Keras functional API - which is the recommended way when using the feature_spec API. Note that we only need to pass the dense_features from the spec we just created.\n\ninput <- layer_input_from_dataset(train_df %>% select(-label))\n\noutput <- input %>% \n  layer_dense_features(dense_features(spec)) %>% \n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 1) \n\nmodel <- keras_model(input, output)\n\nsummary(model)\n\nWe then compile the model with:\n\nmodel %>% \n  compile(\n    loss = \"mse\",\n    optimizer = optimizer_rmsprop(),\n    metrics = list(\"mean_absolute_error\")\n  )\n\nWe will wrap the model building code into a function in order to be able to reuse it for different experiments. Remember that Keras fit modifies the model in-place.\n\nbuild_model <- function() {\n  input <- layer_input_from_dataset(train_df %>% select(-label))\n  \n  output <- input %>% \n    layer_dense_features(dense_features(spec)) %>% \n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dense(units = 1) \n  \n  model <- keras_model(input, output)\n  \n  model %>% \n    compile(\n      loss = \"mse\",\n      optimizer = optimizer_rmsprop(),\n      metrics = list(\"mean_absolute_error\")\n    )\n  \n  model\n}"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#train-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#train-the-model",
    "title": "Basic Regression",
    "section": "Train the model",
    "text": "Train the model\nThe model is trained for 500 epochs, recording training and validation accuracy in a keras_training_history object. We also show how to use a custom callback, replacing the default training output by a single dot per epoch.\n\n# Display training progress by printing a single dot for each completed epoch.\nprint_dot_callback <- callback_lambda(\n  on_epoch_end = function(epoch, logs) {\n    if (epoch %% 80 == 0) cat(\"\\n\")\n    cat(\".\")\n  }\n)    \n\nmodel <- build_model()\n\nhistory <- model %>% fit(\n  x = train_df %>% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(print_dot_callback)\n)\n\nNow, we visualize the model’s training progress using the metrics stored in the history variable. We want to use this data to determine how long to train before the model stops making progress.\n\nlibrary(ggplot2)\nplot(history)\n\nThis graph shows little improvement in the model after about 200 epochs. Let’s update the fit method to automatically stop training when the validation score doesn’t improve. We’ll use a callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, it automatically stops the training.\n\n# The patience parameter is the amount of epochs to check for improvement.\nearly_stop <- callback_early_stopping(monitor = \"val_loss\", patience = 20)\n\nmodel <- build_model()\n\nhistory <- model %>% fit(\n  x = train_df %>% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(early_stop)\n)\n\nplot(history)\n\nThe graph shows the average error is about $2,500 dollars. Is this good? Well, $2,500 is not an insignificant amount when some of the labels are only $15,000.\nLet’s see how did the model performs on the test set:\n\nc(loss, mae) %<-% (model %>% evaluate(test_df %>% select(-label), test_df$label, verbose = 0))\n\npaste0(\"Mean absolute error on test set: $\", sprintf(\"%.2f\", mae * 1000))"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#predict",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#predict",
    "title": "Basic Regression",
    "section": "Predict",
    "text": "Predict\nFinally, predict some housing prices using data in the testing set:\n\ntest_predictions <- model %>% predict(test_df %>% select(-label))\ntest_predictions[ , 1]"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#conclusion",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_regression.html#conclusion",
    "title": "Basic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook introduced a few techniques to handle a regression problem.\n\nMean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems).\nSimilarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\nWhen input data features have values with different ranges, each feature should be scaled independently.\nIf there is not much training data, prefer a small network with few hidden layers to avoid overfitting.\nEarly stopping is a useful technique to prevent overfitting."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html",
    "title": "Text Classification",
    "section": "",
    "text": "Note: This tutorial requires TensorFlow version >= 2.1\nThis tutorial classifies movie reviews as positive or negative using the text of the review. This is an example of binary — or two-class — classification, an important and widely applicable kind of machine learning problem.\nWe’ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\nLet’s start and load Keras, as well as a few other required libraries."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#download-the-movie-reviews-dataset",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#download-the-movie-reviews-dataset",
    "title": "Text Classification",
    "section": "Download the Movie Reviews dataset",
    "text": "Download the Movie Reviews dataset\nWe will use the Movie Reviews dataset created by Bo Pang and Lillian Lee. This dataset is redistributed with NLTK with permission from the authors.\nThe dataset can be found here and can be downloaded from the Kaggle UI or using the pins package.\nIf you are going to use pins follow this tutorial to register the Kaggle board. Then you can run:\n\npaths <- pins::pin_get(\"nltkdata/movie-review\", \"kaggle\")\n# we only need the movie_review.csv file\npath <- paths[1]\n\nNow let’s read it to R using the read_csv funcntion from the readr package.\n\ndf <- readr::read_csv(path)\nhead(df)"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#explore-the-data",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#explore-the-data",
    "title": "Text Classification",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s take a moment to understand the format of the data. The dataset has 60k rows, each one representing a movie review. The text column has the actual review and the tag represents shows us the classified sentiment for the review.\n\ndf %>% count(tag)\n\nAround half of the reviews are negative and the other half are positive. Here is an example of a review:\n\ndf$text[1]\n\nLet’s also split our dataset into training and testing:\n\ntraining_id <- sample.int(nrow(df), size = nrow(df)*0.8)\ntraining <- df[training_id,]\ntesting <- df[-training_id,]\n\nIt’s also useful to find out what is the distribution of the number of words in each review.\n\ndf$text %>% \n  strsplit(\" \") %>% \n  sapply(length) %>% \n  summary()"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#prepare-the-data",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#prepare-the-data",
    "title": "Text Classification",
    "section": "Prepare the data",
    "text": "Prepare the data\nThe reviews — the text — must be converted to tensors before fed into the neural network. First, we create a dictionary and represent each of the 10,000 most common words by an integer. In this case, every review will be represented by a sequence of integers.\nThen we can represent reviews in a couple of ways:\n\nOne-hot-encode the arrays to convert them into vectors of 0s and 1s. For example, the sequence [3, 5] would become a 10,000-dimensional vector that is all zeros except for indices 3 and 5, which are ones. Then, make this the first layer in our network — a dense layer — that can handle floating point vector data. This approach is memory intensive, though, requiring a num_words * num_reviews size matrix.\nAlternatively, we can pad the arrays so they all have the same length, then create an integer tensor of shape num_examples * max_length. We can use an embedding layer capable of handling this shape as the first layer in our network.\n\nIn this tutorial, we will use the second approach. Now, let’s define our Text Vectorization layer, it will be responsible to take the string input and convert it to a Tensor.\n\nnum_words <- 10000\nmax_length <- 50\ntext_vectorization <- layer_text_vectorization(\n  max_tokens = num_words, \n  output_sequence_length = max_length, \n)\n\nNow, we need to adapt the Text Vectorization layer. It’s when we call adapt that the layer will learn about unique words in our dataset and assign an integer value for each one.\n\ntext_vectorization %>% \n  adapt(df$text)\n\nWe can now see the vocabulary is in our text vectorization layer.\n\n# TODO see https://github.com/tensorflow/tensorflow/pull/34529\nget_vocabulary(text_vectorization)\n\nYou can see how the text vectorization layer transforms it’s inputs:\n\ntext_vectorization(matrix(df$text[1], ncol = 1))"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#build-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#build-the-model",
    "title": "Text Classification",
    "section": "Build the model",
    "text": "Build the model\nThe neural network is created by stacking layers — this requires two main architectural decisions:\n\nHow many layers to use in the model?\nHow many hidden units to use for each layer?\n\nIn this example, the input data consists of an array of word-indices. The labels to predict are either 0 or 1. Let’s build a model for this problem:\n\ninput <- layer_input(shape = c(1), dtype = \"string\")\n\noutput <- input %>% \n  text_vectorization() %>% \n  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dropout(0.5) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\nThe layers are stacked sequentially to build the classifier:\n\nThe first layer is an embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\nNext, a global_average_pooling_1d layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\nThis fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\nThe last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n\n\nHidden units\nThe above model has two intermediate or “hidden” layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\nIf a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns — patterns that improve performance on training data but not on the test data. This is called overfitting, and we’ll explore it later.\n\n\nLoss function and optimizer\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we’ll use the binary_crossentropy loss function.\nThis isn’t the only choice for a loss function, you could, for instance, choose mean_squared_error. But, generally, binary_crossentropy is better for dealing with probabilities — it measures the “distance” between probability distributions, or in our case, between the ground-truth distribution and the predictions.\nLater, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'binary_crossentropy',\n  metrics = list('accuracy')\n)"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#train-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#train-the-model",
    "title": "Text Classification",
    "section": "Train the model",
    "text": "Train the model\nTrain the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the x_train and y_train tensors. While training, monitor the model’s loss and accuracy on the 10,000 samples from the validation set:\n\nhistory <- model %>% fit(\n  training$text,\n  as.numeric(training$tag == \"pos\"),\n  epochs = 10,\n  batch_size = 512,\n  validation_split = 0.2,\n  verbose=2\n)"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#evaluate-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#evaluate-the-model",
    "title": "Text Classification",
    "section": "Evaluate the model",
    "text": "Evaluate the model\nAnd let’s see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nresults <- model %>% evaluate(testing$text, as.numeric(testing$tag == \"pos\"), verbose = 0)\nresults\n\nThis fairly naive approach achieves an accuracy of about 68%. With more advanced approaches, the model should get closer to 85%."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#create-a-graph-of-accuracy-and-loss-over-time",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification.html#create-a-graph-of-accuracy-and-loss-over-time",
    "title": "Text Classification",
    "section": "Create a graph of accuracy and loss over time",
    "text": "Create a graph of accuracy and loss over time\nfit returns a keras_training_history object whose metrics slot contains loss and metrics values recorded during training. You can conveniently plot the loss and metrics curves like so:\n\nplot(history)\n\nThe evolution of loss and metrics can also be seen during training in the RStudio Viewer pane.\nNotice the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected when using gradient descent optimization — it should minimize the desired quantity on every iteration."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html",
    "title": "Transfer learning with tfhub",
    "section": "",
    "text": "This tutorial classifies movie reviews as positive or negative using the text of the review. This is an example of binary — or two-class — classification, an important and widely applicable kind of machine learning problem.\nWe’ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\nWe will use Keras to build and train the model and tfhub for Transfer Learning. We will also use tfds to load the IMDB dataset.\nLet’s start and load the required libraries."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#download-the-imdb-dataset",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#download-the-imdb-dataset",
    "title": "Transfer learning with tfhub",
    "section": "Download the IMDB dataset",
    "text": "Download the IMDB dataset\nThe IMDB dataset is available on imdb reviews or on tfds. The one that comes packaged with Keras is already pre-processed so it’s not useful for this tutorial.\nThe following code downloads the IMDB dataset to your machine:\n\nimdb <- tfds_load(\n  \"imdb_reviews:1.0.0\", \n  split = list(\"train[:60%]\", \"train[-40%:]\", \"test\"), \n  as_supervised = TRUE\n)\nsummary(imdb)\n\ntfds_load returns a TensorFlow Dataset, an abstraction that represents a sequence of elements, in which each element consists of one or more components.\nTo access individual elements of a dataset you can use:\n\nfirst <- imdb[[1]] %>% \n  dataset_batch(1) %>% # Used to get only the first example\n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nstr(first)\n\nWe will see next that Keras knows how to extract elements from TensorFlow Datasets automatically, making it a much more memory efficient alternative than loading the entire dataset into RAM before passing it to Keras."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#build-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#build-the-model",
    "title": "Transfer learning with tfhub",
    "section": "Build the model",
    "text": "Build the model\nThe neural network is created by stacking layers — this requires three main architectural decisions:\n\nHow to represent the text?\nHow many layers to use in the model?\nHow many hidden units to use for each layer?\n\nIn this example, the input data consists of sentences. The labels to predict are either 0 or 1.\nOne way to represent the text is to convert sentences into embedding vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages: * we don’t have to worry about text preprocessing, * we can benefit from transfer learning, * the embedding has a fixed size, so it’s simpler to process.\nFor this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\nThere are three other pre-trained models to test for the sake of this tutorial:\n\ngoogle/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if the vocabulary of the task and the vocabulary of the model don’t fully overlap.\ngoogle/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\ngoogle/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n\nLet’s first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension).\n\nembedding_layer <- layer_hub(handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\nembedding_layer(first[[1]])\n\nLet’s now build the full model:\n\nmodel <- keras_model_sequential() %>% \n  layer_hub(\n    handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\",\n    input_shape = list(),\n    dtype = tf$string,\n    trainable = TRUE\n  ) %>% \n  layer_dense(units = 16, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsummary(model)\n\nThe layers are stacked sequentially to build the classifier:\n\nThe first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using (google/tf2-preview/gnews-swivel-20dim/1) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension).\nThis fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\nThe last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n\nLet’s compile the model."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#loss-function-and-optimizer",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#loss-function-and-optimizer",
    "title": "Transfer learning with tfhub",
    "section": "Loss function and optimizer",
    "text": "Loss function and optimizer\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we’ll use the binary_crossentropy loss function.\nThis isn’t the only choice for a loss function, you could, for instance, choose mean_squared_error. But, generally, binary_crossentropy is better for dealing with probabilities — it measures the “distance” between probability distributions, or in our case, between the ground-truth distribution and the predictions.\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %>% \n  compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = \"accuracy\"\n  )"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#train-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#train-the-model",
    "title": "Transfer learning with tfhub",
    "section": "Train the model",
    "text": "Train the model\nTrain the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the dataset. While training, monitor the model’s loss and accuracy on the 10,000 samples from the validation set:\n\nmodel %>% \n  fit(\n    imdb[[1]] %>% dataset_shuffle(10000) %>% dataset_batch(512),\n    epochs = 20,\n    validation_data = imdb[[2]] %>% dataset_batch(512),\n    verbose = 2\n  )"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#evaluate-the-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub.html#evaluate-the-model",
    "title": "Transfer learning with tfhub",
    "section": "Evaluate the model",
    "text": "Evaluate the model\nAnd let’s see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nmodel %>% \n  evaluate(imdb[[3]] %>% dataset_batch(512), verbose = 0)\n\nThis fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html",
    "title": "Tutorial: Overfitting and Underfitting",
    "section": "",
    "text": "In two of the previous tutorails — classifying movie reviews, and predicting housing prices — we saw that the accuracy of our model on the validation data would peak after training for a number of epochs, and would then start decreasing.\nIn other words, our model would overfit to the training data. Learning how to deal with overfitting is important. Although it’s often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to testing data (or data they haven’t seen before).\nThe opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\nIf you train for too long though, the model will start to overfit and learn patterns from the training data that don’t generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we’ll explore below is a useful skill.\nTo prevent overfitting, the best solution is to use more training data. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\nIn this tutorial, we’ll explore two common regularization techniques — weight regularization and dropout — and use them to improve our IMDB movie review classification results."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#download-the-imdb-dataset",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#download-the-imdb-dataset",
    "title": "Tutorial: Overfitting and Underfitting",
    "section": "Download the IMDB dataset",
    "text": "Download the IMDB dataset\n\nnum_words <- 1000\nimdb <- dataset_imdb(num_words = num_words)\n\nc(train_data, train_labels) %<-% imdb$train\nc(test_data, test_labels) %<-% imdb$test\n\nRather than using an embedding as in the previous notebook, here we will multi-hot encode the sentences. This model will quickly overfit to the training set. It will be used to demonstrate when overfitting occurs, and how to fight it.\nMulti-hot-encoding our lists means turning them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones.\n\nmulti_hot_sequences <- function(sequences, dimension) {\n  multi_hot <- matrix(0, nrow = length(sequences), ncol = dimension)\n  for (i in 1:length(sequences)) {\n    multi_hot[i, sequences[[i]]] <- 1\n  }\n  multi_hot\n}\n\ntrain_data <- multi_hot_sequences(train_data, num_words)\ntest_data <- multi_hot_sequences(test_data, num_words)\n\nLet’s look at one of the resulting multi-hot vectors. The word indices are sorted by frequency, so it is expected that there are more 1-values near index zero, as we can see in this plot:\n\nfirst_text <- data.frame(word = 1:num_words, value = train_data[1, ])\nggplot(first_text, aes(x = word, y = value)) +\n  geom_line() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#demonstrate-overfitting",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#demonstrate-overfitting",
    "title": "Tutorial: Overfitting and Underfitting",
    "section": "Demonstrate overfitting",
    "text": "Demonstrate overfitting\nThe simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model’s “capacity”. Intuitively, a model with more parameters will have more “memorization capacity” and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\nAlways keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\nOn the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between “too much capacity” and “not enough capacity”.\nUnfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or what the right size for each layer). You will have to experiment using a series of different architectures.\nTo find an appropriate model size, it’s best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss. Let’s try this on our movie review classification network.\nWe’ll create a simple model using only dense layers, then well a smaller version, and compare them.\n\nCreate a baseline model\n\nbaseline_model <- \n  keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\", input_shape = num_words) %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nbaseline_model %>% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(baseline_model)\n\n\nbaseline_history <- baseline_model %>% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(test_data, test_labels),\n  verbose = 2\n)\n\n\n\nCreate a smaller model\nLet’s create a model with less hidden units to compare against the baseline model that we just created:\n\nsmaller_model <- \n  keras_model_sequential() %>%\n  layer_dense(units = 4, activation = \"relu\", input_shape = num_words) %>%\n  layer_dense(units = 4, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsmaller_model %>% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(smaller_model)\n\nAnd train the model using the same data:\n\nsmaller_history <- smaller_model %>% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(test_data, test_labels),\n  verbose = 2\n)\n\n\n\nCreate a bigger model\nNext, let’s add to this benchmark a network that has much more capacity, far more than the problem would warrant:\n\nbigger_model <- \n  keras_model_sequential() %>%\n  layer_dense(units = 512, activation = \"relu\", input_shape = num_words) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nbigger_model %>% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(bigger_model)\n\nAnd, again, train the model using the same data:\n\nbigger_history <- bigger_model %>% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(test_data, test_labels),\n  verbose = 2\n)\n\n\n\nPlot the training and validation loss\nNow, let’s plot the loss curves for the 3 models. The smaller network begins overfitting a litle later than the baseline model and its performance degrades much more slowly once it starts overfitting. Notice that the larger network begins overfitting almost right away, after just one epoch, and overfits much more severely. The more capacity the network has, the quicker it will be able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).\n\ncompare_cx <- data.frame(\n  baseline_train = baseline_history$metrics$loss,\n  baseline_val = baseline_history$metrics$val_loss,\n  smaller_train = smaller_history$metrics$loss,\n  smaller_val = smaller_history$metrics$val_loss,\n  bigger_train = bigger_history$metrics$loss,\n  bigger_val = bigger_history$metrics$val_loss\n) %>%\n  rownames_to_column() %>%\n  mutate(rowname = as.integer(rowname)) %>%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#strategies",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_overfit_underfit.html#strategies",
    "title": "Tutorial: Overfitting and Underfitting",
    "section": "Strategies",
    "text": "Strategies\n\nAdd weight regularization\nYou may be familiar with Occam’s Razor principle: given two explanations for something, the explanation most likely to be correct is the “simplest” one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\nA “simple model” in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to only take on small values, which makes the distribution of weight values more “regular”. This is called “weight regularization”, and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n\nL1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the “L1 norm” of the weights).\nL2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the “L2 norm” of the weights). L2 regularization is also called weight decay in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n\nIn Keras, weight regularization is added by passing weight regularizer instances to layers. Let’s add L2 weight regularization to the baseline model now.\n\nl2_model <- \n  keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\", input_shape = num_words,\n              kernel_regularizer = regularizer_l2(l = 0.001)) %>%\n  layer_dense(units = 16, activation = \"relu\",\n              kernel_regularizer = regularizer_l2(l = 0.001)) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nl2_model %>% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nl2_history <- l2_model %>% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(test_data, test_labels),\n  verbose = 2\n)\n\nl2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time.\nHere’s the impact of our L2 regularization penalty:\n\ncompare_cx <- data.frame(\n  baseline_train = baseline_history$metrics$loss,\n  baseline_val = baseline_history$metrics$val_loss,\n  l2_train = l2_history$metrics$loss,\n  l2_val = l2_history$metrics$val_loss\n) %>%\n  rownames_to_column() %>%\n  mutate(rowname = as.integer(rowname)) %>%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")\n\nAs you can see, the L2 regularized model has become much more resistant to overfitting than the baseline model, even though both models have the same number of parameters.\n\n\nAdd dropout\nDropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly “dropping out” (i.e. set to zero) a number of output features of the layer during training. Let’s say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. The “dropout rate” is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer’s output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\nIn Keras you can introduce dropout in a network via layer_dropout, which gets applied to the output of the layer right before.\nLet’s add two dropout layers in our IMDB network to see how well they do at reducing overfitting:\n\ndropout_model <- \n  keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\", input_shape = num_words) %>%\n  layer_dropout(0.6) %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dropout(0.6) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\ndropout_model %>% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\ndropout_history <- dropout_model %>% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(test_data, test_labels),\n  verbose = 2\n)\n\nHow well did it work?\n\ncompare_cx <- data.frame(\n  baseline_train = baseline_history$metrics$loss,\n  baseline_val = baseline_history$metrics$val_loss,\n  dropout_train = dropout_history$metrics$loss,\n  dropout_val = dropout_history$metrics$val_loss\n) %>%\n  rownames_to_column() %>%\n  mutate(rowname = as.integer(rowname)) %>%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")\n\nAdding dropout is a clear improvement over the baseline model.\nTo recap: here the most common ways to prevent overfitting in neural networks:\n\nGet more training data.\nReduce the capacity of the network.\nAdd weight regularization.\nAdd dropout.\n\nAnd two important approaches not covered in this guide are data augmentation and batch normalization."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html",
    "title": "Tutorial: Save and Restore Models",
    "section": "",
    "text": "Model progress can be saved after as well as during training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\nSharing this data helps others understand how the model works and try it themselves with new data."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#options",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#options",
    "title": "Tutorial: Save and Restore Models",
    "section": "Options",
    "text": "Options\nThere are many different ways to save TensorFlow models—depending on the API you’re using. This guide uses Keras, a high-level API to build and train models in TensorFlow. For other approaches, see the TensorFlow Save and Restore guide or Saving in eager."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#setup",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#setup",
    "title": "Tutorial: Save and Restore Models",
    "section": "Setup",
    "text": "Setup\nWe’ll use the MNIST dataset to train our model to demonstrate saving weights. To speed up these demonstration runs, only use the first 1000 examples:\n\nlibrary(keras)\n\nmnist <- dataset_mnist()\n\nc(train_images, train_labels) %<-% mnist$train\nc(test_images, test_labels) %<-% mnist$test\n\ntrain_labels <- train_labels[1:1000]\ntest_labels <- test_labels[1:1000]\n\ntrain_images <- train_images[1:1000, , ] %>%\n  array_reshape(c(1000, 28 * 28))\ntrain_images <- train_images / 255\n\ntest_images <- test_images[1:1000, , ] %>%\n  array_reshape(c(1000, 28 * 28))\ntest_images <- test_images / 255"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#define-a-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#define-a-model",
    "title": "Tutorial: Save and Restore Models",
    "section": "Define a model",
    "text": "Define a model\nLet’s build a simple model we’ll use to demonstrate saving and loading weights.\n\n# Returns a short sequential model\ncreate_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(units = 512, activation = \"relu\", input_shape = 784) %>%\n    layer_dropout(0.2) %>%\n    layer_dense(units = 10, activation = \"softmax\")\n  model %>% compile(\n    optimizer = \"adam\",\n    loss = \"sparse_categorical_crossentropy\",\n    metrics = list(\"accuracy\")\n  )\n  model\n}\n\nmodel <- create_model()\nsummary(model)"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#save-the-entire-model",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#save-the-entire-model",
    "title": "Tutorial: Save and Restore Models",
    "section": "Save the entire model",
    "text": "Save the entire model\nCall save_model_* to save the a model’s architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original code*. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\nSaving a fully-functional model is very useful—you can load them in TensorFlow.js (HDF5, Saved Model) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (HDF5, Saved Model)\n*Custom objects (e.g. subclassed models or layers) require special attention when saving and loading. See the “Saving custom objects” section below.\n\nSavedModel format\nThe SavedModel format is a way to serialize models. Models saved in this format can be restored using load_model_tf and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to serve/inspect the SavedModel. The section below illustrates the steps to saving and restoring the model.\n\nmodel <- create_model()\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nmodel %>% save_model_tf(\"model\")\n\nThe SavedModel format is a directory containing a protobuf binary and a Tensorflow checkpoint. Inspect the saved model directory:\n\nlist.files(\"model\")\n\nReload a fresh Keras model from the saved model:\n\nnew_model <- load_model_tf(\"model\")\nsummary(new_model)\n\n\n\nHDF5 format\nKeras provides a basic saving format using the HDF5 standard.\n\nmodel <- create_model()\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nmodel %>% save_model_hdf5(\"my_model.h5\")\n\nNow recreate the model from that file:\n\nnew_model <- load_model_hdf5(\"my_model.h5\")\nsummary(new_model)\n\nThis technique saves everything:\n\nThe weight values\nThe model’s configuration(architecture)\nThe optimizer configuration\n\nKeras saves models by inspecting the architecture. Currently, it is not able to save TensorFlow optimizers (from tf$train). When using those you will need to re-compile the model after loading, and you will lose the state of the optimizer.\n\n\nSaving custom objects\nIf you are using the SavedModel format, you can skip this section. The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph.\nThus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the orginal code.\nTo save custom objects to HDF5, you must do the following:\n\nDefine a get_config method in your object, and optionally a from_config classmethod.\n\nget_config() returns a JSON-serializable dictionary of parameters needed to recreate the object.\nfrom_config(config) uses the returned config from get_config to create a new object. By default, this function will use the config as initialization arguments.\n\nPass the object to the custom_objects argument when loading the model. The argument must be a named list mapping the string class name to the class definition. E.g. load_keras_model_hdf5(path, custom_objects=list(\"CustomLayer\" =  CustomLayer))\n\nSee the Writing layers and models from scratch tutorial for examples of custom_objects and get_config."
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#save-checkpoints-during-training",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#save-checkpoints-during-training",
    "title": "Tutorial: Save and Restore Models",
    "section": "Save checkpoints during training",
    "text": "Save checkpoints during training\nIt is useful to automatically save checkpoints during and at the end of training. This way you can use a trained model without having to retrain it, or pick-up training where you left of, in case the training process was interrupted.\ncallback_model_checkpoint is a callback that performs this task.\nThe callback takes a couple of arguments to configure checkpointing. By default, save_weights_only is set to false, which means the complete model is being saved - including architecture and configuration. You can then restore the model as outlined in the previous paragraph.\nNow here, let’s focus on just saving and restoring weights. In the following code snippet, we are setting save_weights_only to true, so we will need the model definition on restore.\n\nCheckpoint callback usage\nTrain the model and pass it the callback_model_checkpoint:\n\ncheckpoint_path <- \"checkpoints/cp.ckpt\"\n\n# Create checkpoint callback\ncp_callback <- callback_model_checkpoint(\n  filepath = checkpoint_path,\n  save_weights_only = TRUE,\n  verbose = 0\n)\n\nmodel <- create_model()\n\nmodel %>% fit(\n  train_images,\n  train_labels,\n  epochs = 10, \n  validation_data = list(test_images, test_labels),\n  callbacks = list(cp_callback),  # pass callback to training\n  verbose = 2\n)\n\nInspect the files that were created:\n\nlist.files(dirname(checkpoint_path))\n\nCreate a new, untrained model. When restoring a model from only weights, you must have a model with the same architecture as the original model. Since it’s the same model architecture, we can share weights despite that it’s a different instance of the model.\nNow rebuild a fresh, untrained model, and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):\n\nfresh_model <- create_model()\nfresh_model %>% evaluate(test_images, test_labels, verbose = 0)\n\nThen load the weights from the latest checkpoint (epoch 10), and re-evaluate:\n\nfresh_model %>% load_model_weights_tf(filepath = checkpoint_path)\nfresh_model %>% evaluate(test_images, test_labels, verbose = 0)\n\n\n\nCheckpoint callback options\nAlternatively, you can decide to save only the best model, where best by default is defined as validation loss. See the documentation for callback_model_checkpoint for further information.\n\ncheckpoint_path <- \"checkpoints/cp.ckpt\"\n\n# Create checkpoint callback\ncp_callback <- callback_model_checkpoint(\n  filepath = checkpoint_path,\n  save_weights_only = TRUE,\n  save_best_only = TRUE,\n  verbose = 1\n)\n\nmodel <- create_model()\n\nmodel %>% fit(\n  train_images,\n  train_labels,\n  epochs = 10, \n  validation_data = list(test_images, test_labels),\n  callbacks = list(cp_callback), # pass callback to training,\n  verbose = 2\n)\n\nlist.files(dirname(checkpoint_path))\n\n\n\nWhat are these files?\nThe above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain:\n\nOne or more shards that contain your model’s weights.\nAn index file that indicates which weights are stored in a which shard.\n\nIf you are only training a model on a single machine, you’ll have one shard with the suffix: .data-00000-of-00001"
  },
  {
    "objectID": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#manually-save-the-weights",
    "href": "v1/tutorials/beginners/basic-ml/tutorial_save_and_restore.html#manually-save-the-weights",
    "title": "Tutorial: Save and Restore Models",
    "section": "Manually save the weights",
    "text": "Manually save the weights\nYou saw how to load the weights into a model. Manually saving them is just as simple with the save_model_weights_tf function.\n\n# Save the weights\nmodel %>% save_model_weights_tf(\"checkpoints/cp.ckpt\")\n\n# Create a new model instance\nnew_model <- create_model()\n\n# Restore the weights\nnew_model %>% load_model_weights_tf('checkpoints/cp.ckpt')\n\n# Evaluate the model\nnew_model %>% evaluate(test_images, test_labels, verbose = 0)"
  },
  {
    "objectID": "v1/tutorials/beginners/index.html",
    "href": "v1/tutorials/beginners/index.html",
    "title": "Overview",
    "section": "",
    "text": "Build a neural network that classifies images.\nTrain this neural network.\nAnd, finally, evaluate the accuracy of the model.\nSave and restore the created model.\n\nBefore running the quickstart you need to have Keras installed. Please refer to the installation section for installation instructions.\n\nlibrary(keras)\n\nLet’s start by loading and preparing the MNIST dataset. The values of the pixels are integers between 0 and 255, and we will convert them to floats between 0 and 1.\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\nNow, let’s define the a Keras model using the sequential API.\n\nmodel <- keras_model_sequential() %>% \n  layer_flatten(input_shape = c(28, 28)) %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dropout(0.2) %>% \n  layer_dense(10, activation = \"softmax\")\n\nNote that when using the Sequential API the first layer must specify the input_shape argument which represents the dimensions of the input. In our case, the images are 28x28.\nAfter defining the model, you can see information about layers, number of parameters, etc. with the summary function:\n\nsummary(model)\n\nThe next step after building the model is to compile it. It’s at compile time that we define what loss will be optimized and what optimizer will be used. You can also specify metrics, callbacks etc. that are meant to be run during the model fitting.\nCompiling is done with the compile function:\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNote that compile and fit (which we are going to see next) modify the model object in place, unlike most R functions.\nNow let’s fit our model:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    epochs = 5,\n    validation_split = 0.3,\n    verbose = 2\n  )\n\nWe can now make predictions with our model using the predict function:\n\npredictions <- predict(model, mnist$test$x)\nhead(predictions, 2)\n\nBy default predict will return the output of the last Keras layer. In our case this is the probability for each class. You can also use predict_classes and predict_proba to generate class and probability outputs - these functions are slightly different from predict, since they will be run in batches.\nYou can assess model performance on a different dataset using the evaluate function, for example:\n\nmodel %>% \n  evaluate(mnist$test$x, mnist$test$y, verbose = 0)\n\nOur model achieved ~90% accuracy on the test set.\nUnlike models built with the lm function, to save Keras models for later prediction, you need to use specialized functions, like save_model_tf:\n\nsave_model_tf(object = model, filepath = \"model\")\n\nYou can then reload the model and use it to make predictions:\n\nreloaded_model <- load_model_tf(\"model\")\nall.equal(predict(model, mnist$test$x), predict(reloaded_model, mnist$test$x))\n\nMore information about saving and serializing models, as well as about different model types, is available in the guides."
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_csv.html",
    "href": "v1/tutorials/beginners/load/load_csv.html",
    "title": "Loading CSV data",
    "section": "",
    "text": "Note: this is the R version of this tutorial in the TensorFlow official webiste.\nThis tutorial provides an example of how to load CSV data from a file into a TensorFlow Dataset using tfdatasets.\nThe data used in this tutorial are taken from the Titanic passenger list. The model will predict the likelihood a passenger survived based on characteristics like age, gender, ticket class, and wether the person was traveling alone."
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_csv.html#setup",
    "href": "v1/tutorials/beginners/load/load_csv.html#setup",
    "title": "Loading CSV data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n\nTRAIN_DATA_URL <- \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\nTEST_DATA_URL <- \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n\ntrain_file_path <- get_file(\"train_csv\", TRAIN_DATA_URL)\ntest_file_path <- get_file(\"eval.csv\", TEST_DATA_URL)\n\nYou coud load this using read.csv, and pass the arrays to TensorFlow. If you need to scale up to a large set of files, or need a loader that integrates with TensorFlow and tfdatasets then use the make_csv_dataset function:\nNow read the CSV data from the file and create a dataset.\n\ntrain_dataset <- make_csv_dataset(\n  train_file_path, \n  field_delim = \",\",\n  batch_size = 5, \n  num_epochs = 1\n)\n\ntest_dataset <- train_dataset <- make_csv_dataset(\n  test_file_path, \n  field_delim = \",\",\n  batch_size = 5, \n  num_epochs = 1\n)\n\nWe can see an element of the dataset with:\n\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  reticulate::py_to_r()\n\nYou can see that make_csv_dataset creates a list of Tensors each representing a column. This resembles a lot like R’s data.frame, the most significative difference is that a TensorFlow dataset is an iterator - meaning that each time you call iter_next it will yield a different batch of rows from the dataset.\nAs you can see above, the columns in the CSV are named. The dataset constructor will pick these names up automatically. If the file you are working with does not contain the column names in the first line, pass them in a character vector to the column_names argument in the make_csv_dataset function.\nIf you need to omit some columns from the dataset, create a list of just the columns you plan to use, and pass it into the (optional) select_columns argument of the constructor."
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_csv.html#data-preprocessing",
    "href": "v1/tutorials/beginners/load/load_csv.html#data-preprocessing",
    "title": "Loading CSV data",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nA CSV file can contain a variety of data types. Typically you want to convert from those mixed types to a fixed length vector before feeding the data into your model.\nYou can preprocess your data using any tool you like (like nltk or sklearn), and just pass the processed output to TensorFlow.\nTensorFlow has a built-in system for describing common input conversions: feature_column, which we are going to use via the high-level interface called feature_spec.\nThe primary advantage of doing the preprocessing inside your model is that when you export the model it includes the preprocessing. This way you can pass the raw data directly to your model.\nFirst let’s define the spec.\n\nspec <- feature_spec(train_dataset, survived ~ .)\n\nWe can now add steps to our spec telling how to transform our data.\n\nContinuous data\nFor continuous data we use the step_numeric_column:\n\nspec <- spec %>% \n  step_numeric_column(all_numeric())\n\nAfter adding a step we need to fit our spec:\n\nspec <- fit(spec)\n\nWe can then create a layer_dense_features that receives our dataset as input and returns an array containing all dense features:\n\nlayer <- layer_dense_features(feature_columns = dense_features(spec))\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  layer()\n\nIt’s usually a good idea to normalize all numeric features in a neural network. We can use the same step_numeric_column with an additional argument ``:\n\nspec <- feature_spec(train_dataset, survived ~ .)\nspec <- spec %>% \n  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard())\n\nWe can then fit and creat the layer_dense_features to take a look at the output:\n\nspec <- fit(spec)\nlayer <- layer_dense_features(feature_columns = dense_features(spec))\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  layer()\n\nNow, the outputs are scaled.\n\n\nCategorical data\nCategorical data can’t be directly included in the model matrix - we need to perform some kind of transformation in order to represent them as numbers. Representing categorical variables as a set of one-hot encoded columns is very common in practice.\nWe can also perform this transformation using the feature_spec API:\nLet’s again define our spec and add some steps:\n\nspec <- feature_spec(train_dataset, survived ~ .)\nspec <- spec %>% \n  step_categorical_column_with_vocabulary_list(sex) %>% \n  step_indicator_column(sex)\n\nWe can now see the output with:\n\nspec <- fit(spec)\nlayer <- layer_dense_features(feature_columns = dense_features(spec))\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  layer()\n\nWe can see that this generates 2 columns, one for each different category in the column sex of the dataset.\nIt’s straightforward to make this transformation for all the categorical features in the dataset:\n\nspec <- feature_spec(train_dataset, survived ~ .)\nspec <- spec %>% \n  step_categorical_column_with_vocabulary_list(all_nominal()) %>% \n  step_indicator_column(all_nominal())\n\nNow let’s see the output:\n\nspec <- fit(spec)\nlayer <- layer_dense_features(feature_columns = dense_features(spec))\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  layer()\n\n\n\nCombining everything\nWe demonstrated how to use the feature_spec interface both for continuous and categorical data separetedly. It’s also possible to combine all transformations in a single spec:\n\nspec <- feature_spec(train_dataset, survived ~ .) %>% \n  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% \n  step_categorical_column_with_vocabulary_list(all_nominal()) %>% \n  step_indicator_column(all_nominal())\n\nNow, let’s fit the spec and take a look at the output:\n\nspec <- fit(spec)\nlayer <- layer_dense_features(feature_columns = dense_features(spec))\ntrain_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  layer()\n\nThis concludes our data preprocessing step and we can now focus on building a training a model."
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_csv.html#building-the-model",
    "href": "v1/tutorials/beginners/load/load_csv.html#building-the-model",
    "title": "Loading CSV data",
    "section": "Building the model",
    "text": "Building the model\nWe will use the Keras sequential API do build a model that uses the dense features we have defined in the spec:\n\nmodel <- keras_model_sequential() %>% \n  layer_dense_features(feature_columns = dense_features(spec)) %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)"
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_csv.html#train-evaluate-and-predict",
    "href": "v1/tutorials/beginners/load/load_csv.html#train-evaluate-and-predict",
    "title": "Loading CSV data",
    "section": "Train, evaluate and predict",
    "text": "Train, evaluate and predict\nNow the model can be instantiated and trained.\n\nmodel %>% \n  fit(\n    train_dataset %>% dataset_use_spec(spec) %>% dataset_shuffle(500),\n    epochs = 20,\n    validation_data = test_dataset %>% dataset_use_spec(spec),\n    verbose = 2\n  )\n\nOnce the model is trained, you can check its accuracy on the test_data set.\n\nmodel %>% evaluate(test_dataset %>% dataset_use_spec(spec), verbose = 0)\n\nYou can also use predict to infer labels on a batch or a dataset of batches:\n\nbatch <- test_dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  reticulate::py_to_r()\npredict(model, batch)"
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_image.html",
    "href": "v1/tutorials/beginners/load/load_image.html",
    "title": "Loading image data",
    "section": "",
    "text": "Note: this is the R version of this tutorial in the TensorFlow oficial webiste.\nThis tutorial provides a simple example of how to load an image dataset using tfdatasets.\nThe dataset used in this example is distributed as directories of images, with one class of image per directory."
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_image.html#setup",
    "href": "v1/tutorials/beginners/load/load_image.html#setup",
    "title": "Loading image data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(keras)\nlibrary(tfdatasets)"
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_image.html#retrieve-the-images",
    "href": "v1/tutorials/beginners/load/load_image.html#retrieve-the-images",
    "title": "Loading image data",
    "section": "Retrieve the images",
    "text": "Retrieve the images\nBefore you start any training, you will need a set of images to teach the network about the new classes you want to recognize. You can use an archive of creative-commons licensed flower photos from Google.\n\nNote: all images are licensed CC-BY, creators are listed in the LICENSE.txt file.\n\n\ndata_dir <- get_file(\n  origin = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n  fname = \"flower_photos.tgz\",\n  extract = TRUE\n)\ndata_dir <- file.path(dirname(data_dir), \"flower_photos\")\n\nAfter downloading (218MB), you should now have a copy of the flower photos available.\nThe directory contains 5 sub-directories, one per class:\n\nimages <- list.files(data_dir, pattern = \".jpg\", recursive = TRUE)\nlength(images)\n\nclasses <- list.dirs(data_dir, full.names = FALSE, recursive = FALSE)\nclasses"
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_image.html#load-using-tfdatasets",
    "href": "v1/tutorials/beginners/load/load_image.html#load-using-tfdatasets",
    "title": "Loading image data",
    "section": "Load using tfdatasets",
    "text": "Load using tfdatasets\nTo load the files as a TensorFlow Dataset first create a dataset of the file paths:\n\nlist_ds <- file_list_dataset(file_pattern = paste0(data_dir, \"/*/*\"))\n\n\nlist_ds %>% reticulate::as_iterator() %>% reticulate::iter_next()\n\nWrite a short pure-tensorflow function that converts a file paths to an (image_data, label) pair:\n\nget_label <- function(file_path) {\n  parts <- tf$strings$split(file_path, \"/\")\n  parts[-2] %>% \n    tf$equal(classes) %>% \n    tf$cast(dtype = tf$float32)\n}\n\ndecode_img <- function(file_path, height = 224, width = 224) {\n  \n  size <- as.integer(c(height, width))\n  \n  file_path %>% \n    tf$io$read_file() %>% \n    tf$image$decode_jpeg(channels = 3) %>% \n    tf$image$convert_image_dtype(dtype = tf$float32) %>% \n    tf$image$resize(size = size)\n}\n\npreprocess_path <- function(file_path) {\n  list(\n    decode_img(file_path),\n    get_label(file_path)\n  )\n}\n\nUse dataset_map to create a dataset of image, label pairs:\n\n# num_parallel_calls are going to be autotuned\nlabeled_ds <- list_ds %>% \n  dataset_map(preprocess_path, num_parallel_calls = tf$data$experimental$AUTOTUNE)\n\nLet’s see what the output looks like:\n\nlabeled_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()"
  },
  {
    "objectID": "v1/tutorials/beginners/load/load_image.html#training-a-model",
    "href": "v1/tutorials/beginners/load/load_image.html#training-a-model",
    "title": "Loading image data",
    "section": "Training a model",
    "text": "Training a model\nTo train a model with this dataset you will want the data:\n\nTo be well shuffled.\nTo be batched.\nBatches to be available as soon as possible.\n\nThese features can be easily added using tfdatasets.\nFirst, let’s define a function that prepares a dataset in order to feed to a Keras model.\n\nprepare <- function(ds, batch_size, shuffle_buffer_size) {\n  \n  if (shuffle_buffer_size > 0)\n    ds <- ds %>% dataset_shuffle(shuffle_buffer_size)\n  \n  ds %>% \n    dataset_batch(batch_size) %>% \n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    dataset_prefetch(buffer_size = tf$data$experimental$AUTOTUNE)\n}\n\nNow let’s define a Keras model to classify the images:\n\nmodel <- keras_model_sequential() %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 5, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nWe can then fit the model feeding the dataset we just created:\n\nNote We are fitting this model as an example of how to the pipeline built with Keras. In real use cases you should always use validation datasets in order to verify your model performance.\n\n\nmodel %>% \n  fit(\n    prepare(labeled_ds, batch_size = 32, shuffle_buffer_size = 1000),\n    epochs = 5,\n    verbose = 2\n  )"
  },
  {
    "objectID": "v1/tutorials/index.html",
    "href": "v1/tutorials/index.html",
    "title": "Overview",
    "section": "",
    "text": "In this section you will find tutorials that can be used to get started with TensorFlow for R or, for more advanced users, to discover best practices for loading data, building complex models and solving common problems.\nThe best place to get started with TensorFlow is using Keras - a Deep Learning API created by François Chollet and ported to R by JJ Allaire. Keras makes it easy to get started, and it allows you to progressively build more complex workflows as you need to use advanced models and techniques."
  },
  {
    "objectID": "v1/tutorials/index.html#for-beginners",
    "href": "v1/tutorials/index.html#for-beginners",
    "title": "Overview",
    "section": "For beginners",
    "text": "For beginners\nWe recommend the following tutorials for your first contact with TensorFlow. Feel free to navigate through the ‘beginners’ section in the sidebar.\n\nQuickstart: the minimal getting started guide to Keras.\nBasic ML with Keras: use Keras to solve basic Machine Learning tasks.\nLoad data: learn to efficiently load data to TensorFlow using tfdatasets."
  },
  {
    "objectID": "v1/tutorials/index.html#for-experts",
    "href": "v1/tutorials/index.html#for-experts",
    "title": "Overview",
    "section": "For experts",
    "text": "For experts\n\nAdvanced Quickstart: learn the subclassing API and how to create custom loops.\nCustomization: build custom layers and training loops in TensorFlow.\nDistributed Training: distribute your model training across multiple GPU’s or machines.\n\nWe also provide tutorials focused on different types of data:\n\nImages: Build more advanced models for classification and segmentation of images.\nStructured Data: Build models for structured data."
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html",
    "href": "guides/tensorflow/tensor_slicing.html",
    "title": "Tensor Slicing",
    "section": "",
    "text": "# Copyright 2020 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#setup",
    "href": "guides/tensorflow/tensor_slicing.html#setup",
    "title": "Tensor Slicing",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "href": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "title": "Tensor Slicing",
    "section": "Extract tensor slices",
    "text": "Extract tensor slices\nPerform slicing using the [ operator:\n\nt1 <- as_tensor(c(1, 2, 3, 4, 5, 6, 7))\nt1[1:3]\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike base R’s [ operator, TensorFlow’s [ uses negative indexes for selecting starting from the end.\nNULL can be used instead of the last dimension or first, depending if it appears before or after the :.\n\n\n\nt1[-3:NULL]\n\n\nFor 2-dimensional tensors,you can use something like:\n\nt2 <- as_tensor(rbind(c(0, 1, 2, 3, 4),\n                      c(5, 6, 7, 8, 9),\n                      c(10, 11, 12, 13, 14),\n                      c(15, 16, 17, 18, 19)))\n\nt2[NULL:-1, 2:3]\n\n\n\n\n\n\n\n\nNote\n\n\n\ntf$slice can be used instead of the [ operator. However, not that when using functions directly from the tf module, dimensions and indexes will start from 0, unlike in R.\nYou also need to make sure that indexes are passed to TensorFlow with the integer type, for example using the L suffix notation.\n\n\nYou can use tf$slice on higher dimensional tensors as well.\n\nt3 <- as_tensor(array(seq(from=1, to = 31, by = 2), dim = c(2,2,4)))\ntf$slice(\n  t3,\n  begin = list(1L, 1L, 0L),\n  size = list(1L, 1L, 2L)\n)\n\nYou can also use tf$strided_slice to extract slices of tensors by ‘striding’ over the tensor dimensions.\nUse tf$gather to extract specific indices from a single axis of a tensor.\n\ntf$gather(t1, indices = c(0L, 3L, 6L))\n\n\ntf$gather does not require indices to be evenly spaced.\n\nalphabet <- as_tensor(strsplit(\"abcdefghijklmnopqrstuvwxyz\", \"\")[[1]])\ntf$gather(alphabet, indices = c(2L, 0L, 19L, 18L))\n\n\nTo extract slices from multiple axes of a tensor, use tf$gather_nd. This is useful when you want to gather the elements of a matrix as opposed to just its rows or columns.\n\nt4 <- as_tensor(rbind(c(0, 5),\n                      c(1, 6),\n                      c(2, 7),\n                      c(3, 8),\n                      c(4, 9)))\n\ntf$gather_nd(t4, indices = list(list(2L), list(3L), list(0L)))\n\n\n\nt5 <- array(1:18, dim = c(2,3,3))\ntf$gather_nd(t5, indices = list(c(0L, 0L, 0L), c(1L, 2L, 1L)))\n\n\n# Return a list of two matrices\ntf$gather_nd(\n  t5,\n  indices = list(\n    list(c(0L, 0L), c(0L, 2L)), \n    list(c(1L, 0L), c(1L, 2L)))\n)\n\n\n# Return one matrix\ntf$gather_nd(\n  t5,\n  indices = list(c(0L, 0L), c(0L, 2L), c(1L, 0L), c(1L, 2L))\n)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "href": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "title": "Tensor Slicing",
    "section": "Insert data into tensors",
    "text": "Insert data into tensors\nUse tf$scatter_nd to insert data at specific slices/indices of a tensor. Note that the tensor into which you insert values is zero-initialized.\n\nt6 <- as_tensor(list(10L))\nindices <- as_tensor(list(list(1L), list(3L), list(5L), list(7L), list(9L)))\ndata <- as_tensor(c(2, 4, 6, 8, 10))\n\ntf$scatter_nd(\n  indices = indices,\n  updates = data,\n  shape = t6\n)\n\nMethods like tf$scatter_nd which require zero-initialized tensors are similar to sparse tensor initializers. You can use tf$gather_nd and tf$scatter_nd to mimic the behavior of sparse tensor ops.\nConsider an example where you construct a sparse tensor using these two methods in conjunction.\n\n# Gather values from one tensor by specifying indices\nnew_indices <- as_tensor(rbind(c(0L, 2L), c(2L, 1L), c(3L, 3L)))\nt7 <- tf$gather_nd(t2, indices = new_indices)\n\n\n\n# Add these values into a new tensor\nt8 <- tf$scatter_nd(\n  indices = new_indices, \n  updates = t7, \n  shape = as_tensor(c(4L, 5L))\n)\nt8\n\nThis is similar to:\n\nt9 <- tf$SparseTensor(\n  indices = list(c(0L, 2L), c(2L, 1L), c(3L, 3L)),\n  values = c(2, 11, 18),\n  dense_shape = c(4L, 5L)\n)\nt9\n\n\n# Convert the sparse tensor into a dense tensor\nt10 <- tf$sparse$to_dense(t9)\nt10\n\nTo insert data into a tensor with pre-existing values, use tf$tensor_scatter_nd_add.\n\nt11 <- as_tensor(rbind(c(2, 7, 0),\n                       c(9, 0, 1),\n                       c(0, 3, 8)))\n\n# Convert the tensor into a magic square by inserting numbers at appropriate indices\nt12 <- tf$tensor_scatter_nd_add(\n  t11,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt12\n\nSimilarly, use tf$tensor_scatter_nd_sub to subtract values from a tensor with pre-existing values.\n\n# Convert the tensor into an identity matrix\nt13 <- tf$tensor_scatter_nd_sub(\n  t11,\n  indices = list(c(0L, 0L), c(0L, 1L), c(1L, 0L), c(1L, 1L), c(1L, 2L), c(2L, 1L), c(2L, 2L)),\n  updates = c(1, 7, 9, -1, 1, 3, 7)\n)\n\nprint(t13)\n\nUse tf$tensor_scatter_nd_min to copy element-wise minimum values from one tensor to another.\n\nt14 <- as_tensor(rbind(c(-2, -7, 0),\n                       c(-9, 0, 1),\n                       c(0, -3, -8)))\n\nt15 <- tf$tensor_scatter_nd_min(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(-6, -5, -4)\n)\nt15\n\nSimilarly, use tf$tensor_scatter_nd_max to copy element-wise maximum values from one tensor to another.\n\nt16 <- tf$tensor_scatter_nd_max(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt16"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "href": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "title": "Tensor Slicing",
    "section": "Further reading and resources",
    "text": "Further reading and resources\nIn this guide, you learned how to use the tensor slicing ops available with TensorFlow to exert finer control over the elements in your tensors.\n\nCheck out the slicing ops available with TensorFlow NumPy such as tf$experimental$numpy$take_along_axis and tf$experimental$numpy$take.\nAlso check out the Tensor guide and the Variable guide."
  }
]