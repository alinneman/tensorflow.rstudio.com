[
  {
    "objectID": "deploy/docker.html",
    "href": "deploy/docker.html",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using TensorFlow serving.\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\nUsing the Docker container is a an easy way to test the API locally and then deploy it to any cloud provider."
  },
  {
    "objectID": "deploy/docker.html#building-the-model",
    "href": "deploy/docker.html#building-the-model",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.03585155 0.98879999 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/docker.html#running-locally",
    "href": "deploy/docker.html#running-locally",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Running locally",
    "text": "Running locally\nYou can run the tensorflow/serving Docker image locally using the great stevedore package. For example:\n\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside `/models`\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n\nNow we have initialized the container serving the model. You can see the container logs with:\n\ncontainer$logs()\n\nNow you can make POST requests no the following endpoint : http://localhost:8501/v1/models/model/versions/1:predict. The input data must be passed in a special format 0 - see the format definition here, which may seem unnatural for R users. Here is an example:\n\ninstances <- purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %>% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nThis is how you can serve TensorFlow models with TF serving locally. Additionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\nWhen done, you can stop the container with:\n\ncontainer$stop()"
  },
  {
    "objectID": "deploy/docker.html#deploying-to-google-cloud-run",
    "href": "deploy/docker.html#deploying-to-google-cloud-run",
    "title": "Deploying a TensorFlow model using TensorFlow serving",
    "section": "Deploying to Google Cloud Run",
    "text": "Deploying to Google Cloud Run\nTHe first thing you need to do is to follow the section Before you begin in this page.\nNow let’s create a Dockerfile that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\nHere’s an example - create a file called Dockerfile in the same root folder as your SavedModel and paste the following:\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\", \"--rest_api_port=8080\"]\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io. Run the following in your terminal:\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\nYou can get your PROJECT-ID by running:\ngcloud config get-value project\nNext, we can create the service in Google Cloud Run using:\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly you will get a url like https://cnn-mnist-ld4lzfalyq-ue.a.run.app which you can now use to make requests to your model. For example:\n\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API."
  },
  {
    "objectID": "deploy/index.html",
    "href": "deploy/index.html",
    "title": "Overview",
    "section": "",
    "text": "Plumber API: Create a REST API using Plumber to deploy your TensorFlow model. With Plumber you will still depend on having an R runtime which be useful when you want to make the data pre-processing in R.\nShiny: Create a Shiny app that uses a TensorFlow model to generate outputs.\nTensorFlow Serving: This is the most performant way of deploying TensorFlow models since it’s based only inn the TensorFlow serving C++ server. With TF serving you don’t depend on an R runtime, so all pre-processing must be done in the TensorFlow graph.\nRStudio Connect: RStudio Connect makes it easy to deploy TensorFlow models and uses TensorFlow serving in the backend.\n\nThere are many other options to deploy TensorFlow models built with R that are not covered in this section. For example:\n\nDeploy it using a Python runtime.\nDeploy using a JavaScript runtime.\nDeploy to a mobile phone app using TensorFlow Lite.\nDeploy to a iOS app using Apple’s Core ML tool.\nUse plumber and Docker to deploy your TensorFlow model (by T-Mobile)."
  },
  {
    "objectID": "deploy/plumber.html",
    "href": "deploy/plumber.html",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model using a plumber API.\nIn this example we will build an endpoint that takes POST requests sending images containing handwritten digits and returning the predicted number."
  },
  {
    "objectID": "deploy/plumber.html#building-the-model",
    "href": "deploy/plumber.html#building-the-model",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. W We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.02827194 0.99059999 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/plumber.html#plumber-api",
    "href": "deploy/plumber.html#plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Plumber API",
    "text": "Plumber API\nA plumber API is defined by a .R file with a few annotations. Here’s is how we can write our api.R file:\n\nlibrary(keras)\n\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n#* Predicts the number in an image\n#* @param enc a base64  encoded 28x28 image\n#* @post /cnn-mnist\nfunction(enc) {\n  # decode and read the jpeg image\n  img <- jpeg::readJPEG(source = base64enc::base64decode(enc))\n  \n  # reshape\n  img <- img %>% \n    array_reshape(., dim = c(1, dim(.), 1))\n  \n  # make the prediction\n  predict_classes(model, img)\n}\n\nMake sure to have the your SavedModel in the same folder as api.R and call:\n\np <- plumber::plumb(\"api.R\")\np$run(port = 8000)\n\nYou can now make requests to the http://lcoalhost:8000/cnn-minist/ endpoint. For example, let’s verify we can make a POST request to the API sending the first image from the test set:\n\nimg <- mnist$test$x[1,,,]\nmnist$test$y[1]\n\n[1] 7\n\n\nFirst let’s encode the image:\n\nencoded_img <- img %>% \n  jpeg::writeJPEG() %>% \n  base64enc::base64encode()\nencoded_img\n\n[1] \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APHIYJbmZYYInllc4VEUszH2A6064tbi0k8q5glgcfwyIVP5GoqKsWGoXel3sd7YXD29zESUljOGXIwf0JrpLf4o+MLeNUbVftAU5BuIUlPTplhn/wDVXQ+PNcvk8D6bpmtPbz6vqDi9lCwqhtocYRRtHU8k9+orzKius8CeHrS/nutd1pWGiaQnm3GBnznyNsQ+p/w4zWL4h1u48Ra7d6rcDa1w+VQdEUcKo+gAFZtFbsfjDVIPB7+FoRBHYyymWVlT95JyDgknGMgdADx1rCor/9k=\"\n\n\n\nreq <- httr::POST(\"http://localhost:8000/cnn-mnist\",\n           body = list(enc = encoded_img), \n           encode = \"json\")\nhttr::content(req)\n\n[[1]]\n[1] 7\nYou can also access the Swagger interface by accessing http://127.0.0.1:8000/swagger/ and paste the encoded string in the UI to visualize the result."
  },
  {
    "objectID": "deploy/plumber.html#more-advanced-models",
    "href": "deploy/plumber.html#more-advanced-models",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/plumber.html#hosting-the-plumber-api",
    "href": "deploy/plumber.html#hosting-the-plumber-api",
    "title": "Deploying a TensorFlow API with Plumber",
    "section": "Hosting the plumber API",
    "text": "Hosting the plumber API\nPlumber is very flexible and allows multiple hosting options. See the plumber Hostinng documentation for more information."
  },
  {
    "objectID": "deploy/rsconnect.html",
    "href": "deploy/rsconnect.html",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model to RStudio Connect. RStudio Connect uses TensorFlow Serving for performance but makes it much easier for R users to manage their deployment."
  },
  {
    "objectID": "deploy/rsconnect.html#building-the-model",
    "href": "deploy/rsconnect.html#building-the-model",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y, verbose = 0)\n\n      loss   accuracy \n0.03402501 0.98860002 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "href": "deploy/rsconnect.html#deployiong-to-rstudio-connect",
    "title": "Deploying a TensorFlow Model to RStudio Connect",
    "section": "Deployiong to RStudio Connect",
    "text": "Deployiong to RStudio Connect\nOnce the model is saved to the SavedModel format, the model can be deployed with a single line of code:\n\nrsconnect::deployTFModel(\"cnn-mnist/\")\n\nWhen the deployment is complete you will be redirected to your browser with some instructions on how to call the REST endpoint:"
  },
  {
    "objectID": "deploy/shiny.html",
    "href": "deploy/shiny.html",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "",
    "text": "In this tutorial you will learn how to deploy a TensorFlow model inside a Shiny app. We will build a model that can classify handwritten digits in images, then we will build a Shiny app that let’s you upload an image and get predictions from this model."
  },
  {
    "objectID": "deploy/shiny.html#building-the-model",
    "href": "deploy/shiny.html#building-the-model",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Building the model",
    "text": "Building the model\nThe first thing we are going to do is to build our model. We will use the Keras API to build this model.\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nmnist <- dataset_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural network.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate the results on the test dataset with:\n\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n\n      loss   accuracy \n0.03422958 0.98920000 \n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model. First, let’s save the model in the SavedModel format using:\n\nsave_model_tf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file."
  },
  {
    "objectID": "deploy/shiny.html#shiny-app",
    "href": "deploy/shiny.html#shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Shiny app",
    "text": "Shiny app\nA simple shiny app can be define in an app.R file with a few conventions. Here’s how we can structure our Shiny app.\n\nlibrary(shiny)\nlibrary(keras)\n\n# Load the model\nmodel <- load_model_tf(\"cnn-mnist/\")\n\n# Define the UI\nui <- fluidPage(\n  # App title ----\n  titlePanel(\"Hello TensorFlow!\"),\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n      # Input: File upload\n      fileInput(\"image_path\", label = \"Input a JPEG image\")\n    ),\n    # Main panel for displaying outputs ----\n    mainPanel(\n      # Output: Histogram ----\n      textOutput(outputId = \"prediction\"),\n      plotOutput(outputId = \"image\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram ----\nserver <- function(input, output) {\n  \n  image <- reactive({\n    req(input$image_path)\n    jpeg::readJPEG(input$image_path$datapath)\n  })\n  \n  output$prediction <- renderText({\n    \n    img <- image() %>% \n      array_reshape(., dim = c(1, dim(.), 1))\n    \n    paste0(\"The predicted class number is \", predict_classes(model, img))\n  })\n  \n  output$image <- renderPlot({\n    plot(as.raster(image()))\n  })\n  \n}\n\nshinyApp(ui, server)\n\nThis app can be used locally or deployed using any Shiny deployment option. If you are deploying to RStudio Connect or Shinnyapps.io, don’t forget to set the RETICULATE_PYTHON environment variable so rsconnect can detect what python packages are required to reproduce your local environment. See the F.A.Q. for more information.\n\nYou can see a live version of this app here. Note that to keep the code simple, it will only accept JPEG images with 28x28 pixels. You can download this file if you want to try the app."
  },
  {
    "objectID": "deploy/shiny.html#more-advanced-models",
    "href": "deploy/shiny.html#more-advanced-models",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "More advanced models",
    "text": "More advanced models\nWhen building more advanced models you may not be able to save the entire model using the save_model_tf function. In this case you can use the save_model_weights_tf function.\nFor example:\n\nsave_model_weights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use load_model_weights_tf to load the model weights.\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nload_model_weights_tf(model, \"cnn-model-weights\")"
  },
  {
    "objectID": "deploy/shiny.html#hosting-the-shiny-app",
    "href": "deploy/shiny.html#hosting-the-shiny-app",
    "title": "Deploying a Shiny app with a TensorFlow model",
    "section": "Hosting the shiny app",
    "text": "Hosting the shiny app\nThis Shiny app can be hosted in any server using the Shiny Server. If you are managing the complete infrastructure, make sure that you have Python and all required Python packages installed in the server.\nIf you are using Shinyapps.io or RStudio Connect the dependencies will be infered when deploying the app. In this case, don’t forget to set the RETICULATE_PYTHON environment variable.\nYou can find more examples of using reticulate in RStudio products here and learn more about Python in RStudio Connect best practices here."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "link"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html",
    "href": "guides/keras/customizing_what_happens_in_fit.html",
    "title": "Customizing what happens in fit()",
    "section": "",
    "text": "When you’re doing supervised learning, you can use fit() and everything works smoothly.\nWhen you need to write your own training loop from scratch, you can use the GradientTape and take control of every little detail.\nBut what if you need a custom training algorithm, but you still want to benefit from the convenient features of fit(), such as callbacks, built-in distribution support, or step fusing?\nA core principle of Keras is progressive disclosure of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn’t fall off a cliff if the high-level functionality doesn’t exactly match your use case. You should be able to gain more control over the small details while retaining a commensurate amount of high-level convenience.\nWhen you need to customize what fit() does, you should override the training step function of the Model class. This is the function that is called by fit() for every batch of data. You will then be able to call fit() as usual – and it will be running your own learning algorithm.\nNote that this pattern does not prevent you from building models with the Functional API. You can do this whether you’re building Sequential models, Functional API models, or subclassed models.\nLet’s see how that works."
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "href": "guides/keras/customizing_what_happens_in_fit.html#setup",
    "title": "Customizing what happens in fit()",
    "section": "Setup",
    "text": "Setup\nRequires TensorFlow 2.2 or later.\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)\n\nWarning: package 'keras' was built under R version 4.1.2"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#a-first-simple-example",
    "title": "Customizing what happens in fit()",
    "section": "A first simple example",
    "text": "A first simple example\nLet’s start from a simple example:\n\nWe create a new model class by calling new_model_class().\nWe just override the method train_step(data).\nWe return a dictionary mapping metric names (including the loss) to their current value.\n\nThe input argument data is what gets passed to fit as training data:\n\nIf you pass arrays, by calling fit(x, y, ...), then data will be the tuple (x, y)\nIf you pass a tf$data$Dataset, by calling fit(dataset, ...), then data will be what gets yielded by dataset at each batch.\n\nIn the body of the train_step method, we implement a regular training update, similar to what you are already familiar with. Importantly, we compute the loss via self$compiled_loss, which wraps the loss(es) function(s) that were passed to compile().\nSimilarly, we call self$compiled_metrics$update_state(y, y_pred) to update the state of the metrics that were passed in compile(), and we query results from self$metrics at the end to retrieve their current value.\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\nLoaded Tensorflow version 2.9.1\n\n\nLet’s try this out:\n\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "href": "guides/keras/customizing_what_happens_in_fit.html#going-lower-level",
    "title": "Customizing what happens in fit()",
    "section": "Going lower-level",
    "text": "Going lower-level\nNaturally, you could just skip passing a loss function in compile(), and instead do everything manually in train_step. Likewise for metrics.\nHere’s a lower-level example, that only uses compile() to configure the optimizer:\n\nWe start by creating Metric instances to track our loss and a MAE score.\nWe implement a custom train_step() that updates the state of these metrics (by calling update_state() on them), then query them (via result()) to return their current average value, to be displayed by the progress bar and to be pass to any callback.\nNote that we would need to call reset_states() on our metrics between each epoch! Otherwise calling result() would return an average since the start of training, whereas we usually work with per-epoch averages. Thankfully, the framework can do that for us: just list any metric you want to reset in the metrics property of the model. The model will call reset_states() on any object listed here at the beginning of each fit() epoch or at the beginning of a call to evaluate().\n\n\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "href": "guides/keras/customizing_what_happens_in_fit.html#supporting-sample_weight-class_weight",
    "title": "Customizing what happens in fit()",
    "section": "Supporting sample_weight & class_weight",
    "text": "Supporting sample_weight & class_weight\nYou may have noticed that our first basic example didn’t make any mention of sample weighting. If you want to support the fit() arguments sample_weight and class_weight, you’d simply do the following:\n\nUnpack sample_weight from the data argument\nPass it to compiled_loss & compiled_metrics (of course, you could also just apply it manually if you don’t rely on compile() for losses & metrics)\nThat’s it. That’s the list.\n\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "href": "guides/keras/customizing_what_happens_in_fit.html#providing-your-own-evaluation-step",
    "title": "Customizing what happens in fit()",
    "section": "Providing your own evaluation step",
    "text": "Providing your own evaluation step\nWhat if you want to do the same for calls to model$evaluate()? Then you would override test_step in exactly the same way. Here’s what it looks like:\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n\n     loss       mae \n0.6081660 0.6558823"
  },
  {
    "objectID": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "href": "guides/keras/customizing_what_happens_in_fit.html#wrapping-up-an-end-to-end-gan-example",
    "title": "Customizing what happens in fit()",
    "section": "Wrapping up: an end-to-end GAN example",
    "text": "Wrapping up: an end-to-end GAN example\nLet’s walk through an end-to-end example that leverages everything you just learned.\nLet’s consider:\n\nA generator network meant to generate 28x28x1 images.\nA discriminator network meant to classify 28x28x1 images into two classes (“fake” and “real”).\nOne optimizer for each.\nA loss function to train the discriminator.\n\n\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n\nHere’s a feature-complete GAN class, overriding compile() to use its own signature, and implementing the entire GAN algorithm in 17 lines in train_step:\n\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n\nLet’s test-drive it:\n\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n\nHappy training!"
  },
  {
    "objectID": "guides/keras/functional_api.html",
    "href": "guides/keras/functional_api.html",
    "title": "The Functional API",
    "section": "",
    "text": "library(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/functional_api.html#introduction",
    "href": "guides/keras/functional_api.html#introduction",
    "title": "The Functional API",
    "section": "Introduction",
    "text": "Introduction\nThe Keras functional API is a way to create models that are more flexible than the sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\nThe main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\nConsider the following model:\n\n(input: 784-dimensional vectors)\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (10 units, softmax activation)]\n       ↧\n(output: logits of a probability distribution over 10 classes)\n\nThis is a basic graph with three layers. To build this model using the functional API, start by creating an input node:\n\ninputs <- layer_input(shape = c(784))\n\nLoaded Tensorflow version 2.9.1\n\n\nThe shape of the data is set as a 784-dimensional vector. The batch size is always omitted since only the shape of each sample is specified.\nIf, for example, you have an image input with a shape of (32, 32, 3), you would use:\n\n# Just for demonstration purposes.\nimg_inputs <- layer_input(shape = c(32, 32, 3))\n\nThe inputs that is returned contains information about the shape and dtype of the input data that you feed to your model. Here’s the shape:\n\ninputs$shape\n\nTensorShape([None, 784])\n\n\nHere’s the dtype:\n\ninputs$dtype\n\ntf.float32\n\n\nYou create a new node in the graph of layers by calling a layer on this inputs object:\n\ndense <- layer_dense(units = 64, activation = \"relu\")\nx <- dense(inputs)\n\nThe “layer call” action is like drawing an arrow from “inputs” to this layer you created. You’re “passing” the inputs to the dense layer, and you get x as the output.\nYou can also conveniently create the layer and compose it with inputs in one step, like this:\n\nx <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\") \n\nLet’s add a few more layers to the graph of layers:\n\noutputs <- x %>% \n  layer_dense(64, activation = \"relu\") %>% \n  layer_dense(10)\n\nAt this point, you can create a Model by specifying its inputs and outputs in the graph of layers:\n\nmodel <- keras_model(inputs = inputs, outputs = outputs, \n                     name = \"mnist_model\")\n\nLet’s check out what the model summary looks like:\n\nmodel\n\nModel: \"mnist_model\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_1 (InputLayer)               [(None, 784)]                   0           \n dense_1 (Dense)                    (None, 64)                      50240       \n dense_3 (Dense)                    (None, 64)                      4160        \n dense_2 (Dense)                    (None, 10)                      650         \n================================================================================\nTotal params: 55,050\nTrainable params: 55,050\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nYou can also plot the model as a graph:\n\nplot(model)\n\n\n\n\nAnd, optionally, display the input and output shapes of each layer in the plotted graph:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nThis figure and the code are almost identical. In the code version, the connection arrows are replaced by %>% operator.\nA “graph of layers” is an intuitive mental image for a deep learning model, and the functional API is a way to create models that closely mirrors this."
  },
  {
    "objectID": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "href": "guides/keras/functional_api.html#training-evaluation-and-inference",
    "title": "The Functional API",
    "section": "Training, evaluation, and inference",
    "text": "Training, evaluation, and inference\nTraining, evaluation, and inference work exactly in the same way for models built using the functional API as for Sequential models.\nThe Model class offers a built-in training loop (the fit() method) and a built-in evaluation loop (the evaluate() method). Note that you can easily customize these loops to implement training routines beyond supervised learning (e.g. GANs).\nHere, load the MNIST image data, reshape it into vectors, fit the model on the data (while monitoring performance on a validation split), then evaluate the model on the test data:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nx_train <- array_reshape(x_train, c(60000, 784)) / 255\nx_test <-  array_reshape(x_test, c(10000, 784)) / 255 \n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_rmsprop(),\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2)\n\ntest_scores <- model %>% evaluate(x_test, y_test, verbose = 2)\nprint(test_scores)\n\n     loss  accuracy \n0.1290688 0.9635000 \n\n\nFor further reading, see the training and evaluation guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#save-and-serialize",
    "href": "guides/keras/functional_api.html#save-and-serialize",
    "title": "The Functional API",
    "section": "Save and serialize",
    "text": "Save and serialize\nSaving the model and serialization work the same way for models built using the functional API as they do for Sequential models. The standard way to save a functional model is to call save_model_tf() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available.\nThis saved file includes the: - model architecture - model weight values (that were learned during training) - model training config, if any (as passed to compile) - optimizer and its state, if any (to restart training where you left off)\n\npath_to_my_model <- tempfile()\nsave_model_tf(model, path_to_my_model)\n\nrm(model)\n# Recreate the exact same model purely from the file:\nmodel <- load_model_tf(path_to_my_model)\n\nFor details, read the model serialization & saving guide."
  },
  {
    "objectID": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "href": "guides/keras/functional_api.html#use-the-same-graph-of-layers-to-define-multiple-models",
    "title": "The Functional API",
    "section": "Use the same graph of layers to define multiple models",
    "text": "Use the same graph of layers to define multiple models\nIn the functional API, models are created by specifying their inputs and outputs in a graph of layers. That means that a single graph of layers can be used to generate multiple models.\nIn the example below, you use the same stack of layers to instantiate two models: an encoder model that turns image inputs into 16-dimensional vectors, and an end-to-end autoencoder model for training.\n\nencoder_input <- layer_input(shape = c(28, 28, 1), \n                             name = \"img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, \n                       name = \"encoder\")\nencoder\n\nModel: \"encoder\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n img (InputLayer)                   [(None, 28, 28, 1)]             0           \n conv2d_3 (Conv2D)                  (None, 26, 26, 16)              160         \n conv2d_2 (Conv2D)                  (None, 24, 24, 32)              4640        \n max_pooling2d (MaxPooling2D)       (None, 8, 8, 32)                0           \n conv2d_1 (Conv2D)                  (None, 6, 6, 32)                9248        \n conv2d (Conv2D)                    (None, 4, 4, 16)                4624        \n global_max_pooling2d (GlobalMaxPoo  (None, 16)                     0           \n ling2D)                                                                        \n================================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n________________________________________________________________________________\n\ndecoder_output <- encoder_output %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\nautoencoder <- keras_model(encoder_input, decoder_output, \n                           name = \"autoencoder\")\nautoencoder\n\nModel: \"autoencoder\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n img (InputLayer)                   [(None, 28, 28, 1)]             0           \n conv2d_3 (Conv2D)                  (None, 26, 26, 16)              160         \n conv2d_2 (Conv2D)                  (None, 24, 24, 32)              4640        \n max_pooling2d (MaxPooling2D)       (None, 8, 8, 32)                0           \n conv2d_1 (Conv2D)                  (None, 6, 6, 32)                9248        \n conv2d (Conv2D)                    (None, 4, 4, 16)                4624        \n global_max_pooling2d (GlobalMaxPoo  (None, 16)                     0           \n ling2D)                                                                        \n reshape (Reshape)                  (None, 4, 4, 1)                 0           \n conv2d_transpose_3 (Conv2DTranspos  (None, 6, 6, 16)               160         \n e)                                                                             \n conv2d_transpose_2 (Conv2DTranspos  (None, 8, 8, 32)               4640        \n e)                                                                             \n up_sampling2d (UpSampling2D)       (None, 24, 24, 32)              0           \n conv2d_transpose_1 (Conv2DTranspos  (None, 26, 26, 16)             4624        \n e)                                                                             \n conv2d_transpose (Conv2DTranspose)  (None, 28, 28, 1)              145         \n================================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nHere, the decoding architecture is strictly symmetrical to the encoding architecture, so the output shape is the same as the input shape (28, 28, 1).\nThe reverse of a Conv2D layer is a Conv2DTranspose layer, and the reverse of a MaxPooling2D layer is an UpSampling2D layer."
  },
  {
    "objectID": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "href": "guides/keras/functional_api.html#all-models-are-callable-just-like-layers",
    "title": "The Functional API",
    "section": "All models are callable, just like layers",
    "text": "All models are callable, just like layers\nYou can treat any model as if it were a layer by invoking it on an Input or on the output of another layer. By calling a model you aren’t just reusing the architecture of the model, you’re also reusing its weights.\nTo see this in action, here’s a different take on the autoencoder example that creates an encoder model, a decoder model, and chains them in two calls to obtain the autoencoder model:\n\nencoder_input <- layer_input(shape = c(28, 28, 1), name = \"original_img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, name = \"encoder\")\nencoder\n\nModel: \"encoder\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n original_img (InputLayer)          [(None, 28, 28, 1)]             0           \n conv2d_7 (Conv2D)                  (None, 26, 26, 16)              160         \n conv2d_6 (Conv2D)                  (None, 24, 24, 32)              4640        \n max_pooling2d_1 (MaxPooling2D)     (None, 8, 8, 32)                0           \n conv2d_5 (Conv2D)                  (None, 6, 6, 32)                9248        \n conv2d_4 (Conv2D)                  (None, 4, 4, 16)                4624        \n global_max_pooling2d_1 (GlobalMaxP  (None, 16)                     0           \n ooling2D)                                                                      \n================================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n________________________________________________________________________________\n\ndecoder_input <- layer_input(shape = c(16), name = \"encoded_img\")\ndecoder_output <- decoder_input %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\ndecoder <- keras_model(decoder_input, decoder_output, \n                       name = \"decoder\")\ndecoder\n\nModel: \"decoder\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n encoded_img (InputLayer)           [(None, 16)]                    0           \n reshape_1 (Reshape)                (None, 4, 4, 1)                 0           \n conv2d_transpose_7 (Conv2DTranspos  (None, 6, 6, 16)               160         \n e)                                                                             \n conv2d_transpose_6 (Conv2DTranspos  (None, 8, 8, 32)               4640        \n e)                                                                             \n up_sampling2d_1 (UpSampling2D)     (None, 24, 24, 32)              0           \n conv2d_transpose_5 (Conv2DTranspos  (None, 26, 26, 16)             4624        \n e)                                                                             \n conv2d_transpose_4 (Conv2DTranspos  (None, 28, 28, 1)              145         \n e)                                                                             \n================================================================================\nTotal params: 9,569\nTrainable params: 9,569\nNon-trainable params: 0\n________________________________________________________________________________\n\nautoencoder_input <- layer_input(shape = c(28, 28, 1), name = \"img\")\nencoded_img <- encoder(autoencoder_input)\ndecoded_img <- decoder(encoded_img)\nautoencoder <- keras_model(autoencoder_input, decoded_img, \n                           name = \"autoencoder\")\nautoencoder\n\nModel: \"autoencoder\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n img (InputLayer)                   [(None, 28, 28, 1)]             0           \n encoder (Functional)               (None, 16)                      18672       \n decoder (Functional)               (None, 28, 28, 1)               9569        \n================================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nAs you can see, the model can be nested: a model can contain sub-models (since a model is just like a layer). A common use case for model nesting is ensembling. For example, here’s how to ensemble a set of models into a single model that averages their predictions:\n\nget_model <- function() {\n  inputs <- layer_input(shape = c(128))\n  outputs <- inputs %>% layer_dense(1)\n  keras_model(inputs, outputs)\n}\n\nmodel1 <- get_model()\nmodel2 <- get_model()\nmodel3 <- get_model()\n\ninputs <- layer_input(shape = c(128))\ny1 <- model1(inputs)\ny2 <- model2(inputs)\ny3 <- model3(inputs)\noutputs <- layer_average(list(y1, y2, y3))\nensemble_model <- keras_model(inputs = inputs, outputs = outputs)"
  },
  {
    "objectID": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "href": "guides/keras/functional_api.html#manipulate-complex-graph-topologies",
    "title": "The Functional API",
    "section": "Manipulate complex graph topologies",
    "text": "Manipulate complex graph topologies\n\nModels with multiple inputs and outputs\nThe functional API makes it easy to manipulate multiple inputs and outputs. This cannot be handled with the Sequential API.\nFor example, if you’re building a system for ranking customer issue tickets by priority and routing them to the correct department, then the model will have three inputs:\n\nthe title of the ticket (text input),\nthe text body of the ticket (text input), and\nany tags added by the user (categorical input)\n\nThis model will have two outputs:\n\nthe priority score between 0 and 1 (scalar sigmoid output), and\nthe department that should handle the ticket (softmax output over the set of departments).\n\nYou can build this model in a few lines with the functional API:\n\nnum_tags <- 12  # Number of unique issue tags\nnum_words <- 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments <- 4  # Number of departments for predictions\n\ntitle_input <- layer_input(shape = c(NA), name = \"title\")  # Variable-length sequence of ints\nbody_input <- layer_input(shape = c(NA), name = \"body\")  # Variable-length sequence of ints\ntags_input <- layer_input(shape = c(num_tags), name = \"tags\")  # Binary vectors of size `num_tags`\n\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features <- title_input %>% layer_embedding(num_words, 64)\n\n# Embed each word in the text into a 64-dimensional vector\nbody_features <- body_input %>% layer_embedding(num_words, 64)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features <- title_features %>% layer_lstm(128)\n\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features <- body_features %>% layer_lstm(32)\n\n# Merge all available features into a single large vector via concatenation\nx <- layer_concatenate(list(title_features, body_features, tags_input))\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred <- x %>% layer_dense(1, name = \"priority\")\n\n# Stick a department classifier on top of the features\ndepartment_pred <- x %>% layer_dense(num_departments, name = \"department\")\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel <- keras_model(\n  inputs <- list(title_input, body_input, tags_input),\n  outputs <- list(priority_pred, department_pred)\n)\n\nNow plot the model:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nWhen compiling this model, you can assign different losses to each output. You can even assign different weights to each loss – to modulate their contribution to the total training loss.\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_binary_crossentropy(from_logits = TRUE),\n    loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights <- c(1, 0.2)\n)\n\nSince the output layers have different names, you could also specify the losses and loss weights with the corresponding layer names:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    priority = loss_binary_crossentropy(from_logits = TRUE),\n    department = loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights = c(priority =  1.0, department = 0.2),\n)\n\nTrain the model by passing lists of NumPy arrays of inputs and targets:\n\n# some helpers to generate dummy input data\nrandom_uniform_array <- function(dim) \n  array(runif(prod(dim)), dim)\n\nrandom_vectorized_array <- function(num_words, dim)\n  array(sample(0:(num_words - 1), prod(dim), replace = TRUE), dim)\n\n# Dummy input data\ntitle_data <- random_vectorized_array(num_words, c(1280, 10))\nbody_data <- random_vectorized_array(num_words, c(1280, 100))\ntags_data <- random_vectorized_array(2, c(1280, num_tags))\n# storage.mode(tags_data) <- \"double\" # from integer\n\n# Dummy target data\npriority_targets <- random_uniform_array(c(1280, 1))\ndept_targets <- random_vectorized_array(2, c(1280, num_departments))\n\nmodel %>% fit(\n  list(title = title_data, body = body_data, tags = tags_data),\n  list(priority = priority_targets, department = dept_targets),\n  epochs = 2,\n  batch_size = 32\n)\n\nWhen calling fit with a tfdataset object, it should yield either a tuple of lists like tuple(list(title_data, body_data, tags_data), list(priority_targets, dept_targets)) or a tuple of named lists like tuple(list(title = title_data, body = body_data, tags = tags_data), list(priority= priority_targets, department= dept_targets)).\nFor more detailed explanation, refer to the training and evaluation guide.\n\n\nA toy ResNet model\nIn addition to models with multiple inputs and outputs, the functional API makes it easy to manipulate non-linear connectivity topologies – these are models with layers that are not connected sequentially, which the Sequential API cannot handle.\nA common use case for this is residual connections. Let’s build a toy ResNet model for CIFAR10 to demonstrate this:\n\ninputs <- layer_input(shape = c(32, 32, 3), name = \"img\")\nblock_1_output <- inputs %>% \n  layer_conv_2d(32, 3, activation = \"relu\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\") %>% \n  layer_max_pooling_2d(3)\n\nblock_2_output <- block_1_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_1_output)\n\nblock_3_output <- block_2_output %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>% \n  layer_add(block_2_output) \n\noutputs <- block_3_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\") %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(10)\n\nmodel <- keras_model(inputs, outputs, name = \"toy_resnet\")\nmodel\n\nModel: \"toy_resnet\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n img (InputLayer)         [(None, 32, 32,   0        []                         \n                          3)]                                                   \n conv2d_9 (Conv2D)        (None, 30, 30, 3  896      ['img[0][0]']              \n                          2)                                                    \n conv2d_8 (Conv2D)        (None, 28, 28, 6  18496    ['conv2d_9[0][0]']         \n                          4)                                                    \n max_pooling2d_2 (MaxPool  (None, 9, 9, 64)  0       ['conv2d_8[0][0]']         \n ing2D)                                                                         \n conv2d_11 (Conv2D)       (None, 9, 9, 64)  36928    ['max_pooling2d_2[0][0]']  \n conv2d_10 (Conv2D)       (None, 9, 9, 64)  36928    ['conv2d_11[0][0]']        \n add (Add)                (None, 9, 9, 64)  0        ['conv2d_10[0][0]',        \n                                                      'max_pooling2d_2[0][0]']  \n conv2d_13 (Conv2D)       (None, 9, 9, 64)  36928    ['add[0][0]']              \n conv2d_12 (Conv2D)       (None, 9, 9, 64)  36928    ['conv2d_13[0][0]']        \n add_1 (Add)              (None, 9, 9, 64)  0        ['conv2d_12[0][0]',        \n                                                      'add[0][0]']              \n conv2d_14 (Conv2D)       (None, 7, 7, 64)  36928    ['add_1[0][0]']            \n global_average_pooling2d  (None, 64)       0        ['conv2d_14[0][0]']        \n  (GlobalAveragePooling2D                                                       \n )                                                                              \n dense_8 (Dense)          (None, 256)       16640    ['global_average_pooling2d[\n                                                     0][0]']                    \n dropout (Dropout)        (None, 256)       0        ['dense_8[0][0]']          \n dense_7 (Dense)          (None, 10)        2570     ['dropout[0][0]']          \n================================================================================\nTotal params: 223,242\nTrainable params: 223,242\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nPlot the model:\n\nplot(model, show_shapes = TRUE)\n\n\n\n\nNow train the model:\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()  \n\nx_train <- x_train / 255\nx_test <- x_test / 255\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = loss_categorical_crossentropy(from_logits = TRUE),\n  metrics = \"acc\"\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# for this guide. Try to train on the entire dataset until convergence!\nmodel %>% fit(\n  x_train[1:1000, , , ],\n  y_train[1:1000, ],\n  batch_size = 64,\n  epochs = 1,\n  validation_split = 0.2\n)"
  },
  {
    "objectID": "guides/keras/functional_api.html#shared-layers",
    "href": "guides/keras/functional_api.html#shared-layers",
    "title": "The Functional API",
    "section": "Shared layers",
    "text": "Shared layers\nAnother good use for the functional API are models that use shared layers. Shared layers are layer instances that are reused multiple times in the same model – they learn features that correspond to multiple paths in the graph-of-layers.\nShared layers are often used to encode inputs from similar spaces (say, two different pieces of text that feature similar vocabulary). They enable sharing of information across these different inputs, and they make it possible to train such a model on less data. If a given word is seen in one of the inputs, that will benefit the processing of all inputs that pass through the shared layer.\nTo share a layer in the functional API, call the same layer instance multiple times. For instance, here’s an Embedding layer shared across two different text inputs:\n\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)\n\n# Variable-length sequence of integers\ntext_input_a <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Variable-length sequence of integers\ntext_input_b <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a <- shared_embedding(text_input_a)\nencoded_input_b <- shared_embedding(text_input_b)"
  },
  {
    "objectID": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "href": "guides/keras/functional_api.html#extract-and-reuse-nodes-in-the-graph-of-layers",
    "title": "The Functional API",
    "section": "Extract and reuse nodes in the graph of layers",
    "text": "Extract and reuse nodes in the graph of layers\nBecause the graph of layers you are manipulating is a static data structure, it can be accessed and inspected. And this is how you are able to plot functional models as images.\nThis also means that you can access the activations of intermediate layers (“nodes” in the graph) and reuse them elsewhere – which is very useful for something like feature extraction.\nLet’s look at an example. This is a VGG19 model with weights pretrained on ImageNet:\n\nvgg19 <- application_vgg19()\n\nAnd these are the intermediate activations of the model, obtained by querying the graph data structure:\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\n\nUse these features to create a new feature-extraction model that returns the values of the intermediate layer activations:\n\nfeat_extraction_model <-  keras_model(inputs = vgg19$input, \n                                      outputs = features_list)\n\nimg <- random_uniform_array(c(1, 224, 224, 3))\nextracted_features <- feat_extraction_model(img)\n\nThis comes in handy for tasks like neural style transfer, among other things."
  },
  {
    "objectID": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "href": "guides/keras/functional_api.html#extend-the-api-using-custom-layers",
    "title": "The Functional API",
    "section": "Extend the API using custom layers",
    "text": "Extend the API using custom layers\ntf$keras includes a wide range of built-in layers, for example:\n\nConvolutional layers: Conv1D, Conv2D, Conv3D, Conv2DTranspose\nPooling layers: MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D\nRNN layers: GRU, LSTM, ConvLSTM2D\nBatchNormalization, Dropout, Embedding, etc.\n\nBut if you don’t find what you need, it’s easy to extend the API by creating your own layers. All layers subclass the Layer class and implement:\n\ncall method, that specifies the computation done by the layer.\nbuild method, that creates the weights of the layer (this is just a style convention since you can create weights in __init__, as well).\n\nTo learn more about creating layers from scratch, read custom layers and models guide.\nThe following is a basic implementation of layer_dense():\n\nlibrary(tensorflow)\nlibrary(keras)\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units = as.integer(units)\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nmodel <- keras_model(inputs, outputs)\n\nFor serialization support in your custom layer, define a get_config method that returns the constructor arguments of the layer instance:\n\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units <- as.integer(units)\n  },\n  \n  build = function(input_shape) {\n    self$w <-\n      self$add_weight(\n        shape = shape(tail(input_shape, 1), self$units),\n        initializer = \"random_normal\",\n        trainable = TRUE\n      )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  \n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  \n  get_config = function() {\n    list(units = self$units)\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nmodel <- keras_model(inputs, outputs)\nconfig <- model %>% get_config()\n\nnew_model <- from_config(config, custom_objects = list(layer_custom_dense))\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\n\nOptionally, implement the class method from_config(class_constructor, config) which is used when recreating a layer instance given its config. The default implementation of from_config is approximately:\n\nfrom_config <- function(layer_constructor, config) \n  do.call(layer_constructor, config)"
  },
  {
    "objectID": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "href": "guides/keras/functional_api.html#when-to-use-the-functional-api",
    "title": "The Functional API",
    "section": "When to use the functional API",
    "text": "When to use the functional API\nShould you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier and safer, and has a number of features that subclassed models do not support.\nHowever, model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass Model directly.\nFor an in-depth look at the differences between the functional API and model subclassing, read What are Symbolic and Imperative APIs in TensorFlow 2.0?.\n\nFunctional API strengths:\nThe following properties are also true for Sequential models (which are also data structures), but are not true for subclassed models (which are R code, not data structures).\n\nLess verbose\nThere is no super$initialize(...), no call <- function(...) {   }, etc.\nCompare:\n\ninputs <- layer_input(shape = c(32))\noutputs <- inputs %>% \n  layer_dense(64, activation = 'relu') %>% \n  layer_dense(10)\nmlp <- keras_model(inputs, outputs)\n\nWith the subclassed version:\n\nMLP <- new_model_class(\n  classname = \"MLP\",\n  \n  initialize = function(...) {\n    super$initialize(...)\n    self$dense_1 <- layer_dense(units = 64, activation = 'relu')\n    self$dense_2 <- layer_dense(units = 10)\n  },\n  \n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2()\n  }\n)\n\n# Instantiate the model.\nmlp <- MLP()\n\n# Necessary to create the model's state.\n# The model doesn't have a state until it's called at least once.\ninvisible(mlp(tf$zeros(shape(1, 32))))\n\n\n\nModel validation while defining its connectivity graph\nIn the functional API, the input specification (shape and dtype) is created in advance (using layer_input). Every time you call a layer, the layer checks that the specification passed to it matches its assumptions, and it will raise a helpful error message if not.\nThis guarantees that any model you can build with the functional API will run. All debugging – other than convergence-related debugging – happens statically during the model construction and not at execution time. This is similar to type checking in a compiler.\n\n\nA functional model is plottable and inspectable\nYou can plot the model as a graph, and you can easily access intermediate nodes in this graph. For example, to extract and reuse the activations of intermediate layers (as seen in a previous example):\n\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\nfeat_extraction_model <- keras_model(inputs = vgg19$input,\n                                     outputs = features_list)\n\n\n\nA functional model can be serialized or cloned\nBecause a functional model is a data structure rather than a piece of code, it is safely serializable and can be saved as a single file that allows you to recreate the exact same model without having access to any of the original code. See the serialization & saving guide.\nTo serialize a subclassed model, it is necessary for the implementer to specify a get_config() and from_config() method at the model level.\n\n\n\nFunctional API weakness:\n\nIt does not support dynamic architectures\nThe functional API treats models as DAGs of layers. This is true for most deep learning architectures, but not all – for example, recursive networks or Tree RNNs do not follow this assumption and cannot be implemented in the functional API."
  },
  {
    "objectID": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "href": "guides/keras/functional_api.html#mix-and-match-api-styles",
    "title": "The Functional API",
    "section": "Mix-and-match API styles",
    "text": "Mix-and-match API styles\nChoosing between the functional API or Model subclassing isn’t a binary decision that restricts you into one category of models. All models in the tf$keras API can interact with each other, whether they’re Sequential models, functional models, or subclassed models that are written from scratch.\nYou can always use a functional model or Sequential model as part of a subclassed model or layer:\n\nunits <- 32L\ntimesteps <- 10L\ninput_dim <- 5L\n\n# Define a Functional model\n\ninputs <- layer_input(c(NA, units))\noutputs <- inputs %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\n\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <-\n      layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <-\n      layer_dense(units = units, activation = \"tanh\")\n    # Our previously-defined Functional model\n    self$classifier <- model\n  },\n  \n  call = function(inputs) {\n    message(\"inputs shape: \", format(inputs$shape))\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis is 1-based\n    message(\"features shape: \", format(features$shape))\n    self$classifier(features)\n  }\n)\n\nlayer_custom_rnn(tf$zeros(shape(1, timesteps, input_dim)))\n\ninputs shape: (1, 10, 5)\n\n\nfeatures shape: (1, 10, 32)\n\n\nYou can use any subclassed layer or model in the functional API as long as it implements a call method that follows one of the following patterns:\n\ncall(inputs, ..., training = NULL, mask = NULL) – Where inputs is a tensor or a nested structure of tensors (e.g. a list of tensors), and where optional named arguments training and mask can be present.\nare non-tensor arguments (non-inputs).\ncall(self, inputs, training = NULL, **kwargs) – Where training is a boolean indicating whether the layer should behave in training mode and inference mode.\ncall(self, inputs, mask = NULL, **kwargs) – Where mask is a boolean mask tensor (useful for RNNs, for instance).\ncall(self, inputs, training = NULL, mask = NULL, **kwargs) – Of course, you can have both masking and training-specific behavior at the same time.\n\nAdditionally, if you implement the get_config method on your custom Layer or model, the functional models you create will still be serializable and cloneable.\nHere’s a quick example of a custom RNN, written from scratch, being used in a functional model:\n\nunits <- 32 \ntimesteps <- 10 \ninput_dim <- 5 \nbatch_size <- 16\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units  \n    self$projection_1 <- layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <- layer_dense(units = units, activation = \"tanh\")\n    self$classifier <- layer_dense(units = 1)\n  },\n  \n  call = function(inputs) {\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis arg is 1-based\n    self$classifier(features)\n  }\n)\n    \n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs <- layer_input(batch_shape = c(batch_size, timesteps, input_dim))\noutputs <- inputs %>% \n  layer_conv_1d(32, 3) %>% \n  layer_custom_rnn()\n\nmodel <- keras_model(inputs, outputs)\nmodel(tf$zeros(shape(1, 10, 5)))\n\ntf.Tensor(\n[[[0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]]], shape=(1, 8, 1), dtype=float32)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "",
    "text": "library(magrittr)\n\nWarning: package 'magrittr' was built under R version 4.1.2\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n\nLoaded Tensorflow version 2.9.1\n\n\n[1] '2.9'"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-layer-class-a-combination-of-state-weights-and-some-computation",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Layer class: a combination of state (weights) and some computation",
    "text": "The Layer class: a combination of state (weights) and some computation\nOne of the central abstractions in Keras is the Layer class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).\nHere’s a densely-connected layer. It has a state: the variables w and b.\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \"float32\"\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \"float32\"),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nYou would use a layer by calling it on some tensor input(s), much like a regular function.\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[-0.06255919  0.1504314  -0.05702312 -0.08124617]\n [-0.06255919  0.1504314  -0.05702312 -0.08124617]], shape=(2, 4), dtype=float32)\n\n\nLinear behaves similarly to a layer present in the Python interface to keras (e.g., keras$layers$Dense).\nHowever, one additional step is needed to make it behave like the builtin layers present in the keras R package (e.g., layer_dense()).\nKeras layers in R are designed to compose nicely with the pipe operator (%>%), so that the layer instance is conveniently created on demand when an existing model or tensor is piped in. In order to make a custom layer similarly compose nicely with the pipe, you can call create_layer_wrapper() on the layer class constructor.\n\nlayer_linear <- create_layer_wrapper(Linear)\n\nNow layer_linear is a layer constructor that composes nicely with %>%, just like the built-in layers:\n\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n\ntf.Tensor(\n[[-0.02026559 -0.04281025  0.04358904 -0.08537429]\n [-0.02026559 -0.04281025  0.04358904 -0.08537429]], shape=(2, 4), dtype=float32)\n\nmodel\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n linear_1 (Linear)                  (2, 4)                          12          \n================================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nBecause the pattern above is so common, there is a convenience function that combines the steps of subclassing keras$layers$Layer and calling create_layer_wrapper on the output: the Layer function. The layer_linear defined below is identical to the layer_linear defined above.\n\nlayer_linear <- Layer(\n  \"Linear\",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \"float32\"),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\nFor the remainder of this vignette we’ll be using the %py_class% constructor. However, in your own code feel free to use create_layer_wrapper and/or Layer if you prefer.\nNote that the weights w and b are automatically tracked by the layer upon being set as layer attributes:\n\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n\nYou also have access to a quicker shortcut for adding a weight to a layer: the add_weight() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n\ntf.Tensor(\n[[ 0.04569551  0.17757826 -0.04430382 -0.0238559 ]\n [ 0.04569551  0.17757826 -0.04430382 -0.0238559 ]], shape=(2, 4), dtype=float32)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-can-have-non-trainable-weights",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers can have non-trainable weights",
    "text": "Layers can have non-trainable weights\nBesides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\nHere’s how to add and use a non-trainable weight:\n\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 2 2\n\ny <- my_sum(x)\nprint(as.numeric(y))\n\n[1] 4 4\n\n\nIt’s part of layer$weights, but it gets categorized as a non-trainable weight:\n\ncat(\"weights:\", length(my_sum$weights), \"\\n\")\n\nweights: 1 \n\ncat(\"non-trainable weights:\", length(my_sum$non_trainable_weights), \"\\n\")\n\nnon-trainable weights: 1 \n\n# It's not included in the trainable weights:\ncat(\"trainable_weights:\", my_sum$trainable_weights, \"\\n\")\n\ntrainable_weights:"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Best practice: deferring weight creation until the shape of the inputs is known",
    "text": "Best practice: deferring weight creation until the shape of the inputs is known\nOur Linear layer above took an input_dimargument that was used to compute the shape of the weights w and b in initialize():\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \"zeros\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nIn many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\nIn the Keras API, we recommend creating layer weights in the build(self, inputs_shape) method of your layer. Like this:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nThe build() method of your layer will automatically run the first time your layer instance is called. You now have a layer that can handle an arbitrary number of input features:\n\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\n\nImplementing build() separately as shown above nicely separates creating weights only once from using weights in every call. However, for some advanced custom layers, it can become impractical to separate the state creation and computation. Layer implementers are allowed to defer weight creation to the first call(), but need to take care that later calls use the same weights. In addition, since call() is likely to be executed for the first time inside a tf_function(), any variable creation that takes place in call() should be wrapped in a tf$init_scope()."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#layers-are-recursively-composable",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Layers are recursively composable",
    "text": "Layers are recursively composable\nIf you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer.\nWe recommend creating such sublayers in the initialize() method and leave it to the first call() to trigger building their weights.\n\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\ncat(\"weights:\", length(mlp$weights), \"\\n\")\n\nweights: 6 \n\ncat(\"trainable weights:\", length(mlp$trainable_weights), \"\\n\")\n\ntrainable weights: 6"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_loss-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_loss() method",
    "text": "The add_loss() method\nWhen writing the call() method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling self$add_loss(value):\n\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n\nThese losses (including those created by any inner layer) can be retrieved via layer$losses. This property is reset at the start of every call() to the top-level layer, so that layer$losses always contains the loss values created during the last forward pass.\n\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n\nIn addition, the loss property also contains regularization losses created for the weights of any inner layer:\n\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n\n[[1]]\ntf.Tensor(0.0021208026, shape=(), dtype=float32)\n\n\nThese losses are meant to be taken into account when writing training loops, like this:\n\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n\nFor a detailed guide about writing training loops, see the guide to writing a training loop from scratch.\nThese losses also work seamlessly with fit() (they get automatically summed and added to the main loss, if any):\n\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \"adam\")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-add_metric-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The add_metric() method",
    "text": "The add_metric() method\nSimilarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training.\nConsider the following layer: a “logistic endpoint” layer. It takes as inputs predictions and targets, it computes a loss which it tracks via add_loss(), and it computes an accuracy scalar, which it tracks via add_metric().\n\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n\nMetrics tracked in this way are accessible via layer$metrics:\n\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\"layer$metrics: \")\n\nlayer$metrics: \n\nstr(layer$metrics)\n\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n\ncat(\"current accuracy value:\", as.numeric(layer$metrics[[1]]$result()), \"\\n\")\n\ncurrent accuracy value: 1 \n\n\nJust like for add_loss(), these metrics are tracked by fit():\n\ninputs <- layer_input(shape(3), name = \"inputs\")\ntargets <- layer_input(shape(10), name = \"targets\")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#you-can-optionally-enable-serialization-on-your-layers",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "You can optionally enable serialization on your layers",
    "text": "You can optionally enable serialization on your layers\nIf you need your custom layers to be serializable as part of a Functional model, you can optionally implement a get_config() method:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n\n$units\n[1] 64\n\nnew_layer <- Linear$from_config(config)\n\nNote that the initialize() method of the base Layer class takes some additional named arguments, in particular a name and a dtype. It’s good practice to pass these arguments to the parent class in initialize() and to include them in the layer config:\n\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n\nList of 4\n $ name     : chr \"linear_9\"\n $ trainable: logi TRUE\n $ dtype    : chr \"float32\"\n $ units    : num 64\n\nnew_layer <- Linear$from_config(config)\n\nIf you need more flexibility when deserializing the layer from its config, you can also override the from_config() class method. This is the base implementation of from_config():\n\nfrom_config <- function(cls, config) do.call(cls, config)\n\nTo learn more about serialization and saving, see the complete guide to saving and serializing models."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-training-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged training argument in the call() method",
    "text": "Privileged training argument in the call() method\nSome layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call() method.\nBy exposing this argument in call(), you enable the built-in training and evaluation loops (e.g. fit()) to correctly use the layer in training and inference. Note, the default of NULL means that the training parameter will be inferred by keras from the training context (e.g., it will be TRUE if called from fit(), FALSE if called from predict())\n\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#privileged-mask-argument-in-the-call-method",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Privileged mask argument in the call() method",
    "text": "Privileged mask argument in the call() method\nThe other privileged argument supported by call() is the mask argument.\nYou will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\nKeras will automatically pass the correct mask argument to call() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\nTo learn more about masking and how to write masking-enabled layers, please check out the guide “understanding padding and masking”."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#the-model-class",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "The Model class",
    "text": "The Model class\nIn general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model – the object you will train.\nFor instance, in a ResNet50 model, you would have several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.\nThe Model class has the same API as Layer, with the following differences:\n\nIt has support for built-in training, evaluation, and prediction methods (fit(), evaluate(), predict()).\nIt exposes the list of its inner layers, via the model$layers property.\nIt exposes saving and serialization APIs (save_model_tf(), save_model_weights_tf(), …)\n\nEffectively, the Layer class corresponds to what we refer to in the literature as a “layer” (as in “convolution layer” or “recurrent layer”) or as a “block” (as in “ResNet block” or “Inception block”).\nMeanwhile, the Model class corresponds to what is referred to in the literature as a “model” (as in “deep learning model”) or as a “network” (as in “deep neural network”).\nSo if you’re wondering, “should I use the Layer class or the Model class?”, ask yourself: will I need to call fit() on it? Will I need to call save() on it? If so, go with Model. If not (either because your class is just a block in a bigger system, or because you are writing training & saving code yourself), use Layer.\nFor instance, we could take our mini-resnet example above, and use it to build a Model that we could train with fit(), and that we could save with save_model_weights_tf():\n\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#putting-it-all-together-an-end-to-end-example",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Putting it all together: an end-to-end example",
    "text": "Putting it all together: an end-to-end example\nHere’s what you’ve learned so far:\n\nA Layer encapsulates a state (created in initialize() or build()), and some computation (defined in call()).\nLayers can be recursively nested to create new, bigger computation blocks.\nLayers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\nThe outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n\nLet’s put all of these things together into an end-to-end example: we’re going to implement a Variational AutoEncoder (VAE). We’ll train it on MNIST digits.\nOur VAE will be a subclass of Model, built as a nested composition of layers that subclass Layer. It will feature a regularization loss (KL divergence).\n\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \"encoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \"Converts z, the encoded digit vector, back into a readable digit.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \"decoder\", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")\n    self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \"Combines the encoder and decoder into an end-to-end model for training.\"\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \"autoencoder\", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n\nLet’s write a simple training loop on MNIST:\n\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\"Start of epoch %d\\n\", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\"step %d: mean loss = %.4f\\n\", step, loss_metric$result()))\n      }\n    }\n  })\n}\n\nStart of epoch 1\nstep 100: mean loss = 0.1267\nstep 200: mean loss = 0.0997\nstep 300: mean loss = 0.0894\nstep 400: mean loss = 0.0844\nstep 500: mean loss = 0.0811\nstep 600: mean loss = 0.0789\nstep 700: mean loss = 0.0773\nstep 800: mean loss = 0.0761\nstep 900: mean loss = 0.0750\nStart of epoch 2\nstep 100: mean loss = 0.0741\nstep 200: mean loss = 0.0736\nstep 300: mean loss = 0.0731\nstep 400: mean loss = 0.0727\nstep 500: mean loss = 0.0723\nstep 600: mean loss = 0.0720\nstep 700: mean loss = 0.0717\nstep 800: mean loss = 0.0715\nstep 900: mean loss = 0.0712\n\n\nNote that since the VAE is subclassing Model, it features built-in training loops. So you could also have trained it like this:\n\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)"
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#beyond-object-oriented-development-the-functional-api",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Beyond object-oriented development: the Functional API",
    "text": "Beyond object-oriented development: the Functional API\nIf you prefer a less object-oriented way of programming, you can also build models using the Functional API. Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\nFor instance, the Functional API example below reuses the same Sampling layer we defined in the example above:\n\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \"encoder_input\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \"z_mean\")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \"z_log_var\")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \"encoder\")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \"z_sampling\")\nx <- layer_dense(units = intermediate_dim, activation = \"relu\")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \"sigmoid\")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \"decoder\")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \"vae\")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n\nFor more information, make sure to read the Functional API guide."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#defining-custom-layers-and-models-in-an-r-package",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Defining custom layers and models in an R package",
    "text": "Defining custom layers and models in an R package\nUnfortunately you can’t use anything that creates references to Python objects, at the top-level of an R package.\nHere is why: when you build an R package, all the R files in the R/ directory get sourced in an R environment (the package namespace), and then that environment is saved as part of the package bundle. Loading the package means restoring the saved R environment. This means that the R code only gets sourced once, at build time. If you create references to external objects (e.g., Python objects) at package build time, they will be NULL pointers when the package is loaded, because the external objects they pointed to at build time no longer exist at load time.\nThe solution is to delay creating references to Python objects until run time. Fortunately, %py_class%, Layer(), and create_layer_wrapper(R6Class(...)) are all lazy about initializing the Python reference, so they are safe to define and export in an R package.\nIf you’re writing an R package that uses keras and reticulate, this article might be helpful to read over."
  },
  {
    "objectID": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "href": "guides/keras/making_new_layers_and_models_via_subclassing.html#summary",
    "title": "Writing Layer and Model objects from scratch.",
    "section": "Summary",
    "text": "Summary\nIn this guide you learned about creating custom layers and models in keras.\n\nThe constructors available: new_layer_class(), %py_class%, create_layer_wrapper(), R6Class(), Layer().\nWhat methods to you might want to define to your model: initialize(), build(), call(), and get_config().\nWhat convenience methods are available when you subclass keras$layers$Layer: add_weight(), add_loss(), and add_metric()"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html",
    "href": "guides/keras/preprocessing_layers.html",
    "title": "Working with preprocessing layers",
    "section": "",
    "text": "library(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "href": "guides/keras/preprocessing_layers.html#keras-preprocessing",
    "title": "Working with preprocessing layers",
    "section": "Keras preprocessing",
    "text": "Keras preprocessing\nThe Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "href": "guides/keras/preprocessing_layers.html#available-preprocessing-layers",
    "title": "Working with preprocessing layers",
    "section": "Available preprocessing layers",
    "text": "Available preprocessing layers\n\nText preprocessing\n\nlayer_text_vectorization(): turns raw strings into an encoded representation that can be read by a layer_embedding() or layer_dense() layer.\n\n\n\nNumerical features preprocessing\n\nlayer_normalization(): performs feature-wise normalization of input features.\nlayer_discretization(): turns continuous numerical features into integer categorical features.\n\n\n\nCategorical features preprocessing\n\nlayer_category_encoding(): turns integer categorical features into one-hot, multi-hot, or count-based, dense representations.\nlayer_hashing(): performs categorical feature hashing, also known as the “hashing trick”.\nlayer_string_lookup(): turns string categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\nlayer_integer_lookup(): turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\n\n\nImage preprocessing\nThese layers are for standardizing the inputs of an image model.\n\nlayer_resizing(): resizes a batch of images to a target size.\nlayer_rescaling(): rescales and offsets the values of a batch of images (e.g., going from inputs in the [0, 255] range to inputs in the [0, 1] range.\nlayer_center_crop(): returns a center crop of a batch of images.\n\n\n\nImage data augmentation\nThese layers apply random augmentation transforms to a batch of images. They are only active during training.\n\nlayer_random_crop()\nlayer_random_flip()\nlayer_random_flip()\nlayer_random_translation()\nlayer_random_rotation()\nlayer_random_zoom()\nlayer_random_height()\nlayer_random_width()\nlayer_random_contrast()"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "href": "guides/keras/preprocessing_layers.html#the-adapt-function",
    "title": "Working with preprocessing layers",
    "section": "The adapt() function",
    "text": "The adapt() function\nSome preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n\nlayer_text_vectorization(): holds a mapping between string tokens and integer indices\nlayer_string_lookup() and layer_integer_lookup(): hold a mapping between input values and integer indices.\nlayer_normalization(): holds the mean and standard deviation of the features.\nlayer_discretization(): holds information about value bucket boundaries.\n\nCrucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by “adapting” them on data.\nYou set the state of a preprocessing layer by exposing it to training data, via adapt():\n\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n\nLoaded Tensorflow version 2.9.1\n\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\"Features mean: %.2f\", mean(normalized_data))\n\n[1] \"Features mean: -0.00\"\n\nsprintf(\"Features std: %.2f\", sd(normalized_data))\n\n[1] \"Features std: 1.06\"\n\n\nadapt() takes either an array or a tf_dataset. In the case of layer_string_lookup() and layer_text_vectorization(), you can also pass a character vector:\n\ndata <- c(\n  \"Congratulations!\",\n  \"Today is your day.\",\n  \"You're off to Great Places!\",\n  \"You're off and away!\",\n  \"You have brains in your head.\",\n  \"You have feet in your shoes.\",\n  \"You can steer yourself\",\n  \"any direction you choose.\",\n  \"You're on your own. And you know what you know.\",\n  \"And YOU are the one who'll decide where to go.\"\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n\n\nIn addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the adapt() call, they can be set without relying on the layer’s internal computation. For instance, if external vocabulary files for the layer_text_vectorization(), layer_string_lookup(), or layer_integer_lookup() layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer’s constructor arguments.\nHere’s an example where we instantiate a layer_string_lookup() layer with precomputed vocabulary:\n\nvocab <- c(\"a\", \"b\", \"c\", \"d\")\ndata <- as_tensor(rbind(c(\"a\", \"c\", \"d\"),\n                        c(\"d\", \"z\", \"b\")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-data-before-the-model-or-inside-the-model",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing data before the model or inside the model",
    "text": "Preprocessing data before the model or inside the model\nThere are two ways you could be using preprocessing layers:\nOption 1: Make them part of the model, like this:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n\nWith this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you’re training on GPU, this is the best option for the layer_normalization() layer, and for all image preprocessing and data augmentation layers.\nOption 2: apply it to your tf_dataset, so as to obtain a dataset that yields batches of preprocessed data, like this:\n\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\nWith this option, your preprocessing will happen on CPU, asynchronously, and will be buffered before going into the model. In addition, if you call tfdatasets::dataset_prefetch() on your dataset, the preprocessing will happen efficiently in parallel with training:\n\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n\nThis is the best option for layer_text_vectorization(), and all structured data preprocessing layers. It can also be a good option if you’re training on CPU and you use image preprocessing layers."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "href": "guides/keras/preprocessing_layers.html#benefits-of-doing-preprocessing-inside-the-model-at-inference-time",
    "title": "Working with preprocessing layers",
    "section": "Benefits of doing preprocessing inside the model at inference time",
    "text": "Benefits of doing preprocessing inside the model at inference time\nEven if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew.\nWhen all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to [-1, +1] or to [0, 1], etc. This is especially powerful if you’re exporting your model to another runtime, such as TensorFlow.js: you won’t have to reimplement your preprocessing pipeline in JavaScript.\nIf you initially put your preprocessing layers in your tf_dataset pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "href": "guides/keras/preprocessing_layers.html#preprocessing-during-multi-worker-training",
    "title": "Working with preprocessing layers",
    "section": "Preprocessing during multi-worker training",
    "text": "Preprocessing during multi-worker training\nPreprocessing layers are compatible with the tf.distribute API for running training across multiple machines.\nIn general, preprocessing layers should be placed inside a strategy$scope() and called either inside or before the model as discussed above.\n\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n\nFor more details, refer to the preprocessing section of the distributed input guide."
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#quick-recipes",
    "href": "guides/keras/preprocessing_layers.html#quick-recipes",
    "title": "Working with preprocessing layers",
    "section": "Quick recipes",
    "text": "Quick recipes\n\nImage data augmentation\nNote that image data augmentation layers are only active during training (similar to the layer_dropout() layer).\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n\nYou can see a similar setup in action in the example image classification from scratch.\n\n\nNormalizing numerical features\n\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"adam\",\n          loss = \"sparse_categorical_crossentropy\")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n\n\n\nEncoding string categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(c(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\"a\", \"b\", \"c\", \"d\", \"e\", \"\")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n\n\nNote that, here, index 0 is reserved for out-of-vocabulary values (values that were not seen during adapt()).\nYou can see the layer_string_lookup() in action in the Structured data classification from scratch example.\n\n\nEncoding integer categorical features via one-hot encoding\n\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \"int32\")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \"int32\")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n\n\nNote that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_token and oov_token constructor arguments of layer_integer_lookup().\nYou can see the layer_integer_lookup() in action in the example structured data classification from scratch.\n\n\nApplying the hashing trick to an integer categorical feature\nIf you have a categorical feature that can take many different values (on the order of 10e3 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the “hashing trick”: hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing.\n\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \"int64\")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n\nTensorShape([10000, 64])\n\n\n\n\nEncoding text as a sequence of token indices\nThis is how you should preprocess text to be passed to an Embedding layer.\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\",\n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 2 19 14 1 9 2 1\n\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer = \"rmsprop\", loss = \"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output:\", as.array(test_output), \"\\n\")\n\nModel output: 0.1284049 \n\n\nYou can see the layer_text_vectorization() layer in action, combined with an Embedding mode, in the example text classification from scratch.\nNote that when training such a model, for best performance, you should always use the layer_text_vectorization() layer as part of the input pipeline.\n\n\nEncoding text as a dense matrix of ngrams with multi-hot encoding\nThis is how you can preprocess text to be passed to a Dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"multi_hot\" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output); cat(\"\\n\")\n\nModel output: \n\n\ntf.Tensor([[-1.2621338]], shape=(1, 1), dtype=float32)\n\n\n\n\nEncoding text as a dense matrix of ngrams with TF-IDF weighting\nThis is an alternative way of preprocessing text before passing it to a layer_dense layer.\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"tf-idf\" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\"CPU\"), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\"Training model...\")\n\nTraining model...\n\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n\nCalling end-to-end model on test string...\n\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output)\n\nModel output: \n\n\ntf.Tensor([[-1.0513749]], shape=(1, 1), dtype=float32)"
  },
  {
    "objectID": "guides/keras/preprocessing_layers.html#important-gotchas",
    "href": "guides/keras/preprocessing_layers.html#important-gotchas",
    "title": "Working with preprocessing layers",
    "section": "Important gotchas",
    "text": "Important gotchas\n\nWorking with lookup layers with very large vocabularies\nYou may find yourself working with a very large vocabulary in a layer_text_vectorization(), a layer_string_lookup() layer, or an layer_integer_lookup() layer. Typically, a vocabulary larger than 500MB would be considered “very large”.\nIn such case, for best performance, you should avoid using adapt(). Instead, pre-compute your vocabulary in advance (you could use Apache Beam or TF Transform for this) and store it in a file. Then load the vocabulary into the layer at construction time by passing the filepath as the vocabulary argument."
  },
  {
    "objectID": "guides/keras/python_subclasses.html",
    "href": "guides/keras/python_subclasses.html",
    "title": "Python Subclasses",
    "section": "",
    "text": "When using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses R6::R6Class as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R."
  },
  {
    "objectID": "guides/keras/python_subclasses.html#examples",
    "href": "guides/keras/python_subclasses.html#examples",
    "title": "Python Subclasses",
    "section": "Examples",
    "text": "Examples\n\nA custom constraint (R6)\nFor demonstration purposes, let’s say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\nNonNegative <- R6::R6Class(\"NonNegative\",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \"__call__\" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n\nLoaded Tensorflow version 2.9.1\n\n\nThe r_to_py method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\nNew class instances are generated by calling the class directly: NonNegative() (not NonNegative$new())\nAll methods (functions) are (potentially) modified to ensure their first argument is self.\nAll methods have in scope __class__, super and the class name (NonNegative).\nFor convenience, some method names are treated as aliases:\n\ninitialize is treated as an alias for __init__()\nfinalize is treated as an alias for __del__()\n\nsuper can be accessed in 3 ways:\n\nR6 style, which supports only single inheritance (the most common type)\n\nsuper$initialize()\n\nPython 2 style, which requires explicitly providing the class generator and instance\n\nsuper(NonNegative, self)$`__init__`()\n\nPython 3 style\n\nsuper()$`__init__`()\nWhen subclassing Keras base classes, it is generally your responsibility to call super$initialize() if you are masking a superclass initializer by providing your own initialize method.\nPassing convert=FALSE to r_to_py() will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with convert=TRUE, namely, modifying some Python objects, like dictionaries or lists, in-place.\nActive bindings (methods supplied to R6Class(active=...)) are converted to Python @property-decorated methods.\nR6 classes with private methods or attributes are not supported.\nThe argument supplied to inherit can be:\n\nmissing or NULL\na Python class generator\nan R6 class generator, as long as it can be converted to a Python class generator as well\na list of Python/R6 classes (for multiple inheritance)\nA list of superclasses, with optional additional keywords (e.g., metaclass=, only for advanced Python use cases)\n\n\n\n\nA custom constraint (%py_class%)\nAs an alternative to r_to_py(R6Class(...)), we also provide %py_class%, a more concise alternative syntax for achieving the same outcome. %py_class% is heavily inspired by the Python class statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using %py_class%:\n\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \"__call__\" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n\nNotice, this is very similar to the equivalent Python code:\n\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n\nSome (potentially surprising) notes about %py_class%:\n\nJust like the Python class statement, it assigns the constructed class in the current scope! (There is no need to write NonNegative <- ...).\nThe left hand side can be:\n\nA bare symbol, ClassName\nA pseudo-call, with superclasses and keywords as arguments: ClassName(Superclass1, Superclass2, metaclass=my_metaclass)\n\nThe right hand side is evaluated in a new environment to form the namespace for the class methods.\n%py_class% objects can be safely defined at the top level of an R package. (see details about delay_load below)\nTwo keywords are treated specially: convert and delay_load.\nIf you want to call r_to_py with convert=FALSE, pass it as a keyword:\n\n\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n\n\nYou can delay creating the python type object until this first time a class instance is created by passing delay_load=TRUE. The default value is FALSE for most contexts, but TRUE if you are in an R package. (The actual test performed is identical(topenv(), globalenv())). If a %py_class% type object is delayed, it will display \"<<R6type>.ClassName> (delayed)\" when printed.\nAn additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the __doc__ attribute of the class or method. The doc string will then be visible to both python and R tools e.g. reticulate::py_help(). See ?py_class for an example.\n\nIn all other regards, %py_class% is equivalent to r_to_py(R6Class()) (indeed, under the hood, they do the same thing).\n\n\nA custom layer (R6)\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \"CustomLayer\",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n\n\n\nA custom layer (%py_class%)\nor using %py_class%:\n\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}"
  },
  {
    "objectID": "guides/keras/sequential_model.html",
    "href": "guides/keras/sequential_model.html",
    "title": "The Sequential model",
    "section": "",
    "text": "library(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "href": "guides/keras/sequential_model.html#when-to-use-a-sequential-model",
    "title": "The Sequential model",
    "section": "When to use a Sequential model",
    "text": "When to use a Sequential model\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\nSchematically, the following Sequential model:\n\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>% \n  layer_dense(2, activation = \"relu\", name = \"layer1\") %>% \n  layer_dense(3, activation = \"relu\", name = \"layer2\") %>% \n  layer_dense(4, name = \"layer3\")\n\nLoaded Tensorflow version 2.9.1\n\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n\nis equivalent to this function:\n\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \"relu\", name = \"layer1\")\nlayer2 <- layer_dense(units = 3, activation = \"relu\", name = \"layer2\")\nlayer3 <- layer_dense(units = 4, name = \"layer3\")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n\nA Sequential model is not appropriate when:\n\nYour model has multiple inputs or multiple outputs\nAny of your layers has multiple inputs or multiple outputs\nYou need to do layer sharing\nYou want non-linear topology (e.g. a residual connection, a multi-branch model)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "href": "guides/keras/sequential_model.html#creating-a-sequential-model",
    "title": "The Sequential model",
    "section": "Creating a Sequential model",
    "text": "Creating a Sequential model\nYou can create a Sequential model by piping a model through a series layers.\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(4)\n\nIts layers are accessible via the layers attribute:\n\nmodel$layers\n\n[[1]]\n<keras.layers.core.dense.Dense object at 0x7f9a2cb13d60>\n\n[[2]]\n<keras.layers.core.dense.Dense object at 0x7f9a2cb13d90>\n\n[[3]]\n<keras.layers.core.dense.Dense object at 0x7f9a2cb13e50>\n\n\nYou can also create a Sequential model incrementally:\n\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \"relu\")\nmodel %>% layer_dense(3, activation = \"relu\")\nmodel %>% layer_dense(4)\n\nNote that there’s also a corresponding pop() method to remove layers: a Sequential model behaves very much like a stack of layers.\n\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n\n[1] 2\n\n\nAlso note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nmodel <- keras_model_sequential(name = \"my_sequential\")\nmodel %>% layer_dense(2, activation = \"relu\", name = \"layer1\")\nmodel %>% layer_dense(3, activation = \"relu\", name = \"layer2\")\nmodel %>% layer_dense(4, name = \"layer3\")"
  },
  {
    "objectID": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "href": "guides/keras/sequential_model.html#specifying-the-input-shape-in-advance",
    "title": "The Sequential model",
    "section": "Specifying the input shape in advance",
    "text": "Specifying the input shape in advance\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n\nlist()\n\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n\n[[1]]\n<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.15048718, -0.49884558, -0.31632543],\n       [ 0.74115336, -0.22117859, -0.15222079],\n       [ 0.07044244,  0.13284206,  0.42798996],\n       [-0.4665495 , -0.08431947, -0.10449022]], dtype=float32)>\n\n[[2]]\n<tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn’t “built”: it has no weights (and calling model$weights results in an error stating just this). The weights are created when the model first sees some input data:\n\nmodel <- keras_model_sequential() %>% \n        layer_dense(2, activation = \"relu\") %>% \n        layer_dense(3, activation = \"relu\") %>% \n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n\nError in py_get_attr_impl(x, name, silent) : \n  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n\n# The model summary is also not available:\nsummary(model)\n\nModel: <no summary available, model was not built>\n\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\"Number of weights after calling the model:\", length(model$weights), \"\\n\")  # 6\n\nNumber of weights after calling the model: 6 \n\n\nOnce a model is “built”, you can call its summary() method to display its contents (the summary() method is also called by the default print() method:\n\nsummary(model)\n\nModel: \"sequential_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_9 (Dense)                    (1, 2)                          10          \n dense_8 (Dense)                    (1, 3)                          9           \n dense_7 (Dense)                    (1, 4)                          16          \n================================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an input_shape argument to your model, so that it knows its input shape from the start:\n\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \"relu\")\n\nmodel\n\nModel: \"sequential_4\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_10 (Dense)                   (None, 2)                       10          \n================================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\nIn general, it’s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is."
  },
  {
    "objectID": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "href": "guides/keras/sequential_model.html#a-common-debugging-workflow-summary",
    "title": "The Sequential model",
    "section": "A common debugging workflow: %>% + summary()",
    "text": "A common debugging workflow: %>% + summary()\nWhen building a new Sequential architecture, it’s useful to incrementally stack layers and print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n  \nmodel %>% \n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) \n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n\nModel: \"sequential_5\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 123, 123, 32)            2432        \n conv2d (Conv2D)                    (None, 121, 121, 32)            9248        \n max_pooling2d (MaxPooling2D)       (None, 40, 40, 32)              0           \n================================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n________________________________________________________________________________\n\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(2) \n\n# And now?\nmodel\n\nModel: \"sequential_5\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 123, 123, 32)            2432        \n conv2d (Conv2D)                    (None, 121, 121, 32)            9248        \n max_pooling2d (MaxPooling2D)       (None, 40, 40, 32)              0           \n conv2d_5 (Conv2D)                  (None, 38, 38, 32)              9248        \n conv2d_4 (Conv2D)                  (None, 36, 36, 32)              9248        \n max_pooling2d_2 (MaxPooling2D)     (None, 12, 12, 32)              0           \n conv2d_3 (Conv2D)                  (None, 10, 10, 32)              9248        \n conv2d_2 (Conv2D)                  (None, 8, 8, 32)                9248        \n max_pooling2d_1 (MaxPooling2D)     (None, 4, 4, 32)                0           \n================================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n________________________________________________________________________________\n\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n\nVery practical, right?"
  },
  {
    "objectID": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "href": "guides/keras/sequential_model.html#what-to-do-once-you-have-a-model",
    "title": "The Sequential model",
    "section": "What to do once you have a model",
    "text": "What to do once you have a model\nOnce your model architecture is ready, you will want to:\n\nTrain your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\nSave your model to disk and restore it. See our guide to serialization & saving.\nSpeed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training."
  },
  {
    "objectID": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#feature-extraction-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Feature extraction with a Sequential model",
    "text": "Feature extraction with a Sequential model\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n\nHere’s a similar example that only extract features from one layer:\n\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\", name = \"my_intermediate_layer\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \"my_intermediate_layer\")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)"
  },
  {
    "objectID": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "href": "guides/keras/sequential_model.html#transfer-learning-with-a-sequential-model",
    "title": "The Sequential model",
    "section": "Transfer learning with a Sequential model",
    "text": "Transfer learning with a Sequential model\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren’t familiar with it, make sure to read our guide to transfer learning.\nHere are two common transfer learning blueprint involving Sequential models.\nFirst, let’s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model$layers and set layer$trainable = FALSE on each layer, except the last one. Like this:\n\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html",
    "href": "guides/keras/training_with_built_in_methods.html",
    "title": "Training & evaluation with the built-in methods",
    "section": "",
    "text": "library(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#introduction",
    "href": "guides/keras/training_with_built_in_methods.html#introduction",
    "title": "Training & evaluation with the built-in methods",
    "section": "Introduction",
    "text": "Introduction\nThis guide covers training, evaluation, and prediction (inference) models when using built-in APIs for training & validation.\nIf you are interested in leveraging fit() while specifying your own training step function, see the Customizing what happens in fit() guide.\nIf you are interested in writing your own training & evaluation loops from scratch, see the guide “writing a training loop from scratch”.\nIn general, whether you are using built-in loops or writing your own, model training & evaluation works strictly in the same way across every kind of Keras model – Sequential models, models built with the Functional API, and models written from scratch via model subclassing.\nThis guide doesn’t cover distributed training, which is covered in our guide to multi-GPU & distributed training."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#api-overview-a-first-end-to-end-example",
    "href": "guides/keras/training_with_built_in_methods.html#api-overview-a-first-end-to-end-example",
    "title": "Training & evaluation with the built-in methods",
    "section": "API overview: a first end-to-end example",
    "text": "API overview: a first end-to-end example\nWhen passing data to the built-in training loops of a model, you should either use NumPy arrays (if your data is small and fits in memory) or tf$data Dataset objects. In the next few paragraphs, we’ll use the MNIST dataset as NumPy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\nLet’s consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\n\nLoaded Tensorflow version 2.9.1\n\nx <- inputs %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\noutputs <- x %>% \n  layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nHere’s what the typical end-to-end workflow looks like, consisting of:\n\nTraining\nValidation on a holdout set generated from the original training data\nEvaluation on the test data\n\nWe’ll use MNIST data for this example.\n\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# Transform RGB values into [0,1] range\nx_train <- x_train / 255\nx_test <- x_test / 255\n\n# Reserve 10,000 samples for validation\nx_val <- tail(x_train, 10000)\ny_val <- tail(y_train, 10000)\nx_train <- head(x_train, 50000)\ny_train <- head(y_train, 50000)\n\nWe specify the training configuration (optimizer, loss, metrics):\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(),  # Optimizer\n    # Loss function to minimize\n    loss = loss_sparse_categorical_crossentropy(),\n    # List of metrics to monitor\n    metrics = list(metric_sparse_categorical_accuracy()),\n)\n\nWe call fit(), which will train the model by slicing the data into “batches” of size batch_size, and repeatedly iterating over the entire dataset for a given number of epochs.\n\nhistory <- model %>% fit(\n    x_train,\n    y_train,\n    batch_size = 64,\n    epochs = 2,\n    # We pass some validation for\n    # monitoring validation loss and metrics\n    # at the end of each epoch\n    validation_data = list(x_val, y_val),\n)\n\nThe returned history object holds a record of the loss values and metric values during training:\n\nhistory\n\n\nFinal epoch (plot to see history):\n                           loss: 0.1555\n    sparse_categorical_accuracy: 0.9534\n                       val_loss: 0.1314\nval_sparse_categorical_accuracy: 0.9632 \n\n\nWe evaluate the model on the test data via evaluate():\n\n# Evaluate the model on the test data using `evaluate`\nresults <- model %>% evaluate(x_test, y_test, batch_size = 128)\ncat(\"test loss, test acc:\", results)\n\ntest loss, test acc: 0.1308085 0.9602\n\n# Generate predictions (probabilities -- the output of the last layer)\n\n# on new data using `predict`\n\npredictions <- predict(model, x_test[1:3,])\ndim(predictions)\n\n[1]  3 10\n\n\nNow, let’s review each piece of this workflow in detail."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#the-compile-method-specifying-a-loss-metrics-and-an-optimizer",
    "href": "guides/keras/training_with_built_in_methods.html#the-compile-method-specifying-a-loss-metrics-and-an-optimizer",
    "title": "Training & evaluation with the built-in methods",
    "section": "The compile() method: specifying a loss, metrics, and an optimizer",
    "text": "The compile() method: specifying a loss, metrics, and an optimizer\nTo train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\nYou pass these to the model as arguments to the compile() method:\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_categorical_crossentropy(),\n    metrics = list(metric_sparse_categorical_accuracy())\n)\n\nThe metrics argument should be a list – your model can have any number of metrics.\nIf your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate the contribution of each output to the total loss of the model. You will find more details about this in the Passing data to multi-input, multi-output models section.\nNote that if you’re satisfied with the default settings, in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:\n\nmodel %>% compile(\n    optimizer = \"rmsprop\",\n    loss = \"sparse_categorical_crossentropy\",\n    metrics = list(\"sparse_categorical_accuracy\")\n)\n\nFor later reuse, let’s put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n\nget_uncompiled_model <- function() {\n  inputs <- layer_input(shape = shape(784), name = \"digits\")\n  x <- inputs %>% \n    layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>% \n    layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\n  outputs <- x %>% \n    layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\n  model <- keras_model(inputs = inputs, outputs = outputs)\n  model\n}\n\nget_compiled_model <- function() {\n  model <- get_uncompiled_model()\n  model %>% compile(\n    optimizer = \"rmsprop\",\n    loss = \"sparse_categorical_crossentropy\",\n    metrics = list(\"sparse_categorical_accuracy\"),\n  )\n  model\n}\n\n\nMany built-in optimizers, losses, and metrics are available\nIn general, you won’t have to create your own losses, metrics, or optimizers from scratch, because what you need is likely to be already part of the Keras API:\nOptimizers:\n\noptimizer_sgd() (with or without momentum)\noptimizer_rmsprop()\noptimizer_adam()\netc.\n\nLosses:\n\nloss_mean_squared_error()\nloss_kl_divergence()\nloss_cosine_similarity()\netc.\n\nMetrics:\n\nmetric_auc()\nmetric_precision()\nmetric_recall()\netc.\n\n\n\nCustom losses\nIf you need to create a custom loss, Keras provides two ways to do so.\nThe first method involves creating a function that accepts inputs y_true and y_pred. The following example shows a loss function that computes the mean squared error between the real data and the predictions:\n\ncustom_mean_squared_error <- function(y_true, y_pred) {\n  tf$math$reduce_mean(tf$square(y_true - y_pred))\n}\n\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(optimizer = optimizer_adam(), loss = custom_mean_squared_error)\n\n# We need to one-hot encode the labels to use MSE\n\ny_train_one_hot <- tf$one_hot(y_train, depth = 10L)\nmodel %>% fit(x_train, y_train_one_hot, batch_size = 64, epochs = 1)\n\nIf you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf$keras$losses$Loss class and implement the following two methods:\n\ninitialize(): accept parameters to pass during the call of your loss function\ncall(y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model’s loss\n\nLet’s say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won’t know if it works until we try!).\nHere’s how you would do it:\n\ncustom_mse <- new_loss_class(\n  classname = \"custom_mse\",\n  initialize = function(regularization_factor = 0.1, name = \"custom_mse\") {\n    super()$`__init__`(name = name)\n    self$regularization_factor <- regularization_factor\n  },\n  call = function(y_true, y_pred) {\n    mse <- tf$math$reduce_mean(tf$square(y_true - y_pred))\n    reg <- tf$math$reduce_mean(tf$square(0.5 - y_pred))\n    mse + reg * self$regularization_factor\n  }\n)\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(optimizer = optimizer_adam(), loss = custom_mse())\n\ny_train_one_hot <- tf$one_hot(y_train, depth = 10L)\nmodel %>% fit(x_train, y_train_one_hot, batch_size = 64, epochs = 1)\n\n\n\nCustom metrics\nIf you need a metric that isn’t part of the API, you can easily create custom metrics by subclassing the tf$keras$metrics$Metric class. You will need to implement 4 methods:\n\ninitialize(), in which you will create state variables for your metric.\nupdate_state(y_true, y_pred, sample_weight = NULL), which uses the targets y_true and the model predictions y_pred to update the state variables.\nresult(), which uses the state variables to compute the final results.\nreset_state(), which reinitializes the state of the metric.\n\nState update and results computation are kept separate (in update_state() and result(), respectively) because in some cases, the results computation might be very expensive and would only be done periodically.\nHere’s a simple example showing how to implement a CategoricalTRUEPositives metric that counts how many samples were correctly classified as belonging to a given class:\n\ncategorical_true_positives <- new_metric_class(\n  classname = \"categorical_true_positives\",\n  initialize = function(name = \"categorical_true_positives\", ...) {\n    super()$`__init__`(name, ...)\n    self$true_positives <- self$add_weight(name = \"ctp\", initializer = \"zeros\")\n  },\n  update_state = function(y_true, y_pred, sample_weight = NULL) {\n    y_pred <- tf$reshape(tf$argmax(y_pred, axis = 1L), shape = c(-1L,1L))\n    values <- tf$cast(y_true, \"int32\") == tf$cast(y_pred, \"int32\")\n    values <- tf$cast(values, \"float32\")\n    if (!is.null(sample_weight)) {\n      sample_weight <- tf$cast(sample_weight, \"float32\")\n      values <- tf$multiply(values, sample_weight)\n    }\n\n    self$true_positives$assign_add(tf$reduce_sum(values))\n  },\n  result = function() {\n    self$true_positives\n  },\n  reset_state = function() {\n    self$true_positives$assign(0.0)\n  }\n)\n\nmodel <- get_uncompiled_model()\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(),\n    metrics = list(categorical_true_positives()),\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 3)\n\n\n\nHandling losses and metrics that don’t fit the standard signature\nThe overwhelming majority of losses and metrics can be computed from y_true and y_pred, where y_pred is an output of your model – but not all of them. For instance, a regularization loss may only require the activation of a layer (there are no targets in this case), and this activation may not be a model output.\nIn such cases, you can call self$add_loss(loss_value) from inside the call method of a custom layer. Losses added in this way get added to the “main” loss during training (the one passed to compile()). Here’s a simple example that adds activity regularization (note that activity regularization is built-in in all Keras layers – this layer is just for the sake of providing a concrete example):\n\nlayer_activity_regularization <- new_layer_class(\n  classname = \"activity_regularization\",\n  call = function(inputs) {\n    self$add_loss(tf$reduce_sum(inputs) * 0.1)\n    inputs # Pass-through layer.\n  }\n)\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\n# Insert activity regularization as a layer\nx <- layer_activity_regularization(x)\nx <- layer_dense(x, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x, 10, name = \"predictions\")\n\nmodel <- keras_model(inputs = inputs, outputs = outputs)\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\n\n# The displayed loss will be much higher than before\n# due to the regularization component.\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nYou can do the same for logging metric values, using add_metric():\n\nlayer_metric_logging <- new_layer_class(\n  \"metric_logging\",\n  call = function(inputs) {\n    self$add_metric(\n      keras$backend$std(inputs), \n      name = \"std_of_activation\", \n      aggregation = \"mean\"\n    )\n    inputs\n  }\n)\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\n\n# Insert std logging as a layer.\nx <- layer_metric_logging(x)\nx <- layer_dense(x, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x, 10, name = \"predictions\")\n\nmodel <- keras_model(inputs = inputs, outputs = outputs)\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nIn the Functional API, you can also call model$add_loss(loss_tensor), or model$add_metric(metric_tensor, name, aggregation).\nHere’s a simple example:\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\nx1 <- layer_dense(inputs, 64, activation = \"relu\", name = \"dense_1\")\nx2 <- layer_dense(x1, 64, activation = \"relu\", name = \"dense_2\")\noutputs <- layer_dense(x2, 10, name = \"predictions\")\nmodel <- keras_model(inputs = inputs, outputs = outputs)\n\nmodel$add_loss(tf$reduce_sum(x1) * 0.1)\nmodel$add_metric(\n  keras$backend$std(x1), \n  name = \"std_of_activation\", \n  aggregation = \"mean\"\n)\n\nmodel %>% compile(\n    optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE)\n)\nmodel %>% fit(x_train, y_train, batch_size = 64, epochs = 1)\n\nNote that when you pass losses via add_loss(), it becomes possible to call compile() without a loss function, since the model already has a loss to minimize.\nConsider the following LogisticEndpoint layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via add_loss(). It also tracks classification accuracy via add_metric().\n\nlayer_logistic_endpoint <- new_layer_class(\n  \"logistic_endpoint\",\n  initialize = function(name = NULL) {\n    super()$`__init__`(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  },\n  call = function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n    \n    # Log accuracy as a metric and add it\n    # to the layer using `self$add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \"accuracy\")\n    \n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n)\n\nYou can use it in a model with two inputs (input data & targets), compiled without a loss argument, like this:\n\ninputs <- layer_input(shape = shape(3), name = \"inputs\")\ntargets <- layer_input(shape = shape(10), name = \"targets\")\nlogits <- layer_dense(inputs, 10)\npredictions <- layer_logistic_endpoint(name = \"predictions\")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \"adam\")  # No loss argument!\n\ndata <- list(\n    \"inputs\" = array(runif(3*3), dim = c(3,3)),\n    \"targets\" = array(runif(3*10), dim = c(3, 10))\n)\nmodel %>% fit(data, epochs = 1)\n\nFor more information about training multi-input models, see the section Passing data to multi-input, multi-output models.\n\n\nAutomatically setting apart a validation holdout set\nIn the first end-to-end example you saw, we used the validation_data argument to pass a listy of arrays (x_val, y_val) to the model for evaluating a validation loss and validation metrics at the end of each epoch.\nHere’s another option: the argument validation_split allows you to automatically reserve part of your training data for validation. The argument value represents the fraction of the data to be reserved for validation, so it should be set to a number higher than 0 and lower than 1. For instance, validation_split = 0.2 means “use 20% of the data for validation”, and validation_split = 0.6 means “use 60% of the data for validation”.\nThe way the validation is computed is by taking the last x% samples of the arrays received by the fit() call, before any shuffling.\nNote that you can only use validation_split when training with array data.\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, batch_size = 64, validation_split = 0.2, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#training-evaluation-from-tensorflow-datasets",
    "href": "guides/keras/training_with_built_in_methods.html#training-evaluation-from-tensorflow-datasets",
    "title": "Training & evaluation with the built-in methods",
    "section": "Training & evaluation from TensorFlow Datasets",
    "text": "Training & evaluation from TensorFlow Datasets\nIn the past few paragraphs, you’ve seen how to handle losses, metrics, and optimizers, and you’ve seen how to use the validation_data and validation_split arguments in fit(), when your data is passed as R arrays.\nLet’s now take a look at the case where your data comes in the form of a TensorFlow dataset object.\n\n\n\n\n\n\nNote\n\n\n\nThe tfdatasets package in R is an interface for the tf.data module in Python.\n\n\nThe tf.data API is a set of utilities in TensorFlow 2.0 for loading and preprocessing data in a way that’s fast and scalable.\nFor a complete guide about creating Datasets, see the tf.data documentation.\nYou can pass a Dataset instance directly to the methods fit(), evaluate(), and predict():\n\nlibrary(tfdatasets)\nmodel <- get_compiled_model()\n\n# First, let's create a training Dataset instance.\n# For the sake of our example, we'll use the same MNIST data as before.\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train))\n# Shuffle and slice the dataset.\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Now we get a test dataset.\ntest_dataset <- list(x_test, y_test) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\n# Since the dataset already takes care of batching,\n# we don't pass a `batch_size` argument.\nmodel %>% fit(train_dataset, epochs = 3)\n\n# You can also evaluate or predict on a dataset.\nresult <- model %>% evaluate(test_dataset)\nprint(result)\n\n                       loss sparse_categorical_accuracy \n                   0.120964                    0.963900 \n\n\nNote that the Dataset is reset at the end of each epoch, so it can be reused of the next epoch.\nIf you want to run training only on a specific number of batches from this Dataset, you can pass the steps_per_epoch argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.\nIf you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Only use the 100 batches per epoch (that's 64 * 100 samples)\nmodel %>% fit(train_dataset, epochs = 3, steps_per_epoch = 100)\n\n\nUsing a validation dataset\nYou can pass a Dataset instance as the validation_data argument in fit():\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Prepare the validation dataset\nval_dataset <- list(x_val, y_val) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\nmodel %>% fit(train_dataset, epochs = 1, validation_data = val_dataset)\n\nAt the end of each epoch, the model will iterate over the validation dataset and compute the validation loss and validation metrics.\nIf you want to run validation only on a specific number of batches from this dataset, you can pass the validation_steps argument, which specifies how many validation steps the model should run with the validation dataset before interrupting validation and moving on to the next epoch:\n\nmodel <- get_compiled_model()\n\n# Prepare the training dataset\ntrain_dataset <- list(x_train, y_train) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\n# Prepare the validation dataset\nval_dataset <- list(x_val, y_val) %>% \n  tensor_slices_dataset() %>% \n  dataset_batch(64)\n\nmodel %>% fit(\n    train_dataset,\n    epochs = 1,\n    # Only run validation using the first 10 batches of the dataset\n    # using the `validation_steps` argument\n    validation_data = val_dataset,\n    validation_steps = 10,\n)\n\nNote that the validation dataset will be reset after each use (so that you will always be evaluating on the same samples from epoch to epoch).\nThe argument validation_split (generating a holdout set from the training data) is not supported when training from Dataset objects, since this feature requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-sample-weighting-and-class-weighting",
    "href": "guides/keras/training_with_built_in_methods.html#using-sample-weighting-and-class-weighting",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using sample weighting and class weighting",
    "text": "Using sample weighting and class weighting\nWith the default settings the weight of a sample is decided by its frequency in the dataset. There are two methods to weight the data, independent of sample frequency:\n\nClass weights\nSample weights\n\n\nClass weights\nThis is set by passing a dictionary to the class_weight argument to Model %>% fit(). This dictionary maps class indices to the weight that should be used for samples belonging to this class.\nThis can be used to balance classes without resampling, or to train a model that gives more importance to a particular class.\nFor instance, if class “0” is half as represented as class “1” in your data, you could use Model %>% fit(..., class_weight = list(0= 1., 1= 0.5)).\nHere’s an example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit “5” in the MNIST dataset).\n\nclass_weight <- list(\n    \"0\" = 1.0,\n    \"1\" = 1.0,\n    \"2\" = 1.0,\n    \"3\" = 1.0,\n    \"4\" = 1.0,\n    # Set weight \"2\" for class \"5\",\n    # making this class 2x more important\n    \"5\" = 2.0,\n    \"6\" = 1.0,\n    \"7\" = 1.0,\n    \"8\" = 1.0,\n    \"9\" = 1.0\n)\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, class_weight = class_weight, batch_size = 64, epochs = 1)\n\n\n\nSample weights\nFor fine grained control, or if you are not building a classifier, you can use “sample weights”.\n\nWhen training from R data: Pass the sample_weight argument to Model %>% fit().\nWhen training from tfdatasets or any other sort of iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples.\n\nA “sample weights” array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\nWhen the weights used are ones and zeros, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss).\n\nsample_weight <- rep(1, length(y_train))\nsample_weight[y_train == 5] <- 2.0\n\nmodel <- get_compiled_model()\nmodel %>% fit(x_train, y_train, sample_weight = sample_weight, batch_size = 64, epochs = 1)\n\nHere’s a matching Dataset example:\n\nsample_weight <- rep(1, length(y_train))\nsample_weight[y_train == 5] <- 2.0\n\n# Create a Dataset that includes sample weights\n# (3rd element in the return tuple).\ntrain_dataset <- list(x_train, y_train, sample_weight) %>% \n  tensor_slices_dataset()\n\n# Shuffle and slice the dataset.\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\nmodel <- get_compiled_model()\nmodel %>% fit(train_dataset, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#passing-data-to-multi-input-multi-output-models",
    "href": "guides/keras/training_with_built_in_methods.html#passing-data-to-multi-input-multi-output-models",
    "title": "Training & evaluation with the built-in methods",
    "section": "Passing data to multi-input, multi-output models",
    "text": "Passing data to multi-input, multi-output models\nIn the previous examples, we were considering a model with a single input (a tensor of shape (764)) and a single output (a prediction tensor of shape (10)). But what about models that have multiple inputs or outputs?\nConsider the following model, which has an image input of shape (32, 32, 3) (that’s (height, width, channels)) and a time series input of shape (NULL, 10) (that’s (timesteps, features)). Our model will have two outputs computed from the combination of these inputs: a “score” (of shape (1)) and a probability distribution over five classes (of shape (5)).\n\nimage_input <- layer_input(shape = shape(32, 32, 3), name = \"img_input\")\ntimeseries_input <- layer_input(shape = shape(NULL, 10), name = \"ts_input\")\n\nx1 <- layer_conv_2d(image_input, 3, 3)\nx1 <- layer_global_max_pooling_2d(x1)\n\nx2 <- layer_conv_1d(timeseries_input, 3, 3)\nx2 <- layer_global_max_pooling_1d(x2)\n\nx <- layer_concatenate(list(x1, x2))\n\nscore_output <- layer_dense(x, 1, name = \"score_output\")\nclass_output <- layer_dense(x, 5, name = \"class_output\")\n\nmodel <- keras_model(\n    inputs = list(image_input, timeseries_input), \n    outputs = list(score_output, class_output)\n)\n\nLet’s plot this model, so you can clearly see what we’re doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes).\n\nkeras$utils$plot_model(\n  model, \"img/multi_input_and_output_model.png\", \n  show_shapes = TRUE\n)\n\n<IPython.core.display.Image object>\n\n\n\nAt compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  )\n)\n\nIf we only passed a single loss function to the model, the same loss function would be applied to every output (which is not appropriate here).\nLikewise for metrics:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    ),\n    list(\n      metric_categorical_accuracy()\n    )\n  )\n)\n\nSince we gave names to our output layers, we could also specify per-output losses and metrics via a dict:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    score_output = loss_mean_squared_error(),\n    class_output = loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    class_output = list(\n      metric_categorical_accuracy()\n    ),\n    score_output = list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    )\n  )\n)\n\nWe recommend the use of explicit names and dicts if you have more than 2 outputs.\nIt’s possible to give different weights to different output-specific losses (for instance, one might wish to privilege the “score” loss in our example, by giving to 2x the importance of the class loss), using the loss_weights argument:\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    score_output = loss_mean_squared_error(),\n    class_output = loss_categorical_crossentropy()\n  ),\n  metrics = list(\n    class_output = list(\n      metric_categorical_accuracy()\n    ),\n    score_output = list(\n      metric_mean_absolute_percentage_error(),\n      metric_mean_absolute_error()\n    )\n  ),\n  loss_weights = list(score_output = 2.0, class_output = 1.0)\n)\n\nYou could also choose not to compute a loss for certain outputs, if these outputs are meant for prediction but not for training:\n\n# List loss version\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    NULL,\n    loss_categorical_crossentropy()\n  )\n)\n\n# Or dict loss version\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    class_output = loss_categorical_crossentropy()\n  )\n)\n\nPassing data to a multi-input or multi-output model in fit() works in a similar way as specifying a loss function in compile: you can pass lists of R arrays (with 1:1 mapping to the outputs that received a loss function) or named list mapping output names to R arrays.\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_mean_squared_error(),\n    loss_categorical_crossentropy()\n  )\n)\n# Generate dummy NumPy data\n\nimg_data <- array(runif(100*32*32*3), dim = c(100, 32, 32, 3))\nts_data <- array(runif(100*20*10), dim = c(100, 20, 10))\nscore_targets <- array(runif(100), dim = c(100, 1))\nclass_targets <- array(runif(100*5), dim = c(100, 5))\n\n# Fit on lists\nmodel %>% fit(\n  list(img_data, ts_data), \n  list(score_targets, class_targets), \n  batch_size = 32, \n  epochs = 1\n)\n\n# Alternatively, fit on named lists\nmodel %>% fit(\n    list(\"img_input\" = img_data, \"ts_input\" = ts_data),\n    list(\"score_output\" = score_targets, \"class_output\" = class_targets),\n    batch_size = 32,\n    epochs = 1\n)\n\nHere’s the Dataset use case: similarly as what we did for R arrays, the Dataset should return a tuple of dicts.\n\ntrain_dataset <- list(\n  list(\"img_input\" = img_data, \"ts_input\" = ts_data),\n  list(\"score_output\" = score_targets, \"class_output\" = class_targets)\n) %>% \n  tensor_slices_dataset() %>% \n  dataset_shuffle(1024) %>% \n  dataset_batch(64)\n\nmodel %>% fit(train_dataset, epochs = 1)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-callbacks",
    "href": "guides/keras/training_with_built_in_methods.html#using-callbacks",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using callbacks",
    "text": "Using callbacks\nCallbacks in Keras are objects that are called at different points during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They can be used to implement certain behaviors, such as:\n\nDoing validation at different points during training (beyond the built-in per-epoch validation)\nCheckpointing the model at regular intervals or when it exceeds a certain accuracy threshold\nChanging the learning rate of the model when training seems to be plateauing\nDoing fine-tuning of the top layers when training seems to be plateauing\nSending email or instant message notifications when training ends or where a certain performance threshold is exceeded\nEtc.\n\nCallbacks can be passed as a list to your call to fit():\n\nmodel <- get_compiled_model()\n\ncallbacks <- list(\n  callback_early_stopping(\n    # Stop training when `val_loss` is no longer improving\n    monitor = \"val_loss\",\n    # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n    min_delta = 1e-2,\n    # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n    patience = 2,\n    verbose = 1,\n  )\n)\n\nmodel %>% fit(\n    x_train,\n    y_train,\n    epochs = 20,\n    batch_size = 64,\n    callbacks = callbacks,\n    validation_split = 0.2,\n)\n\n\nMany built-in callbacks are available\nThere are many built-in callbacks already available in Keras, such as:\n\ncallback_model_checkpoint(): Periodically save the model.\ncallback_early_stopping(): Stop training when training is no longer improving the validation metrics.\ncallback_tensorboard(): periodically write model logs that can be visualized in TensorBoard (more details in the section “Visualization”).\ncallback_csv_logger(): streams loss and metrics data to a CSV file.\netc.\n\nSee the callbacks documentation for the complete list.\n\n\nWriting your own callback\nYou can create a custom callback by extending the base class keras$callbacks$Callback. A callback has access to its associated model through the class property self$model.\nMake sure to read the complete guide to writing custom callbacks.\nHere’s a simple example saving a list of per-batch loss values during training:\n\ncallback_loss_history <- new_callback_class(\n  \"loss_history\",\n  on_train_begin = function(logs) {\n    self$per_batch_losses <- list()\n  },\n  on_batch_end = function(batch, logs) {\n    self$per_batch_losses <- c(\n      self$per_batch_losses,\n      logs$get(\"loss\")\n    )\n  }\n)"
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#checkpointing-models",
    "href": "guides/keras/training_with_built_in_methods.html#checkpointing-models",
    "title": "Training & evaluation with the built-in methods",
    "section": "Checkpointing models",
    "text": "Checkpointing models\nWhen you’re training model on relatively large datasets, it’s crucial to save checkpoints of your model at frequent intervals.\nThe easiest way to achieve this is with the callback_model_checkpoint() callback:\n\nmodel <- get_compiled_model()\n\ncallbacks <- list(\n  callback_model_checkpoint(\n    # Path where to save the model\n    # The two parameters below mean that we will overwrite\n    # the current checkpoint if and only if\n    # the `val_loss` score has improved.\n    # The saved model name will include the current epoch.\n    filepath = \"mymodel_{epoch}\",\n    save_best_only = TRUE,  # Only save a model if `val_loss` has improved.\n    monitor = \"val_loss\",\n    verbose = 1,\n  )\n)\n\nmodel %>% fit(\n    x_train, \n    y_train, \n    epochs = 2, \n    batch_size = 64, \n    callbacks = callbacks, \n    validation_split = 0.2\n)\n\nThe callback_model_checkpoint() callback can be used to implement fault-tolerance: the ability to restart training from the last saved state of the model in case training gets randomly interrupted. Here’s a basic example:\n\n# Prepare a directory to store all the checkpoints.\ncheckpoint_dir <- \"./ckpt\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n\nmake_or_restore_model <- function() {\n  # Either restore the latest model, or create a fresh one\n  # if there is no checkpoint available.\n  checkpoints <- list.files(checkpoint_dir, full.names = TRUE)\n  details <- file.info(checkpoints)\n  if (length(checkpoints) > 0) {\n    latest_checkpoint <- checkpoints[which.max(as.POSIXct(details$mtime))]\n    cat(\"Restoring from\", latest_checkpoint)\n    return(load_model_tf(latest_checkpoint))\n  }\n  \n  cat(\"Creating a new model\")\n  get_compiled_model()\n}\n\nmodel <- make_or_restore_model()\n\nRestoring from ./ckpt/ckpt-loss=0.14\n\ncallbacks <- list(\n    # This callback saves a SavedModel every 100 batches.\n    # We include the training loss in the saved model name.\n    callback_model_checkpoint(\n        filepath = paste0(checkpoint_dir, \"/ckpt-loss={loss:.2f}\"), \n        save_freq = 100\n    )\n)\nmodel %>% fit(x_train, y_train, epochs = 1, callbacks = callbacks)\n\nYou call also write your own callback for saving and restoring models.\nFor a complete guide on serialization and saving, see the guide to saving and serializing Models."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#using-learning-rate-schedules",
    "href": "guides/keras/training_with_built_in_methods.html#using-learning-rate-schedules",
    "title": "Training & evaluation with the built-in methods",
    "section": "Using learning rate schedules",
    "text": "Using learning rate schedules\nA common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as “learning rate decay”.\nThe learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss).\n\nPassing a schedule to an optimizer\nYou can easily use a static learning rate decay schedule by passing a schedule object as the learning_rate argument in your optimizer:\n\ninitial_learning_rate <- 0.1\nlr_schedule <- learning_rate_schedule_exponential_decay(\n  initial_learning_rate = initial_learning_rate,\n  decay_steps = 100000, \n  decay_rate = 0.96, \n  staircase = TRUE\n)\n\noptimizer <- keras$optimizers$RMSprop(learning_rate = lr_schedule)\n\nSeveral built-in schedules are available: learning_rate_schedule_cosine_decay, learning_rate_schedule_cosine_decay_restarts, learning_rate_schedule_exponential_decay, learning_rate_schedule_inverse_time_decay, learning_rate_schedule_piecewise_constant_decay, learning_rate_schedule_polynomial_decay\n\n\nUsing callbacks to implement a dynamic learning rate schedule\nA dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects, since the optimizer does not have access to validation metrics.\nHowever, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the callback_reduce_lr_on_plateau() callback."
  },
  {
    "objectID": "guides/keras/training_with_built_in_methods.html#visualizing-loss-and-metrics-during-training",
    "href": "guides/keras/training_with_built_in_methods.html#visualizing-loss-and-metrics-during-training",
    "title": "Training & evaluation with the built-in methods",
    "section": "Visualizing loss and metrics during training",
    "text": "Visualizing loss and metrics during training\nThe best way to keep an eye on your model during training is to use TensorBoard – a browser-based application that you can run locally that provides you with:\n\nLive plots of the loss and metrics for training and evaluation\n(optionally) Visualizations of the histograms of your layer activations\n(optionally) 3D visualizations of the embedding spaces learned by your Embedding layers\n\nIf you have installed TensorFlow with pip, you should be able to launch TensorBoard from R with:\n\ntensorflow::tensorboard(log_dir = \"/full_path_to_your_logs\")\n\n\nUsing the TensorBoard callback\nThe easiest way to use TensorBoard with a Keras model and the fit() method is the TensorBoard callback.\nIn the simplest case, just specify where you want the callback to write logs, and you’re good to go:\n\ncallback_tensorboard(\n    log_dir = \"/full_path_to_your_logs\",\n    histogram_freq = 0,  # How often to log histogram visualizations\n    embeddings_freq = 0,  # How often to log embedding visualizations\n    update_freq = \"epoch\" # How often to write logs (default: once per epoch)\n)  \n\n<keras.callbacks.TensorBoard object at 0x7fd7cf8ddbe0>\n\n\nFor more information, see the documentation for the TensorBoard callback."
  },
  {
    "objectID": "guides/keras/transfer_learning.html",
    "href": "guides/keras/transfer_learning.html",
    "title": "Transfer learning and fine-tuning",
    "section": "",
    "text": "library(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#introduction",
    "href": "guides/keras/transfer_learning.html#introduction",
    "title": "Transfer learning and fine-tuning",
    "section": "Introduction",
    "text": "Introduction\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\nTake layers from a previously trained model.\nFreeze them, so as to avoid destroying any of the information they contain during future training rounds.\nAdd some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\nTrain the new layers on your dataset.\n\nA last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\nFirst, we will go over the Keras trainable API in detail, which underlies most transfer learning and fine-tuning workflows.\nThen, we’ll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle “cats vs dogs” classification dataset.\nThis is adapted from Deep Learning with R and the 2016 blog post “building powerful image classification models using very little data”."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#freezing-layers-understanding-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Freezing layers: understanding the trainable attribute",
    "text": "Freezing layers: understanding the trainable attribute\nLayers and models have three weight attributes:\n\nweights is the list of all weights variables of the layer.\ntrainable_weights is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\nnon_trainable_weights is the list of those that aren’t meant to be trained. Typically they are updated by the model during the forward pass.\n\nExample: the Dense layer has 2 trainable weights (kernel and bias)\n\nlayer <- layer_dense(units = 3)\n\nLoaded Tensorflow version 2.9.1\n\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 0\n\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is layer_batch_normalization(). It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the guide to writing new layers from scratch.\nExample: The layer instance returned by layer_batch_normalization() has 2 trainable weights and 2 non-trainable weights\n\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 4\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 2\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nLayers and models also feature a boolean attribute trainable. Its value can be changed. Setting layer$trainable to FALSE moves all the layer’s weights from trainable to non-trainable. This is called “freezing” the layer: the state of a frozen layer won’t be updated during training (either when training with fit() or when training with any custom loop that relies on trainable_weights to apply gradient updates).\nExample: setting trainable to False\n\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\"weights: %s\", length(layer$weights))\n\nweights: 2\n\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n\ntrainable_weights: 0\n\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n\nnon_trainable_weights: 2\n\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \"relu\")\nlayer2 <- layer_dense(units = 3, activation = \"sigmoid\")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n\nDo not confuse the layer$trainable attribute with the training argument in a layer instance’s call signature layer(training =) (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the Keras FAQ."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "href": "guides/keras/transfer_learning.html#recursive-setting-of-the-trainable-attribute",
    "title": "Transfer learning and fine-tuning",
    "section": "Recursive setting of the trainable attribute",
    "text": "Recursive setting of the trainable attribute\nIf you set trainable = FALSE on a model or on any layer that has sublayers, all child layers become non-trainable as well.\nExample:\n\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \"sigmoid\")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "href": "guides/keras/transfer_learning.html#the-typical-transfer-learning-workflow",
    "title": "Transfer learning and fine-tuning",
    "section": "The typical transfer-learning workflow",
    "text": "The typical transfer-learning workflow\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\nInstantiate a base model and load pre-trained weights into it.\nFreeze all layers in the base model by setting trainable = FALSE.\nCreate a new model on top of the output of one (or several) layers from the base model.\nTrain your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\nInstantiate a base model and load pre-trained weights into it.\nRun your new dataset through it and record the output of one (or several) layers from the base model. This is called feature extraction.\nUse that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it’s a lot faster and cheaper.\nAn issue with that second workflow, though, is that it doesn’t allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\nHere’s what the first workflow looks like in Keras:\nFirst, instantiate a base model with pre-trained weights.\n\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\nThen, freeze the base model.\n\nbase_model$trainable <- FALSE\n\nCreate a new model on top.\n\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n\nTrain the model on new data.\n\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#fine-tuning",
    "href": "guides/keras/transfer_learning.html#fine-tuning",
    "title": "Transfer learning and fine-tuning",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting – keep that in mind.\nIt is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\nIt’s also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\nThis is how to implement fine-tuning of the whole base model:\n\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n\nImportant note about compile() and trainable\nCalling compile() on a model is meant to “freeze” the behavior of that model. This implies that the trainable attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until compile is called again. Hence, if you change any trainable value, make sure to call compile() again on your model for your changes to be taken into account.\nImportant notes about layer_batch_normalization()\nMany image models contain BatchNormalization layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\nBatchNormalization contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\nWhen you set bn_layer$trainable = FALSE, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as weight trainability and inference/training modes are two orthogonal concepts. But the two are tied in the case of the BatchNormalization layer.\nWhen you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = FALSE when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou’ll see this pattern in action in the end-to-end example at the end of this guide."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "href": "guides/keras/transfer_learning.html#transfer-learning-and-fine-tuning-with-a-custom-training-loop",
    "title": "Transfer learning and fine-tuning",
    "section": "Transfer learning and fine-tuning with a custom training loop",
    "text": "Transfer learning and fine-tuning with a custom training loop\nIf instead of fit(), you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list model$trainable_weights when applying gradient updates:\n\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n\nLikewise for fine-tuning."
  },
  {
    "objectID": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "href": "guides/keras/transfer_learning.html#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs.-dogs-dataset",
    "title": "Transfer learning and fine-tuning",
    "section": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset",
    "text": "An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\nTo solidify these concepts, let’s walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats vs. dogs” classification dataset.\n\nGetting the data\nFirst, let’s fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you’ll probably want to use the utility image_dataset_from_directory() to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n# reticulate::py_install(\"tensorflow_datasets\", pip = TRUE)\ntfds <- reticulate::import(\"tensorflow_datasets\")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\"Number of training samples: %d\", length(train_ds))\n\nNumber of training samples: 9305\n\nprintf(\"Number of validation samples: %d\", length(validation_ds) )\n\nNumber of validation samples: 2326\n\nprintf(\"Number of test samples: %d\", length(test_ds))\n\nNumber of test samples: 2326\n\n\nThese are the first 9 images in the training dataset – as you can see, they’re all different sizes.\n\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\"label: %s   size: %s\",\n                  label, paste(dim(image), collapse = \" x \")))\n  })\n\n\n\n\nWe can also see that label 1 is “dog” and label 0 is “cat”.\n\n\nStandardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn’t a great fit for feeding a neural network. We need to do 2 things:\n\nStandardize to a fixed image size. We pick 150x150.\nNormalize pixel values between -1 and 1. We’ll do this using a layer_normalization() as part of the model itself.\n\nIn general, it’s a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you’ll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\nHere, we’ll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we’ll do the input value scaling as part of the model, when we create it.\nLet’s resize images to 150x150:\n\nlibrary(magrittr, include.only = \"%<>%\")\n\nWarning: package 'magrittr' was built under R version 4.1.2\n\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n\nBesides, let’s batch the data and use caching and prefetching to optimize loading speed.\n\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n\n\n\nUsing random data augmentation\nWhen you don’t have a large image dataset, it’s a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(.1)\n\nLet’s visualize what the first image of the first batch looks like after various random transformations:\n\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\")\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#build-a-model",
    "href": "guides/keras/transfer_learning.html#build-a-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Build a model",
    "text": "Build a model\nNow let’s build a model that follows the blueprint we’ve explained earlier.\nNote that:\n\nWe add layer_rescaling() to scale input values (initially in the [0, 255] range) to the [-1, 1] range.\nWe add a layer_dropout() before the classification layer, for regularization.\nWe make sure to pass training = FALSE when calling the base model, so that it runs in inference mode, so that batchnorm statistics don’t get updated even after we unfreeze the base model for fine-tuning.\n\n\nbase_model = application_xception(\n  weights = \"imagenet\", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs <- layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n input_7 (InputLayer)          [(None, 150, 150, 3)]      0          Y          \n sequential_3 (Sequential)     (None, 150, 150, 3)        0          Y          \n rescaling (Rescaling)         (None, 150, 150, 3)        0          Y          \n xception (Functional)         (None, 5, 5, 2048)         20861480   N          \n global_average_pooling2d_1 (G  (None, 2048)              0          Y          \n lobalAveragePooling2D)                                                         \n dropout (Dropout)             (None, 2048)               0          Y          \n dense_8 (Dense)               (None, 1)                  2049       Y          \n================================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n________________________________________________________________________________"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#train-the-top-layer",
    "href": "guides/keras/transfer_learning.html#train-the-top-layer",
    "title": "Transfer learning and fine-tuning",
    "section": "Train the top layer",
    "text": "Train the top layer\n\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 2\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)"
  },
  {
    "objectID": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "href": "guides/keras/transfer_learning.html#do-a-round-of-fine-tuning-of-the-entire-model",
    "title": "Transfer learning and fine-tuning",
    "section": "Do a round of fine-tuning of the entire model",
    "text": "Do a round of fine-tuning of the entire model\nFinally, let’s unfreeze the base model and train the entire model end-to-end with a low learning rate.\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed training = FALSE when calling it when we built the model. This means that the batch normalization layers inside won’t update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n input_7 (InputLayer)          [(None, 150, 150, 3)]      0          Y          \n sequential_3 (Sequential)     (None, 150, 150, 3)        0          Y          \n rescaling (Rescaling)         (None, 150, 150, 3)        0          Y          \n xception (Functional)         (None, 5, 5, 2048)         20861480   Y          \n global_average_pooling2d_1 (G  (None, 2048)              0          Y          \n lobalAveragePooling2D)                                                         \n dropout (Dropout)             (None, 2048)               0          Y          \n dense_8 (Dense)               (None, 1)                  2049       Y          \n================================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n________________________________________________________________________________\n\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 1\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html",
    "href": "guides/keras/working_with_rnns.html",
    "title": "Working with RNNs",
    "section": "",
    "text": "Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\nThe Keras RNN API is designed with a focus on:\n\nEase of use: the built-in layer_rnn(), layer_lstm(), layer_gru() layers enable you to quickly build recurrent models without having to make difficult configuration choices.\nEase of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic layer_rnn layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#setup",
    "href": "guides/keras/working_with_rnns.html#setup",
    "title": "Working with RNNs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "href": "guides/keras/working_with_rnns.html#built-in-rnn-layers-a-simple-example",
    "title": "Working with RNNs",
    "section": "Built-in RNN layers: a simple example",
    "text": "Built-in RNN layers: a simple example\nThere are three built-in RNN layers in Keras:\n\nlayer_simple_rnn(), a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\nlayer_gru(), first proposed in Cho et al., 2014.\nlayer_lstm(), first proposed in Hochreiter & Schmidhuber, 1997.\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a layer_lstm().\n\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n\nLoaded Tensorflow version 2.9.1\n\nmodel\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n embedding (Embedding)              (None, None, 64)                64000       \n lstm (LSTM)                        (None, 128)                     98816       \n dense (Dense)                      (None, 10)                      1290        \n================================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nBuilt-in RNNs support a number of useful features:\n\nRecurrent dropout, via the dropout and recurrent_dropout arguments\nAbility to process an input sequence in reverse, via the go_backwards argument\nLoop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the unroll argument\n…and more.\n\nFor more information, see the RNN API documentation."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#outputs-and-states",
    "href": "guides/keras/working_with_rnns.html#outputs-and-states",
    "title": "Working with RNNs",
    "section": "Outputs and states",
    "text": "Outputs and states\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is (batch_size, units) where units corresponds to the units argument passed to the layer’s constructor.\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences = TRUE. The shape of this output is (batch_size, timesteps, units).\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n embedding_1 (Embedding)            (None, None, 64)                64000       \n gru (GRU)                          (None, None, 256)               247296      \n simple_rnn (SimpleRNN)             (None, 128)                     49280       \n dense_1 (Dense)                    (None, 10)                      1290        \n================================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\nTo configure a RNN layer to return its internal state, set return_state = TRUE when creating the layer. Note that LSTM has 2 state tensors, but GRU only has one.\nTo configure the initial state of the layer, call the layer instance with the additional named argument initial_state. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\"encoder\")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \"decoder\")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n input_1 (InputLayer)     [(None, None)]    0        []                         \n input_2 (InputLayer)     [(None, None)]    0        []                         \n embedding_2 (Embedding)  (None, None, 64)  64000    ['input_1[0][0]']          \n embedding_3 (Embedding)  (None, None, 64)  128000   ['input_2[0][0]']          \n encoder (LSTM)           [(None, 64),      33024    ['embedding_2[0][0]']      \n                           (None, 64),                                          \n                           (None, 64)]                                          \n decoder (LSTM)           (None, 64)        33024    ['embedding_3[0][0]',      \n                                                      'encoder[0][1]',          \n                                                      'encoder[0][2]']          \n dense_2 (Dense)          (None, 10)        650      ['decoder[0][0]']          \n================================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n________________________________________________________________________________"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "href": "guides/keras/working_with_rnns.html#rnn-layers-and-rnn-cells",
    "title": "Working with RNNs",
    "section": "RNN layers and RNN cells",
    "text": "RNN layers and RNN cells\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\nThe cell is the inside of the for loop of a RNN layer. Wrapping a cell inside a layer_rnn() layer gives you a layer capable of processing a sequence, e.g. layer_rnn(layer_lstm_cell(10)).\nMathematically, layer_rnn(layer_lstm_cell(10)) produces the same result as layer_lstm(10). In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in layer_gru() and layer_lstm() layers enable the use of CuDNN and you may see better performance.\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\nlayer_simple_rnn_cell() corresponds to the layer_simple_rnn() layer.\nlayer_gru_cell corresponds to the layer_gru layer.\nlayer_lstm_cell corresponds to the layer_lstm layer.\n\nThe cell abstraction, together with the generic layer_rnn() class, makes it very easy to implement custom RNN architectures for your research."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "href": "guides/keras/working_with_rnns.html#cross-batch-statefulness",
    "title": "Working with RNNs",
    "section": "Cross-batch statefulness",
    "text": "Cross-batch statefulness\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of cross-batch statefulness.\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer’s state. That way, the layer can retain information about the entirety of the sequence, even though it’s only seeing one sub-sequence at a time.\nYou can do this by setting stateful = TRUE in the constructor.\nIf you have a sequence s = c(t0, t1, ... t1546, t1547), you would split it into e.g.\n\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n\nThen you would process it via:\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n\nWhen you want to clear the state, you can use layer$reset_states().\n\nNote: In this setup, sample i in a given batch is assumed to be the continuation of sample i in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], the next batch should contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200].\n\nHere is a complete example:\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n\n\nRNN State Reuse\nThe recorded states of the RNN layer are not included in the layer$weights(). If you would like to reuse the state from a RNN layer, you can retrieve the states value by layer$states and use it as the initial state of a new layer instance via the Keras functional API like new_layer(inputs, initial_state = layer$states), or model subclassing.\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "href": "guides/keras/working_with_rnns.html#bidirectional-rnns",
    "title": "Working with RNNs",
    "section": "Bidirectional RNNs",
    "text": "Bidirectional RNNs\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\nKeras provides an easy API for you to build such bidirectional RNNs: the bidirectional() wrapper.\n\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n bidirectional_1 (Bidirectional)    (None, 5, 128)                  38400       \n bidirectional (Bidirectional)      (None, 64)                      41216       \n dense_3 (Dense)                    (None, 10)                      650         \n================================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nUnder the hood, bidirectional() will copy the RNN layer passed in, and flip the go_backwards field of the newly copied layer, so that it will process the inputs in reverse order.\nThe output of the bidirectional RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the merge_mode parameter in the bidirectional wrapper constructor. For more details about bidirectional, please check the API docs."
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "href": "guides/keras/working_with_rnns.html#performance-optimization-and-cudnn-kernels",
    "title": "Working with RNNs",
    "section": "Performance optimization and CuDNN kernels",
    "text": "Performance optimization and CuDNN kernels\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior layer_cudnn_gru/layer_cudnn_lstm layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\nSince the CuDNN kernel is built with certain assumptions, this means the layer will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers. E.g.:\n\nChanging the activation function from \"tanh\" to something else.\nChanging the recurrent_activation function from \"sigmoid\" to something else.\nUsing recurrent_dropout > 0.\nSetting unroll to TRUE, which forces LSTM/GRU to decompose the inner tf$while_loop into an unrolled for loop.\nSetting use_bias to FALSE.\nUsing masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the LSTM and GRU layers.\n\nUsing CuDNN kernels when available\nLet’s build a simple LSTM model to demonstrate the performance difference.\nWe’ll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we’ll predict the digit’s label.\n\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n\nLet’s load the MNIST dataset:\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n\nLet’s create a model instance and train it.\nWe choose sparse_categorical_crossentropy() as the loss function for the model. The output of the model has shape of (batch_size, 10). The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"sgd\",\n    metrics = \"accuracy\"\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nNow, let’s compare to a model that does not use the CuDNN kernel:\n\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\"sgd\",\n    metrics=\"accuracy\",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The tf$device() annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\nYou simply don’t have to worry about the hardware you’re running on anymore. Isn’t that pretty cool?\n\nwith(tf$device(\"CPU:0\"), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \"Predicted result is: %s, target result is: %s\\n\", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n\nPredicted result is: 3, target result is: 5"
  },
  {
    "objectID": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "href": "guides/keras/working_with_rnns.html#rnns-with-listdict-inputs-or-nested-inputs",
    "title": "Working with RNNs",
    "section": "RNNs with list/dict inputs, or nested inputs",
    "text": "RNNs with list/dict inputs, or nested inputs\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\nDefine a custom cell that supports nested input/output\nSee Making new Layers & Models via subclassing for details on writing your own layers.\n\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \"uniform\",\n      name = \"kernel_1\"\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \"uniform\",\n      name = \"kernel_2_3\"\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\"bij,ijkl->bkl\", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\"unit_1\" = self$unit_1,\n         \"unit_2\" = self$unit_2,\n         \"unit_3\" = self$unit_3)\n  }\n}\n\n\n\nBuild a RNN model with nested input/output\nLet’s build a Keras model that uses a layer_rnn layer and the custom cell we just defined.\n\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"accuracy\")\n\n\n\nTrain the model with randomly generated data\nSince there isn’t a good candidate dataset for this model, we use random data for demonstration.\n\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n\nWith keras::layer_rnn(), you are only expected to define the math logic for an individual step within the sequence, and the layer_rnn() will handle the sequence iteration for you. It’s an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\nFor more details, please visit the API docs."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html",
    "href": "guides/keras/writing_your_own_callbacks.html",
    "title": "Writing your own callbacks",
    "section": "",
    "text": "A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include callback_tensorboard() to visualize training progress and results with TensorBoard, or callback_model_checkpoint() to periodically save your model during training.\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#setup",
    "href": "guides/keras/writing_your_own_callbacks.html#setup",
    "title": "Writing your own callbacks",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "href": "guides/keras/writing_your_own_callbacks.html#keras-callbacks-overview",
    "title": "Writing your own callbacks",
    "section": "Keras callbacks overview",
    "text": "Keras callbacks overview\nAll callbacks subclass the keras$callbacks$Callback class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\nYou can pass a list of callbacks (as a named argument callbacks) to the following keras model methods:\n\nfit()\nevaluate()\npredict()"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "href": "guides/keras/writing_your_own_callbacks.html#an-overview-of-callback-methods",
    "title": "Writing your own callbacks",
    "section": "An overview of callback methods",
    "text": "An overview of callback methods\n\nGlobal methods\n\non_(train|test|predict)_begin(self, logs=None)\nCalled at the beginning of fit/evaluate/predict.\n\n\non_(train|test|predict)_end(self, logs=None)\nCalled at the end of fit/evaluate/predict.\n\n\n\nBatch-level methods for training/testing/predicting\n\non_(train|test|predict)_batch_begin(self, batch, logs=None)\nCalled right before processing a batch during training/testing/predicting.\n\n\non_(train|test|predict)_batch_end(self, batch, logs=None)\nCalled at the end of training/testing/predicting a batch. Within this method, logs is a dict containing the metrics results.\n\n\n\nEpoch-level methods (training only)\n\non_epoch_begin(self, epoch, logs=None)\nCalled at the beginning of an epoch during training.\n\n\non_epoch_end(self, epoch, logs=None)\nCalled at the end of an epoch during training."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "href": "guides/keras/writing_your_own_callbacks.html#a-basic-example",
    "title": "Writing your own callbacks",
    "section": "A basic example",
    "text": "A basic example\nLet’s take a look at a concrete example. To get started, let’s import tensorflow and define a simple Sequential Keras model:\n\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \"mean_squared_error\",\n      metrics = \"mean_absolute_error\"\n    )\n  model\n}\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n\nNow, define a simple custom callback that logs:\n\nWhen fit/evaluate/predict starts & ends\nWhen each epoch starts & ends\nWhen each training batch starts & ends\nWhen each evaluation (test) batch starts & ends\nWhen each inference (prediction) batch starts & ends\n\n\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \"got logs: \", sep = \"; \")\n  str(logs); cat(\"\\n\")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\"Starting training\", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\"Stop training\", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\"Start epoch {epoch} of training\", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\"End epoch {epoch} of training\", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\"Start testing\", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\"Stop testing\", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\"Start predicting\", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\"Stop predicting\", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\"...Training: start of batch {batch}\", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\"...Training: end of batch {batch}\",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\"...Evaluating: start of batch {batch}\", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\"...Evaluating: end of batch {batch}\", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\"...Predicting: start of batch {batch}\", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\"...Predicting: end of batch {batch}\", logs)\n}\n\nLet’s try it out:\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n\n\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n\n\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n\n\nUsage of logs dict\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)"
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "href": "guides/keras/writing_your_own_callbacks.html#usage-of-selfmodel-attribute",
    "title": "Writing your own callbacks",
    "section": "Usage of self$model attribute",
    "text": "Usage of self$model attribute\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: self$model.\nHere are of few of the things you can do with self$model in a callback:\n\nSet self$model$stop_training <- TRUE to immediately interrupt training.\nMutate hyperparameters of the optimizer (available as self$model$optimizer), such as self$model$optimizer$learning_rate.\nSave the model at period intervals.\nRecord the output of predict(model) on a few test samples at the end of each epoch, to use as a sanity check during training.\nExtract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\netc.\n\nLet’s see this in action in a couple of examples."
  },
  {
    "objectID": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "href": "guides/keras/writing_your_own_callbacks.html#examples-of-keras-callback-applications",
    "title": "Writing your own callbacks",
    "section": "Examples of Keras callback applications",
    "text": "Examples of Keras callback applications\n\nEarly stopping at minimum loss\nThis first example shows the creation of a Callback that stops training when the minimum of loss has been reached, by setting the attribute self$model$stop_training (boolean). Optionally, you can provide an argument patience to specify how many epochs we should wait before stopping after having reached a local minimum.\nkeras$callbacks$EarlyStopping provides a more complete and general implementation.\n\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \"\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\"Restoring model weights from the end of the best epoch.\\n\")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n\n\n\nLearning rate scheduling\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\nSee keras$callbacks$LearningRateScheduler for a more general implementations (in RStudio, press F1 while the cursor is over LearningRateScheduler and a browser will open to this page).\n\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\"learning_rate\" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \"learning_rate\" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\"\\nEpoch %05d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \"Helper function to retrieve the scheduled learning rate based on epoch.\"\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n\n\n\nBuilt-in Keras callbacks\nBe sure to check out the existing Keras callbacks by reading the API docs. Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html",
    "href": "guides/tensorflow/autodiff.html",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "href": "guides/tensorflow/autodiff.html#automatic-differentiation-and-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Automatic Differentiation and Gradients",
    "text": "Automatic Differentiation and Gradients\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#setup",
    "href": "guides/tensorflow/autodiff.html#setup",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nlibrary(keras)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#computing-gradients",
    "href": "guides/tensorflow/autodiff.html#computing-gradients",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Computing gradients",
    "text": "Computing gradients\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradient-tapes",
    "href": "guides/tensorflow/autodiff.html#gradient-tapes",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradient tapes",
    "text": "Gradient tapes\nTensorFlow provides the tf$GradientTape() API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf$Variables. TensorFlow “records” relevant operations executed inside the context of a tf$GradientTape() onto a “tape”. TensorFlow then uses that tape to compute the gradients of a “recorded” computation using reverse mode differentiation.\nHere is a simple example:\n\nx <- tf$Variable(3)\n\nLoaded Tensorflow version 2.9.1\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n\nOnce you’ve recorded some operations, use GradientTape$gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model’s variables):\n\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\nThe above example uses scalars, but tf$GradientTape works as easily on any tensor:\n\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \"float32\", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf$nest).\n\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n\nThe gradient with respect to each source has the shape of the source:\n\nw$shape\n\nTensorShape([3, 2])\n\ndl_dw$shape\n\nTensorShape([3, 2])\n\n\nHere is the gradient calculation again, this time passing a named list of variables:\n\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b\n\ntf.Tensor([0.6954311 4.652119 ], shape=(2), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "href": "guides/tensorflow/autodiff.html#gradients-with-respect-to-a-model",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients with respect to a model",
    "text": "Gradients with respect to a model\nIt’s common to collect tf$Variables into a tf$Module or one of its subclasses (tf$keras$layers$Layer, tf$keras$Model) for checkpointing and exporting.\nIn most cases, you will want to calculate gradients with respect to a model’s trainable variables. Since all subclasses of tf$Module aggregate their variables in the Module$trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \"float32\", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n\n\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}\n\ndense/kernel:0, shape: (3, 2)\ndense/bias:0, shape: (2)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "href": "guides/tensorflow/autodiff.html#controlling-what-the-tape-watches",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Controlling what the tape watches",
    "text": "Controlling what the tape watches\nThe default behavior is to record all operations after accessing a trainable tf$Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\nThe tape holds references to intermediate outputs, so you don’t want to record unnecessary operations.\nThe most common use case involves calculating the gradient of a loss with respect to all a model’s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf$Tensor is not “watched” by default, and the tf$Variable is not trainable:\n\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n\nList of 4\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nYou can list the variables being watched by the tape using the GradientTape$watched_variables method:\n\ntape$watched_variables()\n\n[[1]]\n<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n\n\ntf$GradientTape provides hooks that give the user control over what is or is not watched.\nTo record gradients with respect to a tf$Tensor, you need to call GradientTape$watch(x):\n\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n\n[1] 6\n\n\nConversely, to disable the default behavior of watching all tf$Variables, set watch_accessed_variables = FALSE when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n\nSince GradientTape$watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\n\ndy/dx0: \n\ncat('dy/dx1: ', as.array(grad$x1))\n\ndy/dx1:  0.9999546"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#intermediate-results",
    "href": "guides/tensorflow/autodiff.html#intermediate-results",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Intermediate results",
    "text": "Intermediate results\nYou can also request gradients of the output with respect to intermediate values computed inside the tf$GradientTape context.\n\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n\n[1] 18\n\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape$gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent = TRUE. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\n\n[1]   4 108\n\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n\n[1] 2 6\n\n\n\nrm(tape)   # Drop the reference to the tape"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#notes-on-performance",
    "href": "guides/tensorflow/autodiff.html#notes-on-performance",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Notes on performance",
    "text": "Notes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\nFor efficiency, some ops (like ReLU) don’t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent = TRUE on your tape, nothing is discarded and your peak memory usage will be higher."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "href": "guides/tensorflow/autodiff.html#gradients-of-non-scalar-targets",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Gradients of non-scalar targets",
    "text": "Gradients of non-scalar targets\nA gradient is fundamentally an operation on a scalar.\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\n\n[1] 4\n\nas.array(tape$gradient(y1, x))\n\n[1] -0.25\n\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\nThe sum of the gradients of each target.\n\n\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n\n[1] 3.75\n\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n\n[1] 7\n\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\nIf you need a separate gradient for each item, refer to Jacobians.\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n\n\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \"royalblue\", lwd = 2)\nlines(x, dy_dx, col = \"coral\", lwd=2)\nlegend(\"topleft\", inset = .05,\n       expression(y, dy/dx),\n       col = c(\"royalblue\", \"coral\"), lwd = 2)"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#control-flow",
    "href": "guides/tensorflow/autodiff.html#control-flow",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Control flow",
    "text": "Control flow\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\ndv1\n\nNULL\n\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1 ^ 2. The gradient with respect to x is always NULL.\n\n(dx <- tape$gradient(result, x))\n\nNULL"
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "href": "guides/tensorflow/autodiff.html#getting-a-gradient-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Getting a gradient of NULL",
    "text": "Getting a gradient of NULL\nWhen a target is not connected to a source you will get a gradient of NULL.\n\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\nIn the section on “controlling what the tape watches” you saw that the tape will automatically watch a tf$Variable but not a tf$Tensor.\nOne common error is to inadvertently replace a tf$Variable with a tf$Tensor, instead of using Variable$assign to update the tf$Variable. Here is an example:\n\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \": \")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n\nResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\nEagerTensor : NULL\n\n\n\n\n2. Did calculations outside of TensorFlow\nThe tape can’t record the gradient path if the calculation exits TensorFlow. For example:\n\nnp <- reticulate::import(\"numpy\", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n\nNULL\n\n\n\n\n3. Took gradients through an integer or string\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\nNobody expects strings to be differentiable, but it’s easy to accidentally create an int constant or variable if you don’t specify the dtype.\n\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nTensorFlow doesn’t automatically cast between types, so, in practice, you’ll often get a type error instead of a missing gradient.\n\n\n4. Took gradients through a stateful object\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\nA tf$Tensor is immutable. You can’t change a tensor once it’s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf$matmul only depends on its inputs.\nA tf$Variable has internal state—its value. When you use the variable, the state is read. It’s normal to calculate a gradient with respect to a variable, but the variable’s state blocks gradient calculations from going farther back. For example:\n\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n\nNULL\n\n\nSimilarly, tf$data$Dataset iterators and tf$queues are stateful, and will stop all gradients on tensors that pass through them."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "href": "guides/tensorflow/autodiff.html#no-gradient-registered",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "No gradient registered",
    "text": "No gradient registered\nSome tf$Operations are registered as being non-differentiable* and will return NULL. Others have no gradient registered**.\nThe tf$raw_ops page shows which low-level ops have gradients registered.\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning NULL. This way you know something has gone wrong.\nFor example, the tf$image$adjust_contrast function wraps raw_ops$AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  LookupError: gradient registry has no entry for: AdjustContrastv2\n\n\nIf you need to differentiate through this op, you’ll either need to implement the gradient and register it (using tf$RegisterGradient) or re-implement the function using other ops."
  },
  {
    "objectID": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "href": "guides/tensorflow/autodiff.html#zeros-instead-of-null",
    "title": "Introduction to gradients and automatic differentiation",
    "section": "Zeros instead of NULL",
    "text": "Zeros instead of NULL\nIn some cases it would be convenient to get 0 instead of NULL for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)\n\ntf.Tensor([0. 0.], shape=(2), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/basics.html",
    "href": "guides/tensorflow/basics.html",
    "title": "Tensorflow Basics",
    "section": "",
    "text": "This guide provides a quick overview of TensorFlow basics. Each section of this doc is an overview of a larger topic—you can find links to full guides at the end of each section.\nTensorFlow is an end-to-end platform for machine learning. It supports the following:\n\nMultidimensional-array based numeric computation (similar to Numpy\nGPU and distributed processing\nAutomatic differentiation\nModel construction, training, and export\nAnd more"
  },
  {
    "objectID": "guides/tensorflow/basics.html#tensors",
    "href": "guides/tensorflow/basics.html#tensors",
    "title": "Tensorflow Basics",
    "section": "Tensors",
    "text": "Tensors\nTensorFlow operates on multidimensional arrays or tensors represented as tensorflow.tensor objects. Here is a two-dimensional tensor:\n\nlibrary(tensorflow)\n\nWarning: package 'tensorflow' was built under R version 4.1.2\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n\nLoaded Tensorflow version 2.9.1\n\nx\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)\n\nx$shape\n\nTensorShape([2, 3])\n\nx$dtype\n\ntf.float32\n\n\nThe most important attributes of a tensor are its shape and dtype:\n\ntensor$shape: tells you the size of the tensor along each of its axes.\ntensor$dtype: tells you the type of all the elements in the tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as well as many operations specialized for machine learning.\nFor example:\n\nx + x\n\ntf.Tensor(\n[[ 2.  4.  6.]\n [ 8. 10. 12.]], shape=(2, 3), dtype=float32)\n\n\n\n5 * x\n\ntf.Tensor(\n[[ 5. 10. 15.]\n [20. 25. 30.]], shape=(2, 3), dtype=float32)\n\n\n\ntf$matmul(x, t(x)) \n\ntf.Tensor(\n[[14. 32.]\n [32. 77.]], shape=(2, 2), dtype=float32)\n\n\n\ntf$concat(list(x, x, x), axis = 0L)\n\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]], shape=(6, 3), dtype=float32)\n\n\n\ntf$nn$softmax(x, axis = -1L)\n\ntf.Tensor(\n[[0.09003057 0.24472848 0.66524094]\n [0.09003057 0.24472848 0.66524094]], shape=(2, 3), dtype=float32)\n\n\n\nsum(x) # same as tf$reduce_sum(x)\n\ntf.Tensor(21.0, shape=(), dtype=float32)\n\n\nRunning large calculations on CPU can be slow. When properly configured, TensorFlow can use accelerator hardware like GPUs to execute operations very quickly.\n\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n\nTensorFlow **IS NOT** using the GPU\n\n\nRefer to the Tensor guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#variables",
    "href": "guides/tensorflow/basics.html#variables",
    "title": "Tensorflow Basics",
    "section": "Variables",
    "text": "Variables\nNormal tensor objects are immutable. To store model weights (or other mutable state) in TensorFlow use a tf$Variable.\n\nvar <- tf$Variable(c(0, 0, 0))\nvar\n\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n\n\n\nvar$assign(c(1, 2, 3))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n\n\n\nvar$assign_add(c(1, 1, 1))\n\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>\n\n\nRefer to the Variables guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#automatic-differentiation",
    "href": "guides/tensorflow/basics.html#automatic-differentiation",
    "title": "Tensorflow Basics",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nGradient descent and related algorithms are a cornerstone of modern machine learning.\nTo enable this, TensorFlow implements automatic differentiation (autodiff), which uses calculus to compute gradients. Typically you’ll use this to calculate the gradient of a model’s error or loss with respect to its weights.\n\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n\n\nf(x)\n\ntf.Tensor(-2.0, shape=(), dtype=float32)\n\n\nAt x = 1.0, y = f(x) = (1^2 + 2*1 - 5) = -2.\nThe derivative of y is y' = f'(x) = (2*x + 2) = 4. TensorFlow can calculate this automatically:\n\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\nThis simplified example only takes the derivative with respect to a single scalar (x), but TensorFlow can compute the gradient with respect to any number of non-scalar tensors simultaneously.\nRefer to the Autodiff guide for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "href": "guides/tensorflow/basics.html#graphs-and-tf_function",
    "title": "Tensorflow Basics",
    "section": "Graphs and tf_function",
    "text": "Graphs and tf_function\nWhile you can use TensorFlow interactively like any R library, TensorFlow also provides tools for:\n\nPerformance optimization: to speed up training and inference.\nExport: so you can save your model when it’s done training.\n\nThese require that you use tf_function() to separate your pure-TensorFlow code from R.\n\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n\nThe first time you run the tf_function, although it executes in R, it captures a complete, optimized graph representing the TensorFlow computations done within the function.\n\nx <- as_tensor(1:3)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(6, shape=(), dtype=int32)\n\n\nOn subsequent calls TensorFlow only executes the optimized graph, skipping any non-TensorFlow steps. Below, note that my_func doesn’t print \"Tracing.\" since message is an R function, not a TensorFlow function.\n\nx <- as_tensor(10:8)\nmy_func(x)\n\ntf.Tensor(27, shape=(), dtype=int32)\n\n\nA graph may not be reusable for inputs with a different signature (shape and dtype), so a new graph is generated instead:\n\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n\nTracing.\n\n\ntf.Tensor(27.3, shape=(), dtype=float32)\n\n\nThese captured graphs provide two benefits:\n\nIn many cases they provide a significant speedup in execution (though not this trivial example).\nYou can export these graphs, using tf$saved_model, to run on other systems like a server or a mobile device, no Python installation required.\n\nRefer to Intro to graphs for more details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#modules-layers-and-models",
    "href": "guides/tensorflow/basics.html#modules-layers-and-models",
    "title": "Tensorflow Basics",
    "section": "Modules, layers, and models",
    "text": "Modules, layers, and models\ntf$Module is a class for managing your tf$Variable objects, and the tf_function objects that operate on them. The tf$Module class is necessary to support two significant features:\n\nYou can save and restore the values of your variables using tf$train$Checkpoint. This is useful during training as it is quick to save and restore a model’s state.\nYou can import and export the tf$Variable values and the tf$function graphs using tf$saved_model. This allows you to run your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple tf$Module object:\n\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n\n\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3)))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nSave the Module:\n\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n\nThe resulting SavedModel is independent of the code that created it. You can load a SavedModel from R, Python, other language bindings, or TensorFlow Serving. You can also convert it to run with TensorFlow Lite or TensorFlow JS.\n\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3)))\n\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n\n\nThe tf$keras$layers$Layer and tf$keras$Model classes build on tf$Module providing additional functionality and convenience methods for building, training, and saving models. Some of these are demonstrated in the next section.\nRefer to Intro to modules for details."
  },
  {
    "objectID": "guides/tensorflow/basics.html#training-loops",
    "href": "guides/tensorflow/basics.html#training-loops",
    "title": "Tensorflow Basics",
    "section": "Training loops",
    "text": "Training loops\nNow put this all together to build a basic model and train it from scratch.\nFirst, create some example data. This generates a cloud of points that loosely follows a quadratic curve:\n\nx <- as_tensor(seq(-2, 2, length.out = 201))\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n\n\n\n\nCreate a model:\n\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n\n\nmodel <- Model(64)\n\n\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n\n\n\n\nWrite a basic training loop:\n\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n\nMean squared error: 0.999\n\n\nMean squared error: 0.961\n\n\nMean squared error: 0.941\n\n\nMean squared error: 0.930\n\n\nMean squared error: 0.922\n\n\nMean squared error: 0.916\n\n\nMean squared error: 0.912\n\n\nMean squared error: 0.908\n\n\nMean squared error: 0.905\n\n\nMean squared error: 0.903\n\n\n\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n\n\n\n\nThat’s working, but remember that implementations of common training utilities are available in the tf$keras module. So consider using those before writing your own. To start with, the compile and fit methods for Keras Models implement a training loop for you:\n\nnew_model <- Model(64)\n\n\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n\n\n\n\n\nplot(history, metrics = 'loss', method = \"base\") \n\n\n\n# see ?plot.keras_training_history for more options.\n\nRefer to Basic training loops and the Keras guide for more details."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html",
    "href": "guides/tensorflow/intro_to_graphs.html",
    "title": "Intro To_graphs",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#overview",
    "href": "guides/tensorflow/intro_to_graphs.html#overview",
    "title": "Intro To_graphs",
    "section": "Overview",
    "text": "Overview\nThis guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the collection of Keras guides.\nIn this guide, you’ll learn how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.\nNote: For those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.\nThis is a big-picture overview that covers how tf_function() allows you to switch from eager execution to graph execution. For a more complete specification of tf_function(), go to the tf_function() guide.\n\nWhat are graphs?\nIn the previous three guides, you ran TensorFlow eagerly. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\nWhile eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. Graph execution means that tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf$Graph or simply a “graph.”\nGraphs are data structures that contain a set of tf$Operation objects, which represent units of computation; and tf$Tensor objects, which represent the units of data that flow between operations. They are defined in a tf$Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original R code.\nThis is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard.\n\n\n\nA simple TensorFlow g\n\n\n\n\nThe benefits of graphs\nWith a graph, you have a great deal of flexibility. You can use your TensorFlow graph in environments that don’t have an R interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for saved models when it exports them from R.\nGraphs are also easily optimized, allowing the compiler to do transformations like:\n\nStatically infer the value of tensors by folding constant nodes in your computation (“constant folding”).\nSeparate sub-parts of a computation that are independent and split them between threads or devices.\nSimplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system, Grappler, to perform this and other speedups.\nIn short, graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices.\nHowever, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#setup",
    "href": "guides/tensorflow/intro_to_graphs.html#setup",
    "title": "Intro To_graphs",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \"%>%\")\n\nWarning: package 'magrittr' was built under R version 4.1.2"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "href": "guides/tensorflow/intro_to_graphs.html#taking-advantage-of-graphs",
    "title": "Intro To_graphs",
    "section": "Taking advantage of graphs",
    "text": "Taking advantage of graphs\nYou create and run a graph in TensorFlow by using tf_function(), either as a direct call or as a decorator. tf_function() takes a regular function as input and returns a Function. A Function is a callable that builds TensorFlow graphs from the R function. You use a Function in the same way as its R equivalent.\n\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\nLoaded Tensorflow version 2.9.1\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \"float64\", shape = c(1, 2))\ny1 <- as_tensor(2:3, \"float64\", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n\nOn the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf$Graphs behind one API. That is how Function is able to give you the benefits of graph execution, like speed and deployability.\ntf_function applies to a function and all other functions it calls:\n\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \"float64\", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \"float64\", shape = c(1, 2))) #%>% as.array()\n\ntf.Tensor([[12.]], shape=(1, 1), dtype=float64)\n\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you need to define a Placeholder or tf$Session().\n\nConverting Python functions to graphs\nAny function you write with TensorFlow will contain a mixture of built-in TF operations and R control-flow logic, such as if-then clauses, loops, break, return, next, and more. While TensorFlow operations are easily captured by a tf$Graph, R-specific logic needs to undergo an extra step in order to become part of the graph. tf_function() uses a library called {tfautograph} to evaluate the R code in a special way so that it generates a graph.\n\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \"First branch, with graph: \", format(tf_simple_relu(as_tensor(1))), \"\\n\",\n  \"Second branch, with graph: \", format(tf_simple_relu(as_tensor(-1))), \"\\n\",\n  sep = \"\"\n)\n\nFirst branch, with graph: tf.Tensor(1.0, shape=(), dtype=float64)\nSecond branch, with graph: tf.Tensor(0.0, shape=(), dtype=float64)\n\n\nThough it is unlikely that you will need to view graphs directly, you can inspect the outputs to check the exact results. These are not easy to read, so no need to look too carefully!\n\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n\nMost of the time, tf_function() will work without special considerations. However, there are some caveats, and the tf_function guide can help here, as well as the tfautograph Getting Started vignette\n\n\nPolymorphism: one Function, many graphs\nA tf$Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()) (i.e, the same memory address).\nEach time you invoke a Function with a set of arguments that can’t be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf$Graph specialized to those new arguments. The type specification of a tf$Graph’s inputs is known as its input signature or just a signature. For more information regarding when a new tf$Graph is generated and how that can be controlled, see the rules of retracing.\nThe Function stores the tf$Graph corresponding to that signature in a ConcreteFunction. A ConcreteFunction is a wrapper around a tf$Graph.\n\nmy_relu <- tf_function(function(x) {\n  message(\"Tracing my_relu(x) with: \", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\n\nTracing my_relu(x) with: Tensor(\"x:0\", shape=(), dtype=float64)\n\n\ntf.Tensor(5.5, shape=(), dtype=float64)\n\nmy_relu(c(1, -1))\n\nTracing my_relu(x) with: 1-1\n\n\ntf.Tensor([1. 0.], shape=(2), dtype=float64)\n\nmy_relu(as_tensor(c(3, -3)))\n\nTracing my_relu(x) with: Tensor(\"x:0\", shape=(2,), dtype=float64)\n\n\ntf.Tensor([3. 0.], shape=(2), dtype=float64)\n\n\nIf the Function has already been called with that signature, Function does not create a new tf$Graph.\n\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\n\ntf.Tensor(0.0, shape=(), dtype=float64)\n\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n\ntf.Tensor([0. 1.], shape=(2), dtype=float64)\n\n\nBecause it’s backed by multiple graphs, a Function is polymorphic. That enables it to support more input types than a single tf$Graph could represent, as well as to optimize each tf$Graph for better performance.\n\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())\n\nfn(x)\n  Args:\n    x: float64 Tensor, shape=()\n  Returns:\n    float64 Tensor, shape=()\n\nfn(x=[1.0, -1.0])\n  Returns:\n    float64 Tensor, shape=(2,)\n\nfn(x)\n  Args:\n    x: float64 Tensor, shape=(2,)\n  Returns:\n    float64 Tensor, shape=(2,)"
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "href": "guides/tensorflow/intro_to_graphs.html#using-tf_function",
    "title": "Intro To_graphs",
    "section": "Using tf_function()",
    "text": "Using tf_function()\nSo far, you’ve learned how to convert a Python function into a graph simply by using tf_function() as function wrapper. But in practice, getting tf_function to work correctly can be tricky! In the following sections, you’ll learn how you can make your code work as expected with tf_function().\n\nGraph execution vs. eager execution\nThe code in a Function can be executed both eagerly and as a graph. By default, Function executes its code as a graph:\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n\n\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\ntf.Tensor([9 0 8 7 4], shape=(5), dtype=int32)\n\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n\ntf.Tensor([2 7 2 4 8], shape=(5), dtype=int32)\n\n\n\nget_MSE(y_true, y_pred)\n\ntf.Tensor(31, shape=(), dtype=int32)\n\n\nTo verify that your Function’s graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with tf$config$run_functions_eagerly(TRUE). This is a switch that turns off Function’s ability to create and run graphs, instead executing the code normally.\n\ntf$config$run_functions_eagerly(TRUE)\n\n\nget_MSE(y_true, y_pred)\n\ntf.Tensor(31, shape=(), dtype=int32)\n\n\n\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n\nHowever, Function can behave differently under graph and eager execution. The R print() function is one example of how these two modes differ. Let’s check out what happens when you insert a print statement to your function and call it repeatedly.\n\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\"Calculating MSE!\")\n  mean((y_true - y_pred) ^ 2)\n  })\n\nObserve what is printed:\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n\nIs the output surprising? get_MSE only printed once even though it was called three times.\nTo explain, the print statement is executed when Function runs the original code in order to create the graph in a process known as “tracing”. Tracing captures the TensorFlow operations into a graph, and print() is not captured in the graph. That graph is then executed for all three calls without ever running the R code again.\nAs a sanity check, let’s turn off graph execution to compare:\n\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n\n\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\nerror <- get_MSE(y_true, y_pred)\n\n[1] \"Calculating MSE!\"\n\n\n\ntf$config$run_functions_eagerly(FALSE)\n\nprint is an R side effect, and there are other differences that you should be aware of when converting a function into a Function. Learn more in the Limitations section of the Better performance with tf_function guide.\n\n\n\n\n\n\nNote\n\n\n\nNote: If you would like to print values in both eager and graph execution, use tf$print() instead.\n\n\n\n\nNon-strict execution\nGraph execution only executes the operations necessary to produce the observable effects, which includes:\n\nThe return value of the function\nDocumented well-known side-effects such as:\n\nInput/output operations, like tf$print()\nDebugging operations, such as the assert functions in tf$debugging() (also, stopifnot())\nMutations of tf$Variable()\n\n\nThis behavior is usually known as “Non-strict execution”, and differs from eager execution, which steps through all of the program operations, needed or not.\nIn particular, runtime error checking does not count as an observable effect. If an operation is skipped because it is unnecessary, it cannot raise any runtime errors.\nIn the following example, the “unnecessary” operation tf$gather() is skipped during graph execution, so the runtime error InvalidArgumentError is not raised as it would be in eager execution. Do not rely on an error being raised while executing a graph.\n\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0] = 2 is not in [0, 1) [Op:GatherV2]\n\n# All operations are run during eager execution so an error is raised.\n\n\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\"CPU\"),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n\ntf.Tensor([0.], shape=(1), dtype=float64)\n\n\n\n\ntf_function() best practices\nIt may take some time to get used to the behavior of Function. To get started quickly, first-time users should play around with wrapping toy functions with tf_function() to get experience with going from eager to graph execution.\nDesigning for tf_function may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n\nToggle between eager and graph execution early and often with tf$config$run_functions_eagerly() to pinpoint if/when the two modes diverge.\nCreate tf$Variables outside the Python function and modify them on the inside. The same goes for objects that use tf$Variable, like keras$layers, keras$Models and tf$optimizers.\nAvoid writing functions that depend on outer Python variables, excluding tf$Variables and Keras objects.\nPrefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful!\nInclude as much computation as possible under a tf_function to maximize the performance gain. For example, wrap a whole training step or the entire training loop."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "href": "guides/tensorflow/intro_to_graphs.html#seeing-the-speed-up",
    "title": "Intro To_graphs",
    "section": "Seeing the speed-up",
    "text": "Seeing the speed-up\ntf_function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:\n\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n\n\nplot(bench::mark(\n  \"Eager execution\" = power(x, 100),\n  \"Graph execution\" = power_as_graph(x, 100)))\n\nLoading required namespace: tidyr\n\n\n\n\n\ntf_function is commonly used to speed up training loops, and you can learn more about it in Writing a training loop from scratch with Keras.\nNote: You can also try tf_function(jit_compile = TRUE) for a more significant performance boost, especially if your code is heavy on TF control flow and uses many small tensors.\n\nPerformance and trade-offs\nGraphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. This investment is usually quickly paid back with the performance boost of subsequent executions, but it’s important to be aware that the first few steps of any large model training can be slower due to tracing.\nNo matter how large your model, you want to avoid tracing frequently. The tf_function() guide discusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it’s a good idea to check if you are retracing accidentally."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "href": "guides/tensorflow/intro_to_graphs.html#when-is-a-function-tracing",
    "title": "Intro To_graphs",
    "section": "When is a Function tracing?",
    "text": "When is a Function tracing?\nTo figure out when your Function is tracing, add a print or message() statement to its code. As a rule of thumb, Function will execute the message statement every time it traces.\n\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\"Tracing!\") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\nTracing!\n\n\ntf.Tensor(6.0, shape=(), dtype=float64)\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n\ntf.Tensor(11.0, shape=(), dtype=float64)\n\n\n\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\n\nTracing!\n\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\na_function_with_r_side_effect(3)\n\nTracing!\n\n\ntf.Tensor(11.0, shape=(), dtype=float32)\n\n\nNew (non-tensor) R arguments always trigger the creation of a new graph, hence the extra tracing."
  },
  {
    "objectID": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "href": "guides/tensorflow/intro_to_graphs.html#next-steps",
    "title": "Intro To_graphs",
    "section": "Next steps",
    "text": "Next steps\nYou can learn more about tf_function() on the API reference page and by following the Better performance with tf_function guide."
  },
  {
    "objectID": "guides/tensorflow/tensor.html",
    "href": "guides/tensorflow/tensor.html",
    "title": "Introduction to Tensors",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nlibrary(tensorflow)\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes with names(tf$dtypes).\nIf you’re familiar with R array or NumPy, tensors are (kind of) like R or NumPy arrays.\nAll tensors are immutable: you can never update the contents of a tensor, only create a new one."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#basics",
    "href": "guides/tensorflow/tensor.html#basics",
    "title": "Introduction to Tensors",
    "section": "Basics",
    "text": "Basics\nLet’s create some basic tensors.\nHere is a “scalar” or “rank-0” tensor . A scalar contains a single value, and no “axes”.\n\n# This will be an float64 tensor by default; see \"dtypes\" below.\nrank_0_tensor <- as_tensor(4)\n\nLoaded Tensorflow version 2.9.1\n\nprint(rank_0_tensor)\n\ntf.Tensor(4.0, shape=(), dtype=float64)\n\n\nA “vector” or “rank-1” tensor is like a list of values. A vector has one axis:\n\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3), dtype=float64)\n\n\nA “matrix” or “rank-2” tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]\n [5. 6.]], shape=(3, 2), dtype=float16)\n\n\n\n\n\n\n\n\n\n\nA scalar, shape: []\nA vector, shape: [3]\nA matrix, shape: [3, 2]\n\n\n\n\n\n\n\n\n\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of\n# axes (sometimes called \"dimensions\")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nThere are many ways you might visualize a tensor with more than two axes.\n\n\n\n\n\n\nA 3-axis tensor, shape: [3, 2, 5]\n\n\n\n\n\n\n\n!  \n\n\n\nYou can convert a tensor to an R array using as.array():\n\nas.array(rank_2_tensor)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nTensors often contain floats and ints, but have many other types, including:\n\ncomplex numbers\nstrings\n\nThe base tf$Tensor class requires tensors to be “rectangular”—that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\n\ntf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n\na * b # element-wise multiplication, same as tf$multiply(a, b)\n\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n\ntf$matmul(a, b) # matrix multiplication\n\ntf.Tensor(\n[[3 3]\n [7 7]], shape=(2, 2), dtype=int32)\n\n\nTensors are used in all kinds of operations (ops).\n\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n\ntf.Tensor(10.0, shape=(), dtype=float64)\n\n# Find the index of the largest value\ntf$math$argmax(x) \n\ntf.Tensor([1 0], shape=(2), dtype=int64)\n\ntf$nn$softmax(x) # Compute the softmax\n\ntf.Tensor(\n[[2.68941421e-01 7.31058579e-01]\n [9.99876605e-01 1.23394576e-04]], shape=(2, 2), dtype=float64)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#about-shapes",
    "href": "guides/tensorflow/tensor.html#about-shapes",
    "title": "Introduction to Tensors",
    "section": "About shapes",
    "text": "About shapes\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\nAxis or Dimension: A particular dimension of a tensor.\nSize: The total number of items in the tensor, the product of the shape vector’s elements.\n\nNote: Although you may see reference to a “tensor of two dimensions”, a rank-2 tensor does not usually describe a 2D space.\nTensors and tf$TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n\n\n\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\n\n\n\n\n\n\n\n\nmessage(\"Type of every element: \", rank_4_tensor$dtype)\n\nType of every element: <dtype: 'float32'>\n\nmessage(\"Number of axes: \", length(dim(rank_4_tensor)))\n\nNumber of axes: 4\n\nmessage(\"Shape of tensor: \", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\n\nShape of tensor: 3245\n\nmessage(\"Elements along axis 0 of tensor: \", dim(rank_4_tensor)[1])\n\nElements along axis 0 of tensor: 3\n\nmessage(\"Elements along the last axis of tensor: \", dim(rank_4_tensor) |> tail(1)) \n\nElements along the last axis of tensor: 5\n\nmessage(\"Total number of elements (3*2*4*5): \", length(rank_4_tensor)) # can also call tf$size()\n\nTotal number of elements (3*2*4*5): 120\n\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\n\n\n\n\n\nTypical axis order"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#indexing",
    "href": "guides/tensorflow/tensor.html#indexing",
    "title": "Introduction to Tensors",
    "section": "Indexing",
    "text": "Indexing\n\nSingle-axis indexing\nSee ?`[.tensorflow.tensor` for details\n\n\nMulti-axis indexing\nHigher rank tensors are indexed by passing multiple indices.\nThe exact same rules as in the single-axis case apply to each axis independently.\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#manipulating-shapes",
    "href": "guides/tensorflow/tensor.html#manipulating-shapes",
    "title": "Introduction to Tensors",
    "section": "Manipulating Shapes",
    "text": "Manipulating Shapes\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n\nTensorShape([1, 3])\n\n\n\n# You can convert this object into an R vector too\nas.integer(x$shape)\n\n[1] 1 3\n\n\nYou can reshape a tensor into a new shape. The tf$reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n\n\nx$shape\n\nTensorShape([1, 3])\n\nreshaped$shape\n\nTensorShape([1, 3])\n\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style “row-major” memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nrank_3_tensor\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says \"Whatever fits\".\ntf$reshape(rank_3_tensor, c(-1L))\n\ntf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29], shape=(30), dtype=int32)\n\n\nA typical and reasonable use of tf$reshape is to combine or split adjacent axes (or add/remove 1s).\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\n\ntf.Tensor(\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [20 21 22 23 24]\n [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\n\n\n\n\n\n\n\nSome good reshapes.\n\n\n\n\n  \n\n\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png https://www.tensorflow.org/guide/ https://www.tensorflow.org/guide/images/tensor/reshape-good2.png\nReshaping will “work” for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\nSwapping axes in tf$reshape does not work; you need tf$transpose for that.\n\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]\n  [10 11 12 13 14]]\n\n [[15 16 17 18 19]\n  [20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32)\n\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n\ntf.Tensor(\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]], shape=(5, 6), dtype=int32)\n\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\n\n\n\n\n\n\n\nSome bad reshapes.\n\n\n\n\n  \n\n\n\nYou may run across not-fully-specified shapes. Either the shape contains a NULL (an axis-length is unknown) or the whole shape is NULL (the rank of the tensor is unknown).\nExcept for tf$RaggedTensor, such shapes will only occur in the context of TensorFlow’s symbolic, graph-building APIs:\n\ntf_function\nThe keras functional API."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#more-on-dtypes",
    "href": "guides/tensorflow/tensor.html#more-on-dtypes",
    "title": "Introduction to Tensors",
    "section": "More on DTypes",
    "text": "More on DTypes\nTo inspect a tf$Tensor’s data type use the Tensor$dtype property.\nWhen creating a tf$Tensor from a Python object you may optionally specify the datatype.\nIf you don’t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts R integers to tf$int32 and R floating point numbers to tf$float64.\nYou can cast from type to type.\n\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor\n\ntf.Tensor([2 3 4], shape=(3), dtype=uint8)"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#broadcasting",
    "href": "guides/tensorflow/tensor.html#broadcasting",
    "title": "Introduction to Tensors",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are recycled automatically to fit larger tensors when running combined operations on them.\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * y\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\nx * z\n\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n\ntf.Tensor(\n[[1.]\n [2.]\n [3.]], shape=(3, 1), dtype=float64)\n\n(y <- tf$range(1, 5,  dtype = \"float64\"))\n\ntf.Tensor([1. 2. 3. 4.], shape=(4), dtype=float64)\n\nx * y\n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\n\n\n\n\n\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\n\n\n\n\\\n\n\n\nHere is the same operation without broadcasting:\n\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\nYou see what broadcasting looks like using tf$broadcast_to.\n\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n\ntf.Tensor(\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]], shape=(3, 3), dtype=float64)\n\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\nIt can get even more complicated. This section of Jake VanderPlas’s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy)."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "href": "guides/tensorflow/tensor.html#tfconvert_to_tensor",
    "title": "Introduction to Tensors",
    "section": "tf$convert_to_tensor",
    "text": "tf$convert_to_tensor\nMost ops, like tf$matmul and tf$reshape take arguments of class tf$Tensor. However, you’ll notice in the above case, objects shaped like tensors are also accepted.\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy’s ndarray, TensorShape, Python lists, and tf$Variable will all convert automatically.\nSee tf$register_tensor_conversion_function for more details, and if you have your own type you’d like to automatically convert to a tensor."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#ragged-tensors",
    "href": "guides/tensorflow/tensor.html#ragged-tensors",
    "title": "Introduction to Tensors",
    "section": "Ragged Tensors",
    "text": "Ragged Tensors\nA tensor with variable numbers of elements along some axis is called “ragged”. Use tf$ragged$RaggedTensor for ragged data.\nFor example, This cannot be represented as a regular tensor:\n\n\n\n\n\n\nA tf$RaggedTensor, shape: [4, NULL]\n\n\n\n\n\n\n\n\n\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n\n\ntry(tensor <- as_tensor(ragged_list))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Can't convert non-rectangular Python sequence to Tensor.\n\n\nInstead create a tf$RaggedTensor using tf$ragged$constant:\n\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n\n<tf.RaggedTensor [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0], [6.0, 7.0, 8.0], [9.0]]>\n\n\nThe shape of a tf$RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor$shape)\n\nTensorShape([4, None])"
  },
  {
    "objectID": "guides/tensorflow/tensor.html#string-tensors",
    "href": "guides/tensorflow/tensor.html#string-tensors",
    "title": "Introduction to Tensors",
    "section": "String tensors",
    "text": "String tensors\ntf$string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\nThe length of the string is not one of the axes of the tensor. See tf$strings for functions to manipulate them.\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\"Gray wolf\"))\n\ntf.Tensor(b'Gray wolf', shape=(), dtype=string)\n\n\nAnd a vector of strings:\n\n\n\n\n\n\nA vector of strings, shape: [3,]\n\n\n\n\n\n\n\n\n\ntensor_of_strings <- as_tensor(c(\"Gray wolf\",\n                                 \"Quick brown fox\",\n                                 \"Lazy dog\"))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n\ntf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3), dtype=string)\n\n\nIn the above printout the b prefix indicates that tf$string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\nIf you pass unicode characters they are utf-8 encoded.\n\nas_tensor(\"🥳👍\")\n\ntf.Tensor(b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d', shape=(), dtype=string)\n\n\nSome basic functions with strings can be found in tf$strings, including tf$strings$split.\n\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\" \")\n\ntf.Tensor([b'Gray' b'wolf'], shape=(2), dtype=string)\n\n\n\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n\n<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n\n\n\n\n\n\n\n\nThree strings split, shape: [3, NULL]\n\n\n\n\n\n\n\n\nAnd tf$string$to_number:\n\ntext <- as_tensor(\"1 10 100\")\ntf$strings$to_number(tf$strings$split(text, \" \"))\n\ntf.Tensor([  1.  10. 100.], shape=(3), dtype=float32)\n\n\nAlthough you can’t use tf$cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings <- tf$strings$bytes_split(as_tensor(\"Duck\"))\nbyte_ints <- tf$io$decode_raw(as_tensor(\"Duck\"), tf$uint8)\ncat(\"Byte strings: \"); print(byte_strings)\n\nByte strings: \n\n\ntf.Tensor([b'D' b'u' b'c' b'k'], shape=(4), dtype=string)\n\ncat(\"Bytes: \"); print(byte_ints)\n\nBytes: \n\n\ntf.Tensor([ 68 117  99 107], shape=(4), dtype=uint8)\n\n\n\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\"アヒル 🦆\")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \"UTF-8\")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \"UTF-8\")\n\ncat(\"Unicode bytes: \"); unicode_bytes\n\nUnicode bytes: \n\n\ntf.Tensor(b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86', shape=(), dtype=string)\n\ncat(\"Unicode chars: \"); unicode_char_bytes\n\nUnicode chars: \n\n\ntf.Tensor([b'\\xe3\\x82\\xa2' b'\\xe3\\x83\\x92' b'\\xe3\\x83\\xab' b' ' b'\\xf0\\x9f\\xa6\\x86'], shape=(5), dtype=string)\n\ncat(\"Unicode values: \"); unicode_values\n\nUnicode values: \n\n\ntf.Tensor([ 12450  12498  12523     32 129414], shape=(5), dtype=int32)\n\n\nThe tf$string dtype is used for all raw bytes data in TensorFlow. The tf$io module contains functions for converting data to and from bytes, including decoding images and parsing csv."
  },
  {
    "objectID": "guides/tensorflow/tensor.html#sparse-tensors",
    "href": "guides/tensorflow/tensor.html#sparse-tensors",
    "title": "Introduction to Tensors",
    "section": "Sparse tensors",
    "text": "Sparse tensors\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf$sparse$SparseTensor and related operations to store sparse data efficiently.\n\n\n\n\n\n\nA tf$SparseTensor, shape: [3, 4]\n\n\n\n\n\n\n\n\n\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n\nSparseTensor(indices=tf.Tensor(\n[[0 0]\n [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2), dtype=int64))\n\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)\n\ntf.Tensor(\n[[1. 0. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html",
    "href": "guides/tensorflow/tensor_slicing.html",
    "title": "Tensor Slicing",
    "section": "",
    "text": "# Copyright 2020 The TensorFlow Authors.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#setup",
    "href": "guides/tensorflow/tensor_slicing.html#setup",
    "title": "Tensor Slicing",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tensorflow)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "href": "guides/tensorflow/tensor_slicing.html#extract-tensor-slices",
    "title": "Tensor Slicing",
    "section": "Extract tensor slices",
    "text": "Extract tensor slices\nPerform slicing using the [ operator:\n\nt1 <- as_tensor(c(1, 2, 3, 4, 5, 6, 7))\n\nLoaded Tensorflow version 2.9.1\n\nt1[1:3]\n\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike base R’s [ operator, TensorFlow’s [ uses negative indexes for selecting starting from the end.\nNULL can be used instead of the last dimension or first, depending if it appears before or after the :.\n\n\n\nt1[-3:NULL]\n\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n\n\ntf.Tensor([5. 6. 7.], shape=(3), dtype=float64)\n\n\n\nFor 2-dimensional tensors,you can use something like:\n\nt2 <- as_tensor(rbind(c(0, 1, 2, 3, 4),\n                      c(5, 6, 7, 8, 9),\n                      c(10, 11, 12, 13, 14),\n                      c(15, 16, 17, 18, 19)))\n\nt2[NULL:-1, 2:3]\n\ntf.Tensor(\n[[ 1.  2.]\n [ 6.  7.]\n [11. 12.]\n [16. 17.]], shape=(4, 2), dtype=float64)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntf$slice can be used instead of the [ operator. However, not that when using functions directly from the tf module, dimensions and indexes will start from 0, unlike in R.\nYou also need to make sure that indexes are passed to TensorFlow with the integer type, for example using the L suffix notation.\n\n\nYou can use tf$slice on higher dimensional tensors as well.\n\nt3 <- as_tensor(array(seq(from=1, to = 31, by = 2), dim = c(2,2,4)))\ntf$slice(\n  t3,\n  begin = list(1L, 1L, 0L),\n  size = list(1L, 1L, 2L)\n)\n\ntf.Tensor([[[ 7. 15.]]], shape=(1, 1, 2), dtype=float64)\n\n\nYou can also use tf$strided_slice to extract slices of tensors by ‘striding’ over the tensor dimensions.\nUse tf$gather to extract specific indices from a single axis of a tensor.\n\ntf$gather(t1, indices = c(0L, 3L, 6L))\n\ntf.Tensor([1. 4. 7.], shape=(3), dtype=float64)\n\n\n\ntf$gather does not require indices to be evenly spaced.\n\nalphabet <- as_tensor(strsplit(\"abcdefghijklmnopqrstuvwxyz\", \"\")[[1]])\ntf$gather(alphabet, indices = c(2L, 0L, 19L, 18L))\n\ntf.Tensor([b'c' b'a' b't' b's'], shape=(4), dtype=string)\n\n\n\nTo extract slices from multiple axes of a tensor, use tf$gather_nd. This is useful when you want to gather the elements of a matrix as opposed to just its rows or columns.\n\nt4 <- as_tensor(rbind(c(0, 5),\n                      c(1, 6),\n                      c(2, 7),\n                      c(3, 8),\n                      c(4, 9)))\n\ntf$gather_nd(t4, indices = list(list(2L), list(3L), list(0L)))\n\ntf.Tensor(\n[[2. 7.]\n [3. 8.]\n [0. 5.]], shape=(3, 2), dtype=float64)\n\n\n\n\nt5 <- array(1:18, dim = c(2,3,3))\ntf$gather_nd(t5, indices = list(c(0L, 0L, 0L), c(1L, 2L, 1L)))\n\ntf.Tensor([ 1 12], shape=(2), dtype=int32)\n\n\n\n# Return a list of two matrices\ntf$gather_nd(\n  t5,\n  indices = list(\n    list(c(0L, 0L), c(0L, 2L)), \n    list(c(1L, 0L), c(1L, 2L)))\n)\n\ntf.Tensor(\n[[[ 1  7 13]\n  [ 5 11 17]]\n\n [[ 2  8 14]\n  [ 6 12 18]]], shape=(2, 2, 3), dtype=int32)\n\n\n\n# Return one matrix\ntf$gather_nd(\n  t5,\n  indices = list(c(0L, 0L), c(0L, 2L), c(1L, 0L), c(1L, 2L))\n)\n\ntf.Tensor(\n[[ 1  7 13]\n [ 5 11 17]\n [ 2  8 14]\n [ 6 12 18]], shape=(4, 3), dtype=int32)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "href": "guides/tensorflow/tensor_slicing.html#insert-data-into-tensors",
    "title": "Tensor Slicing",
    "section": "Insert data into tensors",
    "text": "Insert data into tensors\nUse tf$scatter_nd to insert data at specific slices/indices of a tensor. Note that the tensor into which you insert values is zero-initialized.\n\nt6 <- as_tensor(list(10L))\nindices <- as_tensor(list(list(1L), list(3L), list(5L), list(7L), list(9L)))\ndata <- as_tensor(c(2, 4, 6, 8, 10))\n\ntf$scatter_nd(\n  indices = indices,\n  updates = data,\n  shape = t6\n)\n\ntf.Tensor([ 0.  2.  0.  4.  0.  6.  0.  8.  0. 10.], shape=(10), dtype=float64)\n\n\nMethods like tf$scatter_nd which require zero-initialized tensors are similar to sparse tensor initializers. You can use tf$gather_nd and tf$scatter_nd to mimic the behavior of sparse tensor ops.\nConsider an example where you construct a sparse tensor using these two methods in conjunction.\n\n# Gather values from one tensor by specifying indices\nnew_indices <- as_tensor(rbind(c(0L, 2L), c(2L, 1L), c(3L, 3L)))\nt7 <- tf$gather_nd(t2, indices = new_indices)\n\n\n\n# Add these values into a new tensor\nt8 <- tf$scatter_nd(\n  indices = new_indices, \n  updates = t7, \n  shape = as_tensor(c(4L, 5L))\n)\nt8\n\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float64)\n\n\nThis is similar to:\n\nt9 <- tf$SparseTensor(\n  indices = list(c(0L, 2L), c(2L, 1L), c(3L, 3L)),\n  values = c(2, 11, 18),\n  dense_shape = c(4L, 5L)\n)\nt9\n\nSparseTensor(indices=tf.Tensor(\n[[0 2]\n [2 1]\n [3 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([ 2. 11. 18.], shape=(3), dtype=float32), dense_shape=tf.Tensor([4 5], shape=(2), dtype=int64))\n\n\n\n# Convert the sparse tensor into a dense tensor\nt10 <- tf$sparse$to_dense(t9)\nt10\n\ntf.Tensor(\n[[ 0.  0.  2.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0. 11.  0.  0.  0.]\n [ 0.  0.  0. 18.  0.]], shape=(4, 5), dtype=float32)\n\n\nTo insert data into a tensor with pre-existing values, use tf$tensor_scatter_nd_add.\n\nt11 <- as_tensor(rbind(c(2, 7, 0),\n                       c(9, 0, 1),\n                       c(0, 3, 8)))\n\n# Convert the tensor into a magic square by inserting numbers at appropriate indices\nt12 <- tf$tensor_scatter_nd_add(\n  t11,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt12\n\ntf.Tensor(\n[[2. 7. 6.]\n [9. 5. 1.]\n [4. 3. 8.]], shape=(3, 3), dtype=float64)\n\n\nSimilarly, use tf$tensor_scatter_nd_sub to subtract values from a tensor with pre-existing values.\n\n# Convert the tensor into an identity matrix\nt13 <- tf$tensor_scatter_nd_sub(\n  t11,\n  indices = list(c(0L, 0L), c(0L, 1L), c(1L, 0L), c(1L, 1L), c(1L, 2L), c(2L, 1L), c(2L, 2L)),\n  updates = c(1, 7, 9, -1, 1, 3, 7)\n)\n\nprint(t13)\n\ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float64)\n\n\nUse tf$tensor_scatter_nd_min to copy element-wise minimum values from one tensor to another.\n\nt14 <- as_tensor(rbind(c(-2, -7, 0),\n                       c(-9, 0, 1),\n                       c(0, -3, -8)))\n\nt15 <- tf$tensor_scatter_nd_min(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(-6, -5, -4)\n)\nt15\n\ntf.Tensor(\n[[-2. -7. -6.]\n [-9. -5.  1.]\n [-4. -3. -8.]], shape=(3, 3), dtype=float64)\n\n\nSimilarly, use tf$tensor_scatter_nd_max to copy element-wise maximum values from one tensor to another.\n\nt16 <- tf$tensor_scatter_nd_max(\n  t14,\n  indices = list(c(0L, 2L), c(1L, 1L), c(2L, 0L)),\n  updates = c(6, 5, 4)\n)\nt16\n\ntf.Tensor(\n[[-2. -7.  6.]\n [-9.  5.  1.]\n [ 4. -3. -8.]], shape=(3, 3), dtype=float64)"
  },
  {
    "objectID": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "href": "guides/tensorflow/tensor_slicing.html#further-reading-and-resources",
    "title": "Tensor Slicing",
    "section": "Further reading and resources",
    "text": "Further reading and resources\nIn this guide, you learned how to use the tensor slicing ops available with TensorFlow to exert finer control over the elements in your tensors.\n\nCheck out the slicing ops available with TensorFlow NumPy such as tf$experimental$numpy$take_along_axis and tf$experimental$numpy$take.\nAlso check out the Tensor guide and the Variable guide."
  },
  {
    "objectID": "guides/tensorflow/variable.html",
    "href": "guides/tensorflow/variable.html",
    "title": "Introduction to Variables",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nA TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of tf$Variable in TensorFlow.\nVariables are created and tracked via the tf$Variable class. A tf$Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like tf$keras use tf$Variable to store model parameters."
  },
  {
    "objectID": "guides/tensorflow/variable.html#setup",
    "href": "guides/tensorflow/variable.html#setup",
    "title": "Introduction to Variables",
    "section": "Setup",
    "text": "Setup\nThis notebook discusses variable placement. If you want to see on what device your variables are placed, uncomment this line.\n\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)"
  },
  {
    "objectID": "guides/tensorflow/variable.html#create-a-variable",
    "href": "guides/tensorflow/variable.html#create-a-variable",
    "title": "Introduction to Variables",
    "section": "Create a variable",
    "text": "Create a variable\nTo create a variable, provide an initial value. The tf$Variable will have the same dtype as the initialization value.\n\nmy_tensor <- as_tensor(1:4, \"float32\", shape = c(2, 2))\n\nLoaded Tensorflow version 2.9.1\n\n(my_variable <- tf$Variable(my_tensor))\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=bool, numpy=array([False, False, False,  True])>\n\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n\n\nA variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf$Tensor. Like tensors, they have a dtype and a shape, and can be exported to regular R arrays.\n\ncat(\"Shape: \"); my_variable$shape\n\nShape: \n\n\nTensorShape([2, 2])\n\ncat(\"DType: \"); my_variable$dtype\n\nDType: \n\n\ntf.float32\n\ncat(\"As R array: \"); str(as.array(my_variable))\n\nAs R array: \n\n\n num [1:2, 1:2] 1 3 2 4\n\n\nMost tensor operations work on variables as expected, although variables cannot be reshaped.\n\nmessage(\"A variable: \")\n\nA variable: \n\nmy_variable\n\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n\nmessage(\"Viewed as a tensor: \")\n\nViewed as a tensor: \n\nas_tensor(my_variable)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\nmessage(\"Index of highest value: \")\n\nIndex of highest value: \n\ntf$math$argmax(my_variable)\n\ntf.Tensor([1 1], shape=(2), dtype=int64)\n\n# This creates a new tensor; it does not reshape the variable.\nmessage(\"Copying and reshaping: \") \n\nCopying and reshaping: \n\ntf$reshape(my_variable, c(1L, 4L))\n\ntf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n\n\nAs noted above, variables are backed by tensors. You can reassign the tensor using tf$Variable$assign. Calling assign does not (usually) allocate a new tensor; instead, the existing tensor’s memory is reused.\n\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n\n\nIf you use a variable like a tensor in operations, you will usually operate on the backing tensor.\nCreating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory.\n\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([5., 6.], dtype=float32)>\n\n# a and b are different\n\nas.array(a)\n\n[1] 5 6\n\nas.array(b)\n\n[1] 2 3\n\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\n\n[1] 7 9\n\nas.array(a$assign_sub(c(7,9))) # c(0, 0)\n\n[1] 0 0"
  },
  {
    "objectID": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "href": "guides/tensorflow/variable.html#lifecycles-naming-and-watching",
    "title": "Introduction to Variables",
    "section": "Lifecycles, naming, and watching",
    "text": "Lifecycles, naming, and watching\nIn TensorFlow, tf$Variable instance have the same lifecycle as other R objects. When there are no references to a variable it is automatically deallocated (garbage-collected).\nVariables can also be named which can help you track and debug them. You can give two variables the same name.\n\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \"Mark\")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \"Mark\")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n\ntf.Tensor(\n[[False False]\n [False False]], shape=(2, 2), dtype=bool)\n\n\nVariable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.\nAlthough variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter.\n\n(step_counter <- tf$Variable(1L, trainable = FALSE))\n\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>"
  },
  {
    "objectID": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "href": "guides/tensorflow/variable.html#placing-variables-and-tensors",
    "title": "Introduction to Variables",
    "section": "Placing variables and tensors",
    "text": "Placing variables and tensors\nFor better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype. This means most variables are placed on a GPU if one is available.\nHowever, you can override this. In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available. By turning on device placement logging (see above), you can see where the variable is placed.\nNote: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation.\nIf you run this notebook on different backends with and without a GPU you will see different logging. Note that logging device placement must be turned on at the start of the session.\n\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \"float32\")\n  c <- tf$matmul(a, b)\n})\n\nc\n\ntf.Tensor(\n[[22. 49.]\n [28. 64.]], shape=(2, 2), dtype=float32)\n\n\nIt’s possible to set the location of a variable or tensor on one device and do the computation on another device. This will introduce delay, as data needs to be copied between the devices.\nYou might do this, however, if you had multiple GPU workers but only want one copy of the variables.\n\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \"float32\")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \"float32\")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n\ntf.Tensor(\n[[ 1.  6. 15.]\n [ 2.  8. 18.]], shape=(2, 3), dtype=float32)\n\n\nNote: Because tf$config$set_soft_device_placement() is turned on by default, even if you run this code on a device without a GPU, it will still run. The multiplication step will happen on the CPU.\nFor more on distributed training, refer to the guide."
  },
  {
    "objectID": "guides/tensorflow/variable.html#next-steps",
    "href": "guides/tensorflow/variable.html#next-steps",
    "title": "Introduction to Variables",
    "section": "Next steps",
    "text": "Next steps\nTo understand how variables are typically used, see our guide on automatic differentiation."
  }
]